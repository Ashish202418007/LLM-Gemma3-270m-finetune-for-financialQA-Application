{
  "model_path": "unsloth/gemma-3-270m-it",
  "data_path": "./data/newsQA_validation.jsonl",
  "metrics": {
    "exact_match": 0.0,
    "f1": 0.12831232389013616,
    "total": 4212,
    "samples_per_second": 1.4625422200920704,
    "total_time_seconds": 2879.9168612957,
    "avg_time_per_sample": 0.6837409452268993
  },
  "config": {
    "max_new_tokens": 128,
    "temperature": 1.0
  },
  "predictions": [
    {
      "id": "849240180c584f71bc56c0c92c5df5fd",
      "question": "What will be nominated?",
      "prediction": "The answer is:  \"We want to know which eco videos have got you going by using the Sound Off box.\"",
      "ground_truths": [
        "three different videos"
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "8da0d6902cab4114b140672981c41ece",
      "question": "What does the Harrison Ford video feature?",
      "prediction": "The answer to the question is \"What does the Harrison Ford video feature?\" is \"It often includes footage of actors, particularly people who are known for their intelligence, wit, and sometimes even a bit of a \"cork-pot\" (dark makeup and droopy eyes). The goal is to convey intelligence, but not to make the audience uncomfortable.\"",
      "ground_truths": [
        "getting his chest waxed,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb575e95e21843f9bc7ab8acdde72ee7",
      "question": "What videos will you send?",
      "prediction": "The answer is \"We want to know which eco videos have got you going by using the Sound Off box. We also want you to send in your eco-video film and the iReport form with your environmental efforts. Please do so.\"",
      "ground_truths": [
        "environmental"
      ],
      "em": 0,
      "f1": 0.05128205128205127
    },
    {
      "id": "fc754599d5c048639de9864adbce649a",
      "question": "What is Ford getting waxed?",
      "prediction": "Based on the CNN's response, the answer is: **Jack Ford is waxing his chest.**",
      "ground_truths": [
        "his chest"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "f7b2f89be1724a9c86cbcc347b0c4425",
      "question": "Who got his chest waxed?",
      "prediction": "The answer is: \"I got my chest waxed.\"",
      "ground_truths": [
        "Harrison Ford"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "342cb6f69f6449318f4c4e5385f954fd",
      "question": "How do you send in your video?",
      "prediction": "The answer is: **Do you send in your video?**",
      "ground_truths": [
        "Use the iReport form"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2f07af9f9874dc992ca32c22063ffd4",
      "question": "What type of videos should you nominate?",
      "prediction": "The answer is: \"It's not necessarily 'better' to be funny or shocking. It's a matter of personal preference.\"",
      "ground_truths": [
        "think are the best."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "365c5a43bd704e5bb1bc9e49d28bcf24",
      "question": "What did Steve Bruce describe Amire Zaki as?",
      "prediction": "The answer is:  Wigan Athletic striker Amir Zaki has mended relations with his club manager.",
      "ground_truths": [
        "unprofessional."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d37eef3f1e014b2ebbd84db0e0fd9012",
      "question": "Which team does Zaki play for?",
      "prediction": "The answer is Wigan Athletic.",
      "ground_truths": [
        "Wigan Athletic"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ffef7cbb9e7e4526a9826d07cb730fcd",
      "question": "Which club did Amir Zaki fail to return to?",
      "prediction": "According to Wigan Athletic manager Steve Bruce, Amir Zaki missed international duty, and he was fined the maximum allowed, which is the fourth time he has gone AWOL after international duty.",
      "ground_truths": [
        "Wigan Athletic"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "63fdd20036c74bc585e0a089ac9317df",
      "question": "What punishment will be meted out for his disappearance?",
      "prediction": "The answer is: **Fines for a record-breaking, documented case involving Egyptian striker Amir Zaki, the high-profile striker for Wigan Athletic, and the former manager, Steve Bruce.**",
      "ground_truths": [
        "a fine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d57e430e4bd40e6ab55ba4940df4767",
      "question": "Who went missing for two weeks and said he was taking a break from football?",
      "prediction": "The answer is: Chelsea striker Amir Zaki.",
      "ground_truths": [
        "Adriano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cc0e6b7eaa7409bb2455f1780bb777b",
      "question": "Who said there is no immediate plans for deployment?",
      "prediction": "The answer is \"no, there are no immediate plans to commit more U.S. troops to the ongoing war in Afghanistan. And Canada has 35,000. The Pentagon plans to add 6,000 troops by the end of the year.",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "014e5f5a9a98493ba20e29a3a4c478a5",
      "question": "How many troops does Canada have in Afghanistan?",
      "prediction": "The answer is 35,000.",
      "ground_truths": [
        "35,000."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a5ad768fcc80413387ef56a790b3940f",
      "question": "How many more troops is the US planning to send?",
      "prediction": "The answer is 6,000.",
      "ground_truths": [
        "6,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5c1f77d8de6c4a3886d9fa099e0bc0a0",
      "question": "What are the plans of Obama after this deployment?",
      "prediction": "The answer is: \"We are transitioning from a predominately military mission to a civilian humanitarian mission after 2011,\"",
      "ground_truths": [
        "to commit more U.S. troops to the ongoing war in Afghanistan,"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "9a9e86c0ad0a4a2db791d4f5fd0a8b9b",
      "question": "How many troops are being send to Afghanistan this year?",
      "prediction": "The answer is 6,000.",
      "ground_truths": [
        "6,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a442d4094129405fae0a7d5cdedc0055",
      "question": "Women who don't conform will risk spending how long in jail?",
      "prediction": "The answer is: The answer is 12.",
      "ground_truths": [
        "12 hours"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "b6f4d1fecbb647b0a355efb95d0cd94b",
      "question": "What are the rules of the new order?",
      "prediction": "The answer to the question is: **Somali women are required to cover their bodies and heads from view.**",
      "ground_truths": [
        "to close their shops during daily prayers,"
      ],
      "em": 0,
      "f1": 0.17391304347826086
    },
    {
      "id": "d405d7f57c324ded9029516e71164cc1",
      "question": "What controls Baidoa?",
      "prediction": "The answer is, \"somali women wait to get medicine at a clinic near Mogadishu in July.\"",
      "ground_truths": [
        "Al-Shabaab,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02cc59d96ff94449845329b25359703e",
      "question": "What can happen to the women who don't obey the order?",
      "prediction": "The answer is, because the Somali women in Baidoa are not required to wear Islamic clothing, and their clothes must be black, red, or white, and they can be purchased from a clinic.",
      "ground_truths": [
        "face jail time,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "acb7e1b2249c4df1a0a5d2b0b3b6ed58",
      "question": "Women's clothing must cover what?",
      "prediction": "The answer is \"black, red or white\", as the context indicates.",
      "ground_truths": [
        "their bodies and heads"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd69fc3cb5e442d2b54243f2104eb508",
      "question": "When is the order in effect?",
      "prediction": "The question asks when the order in effect is. The provided context does not explicitly state when the order was issued. Therefore, I cannot answer the question based on the context.",
      "ground_truths": [
        "Tuesday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b8a34a81fd7b4ba19a20a3acf7557a24",
      "question": "What must the clothing cover?",
      "prediction": "The clothing must be black, red, or white, and women in the impoverished city must be able to purchase clothing in the black, red, or white color.",
      "ground_truths": [
        "their bodies and heads from view,"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "237573dcd82d4e30aa08b8411250e3c4",
      "question": "Where was Forrest killed?",
      "prediction": "The answer is **Atlanta, Georgia**.",
      "ground_truths": [
        "in southwest Atlanta, Georgia,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "14e1933785044461a23c36b064f1ca0c",
      "question": "What three men were accused in the death?",
      "prediction": "The three men accused in the death are Charman Sinkfield, Demario Ware, and Jquante Crews.",
      "ground_truths": [
        "and Jquante Crews,"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "69d160735c2c4ecf933d73105ba74079",
      "question": "Which boxing champion was killed?",
      "prediction": "The answer is **C.F.S. Fulton County Superior Court**.",
      "ground_truths": [
        "Vernon Forrest,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cedc58cf8fa3405e8612bbdef684dbcf",
      "question": "What was the age of Vernon Forrest at the time of his death?",
      "prediction": "The age of Vernon Forrest was at the time of his death, not at the time of his death.",
      "ground_truths": [
        "38,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e562f0b103a4805aaa6b2e539c8fb97",
      "question": "What reward was offered?",
      "prediction": "The reward was $17,000.",
      "ground_truths": [
        "$17,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f8037e7835754ed6b00599221a3351b1",
      "question": "How many men were accused of murder?",
      "prediction": "The question asks how many men were accused of murder.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d154c947704e497fb83fe117c589917e",
      "question": "What boxing champion?",
      "prediction": "The boxing champion is Charman Sinkfield.",
      "ground_truths": [
        "Vernon Forrest,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9fb0513fd644a388198e6f6f37096f9",
      "question": "What day in July was Vernon Forrest killed?",
      "prediction": "The answer is Tuesday.",
      "ground_truths": [
        "25."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa73c8d3b86b4f759ac802b5a8ac1112",
      "question": "What charges included  murder?",
      "prediction": "The charges included murder, felony murder, aggravated assault with a deadly weapon, and possession of a firearm.",
      "ground_truths": [
        "aggravated assault with a deadly weapon and possession of a firearm by a convicted felon,"
      ],
      "em": 0,
      "f1": 0.6923076923076924
    },
    {
      "id": "76f6b677a98341849679aa51229f8d69",
      "question": "What football star cleared of charge?",
      "prediction": "The answer is Steven Gerrard.",
      "ground_truths": [
        "Steven Gerrard"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "43d665c7425145ed99cce24c58a8ed91",
      "question": "What was Gerrard's rationale?",
      "prediction": "Based on the provided text, the answer is: **Gerrard's rationale is to change the music on the CD player. He did so with the intention of trying to grab the device.**",
      "ground_truths": [
        "he believed he was about to be attacked himself."
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "45b1985c5f744c6e93be01993d95010d",
      "question": "What did Gerrard admitted?",
      "prediction": "Grettsack admitted to affray, and he admitted to a \"explosion of violence\" in his car.",
      "ground_truths": [
        "throwing three punches"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9660cdcc3e504d37aca14ad746928287",
      "question": "What player has cleared waivers?",
      "prediction": "The answer is: Steven Gerrard.",
      "ground_truths": [
        "Steven Gerrard"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a01ebb0233ee46afb7dc72e7d146dc1e",
      "question": "What was the verdict of the jury?",
      "prediction": "The answer is: **England international Steven Gerrard was cleared by a court in Liverpool of affray.**",
      "ground_truths": [
        "not guilty"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32a86d07321142fab582b0976e876d03",
      "question": "What was the reason for the punishment?",
      "prediction": "The answer is: **In Liverpool Football Club.  The team's captain, Steven Gerrard was cleared of affray for a failed attempt to target Marcus McGhee in a nightclub bar in the north-western of England city on December 29 of last year.**",
      "ground_truths": [
        "threatening behavior."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a36774065e944cc9ba1782363f8a3e1b",
      "question": "Iran criticizes who?",
      "prediction": "The context provides the following:",
      "ground_truths": [
        "U.S. President-elect Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0238ec9386c44699afc3d8a32e0d29e1",
      "question": "What are US and Iran relations tensioned about?",
      "prediction": "The provided text does not contain an answer to the question \"What are US and Iran relations tensioned about?\". It only mentions Obama's remarks about a nuclear-armed Iran and his political considerations, which are not directly linked to the answer.",
      "ground_truths": [
        "nuclear program."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95bb179172e5461b822f2955ab4ea7b3",
      "question": "Who said Obama should apply campaign message?",
      "prediction": "The answer is **Tom Carter.**",
      "ground_truths": [
        "Ali Larijani"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5bbabe002ae84993bf2d810e91f0e01e",
      "question": "What should Obama apply according to speaker?",
      "prediction": "According to the context, the Iranian government's response to Obama's \"change\" requests to U.S. actions, specifically the US's \"nuclear-armed Iran\" statement, is:",
      "ground_truths": [
        "his campaign message of change"
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "e54deaf6751846a48c8ce39e038ea2bf",
      "question": "US - Iran tensions are high over what?",
      "prediction": "The provided text states:",
      "ground_truths": [
        "nuclear program."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e4825a716b7849688276489483e5ee23",
      "question": "Who was the President of the US at this time?",
      "prediction": "The provided text states that Iran's development of a nuclear weapon is unacceptable by the US. It also says that Iran does not mind if the United States provides other Persian Gulf countries with nuclear technology.",
      "ground_truths": [
        "Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0886bb0eeac143bcb37ebd0ecf3e17cd",
      "question": "Who has tensions over Tehran's nuclear ambitions?",
      "prediction": "The provided text contains information from CNN's \"Iran\" news service. It also includes the following:",
      "ground_truths": [
        "U.S."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3861c1666a0d464f82eadeb063bcaf06",
      "question": "Who criticized Obama ?",
      "prediction": "The provided text does not contain the answer to the question. The text simply states that Obama has said that he is \"willing to be conciliatory with the United States.\"",
      "ground_truths": [
        "Iran's parliament speaker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da1702ba63dc476a939605d8e7bac2f2",
      "question": "Parliamentary speaker says who should apply campaign message of change?",
      "prediction": "The correct answer is Iran. Mahmoud Ahmadinejad said he thinks the American campaign message of change should be implemented to reduce the country's reliance on Iran's nuclear program.",
      "ground_truths": [
        "U.S. President-elect Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9872fe4f82246de9a9ba8194206ef69",
      "question": "What kind of weapons are being discussed?",
      "prediction": "The provided text does not contain the answer to the question. Therefore, I am unable to provide a definitive answer. The text does not state the specific items being discussed.",
      "ground_truths": [
        "nuclear"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "36285cfb993846c9878f228ba737a589",
      "question": "Who criticized Obama for saying nuclear weapon development is unaccaptable?",
      "prediction": "The Iranian national news agency, *Theiss News Agency*, criticized U.S. President Barack Obama for saying that Iran's development of a nuclear weapon is unacceptable.",
      "ground_truths": [
        "Iran's parliament speaker"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "be0561947e1d4aeeae20e0bacda01994",
      "question": "Who spent nine years in prison?",
      "prediction": "The answer is: The Colorado prosecutor who dismissed the first-degree murder charge, and the release of the same charge in retaliation for the conviction of Peggy Hettrick's murder.",
      "ground_truths": [
        "Tim Masters,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "14749a452bd346f9abbffc18ac1278e9",
      "question": "Who seeks a dismissal of Tim Masters murder case?",
      "prediction": "Based on the context, the answer is: **Colorado Attorney General John Suthers**.",
      "ground_truths": [
        "Colorado prosecutor"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "32c38f892d754ad89a9cfffa3d6afd7d",
      "question": "What was Masters convicted of?",
      "prediction": "The answer is, \"in the interest of justice.\"",
      "ground_truths": [
        "first-degree murder charge"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8cf9e5b352044c15a7004e39dae7b80b",
      "question": "When was Masters released following the toss of his conviction?",
      "prediction": "The answer is **Friday**.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1afe460fefa4535a3ff8d744086e5d7",
      "question": "Who seeks dismissal?",
      "prediction": "The answer is: The Colorado prosecutor.",
      "ground_truths": [
        "Colorado prosecutor"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ac839a5d98d842e589329484a33d49b6",
      "question": "Who was released on Tuesday?",
      "prediction": "The Colorado prosecutor Friday dismissed MichaelMasters' 1999 murder conviction, in the interest of justice, and stated that the DNA testing results, not all of which was available when he was investigated, were not available when he was investigated and tried.",
      "ground_truths": [
        "Tim Masters,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4159f0217b3f48ddbdee10ee5f4377e6",
      "question": "What was released?",
      "prediction": "The answer is \"in the interest of justice.\"",
      "ground_truths": [
        "new DNA evidence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b1f93e5763f3443e812bb02db0b34dff",
      "question": "What was Masters convicted of in 1999?",
      "prediction": "The answer to the question is \"in the interest of justice,\" and it is not explicitly stated.",
      "ground_truths": [
        "first-degree murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9a189e7d22948c78ac64341a4cb221c",
      "question": "What happened to the U.N. compound?",
      "prediction": "The text is a summary of the events at the London demonstration at a Palestinian protest in Gaza. It includes:",
      "ground_truths": [
        "hit and set on fire,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de2414f2ccc343d4997480eed8d54a02",
      "question": "What does the lawmaker say?",
      "prediction": "The lawmaker states that his grandmother was ill in bed when the Nazis came to her home town of Staszow. He also criticized Israel for its cruelty and its exploitation of the lost innocence of Palestinians.",
      "ground_truths": [
        "Israeli military action in Gaza is comparable to that of German soldiers during"
      ],
      "em": 0,
      "f1": 0.17391304347826086
    },
    {
      "id": "311e5b81a69a40898d8f648ae9db3d58",
      "question": "Who are Israel being asked to talk to",
      "prediction": "The answer is: **The UK's prime minister, Gordon Brown, stated that Israel is responding to militant fire near the complex and to the \"indefensible\" (a \"cease-fire\" order) and a \"delicate, humanitarian norm.\"**",
      "ground_truths": [
        "Hamas,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "997f584b4f774501a7365cf12f2fdbee",
      "question": "What has the UK PM called indefensible",
      "prediction": "The UK's PM, Gordon Brown, said that the UK would increase calls for a cease-fire and deliver aid to Gaza once a cease-fire had taken hold.",
      "ground_truths": [
        "the shelling of the compound"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52c7ca5b6e494e4f908bf7d7c348d2f1",
      "question": "What type of choice has Hamas made",
      "prediction": "The answer is: **(It was not in the context.  Kaufman stated that his grandmother's illness was a \"deeply nasty organization.\")**",
      "ground_truths": [
        "step up attacks against innocent civilians.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb192579a2d0438c935e63a19e1ff1f1",
      "question": "The second shot hit what?",
      "prediction": "The second shot hit what?",
      "ground_truths": [
        "struck Grant in the upper right arm,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5031ac1cc9049608f116e78492a2901",
      "question": "The man was rescued from what in northern Australia?",
      "prediction": "The man was rescued from the jaws of a crocodile in the Adelaide River near Darwin in Northern Australia.",
      "ground_truths": [
        "the jaws of a crocodile"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "c78d7fec07b14ea4aab3bbc8cef4e0ff",
      "question": "The men were collecting what on the river bank in the Northern Territory?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "crocodile eggs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c45d887d2074c3cb9b5c03e58b4c0c5",
      "question": "where Man rescues co-worker?",
      "prediction": "The answer is `A man who rescued a co-worker from the jaws of a crocodile in northern Australia also accidentally shot him in the process, police said.",
      "ground_truths": [
        "northern Australia"
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "08e1f398fc554a2b9cc745c89b8ddd1c",
      "question": "What did Giuliana Rancic do?",
      "prediction": "Giuliana Rancic did not do anything.",
      "ground_truths": [
        "undergoing a double mastectomy and reconstructive surgery,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fccd16a6079343e5ad862a07f82e3f42",
      "question": "What surgery did Rancic have?",
      "prediction": "Answer:  \"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming.\"",
      "ground_truths": [
        "double mastectomy and reconstructive"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "842b90f71d4142229534788a21189892",
      "question": "She says it feels great to be what?",
      "prediction": "To say it feels great to be back at work, she says: \"Thankfully, I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"",
      "ground_truths": [
        "back at work,\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "ecba8f966e03447a940cfff69f9ba8e1",
      "question": "Giuliana Rancic was back on the set where?",
      "prediction": "Answer: \"E! News\"",
      "ground_truths": [
        "\"E! News\""
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "4f1dcb1b8e9745ddb92444ccacb4b1b8",
      "question": "Who was back on the set at E!?",
      "prediction": "Answer: Giuliana Rancic was back on the set at \"E! News\" on Tuesday.",
      "ground_truths": [
        "Rancic"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "f5894ccfbf314a63baaca983e208a43b",
      "question": "Where was Giuliana Rancic?",
      "prediction": "Giuliana Rancic was back on the set at \"E! News\" on Tuesday.",
      "ground_truths": [
        "\"E! News\""
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "d0ef3a30427c4d66a516feb413c6b262",
      "question": "What did Rancic say about it?",
      "prediction": "Rancic said that she felt great being back at work, and she wasn's sure she had returned to \"E! News\" this week due to the previous surgery.",
      "ground_truths": [
        "was a wonderful homecoming,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8834ade537ca4faca9fb3bdd7efcb88b",
      "question": "Rancic, 37, had the surgery after lumpectomies failed to eradicate her what?",
      "prediction": "D'from Cigna-Rancic, 37, had the surgery after lumpectomies failed to eradicate her breast cancer.",
      "ground_truths": [
        "breast cancer."
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "3b8f128366fb4a669715a4b6cfc166cd",
      "question": "What did she say?",
      "prediction": "Giuliana Rancic said she was glad to be back at work.",
      "ground_truths": [
        "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "6e27ab10dda34bac8d427647fb0a3ce4",
      "question": "Who was greeted in Seoul?",
      "prediction": "The answer is \"The South Korean military raised its alert level. The country's media went into overdrive trying to predict how this oblique and erratic state would respond. The South Korean public seemed confident that the country would be able to manage this.",
      "ground_truths": [
        "the announcement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ae9f4575fc1464fa18740502542d2f5",
      "question": "What happened in 1994?",
      "prediction": "The answer is: \"The South Korean military raised its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state of the North Korean state would respond.\"",
      "ground_truths": [
        "Kim Il Sung died"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8aa85c5fbf0b4e21ac1f88d9cfdaf370",
      "question": "What was the reaction of South Korean military?",
      "prediction": "The answer is: \"The South Korean military responded by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond. However, the South Korean public seemed confident that the country would be able to manage this, and many people saw the North Koreans as people they needed to care for and help. The North Korean news immediately followed this with a general astonishment, quickly followed by speculation about what was going to happen next. The South Korean military responded by raising its level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond. But",
      "ground_truths": [
        "raising its alert level,"
      ],
      "em": 0,
      "f1": 0.07476635514018691
    },
    {
      "id": "b3f08e2a732a4b8aac4bdb22efbb6a03",
      "question": "Who died in 1994?",
      "prediction": "The answer is \"Kim Jong-il died in 1994, after being wounded in a car accident.",
      "ground_truths": [
        "Kim Il Sung"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "c20046f2bae84d69a5bc57eb097ed474",
      "question": "When did Kim II Sung die?",
      "prediction": "The context provides information about Kim Jong Il's death, including the news of his absence, and the North Korean government's response. It also mentions the South Korean military's response and the public's reaction to the news.",
      "ground_truths": [
        "1994"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6842c4d6b88841b88274405ab3720cfb",
      "question": "Who says most people in the south are calm about the situation?",
      "prediction": "The answer is: \"There is no rush to buy up food\"",
      "ground_truths": [
        "Woosuk Ken Choi,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5fb124107f024c4592306989f02b53b5",
      "question": "What level was raised?",
      "prediction": "The answer is: \"The South Korean military raised its alert level, while the country's media went into overdrive attempting to predict how this oblique and erratic state would respond. This was driven by the South Korean public's confidence in the North's ability to manage the crisis.",
      "ground_truths": [
        "alert"
      ],
      "em": 0,
      "f1": 0.04878048780487806
    },
    {
      "id": "5f9519908e5c44a48cfdea1bccf3d78c",
      "question": "What was the reaction in Seoul?",
      "prediction": "The city of Seoul's response to Kim Jong-il's death was met with shock, speculation, and a mix of caution, which followed by a wave of widespread panic and a chilling calm in the North's eyes. This was driven by the South Korean public's confidence in the country's ability to manage its affairs.",
      "ground_truths": [
        "astonishment"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da62aa39055e43479d74e92e330cbdd9",
      "question": "There was general astonishment where?",
      "prediction": "The provided text is a news report from South Korea's capital, Seoul, stating that a journalist from the country's largest newspaper's investigation at a cold December Monday confirmed the news of Kim Jong Il's death, though the North Korean leader's response was praised in Seoul.",
      "ground_truths": [
        "Seoul,"
      ],
      "em": 0,
      "f1": 0.05128205128205127
    },
    {
      "id": "f687cfbb7bec408799b8b6f8285b5608",
      "question": "What did cast member A.J. Jewell's death cause?",
      "prediction": "The cast member of \"The Real Housewives of Atlanta\" died at the age of 34, and his death is considered to be a tragedy.",
      "ground_truths": [
        "of \"The Real Housewives of Atlanta\" reunion special,"
      ],
      "em": 0,
      "f1": 0.37037037037037035
    },
    {
      "id": "3e5332718ede4bcb99b7c336ff05b9dd",
      "question": "Who is the former fiance of Kandi?",
      "prediction": "The former fiance of Kandi Burruss is David Eubanks.",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "059055bc2ed24376a2667ee2496aeb0c",
      "question": "Who was the former fiance of Kandi Burruss?",
      "prediction": "The answer is Dwight Eubanks.",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "241efa1cac394ac983b144420a13a162",
      "question": "What show was scheduled to tape its reunion special recently?",
      "prediction": "The Atlanta \"Housewives\" did not tape the reunion special recently.",
      "ground_truths": [
        "\"The Real Housewives of Atlanta\""
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "e365fba82bca48ad8b9ec4fbac8bd3b7",
      "question": "Whose death caused the postponement of taping?",
      "prediction": "The Atlanta \"Housewives\" is postponed the second season of the \"Real Housewives of Atlanta\" reunion special.",
      "ground_truths": [
        "Kandi Burruss' former fianc√©, Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4c3e45a17884ef78442ed584e442741",
      "question": "When was the reunion scheduled for?",
      "prediction": "The Atlanta \"Housewives\" reunion special was scheduled for last week.",
      "ground_truths": [
        "last week,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "828c73b72b124615b8ccc1cf1ab1014b",
      "question": "When was the \"Real Housewives of Atlanta\" scheduled to tape their reunion special?",
      "prediction": "The \"Real Housewives of Atlanta\" reunion special was scheduled for last week.",
      "ground_truths": [
        "last week,"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "1f92d606a35b4cccae34b03477b7f88b",
      "question": "Where did the deadly earthquake happen?",
      "prediction": "The answer is confirmed by the CNN.",
      "ground_truths": [
        "Haiti."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ae1b671353c41eb9fb31fc466e14155",
      "question": "Where did he lead the effort?",
      "prediction": "The answer is \"fed for saving lives,\" not \"leading the effort.\"",
      "ground_truths": [
        "in Haiti."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67a5828b6f5c4b7cbffe811a9e18290d",
      "question": "Is the cause of ibs known?",
      "prediction": "The answer is: The cause of IBS is unknown.",
      "ground_truths": [
        "remains unknown,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "05fd8aeb14d24b5fbb59d596f4aebf8a",
      "question": "What percent of North Americans have ibs?",
      "prediction": "The answer is \"It wasn't appreciated how much of an impact [IBS] can have on a patient's quality of life,\" and \"The exact cause of IBS remains unknown,\" and \"health.com: Is that belly ache in your head?\"",
      "ground_truths": [
        "10 to 15"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4829965548f840809e931e6b957f085c",
      "question": "What does the study analysis say works?",
      "prediction": "The study analysis indicates that 10 to 15 percent of people in North America have IBS, and about one-third of people with the intestinal disorder seek treatment. The exact cause of IBS remains unknown, and the lack of knowledge has led to the use of a variety of treatments, including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.  These treatments are often controversial due to their mixed-type results.  The authors and their colleagues found that, allulose, while effective in reducing constipation, significantly reduced symptoms of IBS, particularly in patients with mild or moderate",
      "ground_truths": [
        "Peppermint oil, soluble fiber, and antispasmodic drugs can indeed help people with irritable bowel syndrome,"
      ],
      "em": 0,
      "f1": 0.0761904761904762
    },
    {
      "id": "7e8255671f0a4fbd88c688b4e6b2bee8",
      "question": "Approximately how many people in North America have IBS?",
      "prediction": "The answer is 10 to 15 percent of people in North America.",
      "ground_truths": [
        "10 to 15 percent"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "739854dee1cc4fbc991ea07a0e593389",
      "question": "What treatments work for ibs?",
      "prediction": "The answer is: **(C)  the exact cause of IBS is unknown, and that lack of knowledge has led to the use of a variety of treatments, including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives. However, many treatments are controversial due to study results have been mixed.**",
      "ground_truths": [
        "including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives."
      ],
      "em": 0,
      "f1": 0.43333333333333335
    },
    {
      "id": "914623f6bde244cab5e20f01a6d2560b",
      "question": "who  has filed suit with international court?",
      "prediction": "The answer is the International Whaling Commission in the Hague, Netherlands.",
      "ground_truths": [
        "Australian officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d974393cef5d43e791127eaed8ddb259",
      "question": "What does the moratorium allow hunting whales for?",
      "prediction": "The answer is: Australia is asking an international court to weigh in on Japan's whale-hunting practices.",
      "ground_truths": [
        "scientific reasons."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b8294085ecc44a89bc6838c7ab93f1e0",
      "question": "who says decision to head to court \"regrettable\"?",
      "prediction": "The answer is Japan.",
      "ground_truths": [
        "Japanese Foreign Ministry spokesman Hidenobu Sobashima"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16083c1098654497a127f77ea191c127",
      "question": "What has Austrailia filed  suit over?",
      "prediction": "The answer is Japan's whaling practice.",
      "ground_truths": [
        "to stop Japan from exploiting the research loophole."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3287651f8746453ba1943191b85095a9",
      "question": "who allows hunting whales for scientific reasons?",
      "prediction": "The answer is: Australia.",
      "ground_truths": [
        "Japanese officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64eba29acb64482196d9e21f5170039d",
      "question": "what resumes TNT?",
      "prediction": "The TNT series has resumed Monday night, with a new episode, as it was last seen in September.",
      "ground_truths": [
        "\"The Closer.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8603756cbaaa464fb160ab2901bfbb00",
      "question": "what is a sag award",
      "prediction": "The question is, what is a sag award?",
      "ground_truths": [
        "Screen Actors Guild"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a6aa177d7f744399fc722a923f38660",
      "question": "When does TNT resume the series?",
      "prediction": "The answer is: TV. (CNN)",
      "ground_truths": [
        "Monday night"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53c67cb7647d4cc4bb23eaf2d317008d",
      "question": "Who did Superman battle in \"Clan of the Fiery Cross\"?",
      "prediction": "The answer is: **Donald Duck. He was a lifelong fan of the \"Clan of the Fierce Cross\" cartoon, and his knowledge of Mischelet, a chemical intermediate, was a key element in his later work, particularly his \"Clan of the Fiery Cross\" series.** He was a master of the \"Donau\" (the \"fiery, smelly, delicious\" sound of the cartoon) and knew how to imbue his characters with a particular quality and charm.",
      "ground_truths": [
        "Ku Klux"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d34861cb019d4deaaeaa08250ca1b4fc",
      "question": "What have affected people in real life?",
      "prediction": "The answer is, \"Among experiments which have not, to our knowledge, been carried out as yet, is one of a most intriguing nature suggested in the literature of no less than 19 years ago.\"",
      "ground_truths": [
        "comic book characters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fee8b22532204c5c87bb96182e128d56",
      "question": "What group did Superman battle in \"Clan of the Fiery Cross\"?",
      "prediction": "Based on the context, the answer is: **The members of the \"Clan of the Fiery Cross,\" and the Man of Steel.**",
      "ground_truths": [
        "the Ku Klux Klan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ebc8ddb0bc9e409e88de995ce9a063ab",
      "question": "Who did Superman battle in the radio series?",
      "prediction": "The answer is, in the 1940s, \"The Adventures of Superman\" was a radio sensation.",
      "ground_truths": [
        "Ku Klux Klan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3811aec484274bd99fb8231b001e9e02",
      "question": "Who blocked a scientist from getting a patent?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "Donald Duck"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ecfb1c98af64534bda5385ce6253550",
      "question": "What cartoon character blocked a scientist from getting a patent?",
      "prediction": "The answer is: **Shaton Kennedy**",
      "ground_truths": [
        "Donald Duck"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c9c76f0655142e8b33e0448f3f94777",
      "question": "Where was the teenage boy shot?",
      "prediction": "The answer is Athens, Greece.",
      "ground_truths": [
        "Athens,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "37b9919bd53e4cd8ad12b4a1d646bf36",
      "question": "How many civilians were injured?",
      "prediction": "The answer is 34.",
      "ground_truths": [
        "34"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3e4c8a8c9e174680a247362b4b0fe77c",
      "question": "What caused protests to explode?",
      "prediction": "The answer is, the government has not accepted what is occurring.",
      "ground_truths": [
        "killing of a 15-year-old boy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01ea084994484e5caa8c9cb317cb11a5",
      "question": "Authorities vow to do what in regards to the rioting?",
      "prediction": "The provided text does not contain any information on the authorities' response to the Knewilly's shooting. It only states that \"The Greek Interior Minister Prokopis Pavlopoulos declared the government will not accept what is occurring\" and that 34 civilians and 16 police officers were injured on Monday in the riots.",
      "ground_truths": [
        "re-impose order"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e54fc7e65e14768a22401ac648db015",
      "question": "Where is the rioting happening?",
      "prediction": "The answer is Greece, Greece.",
      "ground_truths": [
        "across Greece"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "194a8ecbcf0d4d17b469be92d066ad93",
      "question": "Number of civilians injure during riots?",
      "prediction": "The provided text does not contain the answer to the question, but it does include a note on CNN stating that the number of civilians injured (34) and police officers (16) were injured in the riots.",
      "ground_truths": [
        "34"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "708781de86994f85a8cf27d7d4d9552a",
      "question": "How many civilians were injured in the rioting?",
      "prediction": "The answer is 34.",
      "ground_truths": [
        "34"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "815cf0a1a5dd4a31b9b6b94e62e31435",
      "question": "Where was the teenager shot at?",
      "prediction": "The answer is Athens, Greece.",
      "ground_truths": [
        "Athens,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f9d580fe51264fe1948adba52d906ee5",
      "question": "Who is rioting?",
      "prediction": "The answer is \"a large group of citizens of Greece, organizing for a change in government.\"",
      "ground_truths": [
        "young self-styled anarchists"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5252a53cef7d419a8ea37a3beb22e2fa",
      "question": "What do authorities vow to re-impose?",
      "prediction": "The answer is: they vow to re-impose order.",
      "ground_truths": [
        "order"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "4c426e2c625f463f858241fbb08db079",
      "question": "Who is suing the ICE?",
      "prediction": "The answer is the former detainees at the Immigration and Customs Enforcement (ICE) in California.",
      "ground_truths": [
        "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "2ffae4c1a213448c93aae455b39ac77b",
      "question": "WHAT HAVE 1073 DETAINEES HAD SINCE 2003?",
      "prediction": "The answer is: 1,073 immigrants had SIN since 2003.",
      "ground_truths": [
        "\"medical escorts\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d20e69e95bb407d8eb523d6cfeb4c45",
      "question": "Which senator vows to investigate the allegations?",
      "prediction": "The answer is: **Then the former detainees, Raymond Soeoth and Amadou Diouf, did not claim abuse, but they said they were plaintiffs in a class-action lawsuit.**",
      "ground_truths": [
        "Sen. Joe Lieberman,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93fd50e8402b4c80bc553d18f4d6e818",
      "question": "Who did the detainees sue",
      "prediction": "The answer is \"The government is trying to deport them, but not in a normal way.\"",
      "ground_truths": [
        "the government."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "8914c97e8b7143239e4c0f5be45b7a7a",
      "question": "What number of detainees had medical escorts since 2003?",
      "prediction": "The answer is 56.",
      "ground_truths": [
        "1,073"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b9f576ae3984970ae5ee0cbfcbc2af0",
      "question": "What did they tell CNN?",
      "prediction": "Based on the provided context, the answer is directly from the context.",
      "ground_truths": [
        "were injected with the drugs against their will."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa66675458b041748d8d40a8140f82cb",
      "question": "What is ICE alleged to have done to detainees?",
      "prediction": "The answer is: ICE alleged to have done to detainees is not definitively stated but the following:",
      "ground_truths": [
        "forcibly injecting them with psychotropic drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "106958546d0a4d9cb5effa643be73014",
      "question": "What news station interviewed the detainee",
      "prediction": "The answer to the question \"What news station interviewed the detainee\" is likely the **CNN**.",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "bd85694b24f8489e98a5beeeeec3d12b",
      "question": "Who did the detainees file a suit against?",
      "prediction": "The answer is the former detainees of Immigration and Customs Enforcement in California, New York, who filed a lawsuit against the federal government for forcibly injecting them with psychotropic drugs, while trying to shuttle them to the U.S. under the 18 U.S. Code.",
      "ground_truths": [
        "Immigration and Customs Enforcement"
      ],
      "em": 0,
      "f1": 0.1951219512195122
    },
    {
      "id": "32e19ff72cc948d5bba10d5b45071a03",
      "question": "what is the senator doing",
      "prediction": "The answer is: **The senator is making a request to ICE to authorize the drug treatment of undocumented immigrants.**",
      "ground_truths": [
        "intends to follow up with ICE"
      ],
      "em": 0,
      "f1": 0.19047619047619044
    },
    {
      "id": "d9a79c663c2f4359bc85a93bd49e4f90",
      "question": "what were detainees allegedly injected with?",
      "prediction": "The answer is:  The ICE government injected the detainees with a potent anti-psychotic drug, called \" Haldol,\" which is often used for treating schizophrenia.",
      "ground_truths": [
        "psychotropic drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cdb364365444a79a58033a1879248ce",
      "question": "who is lieberman?",
      "prediction": "The answer is: Dr. Paul Appelbaum, a professor of psychiatry at Columbia University.",
      "ground_truths": [
        "\"Senator"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a9b90b0d783413c9c6ff456e7dfe0ee",
      "question": "who had medical escorts?",
      "prediction": "The answer is: \"The government was trying to deport 56 immigration detainees, with 33 receiving medication \"because of combative behavior with the imminent risk of danger to others and/or self,\" said Soeoth, 1,073-1,339 in the American Academy of Psychiatric-15, a member of the American Academy of Psychiatry and the Law.",
      "ground_truths": [
        "1,073 immigration detainees"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "2984697f6d3c4ac79d2c916f3496a4d2",
      "question": "What is Sen. Lieberman vowing?",
      "prediction": "The answer to the question is: \"In this class-action lawsuit, the former detainees, Raymond Soeoth of Indonesia and Amadou Diouf of Senegal, are seeking an end to the alleged practice of forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.\"",
      "ground_truths": [
        "detainees are not drugged unless there"
      ],
      "em": 0,
      "f1": 0.08163265306122448
    },
    {
      "id": "cc289ea875ee4805a12afd70d6c50f6b",
      "question": "What number of detainees have had \"medical escorts\" since 2003?",
      "prediction": "The answer is 56.",
      "ground_truths": [
        "1,073"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b8d6ee43c6941e5a772b201f4865aa9",
      "question": "What is the ICE suit about?",
      "prediction": "The answer is: The ICE suit.",
      "ground_truths": [
        "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bb35180c958469b818d1a49460ce8c0",
      "question": "who files suit?",
      "prediction": "The answer is: the former detainees, Raymond Soeoth of Indonesia and Amadou Diouf of Senegal, who were injected with the potent anti-psychotic drug Haldol.",
      "ground_truths": [
        "American Civil Liberties Union"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0197137fc90f4fe2a809e0651f965ac0",
      "question": "what did he say",
      "prediction": "The answer is:  \"He was injected with a mystery drug that made him groggy for two days.\"",
      "ground_truths": [
        "was injected with drugs by ICE agents against his will."
      ],
      "em": 0,
      "f1": 0.24
    },
    {
      "id": "cdfa787715a043bca7f13b775f44949f",
      "question": "What dog breed is described as \"active athletes\"?",
      "prediction": "The dog breed described as \"active athletes\" is likely to be considered as a Labrador Retriever, but the specific description doesn't specify what's \"active athletes\" in this context. Therefore, we can only say:",
      "ground_truths": [
        "Portuguese water"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4638c3ddb3e49bea26f3ec538123eec",
      "question": "What are Portugese water dogs like?",
      "prediction": "The answer is \"active athletes, far from couch potatoes, they are more likely to rip a couch into pieces than lounge on it.\"",
      "ground_truths": [
        "are \"active athletes,\" far from couch potatoes,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "13dfe50cd8404e7a9ef7f8d3dccae6ee",
      "question": "Who is more likely to rip up the couch, than lounge on it?",
      "prediction": "The answer is \"the most publicity the breed has ever had since its introduction into the U.S. in the late 1960s,\" \"the Portuguese Water Dog Club of America's news release,\" and \"Harry Harding's comment about the new Portuguese water dog's \"Web site's...\",\" which means \"Web site's...\"",
      "ground_truths": [
        "Portuguese water dogs"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "6c20f39bfa2d4fdca81983ab72b56537",
      "question": "What issue is the breeders concerned about?",
      "prediction": "The breeders concerned about are \"the most publicity the breed has ever had since its introduction into the U.S. in the late 1960s,\" \"the Portuguese Water Dog Club of America,\" and \"the American Dogs.\"",
      "ground_truths": [
        "a thorough understanding of the dogs' needs,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "f031f8ed49f8468386dc99013ce53127",
      "question": "Who is Bo the dog?",
      "prediction": "The answer is \"Bo the Portuguese Water Dog Club of America.\"",
      "ground_truths": [
        "gift to the Obama girls from Sen. Ted Kennedy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c479c64936a2439f93eee746440725c3",
      "question": "What is his name?",
      "prediction": "The answer is Jeffrey Jamaleldine.",
      "ground_truths": [
        "Jeffrey Jamaleldine"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bcf61d09606341f89cbf4bf4e6a1c2b1",
      "question": "Where did he go to college?",
      "prediction": "The answer is Jeffrey Jamaleldine, a German-American who left Germany for college in America in 2006. He joined the U.S. Army.",
      "ground_truths": [
        "in Missouri"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "d388f477244347c793cd528aa6e03473",
      "question": "Soldier was one of more than 20,000 \"green-card warriors\"",
      "prediction": "The answer is: Jeffrey Jamaleldine. He is a German U.S. Army scout who, after spending a bullet on his chin, is now stationed in Iraq, and he says he's fighting an unjust war for America. He also states that he has an American passport, and he's leaving his German passport behind.",
      "ground_truths": [
        "Jamaleldine"
      ],
      "em": 0,
      "f1": 0.042553191489361694
    },
    {
      "id": "8173dbf1042c40eeb456d8842e319da0",
      "question": "What do people still believe?",
      "prediction": "The answer is that Jeffrey Jamaleldine, a U.S. Army scout, is not saying that he's going to Iraq again. He says he's fighting for America, and he does so, because he is deeply loyal to the American people. He says that \"it would satisfy me more, it‚Äôs not a German thing.\"",
      "ground_truths": [
        "\"You can go from rags to riches"
      ],
      "em": 0,
      "f1": 0.037037037037037035
    },
    {
      "id": "88f9909fba5a41f0bbaddd549d1003bc",
      "question": "What type of soldier is this?",
      "prediction": "The answer is, Jeffrey Jamaleldine is a German American who was shot in the face during Iraq's 2001 terrorist attacks. He is a 31-year-old U.S. Army scout who was awarded U.S. citizenship in 2006. He was also awarded U.S. citizenship in 2006, but his passport is held by his father, who says it was a \"German passport,\" and he is fighting an \"unjust war for an America that went too far\" to which the majority of Germans believe. His son's testimony is key to",
      "ground_truths": [
        "U.S. Army scout"
      ],
      "em": 0,
      "f1": 0.07594936708860758
    },
    {
      "id": "6ebd5652000e4e6c9d486c4873e23e06",
      "question": "What does his dad wonder?",
      "prediction": "The answer is: Jeff. Jamaleldine's son, Jeffrey Jamaleldine, is going back to Iraq in a second if asked.",
      "ground_truths": [
        "Why he's more American than a German,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa44913cfa244faa88e773dcc3045012",
      "question": "what's the cyclist's Olympic record?",
      "prediction": "The answer is: 1995.",
      "ground_truths": [
        "fifth"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8caeb3cde3074e66af2795f258908f69",
      "question": "who was a three-time road race world champion by 1988?",
      "prediction": "The answer is: **Longo-Ciprelli, a three-time world champion in the women's Tour de France, and a double winner of the women's Tour de France in 1995.**",
      "ground_truths": [
        "Longo-Ciprelli"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "f87ac51d4db74ff7b51a4c5ea00723e8",
      "question": "Where did they win medals?",
      "prediction": "The answer is **Atlanta, Georgia**.",
      "ground_truths": [
        "Atlanta,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f888582a9e324a639852992392b5a0cf",
      "question": "who won the record",
      "prediction": "The answer is: **Longo-Ciprelli won the gold in Seoul at the 1996 Olympic Road Race.**",
      "ground_truths": [
        "Longo-Ciprelli"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "aad4e0033fd1472a8b6b4c3d1eec2096",
      "question": "Who is the president?",
      "prediction": "The answer is: **President Bush.**",
      "ground_truths": [
        "Bush"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "19f2498cca6d46d78d4e3f5bfda08cff",
      "question": "Will there be any restrictions on funding the wars",
      "prediction": "The answer is: Yes, there will be any restrictions on funding the wars.",
      "ground_truths": [
        "without the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aea4400d534b40b5926322dac2091f01",
      "question": "What victory did bush get",
      "prediction": "The answer is: \"We welcome them home.\"",
      "ground_truths": [
        "the bill"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f72b2c5781a44d10a48345cabe13be80",
      "question": "What is the benefit for veterans?",
      "prediction": "The answer is \"our unflinching support, and the best way to demonstrate that support is to give them the resources they need to do their jobs and prevail.\"",
      "ground_truths": [
        "education"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "71827c25726b4b9b863d47e26f55e597",
      "question": "What else does the bill contain?",
      "prediction": "The bill contains a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest.",
      "ground_truths": [
        "that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods."
      ],
      "em": 0,
      "f1": 0.8333333333333334
    },
    {
      "id": "55a7f38bc3564f9eb6657cad876e5228",
      "question": "What causes a victory for President Bush?",
      "prediction": "The answer is \"our unflinching support,\" and the words \"our unflinching support\" are used to describe the Democrats' willingness to support the troops, as they are grateful for the Republicans' \"in-fighting.\" The response is directly based on the context provided.",
      "ground_truths": [
        "the bill"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a380b010fa1471dbf4636c7d6ede83d",
      "question": "what does legislation fund",
      "prediction": "The answer is: The supplemental spending bill provides nearly $162 billion in war funding without the restrictions that were put in place since the national withdrawal of Iraq and Afghanistan.",
      "ground_truths": [
        "wars in Iraq and Afghanistan"
      ],
      "em": 0,
      "f1": 0.25806451612903225
    },
    {
      "id": "8c3fb24a332840c2ba5589d8aabb4d8d",
      "question": "what does bill contains",
      "prediction": "The answer is: \"our unflinching support, and the best way to demonstrate support is to give them the resources they need to do their jobs and prevail.\"",
      "ground_truths": [
        "nearly $162 billion in war funding"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9fba116703f4e9c8fb2a6ac44b98d96",
      "question": "what did bush do",
      "prediction": "The answer is: \"Our unflinching support is to give them the resources they need to do their jobs and to prevail, and to provide them with the means they need to do their jobs and prevail.\"",
      "ground_truths": [
        "signed a bill that will pay for the wars in Iraq and Afghanistan"
      ],
      "em": 0,
      "f1": 0.045454545454545456
    },
    {
      "id": "faf67589ec1f4d73a282e7e977edfa4f",
      "question": "What is the budget for the spending bill?",
      "prediction": "The answer is: the budget for the spending bill.",
      "ground_truths": [
        "$162 billion"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9ef08e2693943069670c3b422f8513a",
      "question": "What did he sign?",
      "prediction": "The answer is \"our unflinching support, and the best way to demonstrate support is to give them the resources they need to do their jobs and prevail.\"",
      "ground_truths": [
        "a bill that will pay for the wars in Iraq and Afghanistan"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "7e4328290edf4f2aac9f8988b5079fe1",
      "question": "Can non European players be in the squad?",
      "prediction": "The answer is: **Yes, Brazilian striker Ronaldinho is granted dual nationality. This is confirmed by Barcelona.**",
      "ground_truths": [
        "frees up a place for another non-European Union"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74f114ab49744e608eafab899371d913",
      "question": "Who is granted dual nationality?",
      "prediction": "The answer is **Brazil**.",
      "ground_truths": [
        "Ronaldinho"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8eab99234f214cfeba8bd193043cc3b8",
      "question": "What does this move mean for the squad?",
      "prediction": "The Madrid forward was granted dual nationality on Monday, the Primera Liga club said.",
      "ground_truths": [
        "frees up a place"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b31c60bda086454a900948cde75eb053",
      "question": "Where was he given dual nationality?",
      "prediction": "The answer is Rio de Janeiro (Brazil).",
      "ground_truths": [
        "Spain"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68b7a9d693f84289b1d8cca76c5daf3b",
      "question": "What has Robaldinho been granted by Spain?",
      "prediction": "Brazil's forward Ronaldinho is granted dual nationality by Spain on Monday.",
      "ground_truths": [
        "dual nationality"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "c17b16d7c4e24f608c9af17cde7c62fb",
      "question": "Who stars in \"The Da Vinci Code\"?",
      "prediction": "The answer is Tom Hanks.",
      "ground_truths": [
        "Ewan McGregor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d40b03e0f9144f2b023013be680715e",
      "question": "What is the name of the other actor?",
      "prediction": "The answer is \"Tom Hanks\" who directed \"Angels & Demons.\"",
      "ground_truths": [
        "Ayelet Zurer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd66428a35a144249fb8f291f3a16519",
      "question": "What is the name of the lead actor in the movie?",
      "prediction": "The answer is Tom Hanks.",
      "ground_truths": [
        "Tom Hanks"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1abdfb6c7fd4491a8ccbf4ebc845ccae",
      "question": "The president of which body said  \"It's all a lie\"?",
      "prediction": "The answer is \"It's all a lie\" by Bill Donohue.",
      "ground_truths": [
        "the Catholic League."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96b7abd0798c466aa7fc3dcc599472ab",
      "question": "Which actor starred in both movies?",
      "prediction": "The answer is \"Tom Hanks,\" as the director of \"Angels & Demons.\"",
      "ground_truths": [
        "Tom Hanks"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "99943e6e0c334768bf246ebb304d242a",
      "question": "Who is Tom's famous cast mate?",
      "prediction": "The answer is Tom Hanks.",
      "ground_truths": [
        "Ewan McGregor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f003b5f374324c6389dffc812158ffa8",
      "question": "What was siad by the Catholic League president?",
      "prediction": "The Catholic League president, Ted Baehr, denied the film's portrayal of the Illuminati, citing that it was a \"lie\" and that the Illuminati had never existed. He also downplayed reports that the Vatican had tried to hinder filming.",
      "ground_truths": [
        "\"I have a strong objection to the genre of mixing fact with fiction,\""
      ],
      "em": 0,
      "f1": 0.09302325581395349
    },
    {
      "id": "07d741fb951242a08936d7b32f7d6235",
      "question": "Angels & Demons was the sequel to which other film?",
      "prediction": "The answer is \"Angels & Demons.\"",
      "ground_truths": [
        "\"The Da Vinci Code\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2433c32ec61452a87d61970da188c26",
      "question": "What HBO show was he on?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "\"The Sopranos,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5df04119979040a09204fba80e103a01",
      "question": "Who did he meet with to discuss the issue?",
      "prediction": "The answer directly from the CNN's response is: \"I've always been fascinated by the political process,\" and the final question asks for the attendees' voting platform. Therefore, the answer is \"I've always been fascinated by the political process,\" and the conclusion is \"I've traveled to Denver to discuss the issues and the successes and failures of the Democratic National Convention.\"",
      "ground_truths": [
        "Obama and McCain camps"
      ],
      "em": 0,
      "f1": 0.037735849056603765
    },
    {
      "id": "e4d4955503d14398a933b782c9c3c7d0",
      "question": "Who did he meet with?",
      "prediction": "The answer is:",
      "ground_truths": [
        "Obama and McCain camps"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "77d545d2f95f41769e14c9207ea9cba9",
      "question": "What does the adovocacy group promote?",
      "prediction": "The answer is, Joe Pantoliano, a founder and president of No Kidding, Me Too, raised his hand in a final, heartfelt message to the Obama and McCain camps.",
      "ground_truths": [
        "mental health and recovery."
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "41f884e430af48e1bc0bd5948b694667",
      "question": "Who is the co-founder of the advocacy group No Kidding, Me Too?",
      "prediction": "The answer to the question is: **The Obama campaign**.",
      "ground_truths": [
        "Joe Pantoliano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0c13f8b533ca4107bff9421662d0354f",
      "question": "What did the actor act in before?",
      "prediction": "The answer is: Joe Pantoliano acted in the HBO series \"The Sopranos.\"",
      "ground_truths": [
        "\"The Sopranos,\""
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "60bbae0fbec24bf2a8de8ac0b7076b24",
      "question": "Who can Dublin rival?",
      "prediction": "The answer is Dublin.",
      "ground_truths": [
        "Silicon Valley."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d7eb4ea422a0440ab1a2ea0d9a313198",
      "question": "Where are headquartered Google and Facebook?",
      "prediction": "The answer is Dublin.",
      "ground_truths": [
        "Dublin."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3e1d564bb5eb4b1ba37db917c472a566",
      "question": "What companies have their headquaters in Ireland?",
      "prediction": "The companies with their heads in Dublin are:",
      "ground_truths": [
        "Facebook and Google,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19d523d3a0824aed8cc62eed5e24f1e4",
      "question": "What provides Dogpatch Labs Europe?",
      "prediction": "Dogpatch Labs Europe is a tech company that provides a space for aspiring entrepreneurs to brainstorm and build, in Dublin, Ireland, with the city as its home.",
      "ground_truths": [
        "a space for aspiring entrepreneurs to brainstorm with like-minded people."
      ],
      "em": 0,
      "f1": 0.42424242424242425
    },
    {
      "id": "4e52f520bbf54c38a827ebb5d2069dd2",
      "question": "Who already has headquarters in Ireland?",
      "prediction": "The answer is Dublin.",
      "ground_truths": [
        "Facebook and Google,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e4f8451992547708f6dac3268baa850",
      "question": "Who provides space for entrepreneurs?",
      "prediction": "Dublin is a major tech hub, providing a space for entrepreneurs to thrive.",
      "ground_truths": [
        "Dogpatch Labs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a7071c8d8c74dac901956228d59fdfb",
      "question": "What did Twitter announce?",
      "prediction": "Twitter announced its intention to set up headquarters in Dublin, Ireland, with Facebook and Google's headquarters in the Irish capital.",
      "ground_truths": [
        "its intention to set up headquarters in Dublin."
      ],
      "em": 0,
      "f1": 0.5925925925925926
    },
    {
      "id": "ccf8e9a161124b058363c911f27cc4bf",
      "question": "Which river has plunged to record low levels?",
      "prediction": "The answer is **Tigris River**.",
      "ground_truths": [
        "Tigris"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "1fed0c068f89431696e283427f59f39d",
      "question": "What does Iraq want Turkey to do?",
      "prediction": "Based on the provided context, the answer is \"We are aware of the water needs of Syria and Iraq.\"",
      "ground_truths": [
        "to increase the flow of water passing through its network of dams."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "f7776c027aca468e87d60716cfe3c1c3",
      "question": "What river has plunged to record lows?",
      "prediction": "Here's the answer, based on the provided context:",
      "ground_truths": [
        "Tigris"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ed20dcee741347bfb50dce447e13bc9b",
      "question": "What happened to the Tigris River?",
      "prediction": "The answer is, Turkish, Iraqi, and Syrian ministers met in Ankara to discuss water shortages in the Tigris and Euphrates rivers.",
      "ground_truths": [
        "has plunged to record low levels,"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "3b6bbfa360e14c8ea4997b82981bf6f7",
      "question": "What country does Iraq and Syria want to increase its water flow?",
      "prediction": "The answer is yes, Iraq and Syria want to increase their water flow.",
      "ground_truths": [
        "Turkey,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72a47042051c41f19706f18a5021cae3",
      "question": "What did Turkey do?",
      "prediction": "The answer is provided, but is not directly stated in the context. The context simply states that \"Datsy and Euphrates rivers run through all three countries,\" and that \"Syria and Iraq are badly in need of water.\" The answer is: \"We are aware of the water needs of Syria and Iraq, but our Iraqi brothers feel the need more. It is why this meeting is important for Turkey.\" The text also mentions that \"the energy minister also offered significant quantities of water to Iraq, but \"central and eastern Turkey had only received 350 cubic meters of water this year.\" This seems to be a factual statement",
      "ground_truths": [
        "provided Syria and Iraq 500 cubic meters of water a second,"
      ],
      "em": 0,
      "f1": 0.14953271028037382
    },
    {
      "id": "ab351f57bcf34ad6a535ed69e1eed04b",
      "question": "What have they withdrawn from each others' capitals?",
      "prediction": "The answer is: Syria and Iraq are not withdrawn from each other's capitals.",
      "ground_truths": [
        "ambassadors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0371094da0dc476383a99661fe910873",
      "question": "Where is the rivers' source located?",
      "prediction": "The answer is in Turkish.",
      "ground_truths": [
        "Turkey,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e9afbf1faa445db8535a1693283216a",
      "question": "Is pre-marital sex legal in Saudi Arabia?",
      "prediction": "The answer is yes, pre-marital sex is legal in Saudi Arabia.",
      "ground_truths": [
        "is illegal"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "4ad550b97d884435b955434af7338463",
      "question": "What did Jawad talk about?",
      "prediction": "Based on the provided text, the answer to the question is:",
      "ground_truths": [
        "foreplay, sexual conquests and how he picks up women,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d23aa2169253403daacd4adcdb967b6f",
      "question": "For what reason did Mazen Abdul Jawad apologize?",
      "prediction": "For what reason did Mazen Abdul Jawad apologize?",
      "ground_truths": [
        "his comments while Saudi authorities discuss whether he should be charged with a crime,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e854a8400dc4ca088df653f568d22b5",
      "question": "What are Saudi authorities debating?",
      "prediction": "Saudi authorities are debating whether to charge Abdul Jawad with a crime, according to local media.",
      "ground_truths": [
        "whether he should be charged with a crime,"
      ],
      "em": 0,
      "f1": 0.27272727272727276
    },
    {
      "id": "d9c8cde9f1c348f1848e4fe717df53b0",
      "question": "What did Mazen Abdul apologize for?",
      "prediction": "Mazen Abdul Jawad apologized for his comments on LBC's \"Red Lines\" show.",
      "ground_truths": [
        "bragging about his sex life on television"
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "102cad20644846c3b2cdfc5747db597b",
      "question": "What show was Jawad on?",
      "prediction": "The answer is \"LBC's show on Lebanese channel \"Red Lines.\"",
      "ground_truths": [
        "\"Red Lines,\""
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "b51dcde14bb747ce8f791ba4ac1bfb2a",
      "question": "North Korea recently threatened to \"wipe out\" what country  if provoked?",
      "prediction": "The answer is, North Korea likely threatened to remove the U.S. from the United States.",
      "ground_truths": [
        "United States"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "6c41bd9457ee40e9b264162b24aa4f4d",
      "question": "Who does Japanese media reporte North Korea may fire a missile at?",
      "prediction": "The answer is: The U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future.",
      "ground_truths": [
        "Hawaii."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a6cb4766efe4fa4b1914c9d24664bd5",
      "question": "Who did North Korea threaten to \"wipe out\" if provoked?",
      "prediction": "The answer is the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future, a U.S. intelligence official told CNN.",
      "ground_truths": [
        "the United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aba879c643644d1998692eb72c3c14d6",
      "question": "U.S. does not believe who intends to launch long-range missile soon?",
      "prediction": "The answer is \"We do not believe.\" The context states that the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future.",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "385fa0ebb7d64fa8bd1df50624998ef7",
      "question": "Who was warned to be clear due to \"military firing exercise\"?",
      "prediction": "The answer is, the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future.",
      "ground_truths": [
        "mariners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "163c4658e8ca400fa498b30004f817b6",
      "question": "What does US believe?",
      "prediction": "The answer to the question is: The U.S. believes North Korea intends to fire a missile at Hawaii on July 4th.",
      "ground_truths": [
        "North Korea intends to launch a long-range missile in the near future,"
      ],
      "em": 0,
      "f1": 0.37037037037037035
    },
    {
      "id": "6e111544f05745d693b3849705dbca73",
      "question": "What did North Korea recently threaten?",
      "prediction": "The answer is \"wipe out the United States if provoked,\" and the U.S. intelligence community does not believe it.",
      "ground_truths": [
        "\"wipe out\" the United States if provoked."
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "e746904f3d2148d599c71540581a6fbf",
      "question": "What may North Korea due to Hawaii on July 4?",
      "prediction": "The answer is, the North Korean government's warning to mariners to avoid a long-range missile test in Hawaii.",
      "ground_truths": [
        "fire a missile toward"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "f959d7fa37cb45e09337e1475d63528e",
      "question": "When did the japanese media report?",
      "prediction": "The answer is: The U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future, but the North Korean regime aims for short-range and medium-range missiles to test its new administration.",
      "ground_truths": [
        "July 4."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "675dc33e5be84d6d924a01c09f145723",
      "question": "Who refuses to broadcast ad?",
      "prediction": "The answer is \"DC\", as they are the DC.",
      "ground_truths": [
        "The BBC"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aff93bef4404bea9b6fe5abdf14bcb3",
      "question": "What is BBC funded by?",
      "prediction": "The answer is the BBC.",
      "ground_truths": [
        "an obligatory license fee paid"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37e1274e4a2b4b0c950789fddf51921c",
      "question": "What is the name of the charity group?",
      "prediction": "The answer to the question is \"Disasters Emergency Committee,\" which is the British Red Cross.",
      "ground_truths": [
        "British Red Cross, Oxfam, Save the Children and 10 other charities,"
      ],
      "em": 0,
      "f1": 0.2727272727272727
    },
    {
      "id": "a21c75fc6865449c8002290923056925",
      "question": "What do the protesters occupy?",
      "prediction": "The BBC's decision to not broadcast a plea from leading charities for aid to Gaza, along with the government's decision to change its stance, has resulted in a decision to not air the ad, with a response from the DEC. The DEC has said that it could limit the reach of the ad's message to the general public, and that it has no view on the possibility of a political response. The comment, \"Bully --baiter,\" prompted by a \"bully,\" was a significant number of people to challenge the BBC's decision.",
      "ground_truths": [
        "Glasgow office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "676b4a9625a446868ff66785db808efe",
      "question": "Where did protestors occupy?",
      "prediction": "The answer is \"Glasgow,\" England.",
      "ground_truths": [
        "the foyer of the BBC building in Glasgow, Scotland"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "5af072c9fcd04a99b64b2546fb15390d",
      "question": "what is the ad about?",
      "prediction": "The answer is: The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza.",
      "ground_truths": [
        "aid to Gaza,"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "46c8ca9995974e1086fae3543f6a8d4d",
      "question": "on which date disasters emergency committee will launch appeal?",
      "prediction": "The answer to the question is: on Monday.",
      "ground_truths": [
        "Monday."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "de5cb70961984db4a811b269f8257857",
      "question": "Carter's sentencing was postponed so he could get what?",
      "prediction": "Answer: Carter's sentencing was postponed, so he could get some dental work done, including removal of his diamond-studded braces.",
      "ground_truths": [
        "some dental work done,"
      ],
      "em": 0,
      "f1": 0.34782608695652173
    },
    {
      "id": "8637839ccc094e64977ef2984182f7d3",
      "question": "Rapper Dwayne Carter will be sentenced for what kind of conviction?",
      "prediction": "Answer: Rapper Dwayne Carter will be sentenced for a one-year prison sentence for felony gun charges.",
      "ground_truths": [
        "gun"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "14e1e5a0902d46a3b256f6cf8bcf5df4",
      "question": "What does the work?",
      "prediction": "The work is a plea agreement.",
      "ground_truths": [
        "removal of his diamond-studded braces."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5249075dcf8646b19ea8e050a4cfc254",
      "question": "Who will be condemned?",
      "prediction": "Answer: Dwane Carter will be condemned to a one-year prison sentence.",
      "ground_truths": [
        "Rapper Lil Wayne"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6a22911be0943c69c493ec69afa746d",
      "question": "Work includes removal of diamond-encrusted what?",
      "prediction": "Answer: Work includes removal of diamond-encrusted.",
      "ground_truths": [
        "braces."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60f6708411f94ae2980e44346f73c8e5",
      "question": "Which gene did the ALS association discover?",
      "prediction": "The ALS association discovered a new gene mutation, which allows those with ALS to be tested for another gene.",
      "ground_truths": [
        "ALS6,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1395b6cae1c4d4ebcf98b76d83b90db",
      "question": "How many people does Lou Gehrig's disease effect?",
      "prediction": "The answer is: Lou Gehrig's disease affects approximately 56,000 people every year.",
      "ground_truths": [
        "5,600"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "14c01f1d03094e0ea0dd5b59f604d270",
      "question": "What number of people get ALS each year?",
      "prediction": "The answer is 5,600.",
      "ground_truths": [
        "5,600"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a0d1230a75b547ca9237bde1e5ad0bf6",
      "question": "What does the ALS call the gene discovery?",
      "prediction": "The ALS gene discovery involves the mutation of a protein in the nucleus of a cell, which is then found outside the nucleus and sometimes forms unusual clumps. This process, which is a process called RNA processing, is thought to be crucial for building and transporting proteins and ensuring their proper location in cell structure.",
      "ground_truths": [
        "\"momentous discovery\""
      ],
      "em": 0,
      "f1": 0.039999999999999994
    },
    {
      "id": "a9d8c69172e3474097e6ad2e51cdaed6",
      "question": "How many people has ALS or Lou Gehrig's disease?",
      "prediction": "The answer is, based on the provided context, the answer is: **2,000.**",
      "ground_truths": [
        "5,600"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03e5521db7994b1d9c01547a2ec51aac",
      "question": "How much was seized?",
      "prediction": "The answer is $7.8 million.",
      "ground_truths": [
        "of methamphetamine and $7.8 million in cash"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "5ce2330296d142a2a154298ea4339e6c",
      "question": "How long was the investigation?",
      "prediction": "The answer is 17 months.",
      "ground_truths": [
        "15-month"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f8d3c8ec8d14efb97d10bd29838df03",
      "question": "Where there any other drugs recovered?",
      "prediction": "The answer is $7.8 million.",
      "ground_truths": [
        "123 pounds of cocaine and 4.5 pounds of heroin,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "58066ea944fe443a9283d135d5dafaac",
      "question": "What else did the authorities recover?",
      "prediction": "The authorities recovered 435 pounds of cocaine and 4.5 pounds of heroin, along with $7.8 million in cash, and another $12 million in $7.8 million in cash.",
      "ground_truths": [
        "123 pounds of cocaine and 4.5 pounds of heroin,"
      ],
      "em": 0,
      "f1": 0.4571428571428572
    },
    {
      "id": "e476ba8414ac461baf6b78958906f247",
      "question": "How much cash did the authorities seize?",
      "prediction": "The authorities seized 44 firearms, 650 pounds of marijuana, and $7.8 million in cash as part of a sting operation.",
      "ground_truths": [
        "$7.8 million"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "4795a6f9e6c24cd194173fe0e0b8b607",
      "question": "How many pounds of marijuana?",
      "prediction": "Answer: 44 pounds of marijuana.",
      "ground_truths": [
        "650"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dbd9d9e9ee1a4314b6e46179fe3db41f",
      "question": "What was the investigation dubbed?",
      "prediction": "The investigation was called \"Operation Crank Call.\"",
      "ground_truths": [
        "\"Operation Crank Call,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2a8e2d0e16d64cc2a03180e7dfb8d745",
      "question": "Where was the overcrowded ferry?",
      "prediction": "The answer is **Bola district of Dhaka.**",
      "ground_truths": [
        "Bangladesh,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99331d54ed084ccf98837531ede526e3",
      "question": "Where were the people traveling to?",
      "prediction": "The answer is **Dhaka**.",
      "ground_truths": [
        "Bhola"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72fa80f3ffa24cadbc2df7c806b0b59b",
      "question": "How many died in ferry capsize?",
      "prediction": "The answer is 28.",
      "ground_truths": [
        "28"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fe231269e136464c857e4cedaa8a523e",
      "question": "Where did the ferry depart from?",
      "prediction": "The ferry departed from Dhaka, Bangladesh, to Bhola, where it was capsized.",
      "ground_truths": [
        "Dhaka,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "4e8d8610abde45809ece08144c48d9e7",
      "question": "By how many was the boat overcrowded?",
      "prediction": "The answer is: The boat was overcrowded.",
      "ground_truths": [
        "2,000 people,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef29092974ce40b980f963ac746247ec",
      "question": "How many people were on board the ferry?",
      "prediction": "The answer is 2,000.",
      "ground_truths": [
        "2,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "22389bcc734d49d98fdccaf2c36e5c7f",
      "question": "What was conveyed?",
      "prediction": "The answer is \"I, the chief executive officer, the one on the very top, should be responsible for this,\" and the company's response to King's comment.",
      "ground_truths": [
        "our sincerity"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4166369ecf494ae998597cee8f2be647",
      "question": "What did the president of Toyota say he takes full responsibility for?",
      "prediction": "The president of Toyota said he takes full responsibility for safety issues in the company's cars.",
      "ground_truths": [
        "cars"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "35d000cd64d14585bd474322dc4f4f70",
      "question": "What did the president of Toyota say?",
      "prediction": "The president of Toyota said he took full responsibility for safety issues in the company's cars.",
      "ground_truths": [
        "he takes full responsibility for safety issues in the company's"
      ],
      "em": 0,
      "f1": 0.6956521739130435
    },
    {
      "id": "811443af769846bd88db9031899cf838",
      "question": "Who should be held responsible?",
      "prediction": "The key to answering the question is to respond to the specific comment by the company's CEO, Akio Toyoda:",
      "ground_truths": [
        "Akio Toyoda"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "2efef198621244dc8f660e49bcb0b54e",
      "question": "Who should be responsible?",
      "prediction": "The answer to the question is: **the chief executive officer.**",
      "ground_truths": [
        "the chief executive officer,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "ffa4d47207614c1a94eed9f7a08fc862",
      "question": "What did Toyota's president say?",
      "prediction": "Toyota's president said he takes full responsibility for safety issues in the company's cars.",
      "ground_truths": [
        "he takes full responsibility for safety issues in the company's"
      ],
      "em": 0,
      "f1": 0.8181818181818181
    },
    {
      "id": "7594504b380443f6920beb03b1379bd7",
      "question": "What are the safety issues?",
      "prediction": "The answer is: \"it is I, the chief executive officer, the one on the very top, should be responsible for this.\"",
      "ground_truths": [
        "related to sudden acceleration."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eee8cb77eccf4a5bab0fb2fe51c21dbf",
      "question": "When was it isolated?",
      "prediction": "The answer is **1983**.",
      "ground_truths": [
        "1983"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "63a371a61f894309b90bdfc835cee0e3",
      "question": "Who pioneered lithium treatment?",
      "prediction": "The answer is the Dutch psychiatrist, Antonie van Leeuwenhoek. He developed new bacteria that could be used to treat bipolar disorder, and he published his findings in the Medical Journal of Australia in 1949, with a note in \"Lithium salts in the treatment of psychotic excitement.\"",
      "ground_truths": [
        "Dr. Cade"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85ea67cf87f543148a3400fbb552a06c",
      "question": "When was HIV isolated?",
      "prediction": "The answer is **1983**.",
      "ground_truths": [
        "1983"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8189238b8f3c401f81c92d4cf28e7324",
      "question": "Many gay rights activists applaud Obama's what?",
      "prediction": "The answer is: \"we have more work to do,\" including on the issue of bullying.",
      "ground_truths": [
        "on supporting full marriage equality,\""
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "4c67b27ffb9d441c9aa84fd03b2de3f9",
      "question": "what did the president do",
      "prediction": "The answer is: Obama stated that he is committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy as an example. He also mentioned his accomplishments on LGBTQ rights issues.",
      "ground_truths": [
        "He acknowledged \"we have more work to do,\" including on the issue of bullying."
      ],
      "em": 0,
      "f1": 0.18604651162790697
    },
    {
      "id": "dbb8fc455e94400d89d4d1542418e945",
      "question": "what will obama do",
      "prediction": "The answer is: Obama will do nothing. He is committed to equality.",
      "ground_truths": [
        "\"we have more work to do,\""
      ],
      "em": 0,
      "f1": 0.23529411764705885
    },
    {
      "id": "2c35b0a8ddbc4c6a8e8cfd53cfa6717e",
      "question": "who do the gay rights activists applaud",
      "prediction": "The answer is: \"We have more work to do.\"",
      "ground_truths": [
        "the administration's progress,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ee81fad60d2d42bcbe7813550fee6aaa",
      "question": "Who says he will continue to advocate for equality?",
      "prediction": "The answer is: \"We don't believe in standing silent when that happens.\"",
      "ground_truths": [
        "President Barack Obama,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c8caac9743984aa4a1719e71cdf2c70c",
      "question": "who will continue to advocate for equality",
      "prediction": "The answer is:  \"We have more work to do,\" including on the issue of bullying.",
      "ground_truths": [
        "President Barack Obama,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2bc0ffd037f4b9397e5d7eb0e547819",
      "question": "What are hot spots for drug use?",
      "prediction": "The answer is \"hot spots for drug use\" ‚Äì a list of countries with higher concentrations of drugs, including narcotics, and also other substances.",
      "ground_truths": [
        "clubs and bars in Hong Kong and Shenzhen,"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "c84dbe83b4934408b32e1799184873fe",
      "question": "Where else are drug hotspots?",
      "prediction": "The answer is: **In China, with other parts of Asia, particularly India and mainland China.**",
      "ground_truths": [
        "public toilets and playgrounds."
      ],
      "em": 0,
      "f1": 0.11111111111111112
    },
    {
      "id": "84087ea7a70e4646ad86e4db8c4b7fa5",
      "question": "what types of drugs consumed",
      "prediction": "The answer is, \"I've watched as drug use has grown, and I've seen the number of students getting into drugs is also younger, but the exact pattern is still being studied by doctors and families.\"",
      "ground_truths": [
        "ketamine."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a369001634a84cb9bf81e4922893fe0e",
      "question": "What is ketamine?",
      "prediction": "The answer is ketamine.",
      "ground_truths": [
        "an animal tranquilizer,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e01e6c6e9ce48b4be86ec2a5010c489",
      "question": "What is the top drug choice in Hong Kong?",
      "prediction": "The top drug choice in Hong Kong is cane.",
      "ground_truths": [
        "ketamine,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ae0a845a533f4a4f86efad0aebbfed7e",
      "question": "what includes an ice sculpture of the Grinch?",
      "prediction": "The answer is **ice sculptures**.",
      "ground_truths": [
        "frozen world located in the Gaslight Theater."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02b909812ebd41d4a7dfd2ebc0bb1f35",
      "question": "What offers great shopping?",
      "prediction": "The answer is, \"The one must-see for a heady dose of holiday spirit. It promises to chill you out. Most of you have read or seen \"How the Grinch Stole Christmas!\" but not had the ultimate Dr. Seuss experience until you meet the green grump carved in ice. Put on a parka -- loaned to you upon admission -- and enter ICE!, a frozen world located in the Gaslight Theater. The attraction will be back next year, but this is the last time you'*ll see the Grinch and Who-ville. There are even frigid sliding boards to try. After a delicious hot",
      "ground_truths": [
        "Opry Mills,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cc706646f61c4f908f968b4b3ef25b68",
      "question": "Where is the show ICE! being held?",
      "prediction": "The answer is: The 25th year of the resort's Santa's bagful of activities and entertainment, with decorations continuing in July.",
      "ground_truths": [
        "Gaslight Theater."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8cb7447b1d56403fb64de07874a3e854",
      "question": "when did the decorations start going up?",
      "prediction": "The answer is, the decorations started going up in July for A Country Christmas.",
      "ground_truths": [
        "in July"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "66e9c731011d45de8ca1e7734c27a935",
      "question": "When did the decorations begin to go up?",
      "prediction": "The answer is a \"Santa's bagful of activities and entertainment.\"",
      "ground_truths": [
        "July"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b0db31bdf7ee43fdaeb49ae9dab8b70f",
      "question": "When do the decorations go up?",
      "prediction": "The answer to the question is \"When the decorations go up.\"",
      "ground_truths": [
        "July"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c50c42865c444b5c95fa97a7db5244e4",
      "question": "length of time pilots to be treated",
      "prediction": "The answer is: 10. A 10-month amnesty period is implemented for all pilots who use antidepressants, without fear of penalties.",
      "ground_truths": [
        "at least 12 months."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "040fd7f085b64969856b71c49b068884",
      "question": "how long is the treatment",
      "prediction": "The answer is: the FAA provides medical certificates to all pilots to ensure they are not prescribed antidepressants.",
      "ground_truths": [
        "12 months."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3fbd68661ad646c5a5e859934c9f9f79",
      "question": "what does faa say",
      "prediction": "The FAA states that the new policy will improve safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly.",
      "ground_truths": [
        "the new policy will improve safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly."
      ],
      "em": 0,
      "f1": 0.9538461538461539
    },
    {
      "id": "1e86e90fef4a4737b6e45530eabf0178",
      "question": "pilots must have been treated for at least 12 months for what reason?",
      "prediction": "The answer is: \"Absolutely,\"",
      "ground_truths": [
        "mild to moderate depression"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74bddc4c04a845bea80ae16c5bd1fd9b",
      "question": "What does de FAA says about the policy?",
      "prediction": "The FAA says that \"absolutely\" improves safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses.",
      "ground_truths": [
        "\"absolutely\" improve safety,"
      ],
      "em": 0,
      "f1": 0.1212121212121212
    },
    {
      "id": "ce0c7cc0513b45b6848c3a9b8c01f813",
      "question": "what has become a way in which to emphasize ideas on Twitter?",
      "prediction": "The answer to the question is, \"Just Sayin'''s Twitter,\" which translates to \"I'm just saying.\"",
      "ground_truths": [
        "the hashtag"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d833472ffb541e59c52411eb2140a6c",
      "question": "What we can use to tag a name on twitter or Facebook?",
      "prediction": "The answer is: \"Aitch-aSketch art is totally dope. But it's also a bit more complex to add to real-life scenarios because the code is inherently less intuitive. Also, it's a slightly less common symbol compared to our familiar #whiskeyandheartache. Therefore, stick to being caustic and dry.\"",
      "ground_truths": [
        "\"@\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "edc8a0f319ed4a4a8933ef300cf3f290",
      "question": "how many were wounded",
      "prediction": "The answer is: A doctor at a 400-bed hospital in Kabul, carrying a bandage, stated, \"We have received three people from the blast at Rabbani's house. Among the injured are Masoom Stanikzai, one bodyguard and an assistant to Rabbani.\"",
      "ground_truths": [
        "Four other people"
      ],
      "em": 0,
      "f1": 0.05714285714285714
    },
    {
      "id": "df1a8977601b4977a5d12f0ecca70c50",
      "question": "What was the number of suicide bombers?",
      "prediction": "The answer is \"Two\", not \"Four\" or \"Three\".",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "7aeaaea83aa14a6cb839bffbb9f350b9",
      "question": "who said 2 suicide bombers carried out the attack",
      "prediction": "The answer is a Taliban.",
      "ground_truths": [
        "NATO's International Security Assistance Force"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fe204be06f04ab487f825e9b6e80984",
      "question": "What did Afghan authorities describe?",
      "prediction": "The Afghan authorities described two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves,\" and four other people, wounded, and a doctor, as \"blast at Rabbani's house.\"",
      "ground_truths": [
        "The attacker hid the explosive device inside his turban,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9203243d8d274c259071d818041c57a5",
      "question": "how many bombers were there",
      "prediction": "The answer is:",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7437ff48e3e942d2b2473df4a6f891a8",
      "question": "who said the face of the peace initiative has been attacked",
      "prediction": "The answer is: \"You will not prevail.\"",
      "ground_truths": [
        "Gen. John R. Allen, commander of ISAF,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d435550c89f41d0b161b5443956aa1f",
      "question": "What did the commander say?",
      "prediction": "The answer is: A doctor at a 400-bed hospital in Kabul said, \"We have received three people from the blast at Rabbani's house, including Masoom Stanikzai, a bodyguard, and an assistant. Rabbani was a long-time key figure in Afghan and coalition efforts to bring Taliban leaders into the reconciliation process.",
      "ground_truths": [
        "\"face of the peace initiative has been attacked.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2aa976c49d943adafac0517e3471eec",
      "question": "who was attacked",
      "prediction": "The answer is \"Afghan people, who begin to take their country's security in their own hands.\"",
      "ground_truths": [
        "Burhanuddin Rabbani,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41d5431a031744a3891f2e86b168ffbe",
      "question": "how many people were wounded in the attack",
      "prediction": "The context describes a military attack on a Afghan political figure. The attack was carried out at a house.",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "967b7a252c2c43e79ee8c94e5d8c7b81",
      "question": "What wa sthe name of the suspect?",
      "prediction": "The suspect's name is Azzam the American.",
      "ground_truths": [
        "Adam Yahiye Gadahn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d5a45a99a744852873f9eae4e7c9694",
      "question": "what does he say about his citizenship",
      "prediction": "The answer is: \"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"",
      "ground_truths": [
        "of America, the symbol of oppression and tyranny and advocate of terror in the world?\""
      ],
      "em": 0,
      "f1": 0.18181818181818185
    },
    {
      "id": "cab69864c0bb431ab2098cac338ac9a1",
      "question": "who is in the video?",
      "prediction": "The answer is: The video is in an English transcript provided by As-Sahab Media, the media production company that used to make the Al Qaeda video.",
      "ground_truths": [
        "Adam Yahiye Gadahn, also known as Azzam the American,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10a9ca4228244de18071f4fccf05b2f2",
      "question": "who criticizes obama",
      "prediction": "The answer is: **U.S. policies and offensive in Gaza, and \"Zio-Crusader\" alliances**",
      "ground_truths": [
        "Adam Yahiye Gadahn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d124064943a2444e86841bc34e25fa81",
      "question": "What did Adam Yahiye Gadahn say?",
      "prediction": "Adam Yahiye Gadahn said in his previous Al Qaeda video, \"In a new anti-Israel, anti-U.S. video,\" he discussed his roots as he castigates U.S. policies and deplores Israel's offensive in Gaza, that began in late December 2008 and continued into January.",
      "ground_truths": [
        "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\""
      ],
      "em": 0,
      "f1": 0.10344827586206896
    },
    {
      "id": "c758a02e5942409599888906bba81b3e",
      "question": "Which area was the convert from?",
      "prediction": "The answer is: The area of the \"Zionist\" and \"Zionist hate organizations,\" which was indeed the city of Tel Aviv, where Gadahn's grandfather was a Zionist, a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations.",
      "ground_truths": [
        "rural California,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e43326fdfa2e4fecb89bc5174d482072",
      "question": "Who says the man denied wife liberty of coming and going with face uncovered?",
      "prediction": "The man denied citizenship because he was depriving his wife's liberty to come and go with her face uncovered.",
      "ground_truths": [
        "Immigration Minister Eric Besson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99a4611c0c0d4131b8813facc14de19b",
      "question": "What did the man deny his wife of?",
      "prediction": "The man denied his wife's citizenship because he was depriving her of the liberty to come and go with her face uncovered.",
      "ground_truths": [
        "liberty to come and go with her face uncovered,"
      ],
      "em": 0,
      "f1": 0.6206896551724138
    },
    {
      "id": "e58f6701ee0a4015a8de2d1f53fd6d6c",
      "question": "Which country is scheduled to vote on whether to ban full veils?",
      "prediction": "The answer is **France**. The question is not explicitly stated in the context.",
      "ground_truths": [
        "France"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "6fd0d95815d34aaa9beee0f56085f012",
      "question": "What did France deny a woman?",
      "prediction": "The French immigration minister said, \"The man was denied citizenship because he was depriving his wife of the liberty to come and go with her face uncovered, and she was prohibited from going out with her face unveiled.\"",
      "ground_truths": [
        "naturalization request"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dce68f614d0d4c1fad1ff4986dcd45fb",
      "question": "Which country denied a Moroccan woman's naturalization request?",
      "prediction": "The answer is France.",
      "ground_truths": [
        "France"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8eb13902054c435786dbfb9d907718ff",
      "question": "what was denied",
      "prediction": "The answer is: \"the full-body covering worn by some Muslim women.\"",
      "ground_truths": [
        "citizenship"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb6855a7c4964e44aa7d97b6bef22d46",
      "question": "what was man denied",
      "prediction": "The answer is: \"The man was refused citizenship because he was depriving his wife of the liberty to come and go with her face uncovered, and he rejected the principles of secularism and equality.\"",
      "ground_truths": [
        "citizenship"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "8c42e3acb5934672bfdc3c713a75d371",
      "question": "Who defeated Froch?",
      "prediction": "The answer is 31-year-old Mikael Froch.",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5d866dd01974343ba3475ba0ca2387c",
      "question": "Who defeated Carl Froch?",
      "prediction": "The answer is Mikael Kessler.",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a63e60b8ef140d28f31325f8211cb10",
      "question": "How many rounds were there in the match?",
      "prediction": "The answer is 12.",
      "ground_truths": [
        "12"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "28ae4259c5bd485a899ec68af2eb4264",
      "question": "Who defeats Carl to win?",
      "prediction": "Answer: Carl Froch defeats Mikael Kessler in a 117-111, 115-113, and 116-112, result.",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ace17f51ddc44d7db28c05c44bc5d658",
      "question": "Kessler secures unanimous points decision after what number of rounds?",
      "prediction": "Kessler secures unanimous points decision after 116-112, 115-113, 116-112, and 112-111.",
      "ground_truths": [
        "12"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aecbba5c017943a79d643d0cc71e57b7",
      "question": "Where are the Thai soldiers accused of crossing into?",
      "prediction": "The Thai soldiers accuse Cambodian troops of crossing into the area, claiming the temple was the site of clashes last year.",
      "ground_truths": [
        "Cambodian territory"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "ab65135d9fc94f16a97555b6b3c54832",
      "question": "Who claimed the soldiers had crossed into the area?",
      "prediction": "The answer is **Thailand**.",
      "ground_truths": [
        "Cambodian officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b627a95012d4f65a70aff1249d43d64",
      "question": "What temple is at the center of the debate?",
      "prediction": "The answer is Cambodia.",
      "ground_truths": [
        "Preah Vihear"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fcab3f4f6484d8992a6c2aa67aeec69",
      "question": "The Thai army said what?",
      "prediction": "The Thai army said that the troop movement was part of a normal rotation and that Thai soldiers did not go anywhere they were not permitted to be.",
      "ground_truths": [
        "soldiers had not gone anywhere they were not permitted to be."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a65e6461f74c47c9a5ffee9366dea0a4",
      "question": "What century is the Preah Vihear temple from?",
      "prediction": "The Preah Vihear temple is from the **11th century**.",
      "ground_truths": [
        "11th"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "d0bae44433814b2da5d10519d7ea3695",
      "question": "What did Thai soldiers cross into?",
      "prediction": "Thai soldiers crossed into the Cambodian area of the 11th century Preah Vihear temple, located on a cliff on Cambodian soil.",
      "ground_truths": [
        "Cambodian territory"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "1cdeb2b345c247bfb083ab363bebd64c",
      "question": "Who is arsenal manager?",
      "prediction": "The answer is **Arsene Wenger.**",
      "ground_truths": [
        "Arsene Wenger"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "23f3abb43b8f4f74bffb018bb3943125",
      "question": "Who is the Arsenal manager?",
      "prediction": "The answer is **Arsene Wenger**.",
      "ground_truths": [
        "Arsene Wenger"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9eecb36f5cb44bb18d24b6efe6824a06",
      "question": "What does Luka Modric suffer from?",
      "prediction": "Luka Modric suffers from a fracture to his right fibula in his right leg.",
      "ground_truths": [
        "a fracture to his right fibula,\""
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "bd356a60992b44f7840f99ffc390382c",
      "question": "Which team beat Arsenal 2-1?",
      "prediction": "The answer is: **Manchester United** defeated Arsenal 2-1 in a 2-1 defeat.",
      "ground_truths": [
        "Manchester United."
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "51ff7cdf97074a07862234487477ef4a",
      "question": "What did Wenger kick?",
      "prediction": "Answer: Wenger kicked an empty water bottle down the touchline.",
      "ground_truths": [
        "an empty water bottle"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "a9dff945426841f8b37069eefd82297b",
      "question": "Who does Luka Modric play for?",
      "prediction": "The answer is Tottenham Hotspur.",
      "ground_truths": [
        "Croatia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20c6639f94d24a91ac79c15024fcd5f7",
      "question": "who will receive an apology?",
      "prediction": "The answer is **Premier Game Match Officials Board**.",
      "ground_truths": [
        "Arsenal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e21128d6906243a58807600c9e0744b4",
      "question": "what was traveling?",
      "prediction": "The answer is: 54 bodies.",
      "ground_truths": [
        "2,000 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e04f0cda81094300afaba328511581fe",
      "question": "How many people were on board?",
      "prediction": "The number of missing people is 54.",
      "ground_truths": [
        "about 2,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66225a9def6a45ddaac68462170b34b7",
      "question": "How much was the ferry capable of carrying?",
      "prediction": "The ferry capacity was 1,500 passengers, which was estimated to be 2,000.",
      "ground_truths": [
        "1,500"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "7cb53bae73d64900a63fde04f93eceea",
      "question": "what did authorities recover",
      "prediction": "Authorities recovered 54 bodies, 22 children, and 15 women.",
      "ground_truths": [
        "54 bodies"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "1fd96469f9a24b37814917f25f8060f2",
      "question": "what is eid al adha?",
      "prediction": "The answer is \"Hopefully, in few hours, we should be able to confirm the exact number of missing (people)...\"",
      "ground_truths": [
        "Muslim festival"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf3bbf5197224180b1a20f44bba8ff58",
      "question": "what capacity did the boat have?",
      "prediction": "The capacity of the boat had 1,500 people.",
      "ground_truths": [
        "1,500"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "f5b0c1a6b5824201baa94cf53c56f0c5",
      "question": "what was the boat capacity?",
      "prediction": "The boat capacity was 1,500 people.",
      "ground_truths": [
        "1,500"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "8e5c3c1a2b514ea2b0d9eb06879364c2",
      "question": "How many bodies were recovered?",
      "prediction": "The number of bodies recovered is 54.",
      "ground_truths": [
        "54"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "b7f2e1ae39b54396b93b90189b8ef3da",
      "question": "Where was the ferry headed?",
      "prediction": "The answer is \"Southend, Dhaka.\"",
      "ground_truths": [
        "Bhola"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a256f8f1dbd644c98a3bc79d072405fc",
      "question": "Taliban plans what month offensive?",
      "prediction": "The Taliban plans what month offensive.",
      "ground_truths": [
        "January"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "23d9c3e938cd4e54a2917488a719b832",
      "question": "When do they feel strong",
      "prediction": "The Taliban are waiting until January for the weather out and will take on the military when Pakistan's tribal region (South Waziristan) arrives.",
      "ground_truths": [
        "during the snowing season,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2af6e73ac32145e9bb843506ce6c6a61",
      "question": "Who is the Taliban leader?",
      "prediction": "The Taliban leader is Hakeemullah Mehsud.",
      "ground_truths": [
        "Hakeemullah Mehsud"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "7c34ec6a706d4aaf96e4f3356fe2da7c",
      "question": "What day was the blast in Peshawar?",
      "prediction": "The day the Pakistani Taliban were waiting for the weather out and taking on the military in Pakistan's tribal region.",
      "ground_truths": [
        "Monday's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d847d59c18884a8eaddd5fc55d047799",
      "question": "How many people were killed in Monday's blast?",
      "prediction": "The number of people killed in the Monday terror attack in Peshawar is 11.",
      "ground_truths": [
        "Eleven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0d3770ca41e4463958dca0c88ce3979",
      "question": "How many were killed  in Peshawar?",
      "prediction": "The number of killed in Peshawar is 18.",
      "ground_truths": [
        "people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b5c249c46644e04a2d28d98bac5f951",
      "question": "Do you know how many were killed",
      "prediction": "Yes, I know the answer is: 11.",
      "ground_truths": [
        "Eleven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef9cc08de0734290840e0ef6219ddf7c",
      "question": "Who led 2-0 at halftime?",
      "prediction": "The answer is **Costa Rica**.",
      "ground_truths": [
        "Ghana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c248d72216c04f8db99757a1a844af49",
      "question": "Who are the four-time champions?",
      "prediction": "The answer is: **Brazil**.",
      "ground_truths": [
        "Brazil"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cdf886b5b0934fde8b0ced32a083b8f5",
      "question": "Who beat Costa Rica?",
      "prediction": "The answer is **Brazil**.",
      "ground_truths": [
        "Brazil"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d293f30681d147138fcc8dd488a33e27",
      "question": "Where is the under-20 World Cup being held?",
      "prediction": "The answer is **Egypt**, where Costa Rica won the 1993 World Cup against Ghana.",
      "ground_truths": [
        "Egypt."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "ec713cfc79cc4dbc8c43ba239a74954b",
      "question": "What was the score of Brazil vs Costa Rica?",
      "prediction": "The answer to the question is:",
      "ground_truths": [
        "5-0,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b1fdfecae5dd453bbba66cbd21e38740",
      "question": "Who did Ghana beat?",
      "prediction": "The answer is **Ghana.**",
      "ground_truths": [
        "Hungary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3db5e82c065b473cbf5627e6433aa143",
      "question": "Who faces the 4 times champion?",
      "prediction": "The answer is **Ghana**.",
      "ground_truths": [
        "Los Ticos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93d27c2a9ec54c57bc14faada354cf3b",
      "question": "What is the price of the Large Hadron Collider?",
      "prediction": "The answer is: the Large Hadron Collider. The Colorado Tunnel is a potential test of the universe's mechanism for forming the universe. It is a major step towards exploring the ways the universe is formed.",
      "ground_truths": [
        "$10 billion"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbc47d7cb9e54eddb352a78e9e2be929",
      "question": "how much are the cost of particle accelerator?",
      "prediction": "The answer is: the LHC's cost is $10 billion.",
      "ground_truths": [
        "$10 billion"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "3d5be1ac654b455db987abc9335857e8",
      "question": "What is the Hadron Collider?",
      "prediction": "The answer is: the Large Hadron Collider, a machine at CERN.",
      "ground_truths": [
        "the world's largest particle"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1173701731e4cfda4d99d5969214317",
      "question": "What does the machine do?",
      "prediction": "The answer is: \"It will look at how the universe formed by analyzing particle collisions.\"",
      "ground_truths": [
        "look at how the universe formed by analyzing particle collisions."
      ],
      "em": 0,
      "f1": 0.8181818181818181
    },
    {
      "id": "a3f3a8c8b40e4f26b7696777583b2a7c",
      "question": "Who worked alongside Tom Cruise?",
      "prediction": "The answer is the **commanding officer of the space shuttle Atlantis**.",
      "ground_truths": [
        "Scott Altman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7eee6e5786e5492ea402aedfbe21fe2d",
      "question": "Who did Altman work with in this film?",
      "prediction": "The answer is: Altman worked with the U.S. space agency, NASA.",
      "ground_truths": [
        "Tom"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa357141135a4d6abc8ead62870d7ffc",
      "question": "What profession did Scott Altman work for?",
      "prediction": "The profession Altman worked for is probably \"space shuttle\" because he was the commander of the space shuttle's mission, and the \"Top Gun\" movie features the command of a space shuttle. He was the commander of the current space shuttle mission.",
      "ground_truths": [
        "retired Navy F-14 fighter pilot"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0ac9981ab774c97a9b9150d190a83fc",
      "question": "Was Scott Altman a fighter pilot?",
      "prediction": "Answer: Yes, Scott Altman is a fighter pilot.",
      "ground_truths": [
        "retired Navy F-14"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d7462fe0cb384c81a672367fb1e3c26f",
      "question": "Which 1986 hit film did Altman perform a stunt double in?",
      "prediction": "Answer: The 1986 hit film that Altman performed a stunt double in is \"Top Gun,\" which did not mention his flying.",
      "ground_truths": [
        "\"Top Gun\""
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "0302f03e0726433ca89b90c145baf5e3",
      "question": "in which hospital the child receives tracheotomy?",
      "prediction": "The answer is: at the London, Ontario, hospital where Joseph was received for a progressive neurological disease, who underwent a tracheotomy.",
      "ground_truths": [
        "SSM Cardinal Glennon Children's Medical Center in St. Louis."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fc8e1310fe949df9825c423cbbba725",
      "question": "where is the hospital?",
      "prediction": "The answer is: \"In St. Louis, Ontario, Canada,\"",
      "ground_truths": [
        "St. Louis, Missouri."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "71a97c5cc88b4e3987dd4a7804a32235",
      "question": "when baby joseph died?",
      "prediction": "The answer is \"Obviously, it's been a very difficult day for the family today,\" and \"In some ways, it was a bit of a relief for the family.\"",
      "ground_truths": [
        "Tuesday afternoon."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "baff036612634a0da96854a984416dec",
      "question": "that suffers Joseph Maraachli?",
      "prediction": "The answer is \"Obviously, it's been a very difficult day for the family today,\" and \"In some ways, it was a bit of a relief for the family.\"",
      "ground_truths": [
        "a progressive neurological disease"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "300a7093186542179fccce8e030fcaa6",
      "question": "The infant received a tracheotomy at",
      "prediction": "The answer directly answers the question, providing the context and the key points:",
      "ground_truths": [
        "a children's hospital in St. Louis, Missouri."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "892e26a6bf9c448c87fadbcb4ec322e9",
      "question": "who suffered from a neurological disease?",
      "prediction": "The answer is \"The Maraachli case caught the attention of the group Priests for Life, which funded Joseph's transfer and treatment at the SSM Cardinal Glennon Children's Medical Center.",
      "ground_truths": [
        "Joseph"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d77c4d812fdf415bb9e09645c84f01bb",
      "question": "What is the reason that women have mammograms?",
      "prediction": "The answer is: \"I wish I could say the mammogram could find 100 percent of breast cancers,\"",
      "ground_truths": [
        "cancer."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f25f23c8bbd4ccd9a3b05e025b7b3fd",
      "question": "what does the cycle have to do with it?",
      "prediction": "The cycle has to do with it.",
      "ground_truths": [
        "\"The best time of your cycle to do a mammogram is going to be when your period is over, maybe the week after your period is done when the breasts are not going to be tender.\""
      ],
      "em": 0,
      "f1": 0.15789473684210525
    },
    {
      "id": "0c31eb03ae2349f49cd9e8bfd1e46a8b",
      "question": "What did the man have a degree in?",
      "prediction": "The answer is a business degree.",
      "ground_truths": [
        "MBA in finance"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3eaa727ee7b94eb6a501ddd864e02c9f",
      "question": "What did the man get stuck in?",
      "prediction": "The man got stuck in a rabbit hole.",
      "ground_truths": [
        "a rabbit hole,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "932256c149594b6cb4ab990092a0e422",
      "question": "What was the man's name?",
      "prediction": "The answer is Karthik Rajaram.",
      "ground_truths": [
        "Karthik Rajaram"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e23ad745e76a4d53b8402b1c7ef40617",
      "question": "What did the father have an MBA in",
      "prediction": "The father had an MBA in finance, but he appeared to have been unemployed for several months and worked for major accounting firms like Price Waterhouse.",
      "ground_truths": [
        "finance"
      ],
      "em": 0,
      "f1": 0.07999999999999999
    },
    {
      "id": "dcd553b5c56f4a3f9eae9e67807c7147",
      "question": "Who was a fulbright schokar",
      "prediction": "The answer is: The man was a fulbright.",
      "ground_truths": [
        "Krishna Rajaram,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03a761e779124cef8d2d9ca903cb2344",
      "question": "Where was one of the son's attending school?",
      "prediction": "The answer is Los Angeles, California.",
      "ground_truths": [
        "UCLA."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7191ed1791dc4e33a7a6e9dbb5ea01e8",
      "question": "How many letters did the man leave",
      "prediction": "The man left the home in a gated community in Porter Ranch, California, and received letters to friends and relatives, including a third letter written by the suspect's will.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ae4333af7ca4c33953c8030c3aed22f",
      "question": "What are government troops and rebels battling for?",
      "prediction": "The answer is: \"We are united in the shade of the national flag.\"",
      "ground_truths": [
        "strongholds in the north of Sri Lanka,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c89d1c366c274f6e8d233be688759386",
      "question": "Number of civilians that are trapped according to aid groups?",
      "prediction": "The number of civilians trapped in the area where the fighting is taking place is 250,000.",
      "ground_truths": [
        "many as 250,000"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "bf9d67fa8b3849ceb410c4dfd7947e5f",
      "question": "How many civilians are trapped?",
      "prediction": "The answer is 250,000.",
      "ground_truths": [
        "as 250,000"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d5938c1e944d44f7ab3ace92abc7d452",
      "question": "How many civilians are trapped, according to aid groups?",
      "prediction": "The answer is 250,000.",
      "ground_truths": [
        "250,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ed28bccab4dc46f0a12e98fd366f2a83",
      "question": "What country is being discussed here?",
      "prediction": "The country being discussed here is Sri Lanka.",
      "ground_truths": [
        "Sri Lanka,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "3e4aa100c744486d883502dd35d3569c",
      "question": "How long have the Ethnic Tamil minority been fighting?",
      "prediction": "The ethnic Tamil minority has been fighting in Sri Lanka for 1983.",
      "ground_truths": [
        "since 1983."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "e0ce4da6cbfe4f26b945f5555901e5c5",
      "question": "How many civilians are trapped due to this conflict?",
      "prediction": "The answer is 250,000.",
      "ground_truths": [
        "as"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "273733b869b741a7b9667927ea701789",
      "question": "What do the Tamils want?",
      "prediction": "The Tamu people want unconditional surrender.",
      "ground_truths": [
        "an independent homeland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf16d51b5b9540ec83fba053bc26a8a9",
      "question": "What happend to the last functioning medical facility in the zone?",
      "prediction": "The last functioning medical facility in the zone of the conflict in Sri Lanka is Pudukkudiyiruppu Hospital, which has closed for the last time.",
      "ground_truths": [
        "The fighting has forced the closure of Pudukkudiyiruppu hospital in the Vanni region,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "c7c63ad1a040451eaf13ad8160295ba0",
      "question": "Year the ethnic Tamil minority have been fighting since?",
      "prediction": "The answer is \"We are a nation that has defeated a powerful enemy that stood before us,\" and \"Our entire nation is now united in the shade of the national flag.\"",
      "ground_truths": [
        "1983."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e75dd8d226914bd9b9070a34d57ff0c9",
      "question": "How long have the Tamil minority been fighting for independence?",
      "prediction": "The Tamil minority has been fighting for independence for 1983.",
      "ground_truths": [
        "an independent homeland since 1983."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "d8d1caff0b4e4c73937106e9fb6f0fc9",
      "question": "What is the date of the case?",
      "prediction": "The answer is the date of the case. The context states the \"case will be argued before a federal judge in Mississippi on March 22, the ACLU said.",
      "ground_truths": [
        "March 22,"
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "d0fc806535774a2cb7fb586c9f87f1e5",
      "question": "Who will it be argued before?",
      "prediction": "The answer is: The American Civil Liberties Union.",
      "ground_truths": [
        "a federal judge in Mississippi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6d36da6f7d6c4ae898960241cc99cd4c",
      "question": "Which group filed a motion?",
      "prediction": "The answer is: The American Civil Liberties Union (ACU).",
      "ground_truths": [
        "The American Civil Liberties Union"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "f8542bff725c4264aaf360ad929a1807",
      "question": "When will the case take place?",
      "prediction": "The answer is based on the provided context.",
      "ground_truths": [
        "March 22,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af77d75481ae4b0391017dfd394050f4",
      "question": "Who files against the school district?",
      "prediction": "The answer is the ACLU.",
      "ground_truths": [
        "rights group"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e111ffbe05e4468c85bfaeceec2ea821",
      "question": "Which school district is subject to the injunction?",
      "prediction": "The answer is the Florida School District of Jackson.",
      "ground_truths": [
        "Mississippi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0480646187a4af3ae3e844f18b1c10f",
      "question": "What did the group file with the court?",
      "prediction": "The American Civil Liberties Union filed a motion for a preliminary injunction against a Mississippi school district and high school.",
      "ground_truths": [
        "a motion for a preliminary injunction against a Mississippi school district and high school"
      ],
      "em": 0,
      "f1": 0.8148148148148148
    },
    {
      "id": "10c2bfdd1e4e4886bf3428f1753757c5",
      "question": "Who made passengers remove nipple rings?",
      "prediction": "The answer is, \"Tasca does have procedures for addressing alarm systems, including visual inspection of the article in lieu of removal of the item.\"",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc9153acb2bd49ed8771b45f7d5359fc",
      "question": "What did the agency say?",
      "prediction": "The TSA's answer to the question \"What did the TSA say?\" is:",
      "ground_truths": [
        "airport appear to have properly followed procedures when they allegedly forced a woman to remove her nipple rings -- one with pliers -- but acknowledged the procedures should be changed."
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "6ffd10e8acea47be8588900424296e22",
      "question": "What backs officers who made passenger remove nipple rings?",
      "prediction": "The answer is: TSA has reviewed the procedures to ensure the removal of the alarm was successful.",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "706a0b4a648c4aa2b6aa5dda328fdb55",
      "question": "Who found piercings at airport?",
      "prediction": "The answer is: The Woman Involved, Mandi Hamlin, who was forced to remove piercings behind a curtain.",
      "ground_truths": [
        "Mandi Hamlin"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "1bb633b7dfbd4bfbb77b7820ce3448d9",
      "question": "Who says she heard male agents snicker?",
      "prediction": "The answer is: The woman had never been specifically asked if she heard any male agents snicker, and she didn's say so.",
      "ground_truths": [
        "Mandi Hamlin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41d4727416c1422db29a26d8d40104f0",
      "question": "What did the woman say?",
      "prediction": "The woman said she was humiliated by last month's incident.",
      "ground_truths": [
        "she was humiliated"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "61a08597e485463f8836d68886a6b8f3",
      "question": "Who said procedures need to be changed?",
      "prediction": "The answer is: **TSA has reviewed the procedures of the airport and agreed that they need to be changed. This is a public apology.**",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6c58352d743439daf313313a04ddcdb",
      "question": "What needs to be changed?",
      "prediction": "The answer is, \"TSA will inform passengers that they have the option to resolve the alarm through a visual inspection of the article in lieu of removing the item in question.\"",
      "ground_truths": [
        "the procedures"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "337fbef73fd746a9ae74ffced3e49223",
      "question": "who is suspended",
      "prediction": "The answer is \"we have a lot of work ahead of us,\" but it's not a direct answer to the context.",
      "ground_truths": [
        "in a campus library,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ebc818947b23459b8e2af504e3ec65e8",
      "question": "What was mocked at the party?",
      "prediction": "The answer is: \"It's unclear how long the student is suspended for. The party was condemned by the school.\"",
      "ground_truths": [
        "Black History Month"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96213a25c60849749fd38b5bbed07a1c",
      "question": "What was the rally opposing?",
      "prediction": "The University of California San Diego has suspended a student who admitted to hanging a noose in the library.",
      "ground_truths": [
        "racial intolerance."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e28e16059c5649a9a2b0b982135f91b7",
      "question": "how many people involved",
      "prediction": "The answer is: \"We are feeling real pain, and we will take real action.\"",
      "ground_truths": [
        "hundreds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b59ea8ed1f8477692529ed59e6ef224",
      "question": "when is the incident",
      "prediction": "The answer is: \"We are feeling real pain, and we will take real action,\" meaning the answer is \"we are feeling real pain.\"",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01519bad5ff04398b6ed82dbf5aeb781",
      "question": "was he punished",
      "prediction": "The answer is \"We are feeling real pain, and we will take real action,\" and the question is not explicitly asked. Therefore, the answer is \"We are feeling real pain, and we will take real action,\" but it is not directly asked.",
      "ground_truths": [
        "suspended"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "35502eb79d91480880ac508d4e2cdb15",
      "question": "Where did the student hang the noose?",
      "prediction": "The answer is: The student hung a noose in the library, at the University of California San Diego.",
      "ground_truths": [
        "campus library,"
      ],
      "em": 0,
      "f1": 0.13333333333333336
    },
    {
      "id": "ef43f4191c424f70aec3ed9eaa357cb0",
      "question": "what was student attempting to accomplish",
      "prediction": "The answer is: \"We are feeling real pain, and we will take real action.\"",
      "ground_truths": [
        "intent to terrorize"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d99e4c4e3ee24996a4c2a5e08d669dfb",
      "question": "What did Bryan ride for more than 11,000 miles?",
      "prediction": "Bryan traveled 11,000 miles to Crawford, Texas.",
      "ground_truths": [
        "a motor scooter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53cdb654e9454fc7b6d7539fbcfbe8d7",
      "question": "What did the activist's route form?",
      "prediction": "The activist's route formed a peace sign.",
      "ground_truths": [
        "a peace sign."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "21d720c9825a49109f21240335bc38bd",
      "question": "What did the activist want to do?",
      "prediction": "The activist wanted to do something.",
      "ground_truths": [
        "meeting with the president to discuss her son."
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "5519a7b4759c4dd087a442a571ef4cb9",
      "question": "How many miles did Bryan ride his scooter?",
      "prediction": "Based on the context, the answer is 11,000 miles.",
      "ground_truths": [
        "11,000"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "37d0106de39b4828ab2db59814510282",
      "question": "When was the tribunal established?",
      "prediction": "The Tribunal for Rwanda on Thursday convicted the \"mastermind\" of the Rwandan genocide, and sentenced him to life in prison for genocide, crimes against humanity, and war crimes.",
      "ground_truths": [
        "in late 1994."
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "ad65a15e844a4f078e4598b8cfe6ab87",
      "question": "when The United Nations established the genocide tribunal?",
      "prediction": "The United Nations established the genocide tribunal in Rwanda in late 1994.",
      "ground_truths": [
        "in late 1994."
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "3c0bb931c2394da1bca011585f60753c",
      "question": "bagosora is charged with which crime?",
      "prediction": "The answer is: bagosora is charged with genocide.",
      "ground_truths": [
        "genocide,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "405919df50b648c3909720727e9d3f16",
      "question": "Who is the mastermind?",
      "prediction": "The answer is: Theoneste Bagosora, the head of the Rwandan military's operations, and Anatole Nsengiyumva, the commander of the Rwandan army, were both convicted of genocide, crimes against humanity, and war crimes.",
      "ground_truths": [
        "Theoneste Bagosora,"
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "cc24f8ce81044ef588f7565d29afaa5d",
      "question": "When did massacres occur?",
      "prediction": "The answer is: The International Criminal Tribunal for Rwanda on Thursday, Thursday, 2002.",
      "ground_truths": [
        "1994"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49c4d8a65f2c4ec7a74535b35335a4f5",
      "question": "how many people dead?",
      "prediction": "The answer is 800,000.",
      "ground_truths": [
        "800,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f5fd77e9ba374d4d89e90f52b3aa681d",
      "question": "what country did massacre occur?",
      "prediction": "The country that massacred.",
      "ground_truths": [
        "Rwanda"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ed37709577d24ab982fafc3b5eb21ba9",
      "question": "what position did bagosora hold?",
      "prediction": "The answer is the United Nations International Criminal Tribunal for Rwanda.",
      "ground_truths": [
        "a colonel in the Rwandan army,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3118a625a3b41c18a04780ff784465b",
      "question": "What have beer drinkers left?",
      "prediction": "The answer is: **Dashes from beer drinkers in the city center, which were broken glass shards, were left on the pavement in the German and Belgian streets, including in Old Town.**",
      "ground_truths": [
        "glass shards"
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "580cfb51fef84f5e9cb2de0be76eaa61",
      "question": "What kind of dogs are wearing protective shoes?",
      "prediction": "The answer is \"Dogs need a month of training to get used to wearing the shoes, Hartwich said.",
      "ground_truths": [
        "police"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89b7733b0a994df7b4250953ccff9c24",
      "question": "What do dog shoes cost?",
      "prediction": "The answer is $89.",
      "ground_truths": [
        "60 euros"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e6bd79cae1cb472c990b5226f2a87152",
      "question": "What are police dogs now wearing?",
      "prediction": "Police dogs are now wearing little blue booties.",
      "ground_truths": [
        "protective shoes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7b7a2491fe134532816141e702cb9b4a",
      "question": "What was the reason that the dogs wear protective shoes?",
      "prediction": "The dogs wear protective shoes in the city center of Duesseldorf, Germany, to protect their feet from glass.",
      "ground_truths": [
        "Too many glass shards left by beer drinkers in the city center,"
      ],
      "em": 0,
      "f1": 0.2962962962962963
    },
    {
      "id": "dcb5b33f7e1848c08ba6d3842490e174",
      "question": "What is the reason for giving them shoes?",
      "prediction": "The answer is: **Their handlers shelled out 60 euros for shoes that are also worn by dogs who walk on ice in Alaska.**",
      "ground_truths": [
        "Too many glass shards left by beer drinkers in the city center,"
      ],
      "em": 0,
      "f1": 0.12121212121212123
    },
    {
      "id": "25eec8384d9d45b8b75affcc9f164be3",
      "question": "What do the shoes cost?",
      "prediction": "The shoes cost $89.",
      "ground_truths": [
        "$89"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "745c742fa8d94abe88d1330cd4cdae1d",
      "question": "What are police dogs wearing?",
      "prediction": "Police dogs in Duesseldorf, Germany are wearing little blue booties.",
      "ground_truths": [
        "protective shoes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b9b15aa02b54f8e82a61c149799963b",
      "question": "Who also wore dog shoes?",
      "prediction": "The answer is: The dogs in Duesseldorf, Germany.",
      "ground_truths": [
        "walk"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a6ced88638f4bf68c6bb3d7a979d48e",
      "question": "Who says America needs a leader who understands the future we seek?",
      "prediction": "The answer is: Barack Obama.",
      "ground_truths": [
        "former Virginia Gov. Mark Warner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5fc7fc4522b43b7a0802acbd3963efd",
      "question": "What political party is Mark Warner associated with?",
      "prediction": "Mark Warner is associated with the Democratic Party.",
      "ground_truths": [
        "Democratic"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5b5d465a7ab2427485a00f3638543996",
      "question": "Bush never asked Americans to do what?",
      "prediction": "Answer: Bush never asked Americans to do what.",
      "ground_truths": [
        "step up.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7bfe3ee0862a4bdbb5e06b894cfefd29",
      "question": "Much of Warner's address focused on what?",
      "prediction": "The answer is: \"We need a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.\"",
      "ground_truths": [
        "bipartisan rhetoric Obama has espoused"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "362b90d2f2de47e1a0d9be76d1383cd5",
      "question": "Warner says America need what?",
      "prediction": "Answer: Warner says America needs what.",
      "ground_truths": [
        "a president who understands the world today, the future we seek and the change we"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef32addb3b9b4c8ab122fd787122a657",
      "question": "Who, specifically, did Mark Warner criticize?",
      "prediction": "Mark Warner criticized the Democratic National Convention, specifically the \"race for the future...\" challenge.",
      "ground_truths": [
        "President Bush"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9bb1b01620ea4ed28be3d13447145272",
      "question": "What type of rhetoric did Warner focus on?",
      "prediction": "The answer is \"convincing the Democratic Party of the future, and our need to change the future, rather than simply focusing on a specific policy.\"",
      "ground_truths": [
        "bipartisan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ae35f52b8069416da505e192383e1cab",
      "question": "Bush never asked what?",
      "prediction": "Answer: Bush never asked what.",
      "ground_truths": [
        "us to step up.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f8bb4dd263246dbac8f331f745d2e5a",
      "question": "What did most of Warner's address focus about?",
      "prediction": "The answer is: \"The future vs. the past.\"",
      "ground_truths": [
        "the kind of bipartisan rhetoric Obama has espoused on the campaign trail."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fee6642b071a46e9b1a005d63b58a98c",
      "question": "What kind of rhetoric did Mark Warner allude to frequently?",
      "prediction": "The answer is \"It's about the future vs. the past.\"",
      "ground_truths": [
        "bipartisan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb1589db33fa4309b9b3f7fb5e4b3480",
      "question": "What did Bush never ask Americans to do?",
      "prediction": "The answer is, \"a plan that explodes the deficit and leaves that to our kids. No real strategy to invest in our infrastructure. And he would continue spending $10 billion in Iraq. I don't know about you, but that's just not right. That's four more years that we can't afford to.  The focus is on the future and the past.",
      "ground_truths": [
        "step up.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64637235282a495585c4b3eca6f776f7",
      "question": "Who does One Laptop per Child target?",
      "prediction": "The answer is One Laptop per Child.",
      "ground_truths": [
        "the world's poorest children."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47ca117f64cd4938bff6ff061e319282",
      "question": "What can laptops do?",
      "prediction": "The answer is that laptops can help children in India to learn and grow, enabling them to engage in math, music, art, and data-based learning.",
      "ground_truths": [
        "allow students to engage in learning differently, enjoy a customized approach and hone critical thinking skills,"
      ],
      "em": 0,
      "f1": 0.25641025641025644
    },
    {
      "id": "21888eb1c822480e8fe265e946838188",
      "question": "Who knows how preachy and awkward the movies get?",
      "prediction": "The answer is: \"I had been affected,\" and I learned that it was not just a paycheck.",
      "ground_truths": [
        "Tripplehorn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "716617662a834ce9a662a4225ccac89e",
      "question": "Who directed the vignettes?",
      "prediction": "The answer is: Tripplehorn, who has starred in cable show \"Big Love.\"",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "725e0fdb36d34af6af01fe99c713ec3b",
      "question": "What does Jeanne Tripplehorn know about the movies?",
      "prediction": "Here's the answer to the question based on the context:",
      "ground_truths": [
        "can get."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d67036c2061345e880ebcd02862a2cf4",
      "question": "What story does \"Five\" tell?",
      "prediction": "The answer to the question is \"The story of women and breast cancer in five vignettes, where Tripplehorn's character, an oncologist, narrates the experiences of women in five vignettes, tells stories of navigating the complexities of cancer, and confronts the challenges of dealing with their illness.\"",
      "ground_truths": [
        "stories of different women coping with breast cancer"
      ],
      "em": 0,
      "f1": 0.25531914893617025
    },
    {
      "id": "84fe1a95dbb449e88149df2e3034787e",
      "question": "Who each directed a vignette?",
      "prediction": "The answer is: Tripplehorn, who has starred in cable show \"Big Love.\"",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93ccadfc45214e47a0e4ee569572379c",
      "question": "Who directed a Vignette?",
      "prediction": "The film is about women and breast cancer. It airs on Lifetime.",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0.11764705882352942
    },
    {
      "id": "12719e1448b74b5d87b14208859f8438",
      "question": "What is the subject of the vignettes?",
      "prediction": "The answer is, \"I had been affected,\" and \"I had been affected,\" and it was a \"personal thing.\"",
      "ground_truths": [
        "different women coping with breast cancer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb602f398bf64c0f8775ec73e80e2f0e",
      "question": "What is \"Five\" about?",
      "prediction": "The answer to the question is \"Five,\" which translates to \"Five of the Broken Ones.\"",
      "ground_truths": [
        "tells stories of different women coping with breast cancer in five vignettes."
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "5664735693d848dbb85b97195420a5a5",
      "question": "What stories does Five tell?",
      "prediction": "Answer to the question directly from the context:",
      "ground_truths": [
        "different women coping with breast cancer in"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13ed8c21d84140549dba809bdca38f6f",
      "question": "What would the news outlet say happened?",
      "prediction": "The news outlet would say \"in fine health\" and \"a total fabrication.\"",
      "ground_truths": [
        "series of summer concerts at the O2."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c6b394f8ec74b8a8e93c3b31624e754",
      "question": "What musician has scheduled a new conference?",
      "prediction": "The answer is \"Pop star Michael Jackson. London's O2 Arena.\"",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "d5532d378e5c463cb39c6e3ef8b47524",
      "question": "In which season will the concerts be held?",
      "prediction": "Answer: On Thursday at London's O2 arena.",
      "ground_truths": [
        "summer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fe9ef278e794c7ab6cc6f78a8714a8b",
      "question": "What has ben the subject of rumors?",
      "prediction": "The subject of rumors about Jackson's health and his comeback.",
      "ground_truths": [
        "Pop star Michael Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53f1d8637f554a908d4c557f43441eb0",
      "question": "Where was the news conference held?",
      "prediction": "London, England.",
      "ground_truths": [
        "London's O2 arena,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90ab85c979ac477ea5a1278034d98e35",
      "question": "What news outlet says he will hold a series of summer concerts?",
      "prediction": "London's Sky News reported Wednesday that Jackson will hold a series of summer concerts at the O2 arena in London.",
      "ground_truths": [
        "Britain's Sky"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "564bd8b4319347d3b2e3be1bc86e1046",
      "question": "In what city is the O2 arena?",
      "prediction": "London, England (CNN)",
      "ground_truths": [
        "London's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd2ab3f3020e49238dfe29398993c918",
      "question": "What has been the subject of rumors?",
      "prediction": "The subject of rumors has been the subject of rumors.",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbbd003f83bd456789b0c7a7b0c4ce8f",
      "question": "What do new materials do?",
      "prediction": "Answer: New materials do not replace the entire frame-rail system, as this is a crucial component.",
      "ground_truths": [
        "truck safer,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "75d42bea14c24d5aa35dd3a4af32755c",
      "question": "What did the Ford expert say?",
      "prediction": "The answer is, \"The Ford expert said that the effort has already led to many new developments in the design of various auto parts and components, resulting in a reduction in repair costs for auto parts and components, and increased innovation in auto parts and components.\"",
      "ground_truths": [
        "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\" said Gerry Bonanni,"
      ],
      "em": 0,
      "f1": 0.25000000000000006
    },
    {
      "id": "c3b05d40bfde4641991d458532505bc2",
      "question": "What will insurers do?",
      "prediction": "Answer: Answer: Ford's insurance rates will likely increase significantly.",
      "ground_truths": [
        "try and reduce the cost of auto repairs and insurance premiums for consumers"
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "96b41a2772e3491ca5a00d89a09f9712",
      "question": "What makes the vehicles safer?",
      "prediction": "From the context, the answer is \"totaled.\"",
      "ground_truths": [
        "ultra-high-strength steel and boron"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e55d296f4a274313b1561742bdfe41c2",
      "question": "What does the Ford expert say?",
      "prediction": "The answer is: Ford's experts say they are working hard to reduce the cost of auto repairs and insurance premiums for consumers and some of the development is brilliant.",
      "ground_truths": [
        "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\""
      ],
      "em": 0,
      "f1": 0.17391304347826086
    },
    {
      "id": "02efcca165d248af83c561cf383e6ef7",
      "question": "Who did UAE deny visa to?",
      "prediction": "The answer is: The Tennis Channel.",
      "ground_truths": [
        "Israeli tennis player Shahar Peer"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "6663783e06e94c3398504e0521a86783",
      "question": "What company is no longer sponsoring the tournament?",
      "prediction": "The answer is: The Wall Street Journal Europe.",
      "ground_truths": [
        "The Wall Street Journal Europe"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "682331495d45489ba6c412a767b47552",
      "question": "What is the sport the player is active in?",
      "prediction": "The player is active in tennis.",
      "ground_truths": [
        "tennis"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "e2472a778b8a44ccba2aaf6e2c6c7b19",
      "question": "what sport did he play",
      "prediction": "The answer is: Tennis Channel.",
      "ground_truths": [
        "tennis"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "1444cd7b36cd495fb7e94b9e4212c5e3",
      "question": "where did this happen",
      "prediction": "The answer is: The Wall Street Journal Europe announced that the UAE is not allowed to host Future tournaments, and the DJ's editorial stance has been criticized by the Journal.",
      "ground_truths": [
        "Dubai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ec113535ab74562b219d362bef03b2b",
      "question": "How many tips did the worker call in?",
      "prediction": "The answer is, \"at a news conference, Capt. Angelo Nieves, an Orange County Sheriff's Department commander, said investigators are looking into whether the tips, called in August 11, 12 and 13, were properly followed up.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2b9e8d1f4fe486d87a9d663503bef07",
      "question": "What child's remains were found?",
      "prediction": "The answer is, \"Caylee Anthony's remains were found in the wooded area of Hidden Oaks Elementary School, near the house where the meter reader first directed police, with the remains consistent with a child of Caylee's age. The search of the area was followed by a dedicated, extended search of the site.",
      "ground_truths": [
        "Caylee Anthony"
      ],
      "em": 0,
      "f1": 0.04545454545454545
    },
    {
      "id": "130f065449984159b589f2af234e3216",
      "question": "How long ago did the friend tell police to check the area?",
      "prediction": "The answer is: the friend told the police to check the area for the remains of Caylee Anthony, whom they later reported to the FBI lab in Quantico, Virginia, due to the same wooded area, and the significant skeletal remains he found near the home of his grandparents.",
      "ground_truths": [
        "month before the meter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e480eabdd5a84307a99dfd94a032efc2",
      "question": "Who went missing?",
      "prediction": "The answer is Caylee Anthony, 2, of a small child.",
      "ground_truths": [
        "Caylee Anthony,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b8edf93c15f94d288bc4fb52fc9da573",
      "question": "What was found in the search area?",
      "prediction": "The answer is: Caylee Anthony's remains are found in a half-mile from Casey Anthony's parents' home in the area where the meter reader first directed police.",
      "ground_truths": [
        "\"significant skeletal remains\""
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "b8cb96fe354b4069bf3175062a10fd03",
      "question": "Who called in several tips?",
      "prediction": "The answer is: **KioMarie Cruz, Casey Anthony's childhood friend.**",
      "ground_truths": [
        "meter reader"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f6f7ad6f6f6a4d0bacf185da89805c6f",
      "question": "Who told police to check the area five months ago?",
      "prediction": "The answer is **Celia Anthony**, 22, and her mother, Casey Anthony, 22.",
      "ground_truths": [
        "KioMarie Cruz,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3ec09495f6a4147824e3c975da11da2",
      "question": "What news station reported on this case?",
      "prediction": "The answer is: The Houston-based meter reader, Caylee Anthony, who led the investigation.",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cb7ed7443af4ee3a65dac3239ea38f2",
      "question": "Who crossed the Atlantic in a micro yacht?",
      "prediction": "The answer is The Pilgrims, sailing to Plymouth Rock in England.",
      "ground_truths": [
        "Hugo Vihlen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "298362fdf91747a582112a69b7df48bf",
      "question": "Who made evolutionary discoveries?",
      "prediction": "The answer is, the Mirabella V was the world's biggest single-masted sailing yacht, with a mast almost 300 feet tall.",
      "ground_truths": [
        "Naturalist Charles Darwin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "647d0669235a41a8a2fe0b37134a6832",
      "question": "What happened in 1620?",
      "prediction": "The answer to the question, based on the context provided, is that \"Magellan navigates Cape Horn\" (South America) and \"Pilgrims sail to Plymouth Rock\" (England and the Caribbean).",
      "ground_truths": [
        "Pilgrims sail to Plymouth Rock"
      ],
      "em": 0,
      "f1": 0.3448275862068966
    },
    {
      "id": "78b488ee4cbe43acb8e7824123baacc5",
      "question": "When did Darwin make his discoveries?",
      "prediction": "The answer to the question is \"When Darwin made his discoveries,\" which would be from the Galapagos Islands.",
      "ground_truths": [
        "1831"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1811934388341ae98b2cd3946c77b45",
      "question": "Where did the Pilgrim's voyage to?",
      "prediction": "The Pilgrim's voyage to Plymouth Rock is found in the United States.",
      "ground_truths": [
        "Plymouth Rock"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "1c128e3cbdcd4b20bf65d89db284e474",
      "question": "When was the Beagle voyage?",
      "prediction": "The Beagle voyage is a testament to the importance of exploration and the triumphs of the British Empire's voyages in the world.",
      "ground_truths": [
        "1831"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aef0cd2a0a8342dfb8c1d1d4b5911e47",
      "question": "When was the Mayflower voyage?",
      "prediction": "The Mayflower voyage was a world-changing voyage, a journey that significantly altered the way humanity looks at the world. The journey, a submerged, small sailboat, was reduced to a collection of specimens and a small, wooden ship, symbolizing the discovery of the Caribbean. The ship, nicknamed the \"Father's Day,\" was a tiny, but highly successful boat, unlike the other ships, that had been abandoned in Haiti. The Mayflower voyage was a crucial event, and the world's first \"micro-yachtsman\" transformed the way people travel the high seas.",
      "ground_truths": [
        "1620"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5953190ee0eb4ada8a3f788a60ba875a",
      "question": "Who did they back?",
      "prediction": "The answer is: Tibetan leaders, meeting in northern India.",
      "ground_truths": [
        "the Dalai Lama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "14fda3c8f7d2443aa2732bd885c8f79a",
      "question": "Who wnated Tibet's independence?",
      "prediction": "The answer is \"the Dalai Lama.\"",
      "ground_truths": [
        "small minority"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "706ce26401d54a5d94687fb541f7350c",
      "question": "What does Lama seek?",
      "prediction": "The answer is: the Dalai Lama seeks autonomy in Tibet.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "5a1406f34ca94051b34e3b4a769669b2",
      "question": "What does a small minority demand for Tibet?",
      "prediction": "The answer is \"the best possible future course of action for Tibet. China could continue to claim Tibet as part of its territory, and the Dalai Lama's call for genuine autonomy is not met.\"",
      "ground_truths": [
        "independence,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "989fede52bf840609bb69d7ba143301b",
      "question": "What do Tibetian leaders back?",
      "prediction": "The answer is: The Tibetan leaders, meeting in northern India to debate a potential new approach to Tibet's autonomy, stated they have sought autonomy.",
      "ground_truths": [
        "the Dalai Lama's current \"middle way approach,\""
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "16e5c8bb8ce841de8613411fcbcfedad",
      "question": "What does the Dalai Lama seek from Bejing?",
      "prediction": "The Dalai Lama seeks from China \"genuine autonomy\" in Tibet.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "d3be7c50925040dca87dc6d971145ccb",
      "question": "What does Dalai Lama seeks?",
      "prediction": "The answer is: the Dalai Lama seeks autonomy.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "8aec4e2910254ab29fd7579a418fe40c",
      "question": "What does the minority want?",
      "prediction": "The answer is: **the Dalai Lama wants genuine autonomy.**",
      "ground_truths": [
        "Tibet's independence,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4491a921e450412fafa90ae9c4a2ce61",
      "question": "What do the Tibetan exile leaders back?",
      "prediction": "The Tibetan exile leaders, meeting in northern India to debate a new approach to Tibet's struggle for autonomy, have voted to stay with the Dalai Lama's \"middle way approach,\" according to a spokesman for the spiritual leader.",
      "ground_truths": [
        "\"middle way approach,\""
      ],
      "em": 0,
      "f1": 0.17142857142857143
    },
    {
      "id": "2da01c4994c34acdb27bf9bedabae346",
      "question": "who designed courses in Eastern Europe?",
      "prediction": "The answer is \"We have some advantages in Bulgaria. We have an excellent coastlines, we are a small country but very diverse, so the country is geographically and naturally perfect for golf.\"",
      "ground_truths": [
        "Gary Player"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8869c54796a0408cb95c46e9542cbed7",
      "question": "where has 6 courses opened",
      "prediction": "The answer is \"We can already see there is a high increase in interest and evolve quite quickly. We have some advantages in Bulgaria. We have an excellent coastlines, a small country but very diverse, so the country is geographically and naturally perfect for golf.\"",
      "ground_truths": [
        "on the Black Sea coast in Bulgaria."
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "69e835bab41c46bf95d6616907f2ca10",
      "question": "what has Player designed",
      "prediction": "The answer is \"A course designed in Poland, built with a focus on the enjoyment of golf and the South Sea coast, in Bulgaria.",
      "ground_truths": [
        "two courses on the Black Sea coast in Bulgaria."
      ],
      "em": 0,
      "f1": 0.37037037037037035
    },
    {
      "id": "18760a916f444d95af7e2f7213887025",
      "question": "Who has been designer for Golf in several Eastern European countries?",
      "prediction": "The answer is: **Gary Player.**",
      "ground_truths": [
        "Gary Player"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "628f1b28c94042b8bf488ef3ebeb70dd",
      "question": "How many courses have opened?",
      "prediction": "Answer: The answer provides a brief overview of the decline in golf in Eastern Europe. It also highlights the increase in the number of games and the high level of interest in the sport. The answer concludes by stating that because of a high-profile name, the development in Eastern Europe has caught the eye of influential figures within the industry, and that the \"knock-on effect of a high-profile name\" ensures the future of the industry in Eastern Europe is bright.",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5927d25618045649eb93255907dd1fe",
      "question": "what has made progress",
      "prediction": "The answer is: \"We have some advantages in Bulgaria. We have an excellent coastlines, a small country but very diverse, so the country is geographically and naturally perfect for golf.\"",
      "ground_truths": [
        "development of two courses on the Black Sea coast in Bulgaria."
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "47a07984eeb64685b92b0c51f5a57a21",
      "question": "All together, how many Golf developments Bulgaria has?",
      "prediction": "The answer is 2.  The context states that Bulgaria has been known as \"one country where the sport is on the up.\"  Therefore, the answer is 2.  There is no golf course, and it is stated that a \"swing to new areas like Poland\" means \"to bring it to Bulgaria, where people never thought it would be a golf course.\"",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "143d368c91d54e6987caebfed87a0f6a",
      "question": "Who is the  Aztecas street gang affiliated with?",
      "prediction": "The answer is: The Aztec street gang is associated with the Chihuahua state attorney general.",
      "ground_truths": [
        "cartel."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "06efd77e664b4991bb1be5c082b3d0b9",
      "question": "Who is the suspect?",
      "prediction": "The suspect is Ricardo Valles de la Rosa, a municipal police spokesman.",
      "ground_truths": [
        "Ricardo Valles de la Rosa,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e353df5892514b8f87494942da1c02bc",
      "question": "What is Ricardo Valles de la Rosa's age?",
      "prediction": "Ricardo Valles de la Rosa's age is 42.",
      "ground_truths": [
        "42 years old"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "50d55f0e179142b88ec8fa428537ca76",
      "question": "Who arrested a suspect on Friday?",
      "prediction": "The arrested suspect was Ricardo Valles de la Rosa, a municipal police spokesman in Ciudad Juarez, Mexico, and was sought in connection with at least one other homicide.",
      "ground_truths": [
        "Mexican military"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c00ec98cc0ad43dcbe419f9a64923f56",
      "question": "Who arrested the suspect Friday?",
      "prediction": "The arrest was made on Friday.",
      "ground_truths": [
        "Mexican military"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "382d699060ef477cbeb6a709468cb4fc",
      "question": "Who is the new suspect sought in connection with the slaying?",
      "prediction": "The new suspect sought in connection with the slaying is Ricardo Valles de la Rosa.",
      "ground_truths": [
        "Ricardo Valles de la Rosa,"
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "1bbffd4ef5094b518beaab077ebf5a50",
      "question": "When was the Mexican military arrested?",
      "prediction": "The answer is \"The Mexican military arrested a suspect in the killings this month of three people with ties to the U.S. Consulate in Ciudad Juarez, Mexico. The arrested man is Ricardo Valles de la Rosa, municipal police spokesman Jacinto Seguro, and was sought in connection with at least one other homicide: the killing of Zapata Reyes, a member of the Mexicles gang, in La colonia Partido Romero, Ciudad Juarez, Mexico.",
      "ground_truths": [
        "Monday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aa93a690e904c89b259f98500106b69",
      "question": "what is Bin's first name?",
      "prediction": "The answer is Omar bin Laden.",
      "ground_truths": [
        "Omar"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "6e38539cc9944d6fa54d4c503a83644a",
      "question": "Omar bin Laden split with whom in 2000?",
      "prediction": "The answer is Omar bin Laden.",
      "ground_truths": [
        "al Qaeda."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a42325e79534043a17bcbb10e870779",
      "question": "Who is Bin Laden?",
      "prediction": "The answer is: Omar bin Laden.",
      "ground_truths": [
        "the most-wanted man in the world"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f6249167ca14f0c966477580281ef72",
      "question": "When did he split from his father?",
      "prediction": "Here's the answer, based on the context:",
      "ground_truths": [
        "in 2000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bbe5fb171c64a7c903b6d71e61d52db",
      "question": "What is the name of Bin Laden's son?",
      "prediction": "The answer is Omar bin Laden's son.",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0.4444444444444444
    },
    {
      "id": "3000fad50c3e4d449a952b1a0b9f7f4f",
      "question": "Bin Laden's son was named who?",
      "prediction": "Answer: Bin Laden's son was named \"Oma\" (who is a British national) and she is not a terrorist. He is also 14.",
      "ground_truths": [
        "Omar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e6e822a6b904dc8afab7de215c4e5e5",
      "question": "when was 9/11?",
      "prediction": "The answer is said to be 2000.",
      "ground_truths": [
        "September 11, 2001."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04110e98bb844515b66207809954c750",
      "question": "Who is CNN's Baghdad correspondent?",
      "prediction": "CNN's Baghdad correspondent Michael Ware says, \"The U.S. subcontracted out an assassination program against al Qaeda...\" and observes that the lack of coordination in the American effort, leading to an escalation in violence, is a major contributing factor.",
      "ground_truths": [
        "Michael Ware"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "29e4bdfd3cc242578410c942a502c7bb",
      "question": "Who is the leader of the Iranian-backed militia?",
      "prediction": "The answer is Michael Ware, a former associate editor at the Washington Post. He says, \"The U.S. subcontracted out an assassination program against al Qaeda in early 2006.  This was a major factor in the decline in Iraq's violence.\"",
      "ground_truths": [
        "Muqtada al-Sadr,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9044663edf944402bc008c70542c6baa",
      "question": "what did CNN Baghdad say?",
      "prediction": "CNN Baghdad said: \"The U.S. subcontracted out an assassination program against al Qaeda...\"",
      "ground_truths": [
        "correspondent Michael Ware cast doubt on Woodward's assertion"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "00d946fd84644d3c9d2f45ba3b5d95fe",
      "question": "What team does Bob Woodward credit?",
      "prediction": "The answer is:  Woodward credits the \"U.S. subcontracted out an assassination program against al Qaeda,\" as the primary reason for the decline in violence in Iraq.",
      "ground_truths": [
        "\"fusion teams,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd193591d3ee4f23b6fc3febb87671e9",
      "question": "What day will the winners be announced?",
      "prediction": "The answer is Friday. The programme will be announced Friday, March 5.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "92586185a2144c5ba7648b93c8033212",
      "question": "when is going to be announced the winner?",
      "prediction": "The answer is Friday, November 28, 2023.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "2e72578e8af74acab4e428e48b6f5ef0",
      "question": "whats the name of the colombian senator?",
      "prediction": "The Colombian senator is the leading candidate for the Nobel Peace Prize.",
      "ground_truths": [
        "Piedad Cordoba,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de80bc2bd912402ba58f27bff9e98925",
      "question": "how many contenders are for nobel peace prize?",
      "prediction": "The answer is 17.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3402c594c0cd444091e58f8e217dcffa",
      "question": "When is the winner going to be announced?",
      "prediction": "The answer is Friday.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0446a0bd16cf4eea818d7dbb932edf35",
      "question": "How many top contenders are there for the Nobel Peace Prize?",
      "prediction": "The answer is 16. The international peace prize nominations are also for 172 people and 33 organizations.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32942a78bfb84a1195aaa580a8bd9fb1",
      "question": "How many years until another hearing for Atkins?",
      "prediction": "The answer is: **8 years.**",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "174f57168a9d482cb523b60f23c48c18",
      "question": "Who held Sharon Tate down and stabbed her 16 times?",
      "prediction": "The answer is \"Sadie Tate,\" who was arrested in 1993 for the Manson murders. She was also imprisoned in a car, and her family's members were killed by the killers.",
      "ground_truths": [
        "Susan Atkins,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44eed06f9e1e49bdbcca72ca2836d01c",
      "question": "When did the panel set another hearing for?",
      "prediction": "The answer to the question is: The panel set another hearing for Atkins in three years.",
      "ground_truths": [
        "in three years,"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "c0aaf4ae674846c3a7ec5bc71c4ba4c0",
      "question": "What happened at Susan Atkins parole hearing?",
      "prediction": "The answer is: \"It's the past I have to write.\"",
      "ground_truths": [
        "denied"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cfdac35dca045f5ac4cdc0aa14a317a",
      "question": "Did Sharon Tate's murderer get parole?",
      "prediction": "The answer to the question is yes, Sharon Tate's murderer is being convicted of murder.",
      "ground_truths": [
        "denied"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3520396bebc1406382993de89ae175ce",
      "question": "How many times was Sharon Tate stabbed?",
      "prediction": "The answer is 16.",
      "ground_truths": [
        "16"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a544b20196e94d7fad2e68b70300054f",
      "question": "What is Atkins battling?",
      "prediction": "Atkins is battling terminal brain cancer.",
      "ground_truths": [
        "terminal brain cancer."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a8ce2d5f5adf469da7c2544958d7b33d",
      "question": "Where did the dogs detect her scent?",
      "prediction": "The answer is: \"In my heart, I fear the worst, that my daughter is gone,\" and \"I fear the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my daughter is gone,\" and \"the worst, that my",
      "ground_truths": [
        "near the George Washington Bridge,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5af90443fde4a6f9f8feba49b266725",
      "question": "What is the disappeared woman's name?",
      "prediction": "The disappeared woman's name is Liza Murphy.",
      "ground_truths": [
        "Liza Murphy"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5347843098a3485eaeef3dc00d24aae7",
      "question": "What number do you call if you have information?",
      "prediction": "The number you call is 201-262-2800.",
      "ground_truths": [
        "201-262-2800."
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "0c7279a28507495abc478963bd318914",
      "question": "When did the woman disappear?",
      "prediction": "The answer is \"Liza Murphy has been missing from her home in Emerson, New Jersey, since August 19, 2007.\"",
      "ground_truths": [
        "August 19, 2007."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "f640e6ad69b54fcd9b606186fb712e07",
      "question": "What did the husband try after wards?",
      "prediction": "The husband tried to take his own life a few days after his wife's disappearance, by walking into oncoming traffic and throwing himself in front of a fire truck.",
      "ground_truths": [
        "to take his own life"
      ],
      "em": 0,
      "f1": 0.32258064516129037
    },
    {
      "id": "b0e63f428d16422db0687b63cd4d96e2",
      "question": "where was a man killed ?",
      "prediction": "The answer is \"at the Champs de Mars, the capital's central plaza, which has turned into a tent city,\" and \"no operating shops on the street.\"",
      "ground_truths": [
        "Port-au-Prince, Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1acd5b23dbcd4af28709c20565a87a36",
      "question": "What was the cause of death?",
      "prediction": "The cause of death was undoubtedly a stolen cash from the United States dollars, extracted by the American citizens.",
      "ground_truths": [
        "\"The people kill him with the blocks,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cc72beb75ea497e930d8667dff597c0",
      "question": "What did the photos show?",
      "prediction": "The photos showed the young man, Frederic Mano, a Haitian sportswriter, who was killed by a mob in Haiti.",
      "ground_truths": [
        "the man facing up, with his arms out to the side."
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "819e9d1061c54fd4bd76023e08d14667",
      "question": "what did the man say?",
      "prediction": "The man said, \"This is robbery. He went to rob the people. He went to steal money. American dollars. And he was one of many prisoners escaped after the earthquake. He did not deserve to be killed, but he is dead.\"",
      "ground_truths": [
        "\"This is robbery. He went to rob the people. He went to steal money -- American dollars,\""
      ],
      "em": 0,
      "f1": 0.5660377358490566
    },
    {
      "id": "93ee454cabff4b79ad4f77b8ac4660ca",
      "question": "what showed the gruesome scene?",
      "prediction": "The gruesome scene in Haiti shows a young man who was killed with blocks, with blood, and with the same, empty hands. The scene is described as a concrete block, near a foot of a concrete block, and a street with a few feet of a concrete block.",
      "ground_truths": [
        "photos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "faf5c483ab2d4cf9b3a8bbecc5442cdb",
      "question": "Who provides humanitarian aid to the Somalis?",
      "prediction": "The answer is the U.N. Development Program.",
      "ground_truths": [
        "Daryeel Bulasho Guud"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2e10abef8034c62b1e1030158e4195c",
      "question": "What is the DBG agency and where do they operate?",
      "prediction": "The DBG agency operates in Somalia.",
      "ground_truths": [
        "Nairobi, Kenya,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba96798c3ba0464e9436aa0d0ea49872",
      "question": "What nationalities were the aid workers?",
      "prediction": "The aid workers who were shot were Somali.",
      "ground_truths": [
        "Somalis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a82c0cb9a3ab4baca2ccd215c7d6a5c6",
      "question": "What aid agency suspended operations?",
      "prediction": "The aid agency suspended operations in Somalia.",
      "ground_truths": [
        "DBG,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "826da1e9a1c04a958f5f314498d08d0d",
      "question": "What are some reasons why Somalis depend on humanitarian aid?",
      "prediction": "Somalis depend on humanitarian aid on the following reasons:",
      "ground_truths": [
        "severe famine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ae7f52f9387841b99305f07a288e38ba",
      "question": "What has the aid agency done?",
      "prediction": "The aid agency has done what?",
      "ground_truths": [
        "suspend all"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8c42925b30c84c02b35aff4b9357010d",
      "question": "Since when do Somalis depend on humanitarian aid?",
      "prediction": "The answer is that Somalis depend on humanitarian aid.",
      "ground_truths": [
        "1991-1993,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ede6871c829b4a80ac507536b3a6d14f",
      "question": "Who was shot in Somalia?",
      "prediction": "The answer is: The United Nations (UN)",
      "ground_truths": [
        "Three aid workers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd35d7a651974bb181849a4486692264",
      "question": "How long have Robinson and Bridges been dating?",
      "prediction": "Robinson and Bridges have been dating for two years.",
      "ground_truths": [
        "two years,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "6254496b903a42e680fd882f3283fb49",
      "question": "how long have they been dating",
      "prediction": "The answer is: \"The baby will be the 42-year-old frontman's second child, he and Hudson have a 5-year-old son, Ryder Russell, together.  The statement was made Tuesday.\"",
      "ground_truths": [
        "two years,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b476d07cc2e747d0bad28a795661540d",
      "question": "how many children does he have",
      "prediction": "Chris Robinson has 5 children.",
      "ground_truths": [
        "second child"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a0b3de216b6498699f9a1745ef51447",
      "question": "Who is having a baby?",
      "prediction": "The answer is Chris Robinson.",
      "ground_truths": [
        "Chris Robinson and girlfriend Allison Bridges"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "3ca19860d31d40d58c3d6018498bbc5e",
      "question": "How long has Robinson and girlfriend been dating?",
      "prediction": "The answer is: Chris Robinson and Allison Bridges have been dating for two years.",
      "ground_truths": [
        "two years,"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "dbc0bbc541124ed0843db76f2e3a1581",
      "question": "Who is Robinson's ex-girlfriend?",
      "prediction": "The answer is Chris Robinson's ex-girlfriend.",
      "ground_truths": [
        "Kate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f092922c6854e3f8bdf22cc50cf71a9",
      "question": "What is the 5 1/2 year old's name?",
      "prediction": "The 5 1/2-year-old's name is Ryder Russell.",
      "ground_truths": [
        "Ryder Russell,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "45c6b42c27e84dc48bc37bccd095980b",
      "question": "What is the name of her show ?",
      "prediction": "The name of O'Donnell's show is \"The Rosie Show.\"",
      "ground_truths": [
        "\"The Rosie Show,\""
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "51a0d143d66d4312b89e4e00929ee837",
      "question": "What O`Donnell said about her love life?",
      "prediction": "O'Donnell's answer to the question about her love life is: \"I've Stopped Being So Angry.\"",
      "ground_truths": [
        "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\""
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "4b7de03eac3642c5aad653d3fe1600dc",
      "question": "What did O'Donnell say recently?",
      "prediction": "O'Donnell said, \"I've stopped being so angry.\"",
      "ground_truths": [
        "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0633af8dabeb4925bf0fb0110cc54247",
      "question": "Where  was O'Donnell  attracted to Rounds  ?",
      "prediction": "The answer is \"I thought she was a 28-year-old heterosexual girl, because that's what she looked like to me. And she's a 40-year-old gay woman.\"",
      "ground_truths": [
        "Starbucks"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "241ae106b1f7409d82a03f8296c8a6ab",
      "question": "Who recently said she's in love ?",
      "prediction": "The answer is \"I've Stopped Being So Angry.\"",
      "ground_truths": [
        "O'Donnell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d94300f7464c401db93e557fec221320",
      "question": "What is the name of her new show?",
      "prediction": "The name of Rosie O'Donnell's new night show is \"The Rosie Show.\"",
      "ground_truths": [
        "\"The Rosie Show,\""
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "6905efa62e924f7c9960bc701d1025bb",
      "question": "On what O`Donnell has been attracted to?",
      "prediction": "Based on the provided text, the answer is Rosie O'Donnell. She was attracted to Michelle Rounds.",
      "ground_truths": [
        "Michelle Rounds"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "160bd2f1f99b4697a5945323087874cb",
      "question": "When did the Rosie Show debut on the OWN network?",
      "prediction": "The Rosie Show debut was on the OWN network on Monday.",
      "ground_truths": [
        "Monday"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "ce867b37ae1d4bc6b35988966e721575",
      "question": "Who was O'donnel attracted to after seeing her in starbucks?",
      "prediction": "The answer is \"I thought she was a 28-year-old heterosexual girl, because that's what she looked like to me. And she's a 40-year-old gay woman,\" O'Donnell says, with a chuckle.",
      "ground_truths": [
        "Michelle Rounds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93f2aed21ef74156be3d976b205a907e",
      "question": "Who was absent from the trial?",
      "prediction": "The answer is \"Illinois Gov. Rod Blagojevich.\"",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b40aa09142ee437bac25b93f6300e20a",
      "question": "who is absent from trial?",
      "prediction": "Based on the provided text, the answer is \"Illinois Gov. Rod Blagojevich.\"",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "9b80ad96c0e04a61bf5221c06c14a9a4",
      "question": "Who heard the recorded phone calls?",
      "prediction": "The answer is, \"The state House of Representatives\" (the House of Representatives).",
      "ground_truths": [
        "state senators"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "bc9d87c351b84540981e32798a4bdc37",
      "question": "Who is holding interviews?",
      "prediction": "The answer is **the State House**.",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fdf3652f9965474eb63b7c5b227983c0",
      "question": "Who did the senators hear from?",
      "prediction": "The answer is: The two-term Democratic House of Representatives, with a single vote.",
      "ground_truths": [
        "Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cc048131d7e410fa3083078bb1fff5a",
      "question": "From who was the testimony?",
      "prediction": "The answer is:  The House impeachment vote was needed because the votes were split among the new House. The second vote was needed because the House of Representatives had no seats, and the impeachment vote was politically motivated.",
      "ground_truths": [
        "FBI Special Agent Daniel Cain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ec4f738dc3143a7bf541019bfe9c705",
      "question": "Who heard recorded phone calls?",
      "prediction": "The answer is \"Congress.\"",
      "ground_truths": [
        "state senators"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a52bd9f9d68446088b731bfd750f6820",
      "question": "What did commentators focus on?",
      "prediction": "The commentators focused on \"why our kids are messed up,\" \"why did these students try to stop the crime,\" and \"what's happening in our schools.\"",
      "ground_truths": [
        "\"bystander effect\":"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b36738aa32d44026be321cdf04b08d39",
      "question": "what are commentators focused on?",
      "prediction": "The commentators focus on the alleged rape of a 15-year-old girl on the campus of Richmond High School in Northern California, while the alleged gang rape of 10 or more students, most of which were students, was another sad example of today's self-absorbed and uncaring youth.",
      "ground_truths": [
        "\"bystander effect\":"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "016465fabbcb45038bfc279cb8190345",
      "question": "what do the studies show?",
      "prediction": "The answer is: The studies show that \"a school staff and security should patrol campuses, especially violence-prone areas, during and after school events.",
      "ground_truths": [
        "students often know ahead of time when and where violence will flare up on campus."
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "98141f5f55af44a589cad71ac101cc96",
      "question": "What crime do students not report?",
      "prediction": "The answer is \"A violent crime, like an act of revenge. It is not a crime.\"",
      "ground_truths": [
        "gang rape"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5bf5a22842224fe4952ea03b53d31fd3",
      "question": "What do studies show?",
      "prediction": "The provided text only answers the question about \"Why are our kids so messed up?\" and the \"What's happening in our schools?\" comment. It does not provide a single, direct answer to the question. It is a complex and controversial issue with many different, but ultimately consistent, potential answers. Therefore, the answer is not a single, definitive answer.",
      "ground_truths": [
        "that students often know ahead of time when and where violence will flare up on campus."
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "f380a5932d4b4a43969a84c3338edae0",
      "question": "Who is Robert Kimmitt?",
      "prediction": "Robert Kimmitt is the U.S. Department of State's Treasury Secretary. He is the head of the U.S. Department of State, and he is the Minister of Foreign Trade for the United Arab Emirates (U.A.E.), where the Sovereign Wealth Funds control up to $3 trillion in assets. He is the head of the U.A.E.",
      "ground_truths": [
        "Deputy Treasury Secretary"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "46f896a31c254fb2bf5b68c060edb437",
      "question": "What does MME talk to UAE's Minister of Foreign Trade about?",
      "prediction": "MME talks to the UAE's Minister of Foreign Trade about future relations between the Middle East and Washington.",
      "ground_truths": [
        "Sovereign Wealth Funds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c67f7f0037b4ab1bb510cba499d11a4",
      "question": "Name of the Deputy Treasury Secretary?",
      "prediction": "The Deputy Treasury Secretary for Foreign Trade, U.A.E., sheikha Lubna Al Qasimi, answers the question about the \"new era and a new President for the United States\" about the Sovereign Wealth Funds.",
      "ground_truths": [
        "Robert Kimmitt."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d83d3eda31c14835877587e71ae8f858",
      "question": "Who is the U.A.E.'s Minister of Foreign Trade?",
      "prediction": "The U.A.E.'s Minister of Foreign Trade, Sheikha Lubna Al Qasimi, talks to CNN about future relations between the Middle East and Washington.",
      "ground_truths": [
        "Sheikha Lubna Al Qasimi,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "6fe2385a3a1f4e7c8b1982cad82c939d",
      "question": "What was the purpose of Brown's Gulf tour?",
      "prediction": "The answer is: \"Back to the United States.\"",
      "ground_truths": [
        "in an attempt to secure more funds from the region."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "6c8e1e1b56bc4eb5b99b83ce6c5f3afd",
      "question": "Where did UK Prime Minister Gordon Brown tour?",
      "prediction": "The answer is: Sheikha Lubna Al Qasimi, Minister of Foreign Trade, U.A.E.",
      "ground_truths": [
        "the Gulf"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f1d761a46c64578849ff7408c6351b2",
      "question": "What did Robert Kimmitt discuss?",
      "prediction": "The answer is \"The region looks for in a new U.S. administration? We talk to United Arab Emirates' Minister of Foreign Trade, Sheikha Lubna Al Qasimi about future relations between the Middle East and Washington.\"",
      "ground_truths": [
        "Sovereign Wealth Funds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e66e99bd83d4ae9abaa6e14c89cddb9",
      "question": "Name of the prime minister of the UK?",
      "prediction": "The answer is: U.S. Prime Minister of the United Arab Emirates.",
      "ground_truths": [
        "Gordon Brown"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d23a30ecc5b8469c8e6324ff5d42409b",
      "question": "What position does Robert Kimmitt hold?",
      "prediction": "Robert Kimmitt holds the position of Minister of Foreign Trade.",
      "ground_truths": [
        "Deputy Treasury Secretary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1699130630f2472cacfcbfe214d40416",
      "question": "what is missing?",
      "prediction": "The missing sketchbook is a red varnished cover with the word \"Album\" in gold lettering.",
      "ground_truths": [
        "A Pablo Picasso sketchbook with 33 pencil drawings"
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "fef91dcc799042b59201fc0f963a8b17",
      "question": "what does it look like?",
      "prediction": "The answer is \"Red varnished cover with the word \"Album\" on it in gold lettering, and measures 16 centimeters by 24 centimeters (6 inches by 9 inches).",
      "ground_truths": [
        "The sketchbook has a red varnished cover with the word \"Album\" inscribed on"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "34a877cef8f4456197484f09d1643e87",
      "question": "how many drawings missing",
      "prediction": "The missing sketches are identified as \"Album\" in gold lettering with the word \"Album\" in red.",
      "ground_truths": [
        "33 pencil"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e497d4f0b4642248a1e1e4b25960ce0",
      "question": "The artist used the sketchbook when?",
      "prediction": "The artist used the sketchbook when he was in his late 30s and early 40s.",
      "ground_truths": [
        "between 1917 and 1924"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "77dc5fcd3f654630a9a8ec90c9375b45",
      "question": "when was the notebook used?",
      "prediction": "The notebook was used between 1917 and 1924 when it was lost.",
      "ground_truths": [
        "between 1917 and 1924"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "cdec18921af845c694384dea037eb6ec",
      "question": "What color is the cover?",
      "prediction": "The cover is red.",
      "ground_truths": [
        "red"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6392f94b4c9f4fd38b24b66e0f2aead8",
      "question": "Who was killed in the hijacking?",
      "prediction": "The answer is \"Reggae legend Lucky Dube,\" who was killed in a car-jacking in Johannesburg.",
      "ground_truths": [
        "Reggae legend Lucky Dube,"
      ],
      "em": 0,
      "f1": 0.47058823529411764
    },
    {
      "id": "76bde369225e4bbda0147da9063ff142",
      "question": "Who has killed in the attempted hijacking?",
      "prediction": "The answer is Lucky Dube, 43, who was killed in a car-jacking in Johannesburg.",
      "ground_truths": [
        "Reggae legend Lucky Dube,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "7c59243c55f6421f8119754dd28c0766",
      "question": "Does this affect the upcoming World Cup?",
      "prediction": "The answer is yes, this impacts the upcoming World Cup.",
      "ground_truths": [
        "his death cast a shadow over festivities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9dd205a2702747e18373629a67b11baf",
      "question": "Where did the hijacking take place?",
      "prediction": "The answer is in CNN's \"Reggae Legend's Aftermath.\"",
      "ground_truths": [
        "Johannesburg"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "63eaf8aa6eda439cb304bb18e12eea91",
      "question": "What did the hijacker try to steal?",
      "prediction": "The answer is: Dube was killed in Johannesburg, South Africa.",
      "ground_truths": [
        "car,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be51da2808964196bf2f456a76db8be9",
      "question": "In what city was Dube killed?",
      "prediction": "The answer is Johannesburg.",
      "ground_truths": [
        "Johannesburg"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c6f1257958664760877836b38f68af88",
      "question": "When was Dube killed?",
      "prediction": "The answer is: Dube was killed in Johannesburg around 8 p.m. Thursday after someone tried to steal his car.",
      "ground_truths": [
        "around 8 p.m. local time Thursday"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "f79b0a4362b34592be510c1391e9f64a",
      "question": "Did hijacker try to steal a car?",
      "prediction": "The answer is yes, he tried to steal a car.",
      "ground_truths": [
        "his"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c974615345cf4c53b0db7413aa72bcfe",
      "question": "What country was reggae legend Lucky Dube from?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa's"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "cd7cad43372b4947834b1f92e929ba96",
      "question": "Who was killed in an attempted hijacking?",
      "prediction": "The answer is \"Reggae legend Lucky Dube, one of South Africa's most famous musicians.\"",
      "ground_truths": [
        "Lucky Dube,"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "c09ddd59a8b14c9281b874e24a2053e4",
      "question": "Did he during an attempted carjacking",
      "prediction": "The answer is yes, Lucky Dube was killed in a car-jacking in Johannesburg.",
      "ground_truths": [
        "Dube, 43, was killed"
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "5ff70db3c11f4c09b46ec32324df940a",
      "question": "Where did 1 million gather?",
      "prediction": "Answer: 1 million",
      "ground_truths": [
        "in Angola"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "263ac68d0d564969908cdec34ad10b8d",
      "question": "What is the name of the Pope?",
      "prediction": "The name of the Pope is John Paul II.",
      "ground_truths": [
        "Benedict XVI"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bd6129a79ac4ba3ae46913fb8243d9c",
      "question": "Amount of people gathered to hear the Pope?",
      "prediction": "The answer is 1 million.",
      "ground_truths": [
        "1 million"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "dc1bcf2ea7d34043b9337438ed4acbf4",
      "question": "where was the event",
      "prediction": "The answer is Africa.",
      "ground_truths": [
        "Angola"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3eb511ac77664650b1b55a19c630b375",
      "question": "What does the Pope express?",
      "prediction": "The Pope's answer to the question is: \"He understands he has to do this. He knows Africa is the future of the Roman Catholic Church, as it is for all of Christianity.\"",
      "ground_truths": [
        "Benedict also expressed \"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "b83cfc8d017346568f72d0993f32a5bd",
      "question": "how many dead?",
      "prediction": "The answer to the question is: 1 million.",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff45f51c967447bbafe8da76140a1a0f",
      "question": "Where was Mass?",
      "prediction": "The answer is Africa.",
      "ground_truths": [
        "in Angola,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d40052ef1ed24b12a67167b232d8d8e1",
      "question": "What continent is the Pope visiting?",
      "prediction": "The Pope's visit to Angola is in Africa, and he did not visit it before.",
      "ground_truths": [
        "Africa."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "1eb7d127003f4185bd7ffa3cc441685e",
      "question": "Which airline has lighting that subtly shifts throughout the day?",
      "prediction": "The answer is: American Airlines.",
      "ground_truths": [
        "Virgin America"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b71158f3ac6c4380aa5d0ca50b47f0dc",
      "question": "Which airline has in-cabin lighting?",
      "prediction": "Based on the context, the answer is Virgin America.",
      "ground_truths": [
        "Virgin America"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "88d0ad4c27184b2886a42cd1aa074e19",
      "question": "How many driverless pods were there?",
      "prediction": "The answer is: There were approximately 18.",
      "ground_truths": [
        "18"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "513409fabe5141609eff1756de476af0",
      "question": "Where are the driverless pods being tested",
      "prediction": "The answer is: \"The drivers are likely being tested at airports.\"",
      "ground_truths": [
        "at airports"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "1df83e933ab24ff0b54b67700f2cfa1b",
      "question": "How many pods are being tested?",
      "prediction": "The answer is 18.",
      "ground_truths": [
        "are"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e349996be424e28a5b92a8b05a0d198",
      "question": "What shifts throughout the day?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "in-cabin lighting system"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa3e80c7f0fc47e49f2db9651533f6f7",
      "question": "In which country is Madonna helping orphans?",
      "prediction": "Answer: In Malawi.",
      "ground_truths": [
        "Malawi,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1d7d93b9022543dca084d0fa1c352457",
      "question": "Who discovers the passion of the Angolan footballers?",
      "prediction": "The answer is David McKenzie, a South African football photographer.",
      "ground_truths": [
        "David McKenzie"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "ceb298b8c9064b5d94824599481fdef8",
      "question": "Who discovered the passion of the Angolan football squad?",
      "prediction": "The answer is David McKenzie.",
      "ground_truths": [
        "David McKenzie"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "409da2ac9b274f3ca50de2b2fcf4d564",
      "question": "Alina Cho speaks to who about her efforts to help other Malawi's orphans?",
      "prediction": "A photographer's powerful pictures cover the conflict in the Democratic Republic of Congo.",
      "ground_truths": [
        "Madonna"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7e5dd4ef82641c6bebdd9a9c4a7ebb0",
      "question": "Who is helping Malawian orphans?",
      "prediction": "Answer: David McKenzie",
      "ground_truths": [
        "Madonna"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b7ae048102f4dcda239ce5315eb5e72",
      "question": "What sport does the World Cup legend play?",
      "prediction": "Answer: Football.",
      "ground_truths": [
        "football"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "274afac6641443f7b27638c937cdb43e",
      "question": "What is the name of the World Cup legend?",
      "prediction": "The World Cup legend is **Diego Maradona.**",
      "ground_truths": [
        "Diego Maradona"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c6340a3591404e35b566ee5b14cf49bd",
      "question": "Inside Africa catches up with a World Cup legend spreading football excitement in which country?",
      "prediction": "Answer: Inside Africa",
      "ground_truths": [
        "South"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c4e8842976246d0adeceba704a8ac8c",
      "question": "Who speaks to Madonna?",
      "prediction": "The answer is **Madonna**.",
      "ground_truths": [
        "Alina Cho"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1364ba1827604428a0d1e1ce8fd8fdc9",
      "question": "Where did he go?",
      "prediction": "The answer is:  \"where he went is not definitively determined by the state's attorney, but until the state's attorney determines if there is enough to charge him.\"",
      "ground_truths": [
        "drove to a gym"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "327e68cc519349b695720f801ef03267",
      "question": "where were the bodies found?",
      "prediction": "The final answer is correct, but the question is asked with \"where were the bodies found\" and the answer is \"Columbia, Illinois.\"",
      "ground_truths": [
        "the bedrooms of their two-floor home"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a94e90ab1f0e4ffa8760e52e917216a9",
      "question": "what city did slayings occur in?",
      "prediction": "The answer is \"South Illinois.\"",
      "ground_truths": [
        "St. Louis,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7944ad8c802d417fb16189db00b2dcd3",
      "question": "What was on the walls of the home?",
      "prediction": "The answer is not directly stated in the context. The context only tells us that the police say they know who killed a 31-year-old Southern Illinois woman and her two young sons, but they are waiting for prosecutors to build a strong forensic case before disclosing Coleman's identity. Therefore, it is not explicitly stated.",
      "ground_truths": [
        "threatening messages"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e370c4ffa374f42ba74df68937b9f6f",
      "question": "Who will decide whether to file charges?",
      "prediction": "The answer is the State's Attorney in St. Louis, Missouri.",
      "ground_truths": [
        "state's attorney"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "5043888646d5421d81fc3b47f8aef68e",
      "question": "Where did the husband say he was?",
      "prediction": "The husband said he was calling his house to \"wasn't answering\" and on his call to the Columbia Police Department.",
      "ground_truths": [
        "gym to work out."
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "707ff5fcde5548819e3c5f3c95a12beb",
      "question": "Who made the documentary?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "Sabina Guzzanti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9f6b0c738be48b5a1fc57326e3a87ba",
      "question": "Documentary was screened where?",
      "prediction": "The answer is \"Draquila,\" a documentary that has created an uproar in Italy.",
      "ground_truths": [
        "Cannes Film Festival,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0fa068fc67094f12bd7bd1147fa6f3ca",
      "question": "What was the name of the documentary?",
      "prediction": "The answer is \"Draquila,\" which translates to \"Dusk of L'Aquila.\"",
      "ground_truths": [
        "\"Draquila"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "950b433fc59c497e93d4e7e4fcb5438c",
      "question": "Who boycotted the film?",
      "prediction": "The answer to the question is \"Guzzanti,\" the Italian political satirist and critic who criticized the government's handling of the L'Aquila earthquake, which killed nearly 300 people.",
      "ground_truths": [
        "Sandro Bondi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21abaf846e2a489a8d36c0c21e27358d",
      "question": "The film, \"Draquila,\" takes issue with the way the prime minister handled what earthquake?",
      "prediction": "The film, \"Draquila,\" highlights the government's use of abuse of power, corruption, and misinformation to create a precarious state of democracy in Italy, especially in the light of the earthquake. The film's commentary on the government's handling of the disaster, particularly the efforts to improve his image and boost the public's opinion of him, is a key argument for the government's recent crackdown. The film's critics then question the government's response, arguing that it was a \"propaganda\" aimed at suppressing truth and the Italian people's \"truth.\"",
      "ground_truths": [
        "L'Aquila"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bdc2f07e48814dec8eb597d9b77a8d11",
      "question": "Italian Culture Minister boycotted Cannes  why?",
      "prediction": "The Italian Culture Minister boycoted Cannes Film Festival in a response to a documentary, \"Draquila,\" which he criticizes for its \"propaganda\" and \"truth,\" and for its alleged influence on the government's image.",
      "ground_truths": [
        "Bondi issued a statement, dismissing the documentary as \"propaganda\" and saying it \"offends the truth and all of the"
      ],
      "em": 0,
      "f1": 0.23255813953488372
    },
    {
      "id": "721e1c8fa27e45dbbbc5261cd2c3d9d4",
      "question": "What is the documentary about?",
      "prediction": "The documentary \"Draquila,\" which is called Italy Trembles, is about how the Italian government has abused its power by indulging in corruption, misrepresentation, and misinformation, resulting in a state of near-collapse in Italy. It's a documentary by Sina Guzzanti, known for her satirical and critical commentary on the government.",
      "ground_truths": [
        "prime minister's handling of the L'Aquila earthquake,"
      ],
      "em": 0,
      "f1": 0.04
    },
    {
      "id": "af32ee59935e4a90b57a6e01d8ae0ced",
      "question": "What is the filmmaker's name?",
      "prediction": "The filmmaker's name is Guido Guzzanti.",
      "ground_truths": [
        "Sabina Guzzanti"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "82210515b3ad4b2bbdad50b7b0f7faff",
      "question": "Who was the documentary about?",
      "prediction": "The answer is \"Draquila,\" which translates to \"D\" after \"Dall'a'a,\" and it is a film about the recovery efforts in L'Aquila, Italy's city after the earthquake. The film criticizes the government's exploitation of power, corruption, and misinformation, and its efforts to improve the public's opinion of Berlusconi.",
      "ground_truths": [
        "Silvio Berlusconi."
      ],
      "em": 0,
      "f1": 0.047619047619047616
    },
    {
      "id": "46c47691cf5b44489b589d6a42b0e438",
      "question": "What is open 24/7?",
      "prediction": "The answer is:  \"The Emergency Operations Center at the Centers for Disease Control and Prevention, Georgia, in Georgia, is a complex system of staffed computers, telephones, and machines, and the central command is a dedicated team. The CDC's operations are staffed 24/7, and the key is to ensure all can work efficiently, even if it means working independently. The government has authorized 50 people to work on the virus and 2009 H1N1 flu, and the CDC's efforts in other areas. The \"Ethics, Immunization, Mental Health, Security, and dozens of other",
      "ground_truths": [
        "The control room"
      ],
      "em": 0,
      "f1": 0.025
    },
    {
      "id": "c8b9bcb9933c4334a5e15347af4e6404",
      "question": "what year was H1N1",
      "prediction": "The answer to the question is \"The CDC is not in the business of mass-producing vaccines.\"",
      "ground_truths": [
        "2009"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "26768a2b3ffd462aa76c0814a0fb67b6",
      "question": "Who needs to use caution?",
      "prediction": "The answer is the U.S. Department of Transportation.",
      "ground_truths": [
        "those traveling near the Somali coast"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f6fd7c37e3c4224892dcacfcdad4a68",
      "question": "Where will passengers fly?",
      "prediction": "The answer is Dubai.",
      "ground_truths": [
        "over the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "321880cf525f47328e5f457f4b2386c4",
      "question": "Where do passengers have to fly to continue their journey?",
      "prediction": "The answer is Dubai.",
      "ground_truths": [
        "Dubai"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d658d3578ccb4607bdbc7b6585bc9a14",
      "question": "Who issued a travel warning?",
      "prediction": "The answer is the U.S. State Department and the British Foreign Office.",
      "ground_truths": [
        "German Foreign Ministry,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "787e42ec72df45998c4507345004caea",
      "question": "How many passengers were mentioned as travelling in this situation?",
      "prediction": "The answer is: 246.",
      "ground_truths": [
        "246"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "99b4eebe743f4bcabce1ec386e81793e",
      "question": "Which countries advise extreme caution while traveling near the Somali coast?",
      "prediction": "The answer is, based on the provided text:",
      "ground_truths": [
        "U.S. State Department and British Foreign Office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b244e94cae0f40d2a5ef6d39687b8eea",
      "question": "Where will passengers fly to continue their journey?",
      "prediction": "The answer is Dubai.",
      "ground_truths": [
        "Dubai"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f2a8589ed0aa4c0e9cfb1446827b09ab",
      "question": "How many passengers does it involve?",
      "prediction": "The answer is 246.",
      "ground_truths": [
        "246"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "793f58910e644f078c60b96a422b9dcc",
      "question": "Which ship was the incident aboard?",
      "prediction": "The answer is: Hapag-Lloyd Cruises.",
      "ground_truths": [
        "MS Columbus,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e006b8380d04df7ac129af93547dc5f",
      "question": "Where did the US and UK say that people should avoid?",
      "prediction": "The US and UK said that people should avoid traveling near the Somali coast due to pirate attacks.",
      "ground_truths": [
        "the Somali coast"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "1b454ee0cee8452986cf0b57342e8534",
      "question": "Where was Omar previously denied?",
      "prediction": "Omar bin Laden was denied asylum in Spain.",
      "ground_truths": [
        "asylum in Britain."
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "540ee9edf9fd4bfc981390d172704bbc",
      "question": "Did Spain give a reason for turning down the asylum?",
      "prediction": "Yes, Spain gave a reason for turning down the asylum.",
      "ground_truths": [
        "was given"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fae579ad240d4227b1301d52e1de5ff3",
      "question": "Who was denied asylum in Britain?",
      "prediction": "The answer is: The Madrid, Spain, Interior Ministry.",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "654cd3a33c974ab989777ea8cd69c130",
      "question": "Where did Omar bin Laden first try to flee to?",
      "prediction": "Omar bin Laden first tried to flee to Cairo, Egypt, in Madrid on Monday.",
      "ground_truths": [
        "Britain."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c9d87584b86413d86f28eb40a1af7cf",
      "question": "Who did Spain deny asylum to?",
      "prediction": "Spain denied asylum to Omar bin Laden.",
      "ground_truths": [
        "Omar bin Laden,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "ed2ba8953b6a49bb8f961969603a956f",
      "question": "Who was denied asylum?",
      "prediction": "The answer is **Bin Laden**.",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "5ed1ab8a035e44ec9200692431509884",
      "question": "Who didn't give a reason?",
      "prediction": "The answer is the spokesperson, who said it's unlikely to be disclosed.",
      "ground_truths": [
        "U.N. High Commissioner for Refugees"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "548b12b048354fa198f25caa234a0081",
      "question": "What family member of Omar bin Laden was associated with terrorism?",
      "prediction": "The answer is:",
      "ground_truths": [
        "his father"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ecb8eb62bc3d4ae280ea3151211089a3",
      "question": "What did he want his father to abandon?",
      "prediction": "The answer is **Bin Laden wanted his father to abandon terrorism.**",
      "ground_truths": [
        "terrorism."
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "00adadfa907f41a0b1bb4e8e4fd98a81",
      "question": "Mugabe and Tsvangirai have signed agreement paving way for what?",
      "prediction": "The answer is: The U.S. is ready to provide a substantial assistance package, development aid, and normalization with international financial institutions.",
      "ground_truths": [
        "power-sharing talks to take place in the next few weeks."
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "e370ff23a5e849569a688e4159142601",
      "question": "Whose government was called to be illegitimate?",
      "prediction": "The answer is: the U.S. has been providing assistance to Zimbabwean refugees and displaced people.",
      "ground_truths": [
        "Zimbabwean"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "9542c8bd1b494422b21cbbbd76fa9ea0",
      "question": "What does order expand?",
      "prediction": "According to the context, order expands by:",
      "ground_truths": [
        "U.S. sanctions against"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cd7056ccab0749d7930b3a75760ff7d8",
      "question": "What did Bush calls Robert Mugabe's government?",
      "prediction": "The answer to the question is: **\"illegitimate.\"**",
      "ground_truths": [
        "\"illegitimate.\""
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "4fedeb2052db4dcbb256d6bd31be5c5d",
      "question": "Who signed the order?",
      "prediction": "The answer is: The U.S.",
      "ground_truths": [
        "President Bush"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83524294a9eb41f5acc541555457dcae",
      "question": "What two countries vetoed UN resolutions relevant to this situation?",
      "prediction": "The answer is: The United States.",
      "ground_truths": [
        "Russia and China"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab4547eb5b754a788730ac927811a4a3",
      "question": "Who is Tsvangirai?",
      "prediction": "The answer is Thamer Bin Saeed Ahmed al-Shanfari, an Omani national who has close ties to Mugabe and his top officials. He was an Omani national who used his company, Oryx Natural Resources, to enable Mugabe to maintain access to mining ventures in Congo.",
      "ground_truths": [
        "opposition candidate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6e0c91b398b44fea89c823558223b92",
      "question": "What did President Bush sign order to expand?",
      "prediction": "The president signed an executive order to expand U.S. sanctions against Zimbabwe, the White House said.",
      "ground_truths": [
        "U.S. sanctions against"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "0715d4474ede4876903c8b68ce6c897a",
      "question": "Who signed an agreement paving way for power-sharing talks?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Mugabe and Tsvangirai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da070cef91304c029a6edab81dec8e02",
      "question": "What country does Robert Mugabe lead?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Zimbabwe,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62f85a5490834ba294bd772557571fca",
      "question": "What country are the sanctions against?",
      "prediction": "The sanctions against Zimbabwe are 17 entities, including government-owned or controlled companies, and one individual.",
      "ground_truths": [
        "Zimbabwe,"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "f2a65bbb2d8e4767ac2fb9423e8c685b",
      "question": "Who vetoed the UN resolution?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Russia and China"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2058b653142d477caffeab8b3e29dbcd",
      "question": "What did tthe CEO say?",
      "prediction": "The CEO of London, England, Pierre-Henri Gourgeon, stated that the Airbus A330-200 encountered heavy turbulence around 02:15 a.m. Monday, 10:15 p.m. ET Sunday, and was unable to communicate its last contact with the maintenance computers.",
      "ground_truths": [
        "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday"
      ],
      "em": 0,
      "f1": 0.37209302325581395
    },
    {
      "id": "758df4eb23104c66950c66ce7d3a78c9",
      "question": "What encountered heavy turbulence?",
      "prediction": "The answer is \"heavy turbulence\".",
      "ground_truths": [
        "the Airbus A330-200"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3617db7062854907aa937695b18e8696",
      "question": "What did the A330-200 encounter?",
      "prediction": "According to Air France, the A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday (10:15 p.m. ET Sunday), three hours after the jet left Rio de Janeiro, Brazil, for Paris, France.",
      "ground_truths": [
        "heavy turbulence"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "7047a1d5e4564611903f19acd9f2e764",
      "question": "What did the Airbus A330-200 encounter",
      "prediction": "The answer is, Airbus A330-200.",
      "ground_truths": [
        "heavy turbulence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e271a1cce4441378acdeabc701dcb04",
      "question": "How many people were on the flight?",
      "prediction": "The answer is: The answer provided is: \"The disappearance of Flight AF 447 over the Atlantic Ocean has fueled speculation among aviation experts about what caused the state-of-the-art airliner to come down.\"",
      "ground_truths": [
        "228"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97f7bf68a05f48958ef3062ec906f6cb",
      "question": "How many people where onboard the flight",
      "prediction": "The answer provided is simply the CEO of London, England (CNN), and the Air France (CNN) CEO.",
      "ground_truths": [
        "228"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b832de505fb545e2995dfc6e50284d8f",
      "question": "What did some consider to be a possible cause",
      "prediction": "The answer is: \"It is possible that lightning struck the plane's electrical components at the tail. And that's a possible cause, even though the aircraft's system, which initiated a four-minute exchange of messages to the maintenance computers at 02:33 a.m., showed \"several pieces of aircraft equipment at fault or had broken down\" in the last 02:33 a.m. Moscow Time.",
      "ground_truths": [
        "lightning strike"
      ],
      "em": 0,
      "f1": 0.03773584905660377
    },
    {
      "id": "27447b04916e4932aa885dc445aa0f68",
      "question": "What did Obama said?",
      "prediction": "Obama stated that key members of his energy team, including Steven Chu, Carol Browner, and Lisa Jackson, named as secretaries of energy, and Nancy Sutley, the Los Angeles deputy mayor for energy and environment, both as the first high-ranking gay aptee to the Obama administration.",
      "ground_truths": [
        "\"We've seen Washington launch policy after policy, yet our dependence on foreign oil has only grown, even as the world's resources are disappearing,\""
      ],
      "em": 0,
      "f1": 0.03125
    },
    {
      "id": "b2e3e2f0039645cbb9bbef363bb33901",
      "question": "How much will be invested?",
      "prediction": "The answer is $150 billion.",
      "ground_truths": [
        "$150 billion"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "77fc4a206e1b4992a867608d3e9cd208",
      "question": "How much would Obama invest?",
      "prediction": "The answer is, Obama would invest $150 billion over 10 years.",
      "ground_truths": [
        "$150 billion over 10 years"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ba8c3e4c32eb4c6c8513fe3ffb6cd12f",
      "question": "Who was named secretary of energy?",
      "prediction": "The answer is Chu, a former EPA administrator.",
      "ground_truths": [
        "Steven Chu"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "660fdaeb1a134f3782b6737147b03bfd",
      "question": "Who named Secretary of energy?",
      "prediction": "The name of the Secretary of Energy for the White House is:",
      "ground_truths": [
        "Steven Chu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b29f0f79a6e24fa695af9ea0ca9818bf",
      "question": "Who created climate policy?",
      "prediction": "The answer is: **Carol Browner**",
      "ground_truths": [
        "Carol Browner"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "85f8058231ee479b8af6624c13a1993f",
      "question": "Who hosted \"Star Search\"?",
      "prediction": "The answer to the question is *Ed McMahon* hosted \"Star Search\".",
      "ground_truths": [
        "Ed McMahon"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "97547bbeb3614747bdb52be1d90c7bc2",
      "question": "Who hosted \"The Tonight Show\"?",
      "prediction": "Answer: The Tonight Show",
      "ground_truths": [
        "Carson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be20cc1e18b54ecc87e4a1e34d45bff6",
      "question": "What tv shows hosted McMahon?",
      "prediction": "Based on the context, the answer is **\"The Tonight Show,\"**",
      "ground_truths": [
        "\"TV's Bloopers and Practical Jokes,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f40951a7d86e43dcb35ade73d74a48a4",
      "question": "What is McMahon most famous for?",
      "prediction": "The answer is, Ed McMahon is famous for his \"Heeeeeeerre's Johnny!\" play, which was a part of the vernacular.",
      "ground_truths": [
        "longtime pitchman and Johnny Carson sidekick"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "e82fa7a590004a5091ed4ff9d55156b5",
      "question": "what health problems suffered McMahon?",
      "prediction": "The answer is: McMahon suffered from neck injuries caused by a 2007 fall.",
      "ground_truths": [
        "pneumonia and other medical"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64395311a5d54d79baae78a2162cf8b9",
      "question": "Which TV programs did McMahon host?",
      "prediction": "Answer: The television shows, including \"Star Search\" and \"TV's Bloopers and Practical Jokes,\" are all known for their depiction of the usual, often humorous, programming of the TV show. The answer is: \"Star Search\" and \"TV's Bloopers and Practical Jokes.\"",
      "ground_truths": [
        "\"TV's Bloopers and Practical Jokes,\""
      ],
      "em": 0,
      "f1": 0.24390243902439027
    },
    {
      "id": "097639dba05445eba050b1141ea106b9",
      "question": "New book quotes Reid discussing what?",
      "prediction": "The answer is \"Game Change,\" a book that quotes Reid in his statement.",
      "ground_truths": [
        "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\""
      ],
      "em": 0,
      "f1": 0.13953488372093023
    },
    {
      "id": "fc1195a9859b43be9b77eff8e9ea98f4",
      "question": "What does Congressional Black Caucus reject?",
      "prediction": "The Congress Black Caucus rejects Reid's ouster from his party.",
      "ground_truths": [
        "calls for Reid's dismissal."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "deabb360cc7344ff91443e4ecc8cb960",
      "question": "What does GOP chair say about the Senate majority leader's language?",
      "prediction": "The GOP chair's statement to Reid, D-Nevada, asks him to \"deal with the issues like the economy, job creation, and energy for these regrettable comments to distract us from the work that must be done on behalf of the American people.\"",
      "ground_truths": [
        "\"embarrassing and racially insensitive,\""
      ],
      "em": 0,
      "f1": 0.049999999999999996
    },
    {
      "id": "9937f92b0d734539ae1d70d9845d5ed4",
      "question": "Congressional Black Caucus rejects calls for?",
      "prediction": "The answer is \"I have no plans to step down.\"",
      "ground_truths": [
        "him to step down as majority leader."
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "1c2ad6efb4e64e0f98d96ec7c5d3ba0b",
      "question": "What are training dogs for?",
      "prediction": "The answer is:  The state of Texas is a growing number of contraband cell phones found behind bars or in transit to Texas inmates in 2008.",
      "ground_truths": [
        "to sniff out cell phones."
      ],
      "em": 0,
      "f1": 0.21428571428571427
    },
    {
      "id": "482ca94d43ea43c481d3e179d4c613bf",
      "question": "What are dogs trained to do?",
      "prediction": "The answer is dogs trained to \"sniff out cell phones.\"",
      "ground_truths": [
        "sniff out cell phones."
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "a22abca53c0c4cbea72109ec7e09b0f3",
      "question": "What clears Texas Legislature?",
      "prediction": "The answer is, \"Texas Legislature.\"",
      "ground_truths": [
        "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6b05edac93854de7bfc7f66698634f62",
      "question": "What does the bill crack down on?",
      "prediction": "The answer is: \"It's en route to Gov. Perry's desk after clearing both houses of the legislature.\"",
      "ground_truths": [
        "convicts caught with phones"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "82f836841fb745ef972057edb01a3892",
      "question": "What are sniffer dogs in Texas prisons looking for?",
      "prediction": "The answer is, in the black market of prison life, cell phones have become the hottest commodity, and Texas is among a growing number of state governments going after them.",
      "ground_truths": [
        "sniff out cell phones."
      ],
      "em": 0,
      "f1": 0.13333333333333336
    },
    {
      "id": "47c655c75bad4ccb96e18b573d3ab162",
      "question": "What do inmates in Texas have?",
      "prediction": "The answer is \"often, but not all,\" and this is because contraband cells have less cell phones than illegal drugs, so the key is to find and locate the missing, dirty, and lost contraband.",
      "ground_truths": [
        "cell phones"
      ],
      "em": 0,
      "f1": 0.12121212121212122
    },
    {
      "id": "7c24847ff8924ded8c7f14ae191cb0f5",
      "question": "What is texas?",
      "prediction": "The answer is Texas.",
      "ground_truths": [
        "state"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2712e3b995a849349843ff869f4c5820",
      "question": "What does the book include?",
      "prediction": "The answer is: \"Verisimilitude.\"",
      "ground_truths": [
        "a paragraph about the king and crown prince"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27ba4dcff67c42e4bd660ed84520cc9d",
      "question": "who was arrested?",
      "prediction": "The answer is Harry Nicolaides, 41, who was arrested in Thailand.",
      "ground_truths": [
        "Harry Nicolaides,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "5579074640614bacbdd03ed34a3ddf08",
      "question": "what did authorities consider it as",
      "prediction": "The answer is: **The authorities considered it a rumor, not a fact.**",
      "ground_truths": [
        "illegal to defame, insult or threaten the crown."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b7be73a8ada1408b95b476b80880309d",
      "question": "When was Harry Nicolaides arrested over his 2005 book?",
      "prediction": "The answer is: **Harry Nicolaides was arrested in Thailand on August 31, 2005, after receiving a pardon from the king.**",
      "ground_truths": [
        "August 31."
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "e854d8deb6cf4af4ac58d44cb3c027c3",
      "question": "what age was Harry Nicolaides",
      "prediction": "The answer is 2005.",
      "ground_truths": [
        "41,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "171414c078ce452d9374be8989aa0147",
      "question": "what does book include?",
      "prediction": "The answer is \"Verisimilitude,\" a story about the king and crown prince of Thailand.",
      "ground_truths": [
        "a paragraph about the king and crown prince"
      ],
      "em": 0,
      "f1": 0.5882352941176471
    },
    {
      "id": "fe0121ef5b874df0bfca5369f2253f03",
      "question": "what did Man's lawyer say",
      "prediction": "The answer is: **\"It was probably not the best time to repeat the passage that was found to be offensive. But it concerned the crown prince of Thailand and a rumor that was being circulated in Thailand about the crown prince.\"**",
      "ground_truths": [
        "he was released Friday and taken to the Australian embassy"
      ],
      "em": 0,
      "f1": 0.13636363636363638
    },
    {
      "id": "6a80ae4b171345c3ba541b8aa343b1be",
      "question": "what did authorites deem?",
      "prediction": "The answer is, the authorites deemed the book a violation of a law that makes it illegal to defame, insult, or threaten the crown.",
      "ground_truths": [
        "a violation of a law that makes it illegal to defame, insult or threaten the crown."
      ],
      "em": 0,
      "f1": 0.8387096774193548
    },
    {
      "id": "5436abb6b9514525b7f31b9e1b4935b7",
      "question": "Where does the husband tweet from?",
      "prediction": "The husband's tweet from the Haitian family's blog, \"Tory Livesay\" on Wednesday morning, mentions the \"deafening tragedy of Haiti,\" emphasizing the fact that \"Life in Haiti left you wondering 'Where are you God?' and other times we witness miracles with our own eyes.\" He also shares his photos and accounts of the earthquake, including a news report of the collapse of Carrfour (South of Port) and the collapse of the National Palace in Carrefour, and his speech to the Haitian people. Finally, he writes about the loss of many other houses, churches, hospitals, schools, and businesses in Haiti",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0.022727272727272724
    },
    {
      "id": "eab5c3e8c5b7496084b5bce4d50369f8",
      "question": "from where Troy issues tweets?",
      "prediction": "The answers to the question are based on the provided text:",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a951f069f82b478a9ec9d63a07844610",
      "question": "who are missionary couple with houseful of children?",
      "prediction": "The provided text is a blog post from a missionary family in Haiti, detailing the devastation and the efforts of the family to provide aid and support to children. Here is the answer directly from the text:",
      "ground_truths": [
        "Tara and Troy Livesay"
      ],
      "em": 0,
      "f1": 0.0606060606060606
    },
    {
      "id": "ee882bdef05c416d8da08f506b7f0abb",
      "question": "When did wife blog?",
      "prediction": "According to Tara Livesay, the family's blog, they wrote about the devastation from Tuesday's earthquake and the outpouring of local and international support. They also report on the world wide village's charitable work, the state of Haiti's education, healthcare, nutrition, and micro-enterprise opportunities, and the efforts of children and families in Haiti. They also describe the lack of a single, dedicated, and \"best-of-all\" type of cry for humanity's attention, and the profound and beautiful sound of faith and good news. They mention the work of the household in Port-au-Prince, and the",
      "ground_truths": [
        "Wednesday morning."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c8145ebc165f4377b68d53d8e91debdf",
      "question": "Where does Troy issue tweets from?",
      "prediction": "The answers to the question are based on the context provided.",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e89f382568a945d8bed79a01ca36defc",
      "question": "What are the couple?",
      "prediction": "The answer is:",
      "ground_truths": [
        "country directors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7fc7459e586348a193b39ef43b3e57bd",
      "question": "who is reputed leader of klan?",
      "prediction": "The answer is \"Raymond \"Chuck\" Foster,\" referring to the leader of the Klan Group.",
      "ground_truths": [
        "Raymond \"Chuck\" Foster"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "da428c5bcb424a7d895a78bed863d644",
      "question": "Where is Cynthia Lynch from?",
      "prediction": "The answer is Louisiana, Louisiana.",
      "ground_truths": [
        "Tulsa, Oklahoma."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "206318a41226426c9daef8ac3ed2ceca",
      "question": "What is the KKK?",
      "prediction": "The KKK is a religious group known for its abolitionist and Klan ideology.",
      "ground_truths": [
        "Ku Klux Klan"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "51ef7c72120942d287ad51db83ccd02b",
      "question": "What are Americans doing as a way to cope with tough economic times?",
      "prediction": "The answer is, Hayes is going to visit Cape Cod.",
      "ground_truths": [
        "bartering"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ff3f2ec9a814df7b4dfaea27fcdd29c",
      "question": "When does bartering rise dramatically?",
      "prediction": "The answer is, in the context of a trip to Cape Cod, when the cost of lodging was $200, or more, and the dream of a vacation seemed out of reach, the dream of bartering, as a way to cope with tough economic times, has increased.",
      "ground_truths": [
        "\"When the economy turns unfriendly,"
      ],
      "em": 0,
      "f1": 0.04761904761904762
    },
    {
      "id": "db9d9939947742aaa3a4f8c7cd946cef",
      "question": "Who is the barter network president?",
      "prediction": "The answer is, the Bushnell and Brown are the Barter Network presidents, and they are the leaders of the American Bartering Network.",
      "ground_truths": [
        "Michael Krane,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9bb38adb3e7e4ee2ba2c7cfbe892ab34",
      "question": "What does bartering involve?",
      "prediction": "At the end of the answer, the answer, based on the context, states that \"I have traveled a lot in prior years, but after [Hurricane] Katrina, there's just no money for traveling,\" and that she still plans to go, but she will not spend any cash at all on lodging.",
      "ground_truths": [
        "trading goods and services without exchanging money"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "2f0f14b7f5824a639aa29f309aa3e903",
      "question": "What does barterng involve?",
      "prediction": "In the context, the answer is: **bartering.**",
      "ground_truths": [
        "trading goods and services without exchanging money"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "82675ed3f91146cda526915da1211f18",
      "question": "What are Americans doing to cope with tough economic times?",
      "prediction": "The answer is, they are bartering goods and services, not money.",
      "ground_truths": [
        "bartering"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "58b38431dd6a401194b71fafd8326ed9",
      "question": "who is not  considered an African-American woman for high court?",
      "prediction": "The answer is \"A Democratic pollster told me that black women have a higher voter turnout than any other ethnic-gender demographic.\"",
      "ground_truths": [
        "Elena Kagan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e772aa8945a4d19aa6b80ab41e15ef1",
      "question": "what are they ticked off ab out",
      "prediction": "The answer is: The nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens has caused a stir on the political left and right, as operatives on both sides try to ascertain exactly where she stands.",
      "ground_truths": [
        "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul"
      ],
      "em": 0,
      "f1": 0.5490196078431372
    },
    {
      "id": "6c064ea2080147e797731b9e36ad5834",
      "question": "What has ticked off a number of leaders?",
      "prediction": "The answer to the question is: Obama has been appointed to the Supreme Court.",
      "ground_truths": [
        "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul"
      ],
      "em": 0,
      "f1": 0.23999999999999996
    },
    {
      "id": "a3961cb61ec74837b21fa52a4f6b518b",
      "question": "what woman has he considered",
      "prediction": "The answer is: Black civil rights and political leaders.",
      "ground_truths": [
        "Elena Kagan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d98133709d87454b8868e46a888416b4",
      "question": "What does the draft report say about Iran?",
      "prediction": "The answer to the question is: \"The draft report, obtained by CNN and not yet approved by the International Atomic Energy Agency's board of governors, is the U.N. nuclear watchdog agency's strongest warning that Iran can be secretly working on a nuclear bomb, but not one that should lead the U.S. to consider a military strike against the Tehran regime. Cartoons by Fareed Zakaria, the author and host of CNN's \"Fareed Zakaria GPS,\" confirm this, stating the report should spur U.S. diplomacy to prevent Iran from developing nuclear weapons but that talk by commentators outside the U.",
      "ground_truths": [
        "could be secretly working on a nuclear weapon"
      ],
      "em": 0,
      "f1": 0.10869565217391305
    },
    {
      "id": "ff6c6eecb3414370b325b900070c2711",
      "question": "What could Iran be secretly working on?",
      "prediction": "The context provided is not directly stated in the text. Therefore, I cannot provide the answer to the question based on the context.",
      "ground_truths": [
        "nuclear weapon"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28c41ce5339b42849560e5551160bf9a",
      "question": "Who said the report is a major development?",
      "prediction": "The answer is: Not definitively.",
      "ground_truths": [
        "Fareed Zakaria."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "536a0720343f436abf39e0409ac9beae",
      "question": "what did zakaria say",
      "prediction": "The answer is:",
      "ground_truths": [
        "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "168b2329b8504ce2a0640c61e14ff536",
      "question": "What is not on the side of the Iranian regime?",
      "prediction": "The answer is: **The Iranian government is not on the side of the U.S.**",
      "ground_truths": [
        "Iran's Green Movement of protesters"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "2a38dff494494797839d2adfe6875577",
      "question": "What does he talk of against Iran?",
      "prediction": "The provided text is a transcript of a CNN interview with Iran's Green Movement of protesters against the regime. The interview focuses on the government's findings of a nuclear weapon development, the \"dangerous\" actions of the United States, and the continued focus on military action. The key themes of the conversation are:",
      "ground_truths": [
        "military strike"
      ],
      "em": 0,
      "f1": 0.047619047619047616
    },
    {
      "id": "d197ff9d87cc47c4ae12db152196fa35",
      "question": "what was the report about",
      "prediction": "The provided text is a news report from CNN, and it does not contain any additional information. Therefore, I cannot provide the answer to the question based on the context.",
      "ground_truths": [
        "Iran could be secretly working on a nuclear weapon"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "4f05df570a93435fb2fe25016fa26166",
      "question": "When was Ma Khin Khin Leh sentenced to life in prison?",
      "prediction": "The answer is, when was Ma Khin Khin Leh sentenced to life in prison in Myanmar. The context states that the sentenced was extreme, and the final sentence was twice as severe.",
      "ground_truths": [
        "July 1999,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12039f9c08bf4751afd6c8396ffecaf4",
      "question": "How many prisoners were freed from Myanmar?",
      "prediction": "The answer is 18.",
      "ground_truths": [
        "Nineteen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "efefcf436f894cbd94bff1c8b03a7458",
      "question": "Where was leader Aung San Suu Kyi confined to?",
      "prediction": "Where was Aung San Suu Kyi was confined to Myanmar. She was in her home in Ineung, Myanmar.",
      "ground_truths": [
        "in her home"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "bd01ef65a2bd4c56accb1f2446318bf9",
      "question": "Number of politicl prisoners freed in Myanmar?",
      "prediction": "The answer is 18.",
      "ground_truths": [
        "Nineteen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "54e2bcd8dc47489a988fd7647b6c6126",
      "question": "Where were the school teacher and 18 other political prisoners freed from?",
      "prediction": "The answer is: Where were the school teacher and 18 other political prisoners freed from Myanmar.",
      "ground_truths": [
        "Myanmar"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "7f334092620f45bba5fa99140c443e3c",
      "question": "Who is still confined to home?",
      "prediction": "The answer is **Maa Khin Leh**.",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e45c3ad14df4921bb75492b6ec4275c",
      "question": "Who was confined to his home?",
      "prediction": "The answer is: Ma Khin Khin Leh was confined to his home.",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f4d2e02ed614e1ba05e280a9d2dd39a",
      "question": "Why was Ma Khin Leh sentenced to life?",
      "prediction": "The answer is: Amnesty International USA's web blog.",
      "ground_truths": [
        "because her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3c473008c4bb49e4b6e79330fe92c32d",
      "question": "Who is the pro-democracy leader confined to her home in Myanmar?",
      "prediction": "The answer is \"The mother, a 33-year-old school teacher, was sentenced to life in prison in Myanmar.\"",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "17d54228f388411d864fcd28aaa0da8a",
      "question": "Who wa sentenced to life?",
      "prediction": "The answer is, \"In Myanmar, the life sentence was extreme.\"",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8557bf45257f477c9da97b4c04560acd",
      "question": "Who was sentenced to life?",
      "prediction": "The answer is \"Myanmar's military rulers have been widely condemned for their human rights abuses.\"",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "094d74e7455d401f84ca459c40084cb5",
      "question": "Who is among the 19 prisoners?",
      "prediction": "Among the 19 prisoners, the answer is **Maha Khin Leh**.",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "e1dac33266d048c89894eab0b0dc1c2a",
      "question": "What is being planned?",
      "prediction": "Answer: The planned Kingdom City project is being planned at Jeddah, Saudi Town of Jeddah, and Dubai, United Arab Emirates.",
      "ground_truths": [
        "tallest building,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e3b57d9763d4f1da04cc784d5944be2",
      "question": "What will reach over a kilometer in height?",
      "prediction": "Answer: The predicted height of the World's tallest skyscraper in the world is 3,281 feet.",
      "ground_truths": [
        "tallest building,"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "695bcb8dd90046bba77ed4fe322db935",
      "question": "a new generation of what?",
      "prediction": "The answer is \"New Generation of Innovative Skyscrapers Set to appear all over the world.\"",
      "ground_truths": [
        "innovative, exciting skyscrapers"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "66671cea0dee4e95ace4c4ccbdcc094b",
      "question": "Which towers have good designs in the US?",
      "prediction": "The answers to the question follow the context:",
      "ground_truths": [
        "highlight of the rebuilt World Trade Center."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9d30d695b524637976deff70f6a7afd",
      "question": "What height will Kingdom City be?",
      "prediction": "Answer: Kingdom City will be at 23 million square meters (248 million square feet) and will be at least 1 kilometer (3,281 feet) tall.",
      "ground_truths": [
        "over a kilometer (3,281 feet)"
      ],
      "em": 0,
      "f1": 0.21428571428571427
    },
    {
      "id": "e3ece1d381234fa9a9402f702b38c5b5",
      "question": "What will the height of Kingdom City be?",
      "prediction": "The height of Kingdom City, a planned 1km tall skyscraper, will be 23 million square feet (248 million square feet). The project, entitled Kingdom City, will also be 23 million square feet.",
      "ground_truths": [
        "over a kilometer (3,281 feet) high."
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "8b0e7ccc65914a2c8d7cbc43b114b06b",
      "question": "How many hostages did the Farc hold?",
      "prediction": "The FARC held 750 hostages in Colombia.",
      "ground_truths": [
        "750"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "8586cbf2042d473196cb9cd0622be798",
      "question": "Who was kidnapped August 4?",
      "prediction": "The answer is:  \"Isaza\" (FARC)",
      "ground_truths": [
        "Oscar Tulio Lizcano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6b7e8fbd8eb7489983dbfac313c5124e",
      "question": "What does FARC stand for?",
      "prediction": "FARC stands for **Colombia's Revolutionary Armed Forces of Colombia**.",
      "ground_truths": [
        "Revolutionary Armed Forces of Colombia,"
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "f4dae4387c854583a99555dab762a1db",
      "question": "Who kidnapped the ex-congressman?",
      "prediction": "The kidnapping of Oscar Tulio Lizcano was committed to mass arrest in the jungle of Colombia, with the help of a former captor.",
      "ground_truths": [
        "leftist rebels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46fd622e5ff544c29074182e84798594",
      "question": "In what country are the estimated 750 hostages being held?",
      "prediction": "The estimated 750 hostages are being held in Colombia.",
      "ground_truths": [
        "Colombia."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "d7ccb38cca004c16af0889a565db7996",
      "question": "When was Lizcano kidnapped?",
      "prediction": "The answer is **August 4, 2000**.",
      "ground_truths": [
        "August 4, 2000"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "d93fb638308e48419553c46f6fd4582e",
      "question": "How many hostages does the FARC hold?",
      "prediction": "The FARC holds 750 hostages in Colombia.",
      "ground_truths": [
        "750"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "b69d29e4720248f283d9ab2913ce47e1",
      "question": "Lizcano fled how long ago?",
      "prediction": "The answer is \"about three days ago.\"",
      "ground_truths": [
        "about three days"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9f6d3bd15c0c4bdf963d31cccb7a8b7b",
      "question": "Who was kidnapped on August 4, 2000?",
      "prediction": "The answer is Fernando Lizcano, who was held in the jungle for more than eight years.",
      "ground_truths": [
        "Oscar Tulio Lizcano"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "b7648c8b476e4482a7fa9a8d4584c551",
      "question": "What was the value of the fortune Daniel Radcliffe received when he turned 18?",
      "prediction": "The answer is: \"I'm not going to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar. I don't think I'll be particularly extravagant, and I don't think I'll be especially extravagant. I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar.\"",
      "ground_truths": [
        "¬£20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8047bb5d810b4c2f967c45b876ff4bb8",
      "question": "What age is Daniel?",
      "prediction": "Daniel Radcliffe is 18 years old. He has been granted access to a fortune for ¬£20 million, which he has not cast, so he can't make any predictions about his future earnings.",
      "ground_truths": [
        "18,"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "db63ac356f6a4df8b59087b055a45357",
      "question": "Where was the money held?",
      "prediction": "The answer is: the money was held in a trust fund, not in Harry Potter's personal savings.",
      "ground_truths": [
        "in a trust fund"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "f3fc1b6ee3b24e47856d9fe7730350a2",
      "question": "What is he going to do?",
      "prediction": "The answer is, \"I'll definitely have some sort of party,\"",
      "ground_truths": [
        "have some sort of party,\""
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "eff7102be794409081a5e3b3ef3c9518",
      "question": "What happens when Daniel Radcliffe turns 18?",
      "prediction": "Here's the answer based on the provided context:",
      "ground_truths": [
        "gains access to a reported ¬£20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2747da8e1e1416cb057ee8651131dd8",
      "question": "Which earnings were held?",
      "prediction": "The answer is: \"I'm not planning to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" and \"the things I like buying are books and CDs and DVDs.\"",
      "ground_truths": [
        "¬£20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba2b3c2aeb464d8f8a949fc75944bcbd",
      "question": "Who stars in Harry Potter?",
      "prediction": "The answer is: Harry Potter.",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f5b923f78d642ebb61f535595115ceb",
      "question": "What day is Daniel Radcliffe's birthday?",
      "prediction": "Daniel Radcliffe's birthday is on Monday.",
      "ground_truths": [
        "Monday,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "3bb0f77a9cbc44fab12fdc8005e69f91",
      "question": "Who inherits ¬£20M on Monday?",
      "prediction": "Answer: The London, England (Reuters)",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dfc54f2afa2849f4a1d9d7095fa37f8d",
      "question": "What is he saving for?",
      "prediction": "The answer is: \"I'll have some sort of party.\"",
      "ground_truths": [
        "books"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a168c89e395546be9605e969f682e554",
      "question": "What happened to all the money Radcliffe made from the Harry Potter movies?",
      "prediction": "The answer is: Radcliffe is kept firmly on the ground.",
      "ground_truths": [
        "held in a trust fund"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "69bda7a3b8cb4e2baecd4fedbe7458ce",
      "question": "Where did the money in the trust fund come from?",
      "prediction": "The answer is \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" and \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" and \"people are always looking to say 'kid star goes off the rails,'\" and \"I've definitely had some sort of party,\" and \"I'm certainly having some sort of party.\"",
      "ground_truths": [
        "the first five Potter films"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b6ae22e3a9d04cfbaf6206a39dd062da",
      "question": "What is Radcliffe's net worth?",
      "prediction": "A British actor, Harry Potter, has gained access to a ¬£20 million fortune as he turns 18, but he has not cast a spell on his earnings.",
      "ground_truths": [
        "¬£20 million"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "b18ad0a41f164debb57278760e011fd1",
      "question": "What are his plans with the money?",
      "prediction": "The answer is: he will definitely have some sort of party.",
      "ground_truths": [
        "books"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25b9ecd1aefd493eae1547302afed49b",
      "question": "Who is the star of Harry Potter?",
      "prediction": "The answer is \"I'm just going to be more sort of fair game.\"",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4ef4313787d49d7851ad2a6c66cf163",
      "question": "What amount of money is Daniel getting?",
      "prediction": "The answer to the question based on the provided context is:",
      "ground_truths": [
        "¬£20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb6590ce2f134b889d853696a1519eca",
      "question": "what did the young actor say?",
      "prediction": "The answer is: he will definitely have some sort of party. He said he does not plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar. He also said he does not think he will be particularly extravagant, and I-Reporter noted his excitement about the first five Potter films' success.",
      "ground_truths": [
        "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\""
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "08ff89d3a48f43799ba35c485995ff19",
      "question": "What is search of the temple expected to turn up?",
      "prediction": "The answer is \"We need to know if they have been abused or neglected.\"",
      "ground_truths": [
        "possible victims of physical and sexual abuse."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6919a1b049514471b42c88f1e89ac267",
      "question": "Where is Jeffs in jail awaiting trial?",
      "prediction": "Answer:  Where is Jeffs in jail awaiting trial.  The information about the yFZ ranch, and the search for Dale Barlow's spouse, is not provided.",
      "ground_truths": [
        "Texas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fb052cb92b64f6eadd68c2a9d16b0bb",
      "question": "What is the number of children removed from ranch?",
      "prediction": "The answer is 183.",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "df799a737f6343a8b4776b01076e3a3c",
      "question": "In which state is Jeffs jailed while awaiting trial?",
      "prediction": "The answer is Texas.",
      "ground_truths": [
        "Arizona"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cdd10c1af31544cdbe58e6e3edd89bde",
      "question": "How many children were removed?",
      "prediction": "The answer is 183.",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0dbf719fe2f54987b1118862ace2d1fb",
      "question": "What is the number of people removed from ranch?",
      "prediction": "The answer is 183.",
      "ground_truths": [
        "183"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "90d73aa610df4fadad68bf28ed91ac0f",
      "question": "How many children were removed from the ranch?",
      "prediction": "The answer is 183.",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "104a4d520b854b7aaae7f12b77c8891a",
      "question": "How many people were removed?",
      "prediction": "The answer is 183.",
      "ground_truths": [
        "183"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5406135ecb9b4257807d6a25521cc062",
      "question": "How many years was Jeffs sentenced to prison last year?",
      "prediction": "The answer is 10 years.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "3db78d9616ac4d99986d3c580ad26549",
      "question": "What state jail is Jeffs waiting trial in?",
      "prediction": "The answer is Kentucky.",
      "ground_truths": [
        "Utah"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10ca15fbc0e84072a3b293df8aef3e99",
      "question": "What instrument does she play?",
      "prediction": "The answer is jazz.",
      "ground_truths": [
        "piano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4a225586c76c416faf13ff63eeac09c8",
      "question": "Whose new album is she producing?",
      "prediction": "The answer is \"Quiet Nights\" is produced by Diana Krall.",
      "ground_truths": [
        "\"Quiet Nights,\""
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "fb54542b6bc748c89b36807ab7b082b2",
      "question": "What is Diana Krall's new album?",
      "prediction": "Diana Krall's new album, \"Quiet Nights,\" was inspired by a trip to Brazil. It is a new concept for the ever-consuming artist.",
      "ground_truths": [
        "\"Quiet Nights,\""
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "a25ba84fca504b2aa4e126007a22a14f",
      "question": "what bridge did Monet draw?",
      "prediction": "The answer is: The bridge across the Thames.",
      "ground_truths": [
        "London's Waterloo"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3febc40bd3ee4d4e9226aa424b60123c",
      "question": "what room did Monet stay in at the Savoy?",
      "prediction": "The answer is: The room at the Savoy, which was the Monet Suite, was the only room in the hotel room for Monet's paintings.",
      "ground_truths": [
        "618"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "63fc6e7a70aa4d60a8cab579a90afb9e",
      "question": "From which room was Monet's Waterloo Bridge painted?",
      "prediction": "From which room was Monet's Waterloo Bridge painted?",
      "ground_truths": [
        "618"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fc9afe35d8c4cdf8115fe9faf6ac96a",
      "question": "what hotel did Monet stay at?",
      "prediction": "The hotel where Monet drew the bridge across the Thames in 110 years ago was the Savoy.",
      "ground_truths": [
        "Savoy"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "400ab40069334ea6a12b1085941a9880",
      "question": "Whose drawing is on display at the Savoy?",
      "prediction": "The answer is the Savoy's drawing of London's Waterloo Bridge, which was created 110 years ago.",
      "ground_truths": [
        "Claude Monet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b0bc5b996f8c43a4a27f7a0a92100227",
      "question": "When did the crash happen?",
      "prediction": "The crash occurred on Monday, in the Mediterranean Sea, at a depth of about 1,300 meters.",
      "ground_truths": [
        "Monday"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "721498d8723844d5b1ab0d15eb8e7699",
      "question": "what is the number of passengers?",
      "prediction": "The number of passengers is 14.",
      "ground_truths": [
        "82"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45e285d6497d478b835ec6f97fe0edb8",
      "question": "what amount of bodies have been found so far?",
      "prediction": "The amount of bodies found so far is 14.",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "b8b75f189ac042c9a664c7bc13bcb936",
      "question": "Where was black box located?",
      "prediction": "The answer is \"at a depth of about 1,300 meters in the Mediterranean Sea,\" which translates to \"the black box was located at a depth of about 1,300 meters in the Mediterranean Sea.\"",
      "ground_truths": [
        "at a depth of about 1,300 meters in the Mediterranean Sea."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cbb5d9c625b74ae1b3426d2a185549c4",
      "question": "What was found?",
      "prediction": "The answer is \"at least 14 bodies were recovered from the crash of the Ethiopian plane, which was bound for Addis Ababa, in Egypt, after 3.5 kilometers (2 miles) west of Na'ameh.\"",
      "ground_truths": [
        "flight data recorder from an Ethiopian Airlines plane"
      ],
      "em": 0,
      "f1": 0.16666666666666663
    },
    {
      "id": "614b047fc1144246af208be2a508d5ce",
      "question": "How many bodies were recovered?",
      "prediction": "The number of bodies recovered is 14.",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "8dcf91cc88c7478fb336f2b856b1f9b8",
      "question": "Where was the flight going?",
      "prediction": "The flight was going to Addis Ababa, Africa.",
      "ground_truths": [
        "Addis Ababa,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "e45fbf36c6d64467b16f13324d22a4c2",
      "question": "where was the back box found?",
      "prediction": "The back box was found at the depths of the Mediterranean Sea, about 1,300 meters.",
      "ground_truths": [
        "at a depth of about 1,300 meters in the Mediterranean Sea."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4ae06e8228ca4ca89224a8598de5e6d9",
      "question": "What did 26 year old return from doing?",
      "prediction": "The answer is 26.",
      "ground_truths": [
        "after giving birth to baby daughter Jada,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47e690d71a1648aba0e8ee4f18ede013",
      "question": "How long did the final last and what was the score",
      "prediction": "The final of the Sony Ericsson Open in Miami held for 26 years. Kim Clijsters, who won her third title in 2005, scored her third win in 58 minutes at Key Bisc la, achieving a score of 29 unforced errors. Williams, a 26-year-old, was the 2009 U.S. Open champion, who scored 37 points, with a score of 28 unforced errors. Clijsters' team achieved a score of 29 unforced errors, making her 12th place in the rankings.",
      "ground_truths": [
        "6-2 6-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f764f3b545744048cf82108c4bb5157",
      "question": "What did Belgian defeat American in?",
      "prediction": "The Belgian defeated Venus Williams in the final of the Sony Ericsson Open in Miami, Miami, USA.",
      "ground_truths": [
        "final of the Sony Ericsson Open"
      ],
      "em": 0,
      "f1": 0.5263157894736842
    },
    {
      "id": "5bdc71bbfc6e4934aa337ab24559eb6c",
      "question": "Who did Kim demolish?",
      "prediction": "Clijsters (Kim Clijsters) continued her successful return to tennis, demolishing Venus Williams in the final of the Sony Ericsson Open in Miami.",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "354948edd4c94ed49d3c744a8213b0e4",
      "question": "who beat williams",
      "prediction": "The answer is: **Kim Clijsters.**",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "4fed7ee44ec9407ebf2f05e33d2f6d54",
      "question": "what was the score",
      "prediction": "The answer is: Kim Clijsters won the US Open, achieving her third title in WTA Tour events. She also won her 2009 U.S. Open title after a 6-2 victory in her first match.",
      "ground_truths": [
        "6-2 6-1"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "ec0a4212a5df47f682fbba53188e7af8",
      "question": "How many titles has the winner of the tournament collected on the tour",
      "prediction": "The answer is: **28**",
      "ground_truths": [
        "third"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0178feda5ef74636ba6e5ba11135ceb6",
      "question": "Which player did Venue Williams lose to in the Miami final",
      "prediction": "The answer is: **Kim Clijsters.**",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "ff83ee0ffed94025b218bc5444b14880",
      "question": "who had a baby",
      "prediction": "The answer is **Clijsters, who returned to tennis after a two-year absence, will move up to 10th in the rankings next week, following her one-sided 6-2 final at Key BiscNeal. She will then move to 10th in the rankings next week.**",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b509458380a84ed5bdbaa84a6a4d1b86",
      "question": "who is al-moayad",
      "prediction": "The answer is Al-Moayad.",
      "ground_truths": [
        "a Yemeni cleric"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57973e7ba373436eb1374990545bcf11",
      "question": "What did the court say?",
      "prediction": "The court said that al-Moayad and Zayed were not received a fair trial.",
      "ground_truths": [
        "not receive a fair trial."
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "f8c03f10b27045d89e068c461ea3d69c",
      "question": "Who did al-Moayad boast about giving money to?",
      "prediction": "The answer is Al-Moayad, who was in his 60s.",
      "ground_truths": [
        "al Qaeda leader Osama bin Laden."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d53777d8a5b407f9330e6fdd7e43214",
      "question": "who supported terrorism?",
      "prediction": "The answer is the New York (CNN) case.",
      "ground_truths": [
        "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25223706065646aebcebc831d267c30d",
      "question": "Who gave money to Osama bin Laden?",
      "prediction": "The answer is al-Moayad, who is in his 60s.",
      "ground_truths": [
        "al-Moayad"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "43204be4d77d421990db36f2d9951e63",
      "question": "What denied the pair a fair trial?",
      "prediction": "The answer is: The pair were arrested in 2003 in a sting operation in Germany.",
      "ground_truths": [
        "prosecutors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca8d39a6862e49b58a03a0da02a7bede",
      "question": "what does court say",
      "prediction": "The court says that the appeals court has vacated the convictions of al-Moayad and Zayed because they were found to have been prejudicial and the trial's outcome was legally and factually correct.",
      "ground_truths": [
        "they did not receive a fair trial."
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "cc5ecf6df2414809b12dd513fdfa7935",
      "question": "Who was convicted of supporting terrorism?",
      "prediction": "The answer is al-Moayad al-Moayad.",
      "ground_truths": [
        "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "afe6d5bb126344f7aed2447a11235964",
      "question": "who is Khalid Sheikh Mohammed ?",
      "prediction": "The answer is Khalid Sheikh Mohammed.",
      "ground_truths": [
        "the mastermind behind the September 11, 2001, terrorist attacks on the United States."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f7b064fa9fe497abdc626bc2f315179",
      "question": "What happened to Khalid Sheikh Mohammed?",
      "prediction": "The answer to the question based on the context is: \"Khalid Sheikh Mohammed was waterboarded 183 times in a month in March 2003.",
      "ground_truths": [
        "was waterboarded 183 times in a month,"
      ],
      "em": 0,
      "f1": 0.4799999999999999
    },
    {
      "id": "87fae21d4ba64ce39473e53303b9a010",
      "question": "What did former CIA officer say?",
      "prediction": "The answer is: Barack Obama described the memos as \"exceptional circumstances surround these memos and require their release.\"",
      "ground_truths": [
        "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day."
      ],
      "em": 0,
      "f1": 0.05555555555555556
    },
    {
      "id": "10711b8a6a60450f97f1d8c41c04b6a1",
      "question": "Who denounced the decision?",
      "prediction": "The answer is:  The memo, dated May 30, 2005, was from then-Deputy Assistant Attorney General Steven G. Bradbury to John Rizzo.",
      "ground_truths": [
        "Hayden"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "24b16656c4004ad897fb617b063b9d13",
      "question": "who did denounces decision to release memos?",
      "prediction": "The answer is: the CIA.",
      "ground_truths": [
        "Hayden"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2b5d2e3dba846f2b501f617fbda65eb",
      "question": "Where did the passenger purchase airline tickets from?",
      "prediction": "The passenger purchased airline tickets from Expedia.",
      "ground_truths": [
        "Expedia."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "0b6a31e91402412db37498cbbf45f68c",
      "question": "What did the passenger purchase?",
      "prediction": "The answer is: **(the ticket to Italy)**.",
      "ground_truths": [
        "two tickets to Italy"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "50df16ae2a574f7c9140cd8242b98a90",
      "question": "What travel company issued a corrected ticket?",
      "prediction": "The answer is Expedia.",
      "ground_truths": [
        "Expedia."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "af70919c8e954162a4fed9325ffff0d4",
      "question": "What was misspelled when received the tickets?",
      "prediction": "The misspelled word is \"Crista,\" and it is needed to be resolved.",
      "ground_truths": [
        "his wife's name,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e089e57101b24a09865fd079c8a853be",
      "question": "Who faces 22 felony counts in connection with sex tape?",
      "prediction": "The answer is **Chester Arthur Stiles, 38, faces 22 felony counts in connection with the videotape, including lewdness with a child, sexual assault with a minor, and attempted sexual assault with a minor.**",
      "ground_truths": [
        "Chester Arthur Stiles,"
      ],
      "em": 0,
      "f1": 0.19354838709677416
    },
    {
      "id": "b267cf430ab441cc8fdb4fe5b62f89ae",
      "question": "Who discovered the tape?",
      "prediction": "The answer is, \"The media for help and the community that they want to do the right thing.\"",
      "ground_truths": [
        "Darrin Tuck,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d22bea148e76450886164c2c2b46c92c",
      "question": "What is the evidence that shows a girl under 3 being assaulted?",
      "prediction": "The answer is \"The American Dream,\" a state of childhood joy and innocence.",
      "ground_truths": [
        "videotape"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8eb2b0d07894b3ca6980fd85b5c1815",
      "question": "How many felony counts does Stiles face?",
      "prediction": "The answer is 22.",
      "ground_truths": [
        "22"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2f13bf99fea7424189c6428348a832bb",
      "question": "What group of people know about the case and hae strong feelings about it?",
      "prediction": "The answer is Chester Arthur Stiles, 38, facing life imprisonment for videtaping a sexual assault on a child.",
      "ground_truths": [
        "a jury"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb6fb9e713e94b0c85efed7896bb364b",
      "question": "What did the tape show?",
      "prediction": "The tape showed images of the girl, who was sexually assaulted.",
      "ground_truths": [
        "images of the small girl being sexually assaulted."
      ],
      "em": 0,
      "f1": 0.6250000000000001
    },
    {
      "id": "04b44e8378454db2b175693882d6f37e",
      "question": "How many felony counts is Arthur Stiles facing?",
      "prediction": "Answer: 12",
      "ground_truths": [
        "22"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "efca00cb58c04369b44b7edd3d9ab167",
      "question": "Who was charged with slaying the woman?",
      "prediction": "The answer is \"Philip Markoff\" (with the \"New York City resident Julissa Brisman\").",
      "ground_truths": [
        "Philip Markoff, a pre-med student at Boston University"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "2c4feb40f8d84a8abd45dbd71fe093e4",
      "question": "who makes public plea?",
      "prediction": "The answer is \"Police.\"",
      "ground_truths": [
        "Boston Police Department,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "da43ca226cb640228c78e6bec528b1fe",
      "question": "who was charged",
      "prediction": "The answer is: \"Philip Markoff.\"",
      "ground_truths": [
        "Philip Markoff,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "cda5c1184ed6422c9a1e554586f6befd",
      "question": "what was he charged with",
      "prediction": "The answer is:  \"It is a compelling case with a myriad of evidence, with computer evidence being part of it,\"",
      "ground_truths": [
        "murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b1ae9084cebb44b48a69c1c52e6408aa",
      "question": "What else was the suspect charged with?",
      "prediction": "The suspect was charged with the armed robbery of New York City resident Julissa Brisman, 26, who was found unconscious with multiple gunshot wounds.",
      "ground_truths": [
        "the armed robbery and kidnapping of another victim,"
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "075c87d32f284998931c5db62c495931",
      "question": "what did the police say",
      "prediction": "The police said that a 22-year-old college student, in Boston, Massachusetts, had been charged with murder in connection with the death of a woman, who may have been contacted through a Craigslist ad, and that police found Julissa Brisman unconscious with multiple gunshot wounds on April 14th. The police found her killed in Copley Marriott Hotel on April 14th, in her injuries shortly afterwards.",
      "ground_truths": [
        "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "46e63673f28e492f89d73d10df691d85",
      "question": "whom did police charge?",
      "prediction": "The police charge is **Col. Dan Conley**, and he is charged with the armed robbery of a 29-year-old woman, Julissa Brisman.",
      "ground_truths": [
        "Philip Markoff,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b82861fa9c7a4b6ab291c2ec24540604",
      "question": "The suspect is how old?",
      "prediction": "The suspect's age is unknown.",
      "ground_truths": [
        "22-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "05528848df284bf1bf0221430f701239",
      "question": "What does the columnist describe draper as?",
      "prediction": "The columnist's description of Don Draper as a complex man with \"confused, secretive, and calculating personalities\" (referring to his onscreen paramours and the show's viewers) is a key to understanding his \"drool-worthy\" nature. The columnist also notes that Don Draper's \"adventuring\" into the \"Mad Men\" landscape, highlighting his \"complex, deep-seated, and flawed man\" personality.",
      "ground_truths": [
        "complicated man"
      ],
      "em": 0,
      "f1": 0.039999999999999994
    },
    {
      "id": "ea4395e24790427eb9016fdc41d97557",
      "question": "What did columnist describe Draper as?",
      "prediction": "The columnist described D'Rage as a complex and flawed man, a columnist warns that his \"coagged exterior\" makes him difficult to live with.",
      "ground_truths": [
        "sexy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f07632e29e364346a67e7e83b6ba57c5",
      "question": "what is frustrating?",
      "prediction": "The answer to the question is: \"what is frustrating.\"",
      "ground_truths": [
        "the refusal or inability to \"turn it off\""
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "24fbb9ffcb9f4ac89a61f53531515f7b",
      "question": "What year was Bhutto's father hanged?",
      "prediction": "The answer is October 16, 1996.",
      "ground_truths": [
        "1979"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c204c9bbf71046878a9b4ea3c122cd9b",
      "question": "What happened to Bhutto in October?",
      "prediction": "Based on the provided text, the answer is: **She returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.**",
      "ground_truths": [
        "narrowly escaped injury"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4df5296566b641b0adc46f5ee2458181",
      "question": "What happened to her father?",
      "prediction": "The answer is Benazir Bhutto died in Pakistan, after a suicide bombing at a political rally in Rawalpindi.",
      "ground_truths": [
        "hanged in 1979"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "de80389f9b874879bd8503f3a8f9d0d9",
      "question": "Who attempted to assassinate Bhutto in October?",
      "prediction": "The context states that Benazir Bhutto was the first female prime minister of Pakistan and of any Islamic nation.",
      "ground_truths": [
        "suicide bombing"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f9377b97fc04b368c8a59a88819df6b",
      "question": "Who was the first female prime minister of a Muslim country?",
      "prediction": "The answer is Benazir Bhutto.",
      "ground_truths": [
        "Bhutto,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f0cb1630307f4842b035bb378e164d3e",
      "question": "Who was hanged in 1979?",
      "prediction": "The answer is **Benazir Bhutto**.",
      "ground_truths": [
        "Zulfikar Ali Bhutto,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "8415726510464e069a79294889f5578c",
      "question": "What did Bhutto refuse to allow?",
      "prediction": "The answer is: she refused to allow assassins to travel with tinted windows.",
      "ground_truths": [
        "assassins"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "3707d857e2c84a94ba2aa8f965ebe359",
      "question": "What is Georgia supposed to join?",
      "prediction": "The answer to the question is: Russia is not a NATO member.",
      "ground_truths": [
        "NATO"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "4fe29932e9544f579ad4de4582172344",
      "question": "what do european countries do",
      "prediction": "The answer is: U.S. and Russia express concerns about the missile defense system, and the US, Russia, and Ukraine are all hopeful for a successful and NATO-led defense system to mitigate Russian missile attacks.",
      "ground_truths": [
        "expressed concerns about the missile defense system."
      ],
      "em": 0,
      "f1": 0.2777777777777778
    },
    {
      "id": "ad4834d86052486e93a0267fa03a013d",
      "question": "what did bush say",
      "prediction": "The answer is: \"We are working hard to ensure Russia's participation in NATO's Membership Action Plan, and we are not going to allow Moscow to block our inclusion.\"",
      "ground_truths": [
        "A planned missile defense system in Eastern Europe poses no threat to Russia,"
      ],
      "em": 0,
      "f1": 0.10256410256410256
    },
    {
      "id": "4f7bda2de72f480fad80c4e296528d29",
      "question": "What do European countries share?",
      "prediction": "Here's the answer, based on the context:",
      "ground_truths": [
        "Russian concerns that the defensive shield could be used for offensive aims."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bd6887183bd4b53aaf174debcb3b3f1",
      "question": "to where they carried the injured?",
      "prediction": "The answer is 116 American tourists.",
      "ground_truths": [
        "Nasser Medical Institute in Cairo,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a36a9969dee14e6f90fc1785e4d8151a",
      "question": "How many tourists are injured?",
      "prediction": "The answer is 116.",
      "ground_truths": [
        "19"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f98e25c885246da8bf351307de869b3",
      "question": "What is \"A Lion Among Men,\" about?",
      "prediction": "The answer to the question is: \"A Lion Among Men,\" a children's book about the life of the Cowardly Lion.",
      "ground_truths": [
        "-- the motherless cub defended by Elphaba in \"Wicked.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb216937fe904ea8bd27fa37bed51ea0",
      "question": "What did Maguire write for 14 years?",
      "prediction": "Maguire wrote \"A Lion Among Men,\" a book that drew in millions of people in New York alone. It is not an allegorical story. It was a commentary on contemporary society, disguised as children's stories, at the expense of the classic \"The Wonderful Wizard of Oz.\"",
      "ground_truths": [
        "children's books"
      ],
      "em": 0,
      "f1": 0.048780487804878044
    },
    {
      "id": "7e2d378b24594539b665d54846855743",
      "question": "How many Maguire books of Wicked has he sold?",
      "prediction": "Maguire's answer to CNN's question about his writing career reveals a mix of:",
      "ground_truths": [
        "2.5 million copies,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3d122f0ff5048a9ae7793e6253fab48",
      "question": "How long has Maguire been writing childrens's books?",
      "prediction": "The answer to the question is: Maguire has written children's books for 14 years.",
      "ground_truths": [
        "14 years"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "2c523aea52aa481e9ce5bb6248498fde",
      "question": "What is the name of Gregory's new book?",
      "prediction": "The answer to the question is \"A Lion Among Men,\" a story about the Cowardly Lion.",
      "ground_truths": [
        "\"A Lion Among Men,\""
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "2297d8f94e90417abf39aea108b1b108",
      "question": "How many years did Maguire write children's books?",
      "prediction": "The answer to the question is: \"I have written children's books for 14 years.\"",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "fd23f19adb1c4bfb8e746e18835fb514",
      "question": "How many copies has \"Wicked\" sold?",
      "prediction": "The answer to the question is: \"Wicked\" has sold over 2.5 million copies worldwide.",
      "ground_truths": [
        "2.5 million"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "63ac58f147fb47969c3b84de7e517fd8",
      "question": "How many times did the novel 'Wicked' sell?",
      "prediction": "The answer to the question is: \"Wicked\" sold more than 2.5 million copies and was seen by over 3.75 million people in New York alone.",
      "ground_truths": [
        "\"Wicked,\" has sold more than 2.5 million copies,"
      ],
      "em": 0,
      "f1": 0.4516129032258065
    },
    {
      "id": "561ae3af3c364b27a82dd5560b22555e",
      "question": "What is Gregory Maguire's new book about?",
      "prediction": "Gregory Maguire's new book, \"A Lion Among Men,\" tells the story of the Cowardly Lion, who defends his motherless cub in \"Wicked,\" recounting the experiences of the motherless cub, Elphaba, and her actions in rescuing the cub, highlighting the challenges of the animal kingdom and the complexities of leadership.",
      "ground_truths": [
        "tells the story of the Cowardly Lion"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "794ebaee3f574f5097807ca0e53abceb",
      "question": "Who is wanted for questioning in the death of a two-year-old girl?",
      "prediction": "The answer is: A group of boys discovered the child's body partially submerged in a stream in Shark River Park in Monmouth County, New Jersey, on Tuesday afternoon.",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dcd806e325b74372a9b842d8cddb0281",
      "question": "where was the child's body found?",
      "prediction": "The child's body was found in Shark River Park, Monmouth County, New Jersey.",
      "ground_truths": [
        "in a stream in Shark River Park in Monmouth County"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "e6a8249c106042fba956548da2b42160",
      "question": "Who is wanted for questioning in the death of a two year old girl?",
      "prediction": "The answer is \"A group of boys discovered the child's body partially submerged in a stream in Shark River Park in Monmouth County, New Jersey, on Tuesday afternoon, and officials said it was still strapped in her car seat.\"",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "466e7f5760a1471b923c3d19b6587f40",
      "question": "What is he wanted for",
      "prediction": "The answer is \"a number of federal and state agencies, including the FBI, are involved in the search.\"",
      "ground_truths": [
        "questioning in the death of a two-year-old girl,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "413b110ca14c475ea7cf2363ab405203",
      "question": "Where was the body discovered",
      "prediction": "The final answer is corroborated by the FBI.",
      "ground_truths": [
        "in a stream in Shark River Park"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16f229e67d1b4807bd7d75bf021fb6b2",
      "question": "The child was last seen with who?",
      "prediction": "The child was last seen with someone who is believed to be Morgan's biological daughter.",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d8bba20206e4231bde074aad69904b4",
      "question": "who is victor?",
      "prediction": "Victor Manuel Mejia Munera was killed in Colombia.",
      "ground_truths": [
        "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,"
      ],
      "em": 0,
      "f1": 0.4210526315789474
    },
    {
      "id": "9cba4af1e5634c5096943b274940a517",
      "question": "Who did the defense minister issue a warning to ?",
      "prediction": "The answer is: The Colombian Defense Minister issued a warning to the presidential statement, saying that drug traffickers who do not turn themselves over to the government will \"end up in the jail or in a tomb.\"",
      "ground_truths": [
        "drug traffickers"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "b8ea1edfd43d41c3bf0bc5317a11b7d5",
      "question": "What was found with a dead drug lord ?",
      "prediction": "The answer is: **Vicu-Mera Munera** is identified as a Colombian-based Caracol Radio, a drug lord with ties to paramilitary groups.",
      "ground_truths": [
        "short- and long-range weapons"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4034aa4e80a64b048d230e50b51bbe69",
      "question": "Where was Manuel Munera on the wanted list ?",
      "prediction": "The answer is Colombia.",
      "ground_truths": [
        "Colombia's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da732bb77e624a11a31f1146aeabdd2c",
      "question": "What did the government find?",
      "prediction": "The government found that Victor Manuel Mejia Munera, a drug lord with ties to paramilitary groups, was killed, and two bodies were found in a farm in Antioquia, Colombia.",
      "ground_truths": [
        "identity documents"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "907592fbdbc74aef8328e9af8a826aeb",
      "question": "What title did Eikenberry hold whilst he served time in Kabul",
      "prediction": "The answer is:",
      "ground_truths": [
        "U.S. security coordinator"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f21b85d55c34ce88a2df48a109f1a9c",
      "question": "When did Eikenberry advise the secretary of defense?",
      "prediction": "The answer is: The U.S. Embassy.",
      "ground_truths": [
        "for strategy, plans and policy on the Army staff."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ba1bd3f1dc54887910a00880e903024",
      "question": "After how many years service did Eikenberry retire from the army",
      "prediction": "The answer is: Eikenberry retired from the Army after nearly 40 years of service with the rank of lieutenant general on April 28, 1 day before he was sworn in as ambassador.",
      "ground_truths": [
        "nearly 40"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "1abb7082fac043689955b298c4454bd9",
      "question": "What did Eikenberry do?",
      "prediction": "The answer is: he served two tours in the country as a general, training Afghan police and troops before trading his uniform for a diplomat's business suit.",
      "ground_truths": [
        "sent private cables to Obama last week,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab977ab989cf498ea08ccbf421490729",
      "question": "what did he advise US secretey of defense",
      "prediction": "The answer is: **Kierneberry advised the U.S. secret services of defense to focus on strengthening the U.S. military, improving its effectiveness, and addressing the challenges of Afghanistan, and provided the U.S. Secretary of Defense with recommendations on China, Taiwan, Hong Kong, and Mongolia, and the Army staff.**",
      "ground_truths": [
        "China, Taiwan, Hong Kong and Mongolia,"
      ],
      "em": 0,
      "f1": 0.2553191489361702
    },
    {
      "id": "467580cb39bf4886805860713e5a2ccc",
      "question": "Name the four countries that he advised the US secretary of defense on",
      "prediction": "The answer is: The US secretary of defense on China.",
      "ground_truths": [
        "China, Taiwan, Hong Kong and Mongolia,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "a92b72e3800a4803afb6e038e0b3d9fb",
      "question": "who retired from the army",
      "prediction": "The answer is: **He retired from the Army after nearly 40 years of service with the rank of lieutenant general on April 28, and one day before he was sworn in as ambassador.**",
      "ground_truths": [
        "Eikenberry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47159fd3a28d47b38edc4ec20f19f4d6",
      "question": "What is Apples iphone 4s voice assistant feature called?",
      "prediction": "The answer is \"It sure looks like rain today,\" as Apple's new phone, the iPhone 4S, is a model with a new function called Siri.",
      "ground_truths": [
        "Siri."
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "a42a58af650a4ec3b6948870d36e95cd",
      "question": "What was siri based on?",
      "prediction": "The answer is: Siri is a program that understands and responds to voice commands using a key to unlock the phone's internal memory. The program uses natural language processing techniques to understand and respond to user requests.",
      "ground_truths": [
        "artificial intelligence."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc2454ca01d44e4e9965d90d5b9f024d",
      "question": "What is siri based on",
      "prediction": "The answer is: Siri is based on artificial intelligence.",
      "ground_truths": [
        "onstage demos."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e72ba2f7e7f54f4d921c0c7049f1c42f",
      "question": "When did apple acquire siri",
      "prediction": "The answer is \"Apple's only model with a sort of robot living inside.\"",
      "ground_truths": [
        "April 2010."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33e4070dbdca45ebb435ba3b44c71771",
      "question": "What is the name of the voice-assistant?",
      "prediction": "The answer is \"I'm not sure. I can't provide a definitive answer to the question because it depends on the context.\"",
      "ground_truths": [
        "Siri"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c21991db995941fa91aa5f02a615503e",
      "question": "What station did the train leave from?",
      "prediction": "The train left from Liverpool Street Station.",
      "ground_truths": [
        "Liverpool Street"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "91ff6f650b2944f3b449902d2bbb724a",
      "question": "What number of suspects are in the videos?",
      "prediction": "The number of suspects in the videos is 2.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ae0c09062ab4d87baaf4edd736aba4e",
      "question": "Where does the train leave?",
      "prediction": "The train leaves Liverpool Street Station in London, England.",
      "ground_truths": [
        "Liverpool Street Station"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "c95da94fd3d949e7ac8a4f73bd5e4452",
      "question": "What did the jurors see?",
      "prediction": "The jurors saw the explosion of a train's circuit board in Liverpool Street Station on July 7, 2005.",
      "ground_truths": [
        "transit bombings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8f6bdcf0c3ed4a21a0e54b26d5186bb4",
      "question": "What did videos also show?",
      "prediction": "The videos also showed the explosion of a train's engines after it leaves the Liverpool Street Station, in Russell Square and Edgware Road.",
      "ground_truths": [
        "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East."
      ],
      "em": 0,
      "f1": 0.48484848484848486
    },
    {
      "id": "d802582c938f4c4589efd03f9b80490f",
      "question": "What does the video show?",
      "prediction": "The video shows the explosion of a train at Liverpool Street Station, then Russell Square and Edgware Road, and Tavistock Square.",
      "ground_truths": [
        "transit bombings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be0883a19590447b8c005d99288c16c1",
      "question": "What will the jurors see?",
      "prediction": "The jurors will see footage of the chaos and horrified reactions after the July 7, 2005 London transit bombings, which were shown to jurors Thursday in the trial of three men convicted of conspiracy in the case.",
      "ground_truths": [
        "Videos of the chaos and horrified reactions after the July 7, 2005, London"
      ],
      "em": 0,
      "f1": 0.4651162790697674
    },
    {
      "id": "736a2ec3de8641228ec051fcc9db843f",
      "question": "What number of men were charged with conspiracy?",
      "prediction": "The number of men charged with conspiracy was:",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f05dc53dcc24ed1a59fdd5d950966d0",
      "question": "What does the painting show?",
      "prediction": "The painting shows \"Nude, Green Leaves and Bust,\" from the collection of Los Angeles philanthropist Frances Lasker Brody, who died in November 1932. The painting was sold for $106.5 million at Christie's New York Auction.",
      "ground_truths": [
        "Picasso's muse and mistress, Marie-Therese Walter."
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "4d50fbdd13184f64aa70b8e68ea872d8",
      "question": "What was the previous record?",
      "prediction": "The answer is $106.5 million.",
      "ground_truths": [
        "$104,327,006"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c8c2af2e7b14a84acf550c9253961d8",
      "question": "How much did the painting sell for?",
      "prediction": "The answer is $106.5 million.",
      "ground_truths": [
        "$106,482,500"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57b5639ab5b441d9956c8bb4d17cf3e2",
      "question": "What painting sold for $106.482,500?",
      "prediction": "The answer is $106.5 million.",
      "ground_truths": [
        "\"Nude, Green Leaves and Bust\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2d0189ba8fd449889044fecb68c28b2",
      "question": "Who was Picasso's muse?",
      "prediction": "The answer is \"Marie-Therese Walter.\"",
      "ground_truths": [
        "Marie-Therese Walter."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "637157837144436a8998cd271d8babda",
      "question": "Where is the facility?",
      "prediction": "The answer is Atlanta, Georgia.",
      "ground_truths": [
        "in Salt Lake City,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93ade19467b641139f41f9fe6e0eb251",
      "question": "What did FAA say about the situation?",
      "prediction": "The FAA said that traffic delays at Atlanta's Hartsfield-Jackson International Airport were caused by communication breakdowns at a federal Aviation Administration facility in Salt Lake City, Utah, overloaded the facility's processing of data, leading to a request to process all flight-plan information through a facility in Salt Lake City, Utah, overloaded the facility.",
      "ground_truths": [
        "\"The"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "daa4cf0cb9dd4a64892c8decb071a087",
      "question": "where is the problem",
      "prediction": "The answer from CNN is directly from the context, describing the cause of the delays. The answer from the U.S. Air Force indicates that the FAA states that the issue is a processing problem at the Salt Lake facility in Utah, overloaded with data, prompting the FAA to process all flight plans and have a facility in Salt Lake City, Utah, processed requests closer to normal speed.",
      "ground_truths": [
        "a Federal Aviation Administration facility,"
      ],
      "em": 0,
      "f1": 0.03389830508474576
    },
    {
      "id": "6974b36e9aa949de8791f1545e7bef62",
      "question": "where is all flight-plan information processed",
      "prediction": "The answer is: Atlanta, Georgia (CNN)",
      "ground_truths": [
        "through a facility in Salt Lake City, Utah,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db07c15f3b7a40c7b61fa05120bfede2",
      "question": "what does faa say",
      "prediction": "The answer is \"We're just waiting for more information.\"",
      "ground_truths": [
        "\"The"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cdc323e4b9f94668b7099f2f958f2a29",
      "question": "Where is flight plan information processed?",
      "prediction": "The answer is: the Atlanta, Georgia (CNN) airport.",
      "ground_truths": [
        "Salt Lake City, Utah,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf3cb36658104db4b9fc76212bd43ef7",
      "question": "What is the problem the facility is having?",
      "prediction": "The answer is: The facility is having problems processing all flight plans for commercial and general aviation flights in the United States, the FAA's website (at the FAA's Web site), and, the last answer, the New York airport.",
      "ground_truths": [
        "processing data,"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "f67479f04dd14ff9829ca588bfb8ebb0",
      "question": "What is there an unknown number of?",
      "prediction": "The answer is: \"No, we don't have an unknown number of flights.\"",
      "ground_truths": [
        "flights affected"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "6079453c108242a4a62fbc139b352026",
      "question": "What problem were FAA having?",
      "prediction": "The answer is: \"The condition of the Atlanta Airport's computer problem, exacerbated by the Federal Aviation Administration (FAA) which implemented a communication breakdown at a facility south of Atlanta, ultimately resulting in some flight-plan processing at Salt Lake City, Utah, overloading the facility. The FAA's response was largely unsuccessful in resolving the issue, with the Salt Lake facility processing requests closer to normal speed.",
      "ground_truths": [
        "communications breakdown"
      ],
      "em": 0,
      "f1": 0.03571428571428571
    },
    {
      "id": "b5f0520db0c94594949ebeb81146b062",
      "question": "What has deteriorated this year?",
      "prediction": "The question asks what has deteriorated this year. The context provides specific details about the assassination, including the scene at the camp de la Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de Camp de",
      "ground_truths": [
        "the peace with Israel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b72bdadb5821459ead2f35971941243a",
      "question": "In what year was the president murder?",
      "prediction": "The answer is in Egypt's first year, in March 1979.",
      "ground_truths": [
        "1981,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "575ab7bd4c394bc6b01b31d572258129",
      "question": "Who was assasinated?",
      "prediction": "The answer is the Egyptian State TV.",
      "ground_truths": [
        "President Mohamed Anwar al-Sadat"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "decfea1e7c1f4fdb89e62f3edf078428",
      "question": "What happened in October 1981?",
      "prediction": "The answer is \"Hosni Mubarak's death at 2.40 p.m. due to intense nervous shock and internal bleeding in his chest cavity.\"",
      "ground_truths": [
        "assassination of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "638d58cec45e4bea83844a2c8acd2a87",
      "question": "Who has Sadt's daughter implicated?",
      "prediction": "The answer is El Zomor, the leader of Al Gamaa al-Islamiyya, who was convicted of plotting the assassination.",
      "ground_truths": [
        "Mubarak,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97d117cc5c8e4480af9110fd44b1ef22",
      "question": "What amount many members of Zoe's Ark are under arrest?",
      "prediction": "The answer is, the Red Cross, UNHCR, and UNICEF.",
      "ground_truths": [
        "Six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d50f3c55c0c433c87aa649e7ef23e74",
      "question": "What president wants the journalists and flight crew released",
      "prediction": "Answer: The Sichuan earthquake of 2011 has not resulted in the release of the hostages from Chad. Instead, the **international Red Cross Committee** and the U.N. High Commissioner for Refugees (UNHCR) have released the hostages in France.",
      "ground_truths": [
        "Chadian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccae12e191874f8c8c929fa1b9b197f4",
      "question": "WHAT DOES THE CHADIAN PRESIDENT WANT?",
      "prediction": "The chadian president wants to legally intervene in the case if it is transferred from a judge in Abeche, where the children were taken, to a judge in N'Djamena, the capital, Hissene, Hadz, and Cajal, Caux and Sagnu, who are also arrested.",
      "ground_truths": [
        "journalists and the flight crew will be freed,"
      ],
      "em": 0,
      "f1": 0.046511627906976744
    },
    {
      "id": "d4f4cf4d890b4190a37e0234f229bc62",
      "question": "Who was arrested in Chad?",
      "prediction": "The answer is: Four French journalists, a seven-member Spanish flight crew, and one Belgian were arrested.",
      "ground_truths": [
        "Six members of Zoe's Ark"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50adaceccbd944119768b4f3f954bda3",
      "question": "Who is under arrest in Chad?",
      "prediction": "The answer is **Chad**.",
      "ground_truths": [
        "But the four women and three men are"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c867c284d21e46119e6538baa179a82c",
      "question": "What is happening in Chad?",
      "prediction": "The context provides information about the activities of the French charity, Chad (CNN), and the international aid agencies. It also includes the release of the 103 children from Chad, and the accusations of kidnapping and corruption by the Congolese national government. The involvement of the Venezuelan president, Idriss Deby, and the French flight crew, along with the Spanish crew, were all arrested. The state of the children (healthy, in orphanages with families) and the intervention by the charity's president, Mahamat Hissene, were also mentioned. The relief of the children was described as a humanitarian mission, and the charity",
      "ground_truths": [
        "Hundreds of women protest"
      ],
      "em": 0,
      "f1": 0.024390243902439022
    },
    {
      "id": "8b33fd5b1bd34682b58b350be208abf2",
      "question": "What is the Chadian president's reaction?",
      "prediction": "The answer is, Chad's President Idriss Deby has vowed to legally intervene in the case if it is transferred from a judge in Abeche, where the children were taken, to a judge in N'Djamena, the capital, Hissene, which is the capital of Chad, the capital of France.",
      "ground_truths": [
        "Idriss Deby hopes the journalists and the flight crew"
      ],
      "em": 0,
      "f1": 0.08695652173913043
    },
    {
      "id": "a53ebe1ca9c84930b1eeb03a7a143cb1",
      "question": "Where are the children from?",
      "prediction": "The answer is:  Chadian villages near the cities of Adre and Tine along the Chadian-Sudanese border.",
      "ground_truths": [
        "Chad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6174cd572d34786b0bdd775d4e52439",
      "question": "Who wants journalists, flight crew released?",
      "prediction": "The answer is **Chadian authorities**.",
      "ground_truths": [
        "Chadian President Idriss Deby"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "8be8b34d71a948568beed0e595760a0b",
      "question": "What number were arrested in Chad",
      "prediction": "The answer provided is:",
      "ground_truths": [
        "Six members"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "669f5a064f5640bfa06abf444e42d066",
      "question": "few were arrested in chad?",
      "prediction": "The text provides information about the detention of the children of Chad's refugees, including the fact that no Sudanese or orphans were arrested, three international aid agencies reported, and the former President of Chad Idriss Deby (Hissene) stated that the hostages were not Sudanese orphans, and were likely from Chadian villages along the border with Sudan. The Argentine foreign ministry also stated that the children were likely from Chadian villages near Adre and Tine. The international Red Cross Committee and UNICEF stated that most children were living with their families, and that the charity's focus was on rescue in France. The Caux's",
      "ground_truths": [
        "Six members of Zoe's Ark"
      ],
      "em": 0,
      "f1": 0.021052631578947368
    },
    {
      "id": "a4a58940ed9d46289d94011a45d3194d",
      "question": "What is the Chadian president asking for?",
      "prediction": "The Chadian president asks for the Chadian president for the release of the children, with a request to be rescued.",
      "ground_truths": [
        "journalists and the flight crew will be freed,"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "d56bdbbd91ce456c86ea9670fc3712ea",
      "question": "Where did they try to fly from",
      "prediction": "The answer is: Chadian villages of Chad, near the border with Sudan.",
      "ground_truths": [
        "Chad"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "72ebeac43a1048c393dc6283c0242f33",
      "question": "Who interviewed the children that tried to fly out of Chad?",
      "prediction": "The answer is the French Catholic charity, Chad.",
      "ground_truths": [
        "The Red Cross, UNHCR and UNICEF"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "77b5870674e94f139fe4160ebc755ecc",
      "question": "What does the president want?",
      "prediction": "The president wants to legally intervene in the case, if it is transferred from a judge in Abeche, where the children were taken, to a judge in N'Djamena, Hissene, and a judge in Chad, to a judge in Ankara, the capital, Hissene, to a judge in Abeche, where the children were taken, in a manner of \"teleporting them to a judge in N'Djamena, Hissene,\" to a judge in Ankara, where the children were taken, and to a judge in Abeche, where the children were taken, in a manner of \"transfer",
      "ground_truths": [
        "hopes the journalists and the flight crew will be freed,"
      ],
      "em": 0,
      "f1": 0.024691358024691357
    },
    {
      "id": "5e923fe0adac4d1bb02e2207f78e5d2f",
      "question": "Which charity organisations have been interviewing children?",
      "prediction": "The answer is: **Chadian authorities, the U.N. High Commissioner for Refugees, and UNICEF.**",
      "ground_truths": [
        "Red Cross, UNHCR and UNICEF"
      ],
      "em": 0,
      "f1": 0.25000000000000006
    },
    {
      "id": "4d593f719345401e85e2baa15492fa88",
      "question": "WHO ARE UNDER ARREST IN CHAD?",
      "prediction": "The answer is, hundreds of women protest child trafficking and shout anti-French slogans.",
      "ground_truths": [
        "Three French journalists, a seven-member Spanish flight crew and one Belgian"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "604c65f6b6044a8bb7c85d5c6fe63ff9",
      "question": "where was the blast",
      "prediction": "The answer is Iraq (CNN).",
      "ground_truths": [
        "a municipal building in Baghdad's Sadr City,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90e20a8ca8b8461994b6731460d74160",
      "question": "Who was killed in the Mosul suicide bomb?",
      "prediction": "The answer is:  A second employee from the U.S. Defense Department.",
      "ground_truths": [
        "two soldiers and two civilians from the Defense and State departments"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "1582018cb34e4a81a4e3bd6c09c30e80",
      "question": "How many americans died",
      "prediction": "The answer is: The U.S. Embassy said that four Americans died.",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "d2db4a1fc8aa479c81d9546911c79dfe",
      "question": "How many were killed in the bombing?",
      "prediction": "The answer is 10.",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45f8a09242f04f1a804f6fce37d3d020",
      "question": "Who was killed in Mosul?",
      "prediction": "The answer is ** Iraq (CNN).**",
      "ground_truths": [
        "two soldiers and two civilians from the Defense and State departments"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d042832ba94a4e10a544d91ee12fa061",
      "question": "Who did the victim work for?",
      "prediction": "The answer is: The U.S. Embassy said the blast struck a meeting of a district advisory council, a neighborhood group looking at local needs and passes on its assessments to the provincial government.",
      "ground_truths": [
        "U.S. Defense Department"
      ],
      "em": 0,
      "f1": 0.06896551724137931
    },
    {
      "id": "fa95ad070965475088a5826805ab3ef9",
      "question": "Who was shot in California?",
      "prediction": "The answer is \"The Costa Mesa Police Department,\" and the question is directly asked.",
      "ground_truths": [
        "Samuel Herr,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f12397bd3304231a5a0145051286d5c",
      "question": "Where is Daniel Wozniak held without bail?",
      "prediction": "The answer is **California**.",
      "ground_truths": [
        "California"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f3a33265c80d4541aed7148b15ff4f15",
      "question": "Police says he shot Herr at California training base for what?",
      "prediction": "Police say Wozniak shot Herr at the California training base for financial gain.",
      "ground_truths": [
        "financial gain,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "fd00fcba4eea469799b95defa152dba8",
      "question": "Who is charged with murder?",
      "prediction": "The answer is **C. Wozniak**.",
      "ground_truths": [
        "Daniel Wozniak,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "66e898eab8f3429cb1d5b893a6348670",
      "question": "Who is charged with two counts of murder?",
      "prediction": "The answer is: CA State Prison.",
      "ground_truths": [
        "Daniel Wozniak,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5e2ed2c52bf4df8a8cad864e6057bf0",
      "question": "What is Daniel Wozniak charged with?",
      "prediction": "Daniel Wozniak is charged with two counts of murder for murder and then luring the victim's friend to her death in an attempt to cover up his crime.",
      "ground_truths": [
        "two counts of murder."
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "11d608c95915496db587a48b7d7e448a",
      "question": "Wozniak is being held where?",
      "prediction": "The answer is CA, California.",
      "ground_truths": [
        "Costa Mesa Police Department"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e7e230cb20c4bbc9a8c6deb8edcd62f",
      "question": "Where did Daniel Wozniak shoot Samuel Herr?",
      "prediction": "The answer is California. The California Department of Motor Vehicles (DCM) is the agency responsible for the death of Samuel Herr.",
      "ground_truths": [
        "Los Alamitos Joint Forces Training Base"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f510e4b92495473db9b86dd2d5b6eb18",
      "question": "Who got engaged to Ryan Adams?",
      "prediction": "Mandy Moore is engaged to Ryan Adams.",
      "ground_truths": [
        "Mandy Moore"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "1d9dc72255774f95b5e274139d8ea9bd",
      "question": "What is Moore famous for?",
      "prediction": "Mandy Moore is famous for being a singer-actress.",
      "ground_truths": [
        "role as a bride in the 2007 movie \"License to Wed\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7abf6763c7d425c8577ed893a0cd182",
      "question": "What is Moore better known for?",
      "prediction": "Moore is better known for marrying Adams.",
      "ground_truths": [
        "success as a recording artist"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "35190f405eb4417497a797f137fb3f1b",
      "question": "Who is she engaged to?",
      "prediction": "Mandy Moore is engaged to Ryan Adams.",
      "ground_truths": [
        "Ryan Adams."
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "aee9e1f69d5048c2a687b4a5acc056be",
      "question": "where Site raised $17,000 before crashing on Tuesday due to high volume?",
      "prediction": "The site raised $17,000 before crashing on Tuesday due to high traffic.",
      "ground_truths": [
        "City of Los Angeles' Web"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b8093ba1236f482a9191dd552f526ec2",
      "question": "How many police were on hand?",
      "prediction": "The answer is: The city of Los Angeles, with $530 million in debt, set up a Web page asking for donations to help with the expenses.",
      "ground_truths": [
        "Three thousand"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "59cee8c898e5429f952a3af057e0d383",
      "question": "Costs include what?",
      "prediction": "The costs include putting extra police on the streets, trash pickup, sanitation, traffic control, and more for the Tuesday event.",
      "ground_truths": [
        "putting extra police on the streets, trash pickup, sanitation, traffic control and more for the Tuesday event,"
      ],
      "em": 0,
      "f1": 0.9375
    },
    {
      "id": "67bee8ca8bf642238b43fd87d1a5405f",
      "question": "How many cops were at the event?",
      "prediction": "The answer is \"hundreds of donors contributed more than $17,000 through the Web site.\"",
      "ground_truths": [
        "Three thousand"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04948ff5f6ba4caea0a805f6c1596d56",
      "question": "What did costs include?",
      "prediction": "The costs included putting extra police on the streets, trash pickup, sanitation, traffic control, and more for the Tuesday event, and the city was unable to collect contributions.",
      "ground_truths": [
        "putting extra police on the streets, trash pickup, sanitation, traffic control and more"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a04a4431e605471480fb64662b38504d",
      "question": "Site raised how much money before crashing?",
      "prediction": "The site raised $1.4 million (a donation page) before crashing.",
      "ground_truths": [
        "$17,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5dc840ded554956be6daa4c6631deb3",
      "question": "what City set up Web page asking Jackson fans to donate money?",
      "prediction": "The City of Los Angeles set up a Web page asking fans to donate money to help with the expenses.",
      "ground_truths": [
        "Los Angeles'"
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "959464bf0c7c44ba8f2b21448867e4ad",
      "question": "How much money was raised?",
      "prediction": "The money raised from the Los Angeles City of Los Angeles was $1.4 million, and the office stated that the website crashed for 12 hours, during which the city paid three thousand police officers for the incident. The City of Los Angeles' official said the city did not want taxpayers to pay a penny for the service.",
      "ground_truths": [
        "$17,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02a006ec43e94d8a8fec1ab448fa5e00",
      "question": "Which political party did Nepal belong to?",
      "prediction": "The political party that Nepal belongs to is **Nepal**.",
      "ground_truths": [
        "Communist"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "652962af37ce4c5799789678fa661553",
      "question": "What was Nepal's old job?",
      "prediction": "Nepal's old job is as the new government's key responsibility.",
      "ground_truths": [
        "former general secretary of the Communist Party,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5500a7bfa4d4b1683dc90808707e306",
      "question": "When did Pushpa Kamal Dahal resign ?",
      "prediction": "Answer: When Pushpa Kamal Dahal resigned as Prime Minister of Nepal.",
      "ground_truths": [
        "May 4"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4740a5c55c1f47bf93a59c65525f43e9",
      "question": "What is Nepal's age?",
      "prediction": "Nepal's age is 56.",
      "ground_truths": [
        "56,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c1b2178b59534886b5fab6b21bb11662",
      "question": "Who was the only candidate?",
      "prediction": "Answer: The only candidate was a former general secretary of the Communist Party of Nepal (Maoist), whose resignation was announced on Saturday.",
      "ground_truths": [
        "Madhav Kumar Nepal"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "ed00bbd0da8841fa9bb15f15859a9876",
      "question": "What is the age of Madhav  Kumar Nepal?",
      "prediction": "The answer is 56.",
      "ground_truths": [
        "56,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6e0186a28acb4427aedaee03db16e623",
      "question": "Who resigned as Prime Minister?",
      "prediction": "The answer is **Nepal, 56, is a former general secretary of the Communist Party of Nepal (Maoist).**",
      "ground_truths": [
        "Pushpa Kamal Dahal, the Maoist chairman,"
      ],
      "em": 0,
      "f1": 0.10526315789473682
    },
    {
      "id": "a0b2f384132d475d9b375afdc5343fb2",
      "question": "When did the Maoist chairman resign?",
      "prediction": "The Maoist chairman resigned as prime minister on May 4, after the president overturned the Cabinet's decision to sack the army chief.",
      "ground_truths": [
        "May 4"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "f002078c2ea2435db1e405606b820e19",
      "question": "What age is Madhav Nepal?",
      "prediction": "Madhav Kumar Nepal, a former Communist Party (Maoist) leader, was the only person to serve as a candidate for the post. The party's strong backing, a 38% vote of 25 parties, and the support of the 601-member constituent assembly, plus the (in 1995) parliament, all contributed to his candidacy.",
      "ground_truths": [
        "56,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d56e8c7e4b6449696113a82ec0da19e",
      "question": "What is the Palm Jumeirah?",
      "prediction": "The Palm Jumeirah is a large, artificial island, and it opened Thursday, with a spectacular fireworks display, with a three-day party.",
      "ground_truths": [
        "A huge man-made island"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "f97696deb9704d2f8ae5834f9fcdd88c",
      "question": "Where is Palm Jumeirah island?",
      "prediction": "The answer is **Dubai, United Arab Emirates.**",
      "ground_truths": [
        "off the coast of Dubai"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "07f2d15033cf4fb6916bed5d56dcb8f5",
      "question": "Who will be at the opening party?",
      "prediction": "The answer is **The Palm Jumeirah.**",
      "ground_truths": [
        "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97034e6a207f4a238a49bf7babb932e9",
      "question": "When was the opening party?",
      "prediction": "The opening party was Thursday night.",
      "ground_truths": [
        "Thursday night."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "dd5a9212340046e2a340103c45fd7f17",
      "question": "Where is Dubai?",
      "prediction": "The answer is **Dubai**.",
      "ground_truths": [
        "Arab Emirates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9186ab50536b4241b5033a6f1bfa8c9b",
      "question": "How much has the addition of the man-made island increased the Dubai coastline?",
      "prediction": "The addition of the man-made island (the Palm Jumeirah) increased Dubai's shoreline by 100 percent after work started on it on Thursday.",
      "ground_truths": [
        "100 percent"
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "1646e87aca7b4f078ae816044f0f5955",
      "question": "What is the name of the man-made island?",
      "prediction": "The name of the man-made island is Dubai.",
      "ground_truths": [
        "Palm Jumeirah"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ff0076b7fe74648806429e78621f265",
      "question": "Where is the island located?",
      "prediction": "The island is located in Dubai, United Arab Emirates.",
      "ground_truths": [
        "off the coast of Dubai"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "6455065759ff4462a651dca3f2bdc610",
      "question": "How many died in mall shooting?",
      "prediction": "According to the context, the answer is \"Eight.\"",
      "ground_truths": [
        "eight."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "43bd6876bebc480d80d49b28c9f6ba5d",
      "question": "What are malls expected to assess?",
      "prediction": "The answer is: \"not all malls are expected to assess.\"",
      "ground_truths": [
        "their emergency plans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81d4deec3837435fb36d6d66a0514010",
      "question": "How many people did the gunman kill?",
      "prediction": "The gunman killed 8 people in Omaha, Nebraska.",
      "ground_truths": [
        "eight."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b0d0110d85b4616b9554ee617750926",
      "question": "What day did the shooting occur?",
      "prediction": "The shooting occurred on Wednesday, Washington, Nebraska.",
      "ground_truths": [
        "Wednesday."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "492061a0ce704e7790ebe5e33f796052",
      "question": "How many people did the gunman shoot?",
      "prediction": "The gunman shot 1,200 people.",
      "ground_truths": [
        "eight.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c723a032ea4e457b8fac1068352121c9",
      "question": "What do security experts say abut such incidents?",
      "prediction": "Security experts say that, despite the loss of eight people, malls in the United States will continue to review their emergency plans and consider additional security measures.",
      "ground_truths": [
        "it's a matter of money."
      ],
      "em": 0,
      "f1": 0.06896551724137932
    },
    {
      "id": "266894f86fe6470bac36f71baa2b9ea9",
      "question": "What is impossible to anticipate?",
      "prediction": "The answer is: \"Nothing is impossible.\"",
      "ground_truths": [
        "murderous rampage"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6634ef970c9342ab9fe5f7a6ef1d72f7",
      "question": "Where was the shooting?",
      "prediction": "The answer is Washington, Nebraska.",
      "ground_truths": [
        "Westroads Mall in Omaha, Nebraska,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "9f4b7421498641fe88cfb553ed3fa756",
      "question": "What are malls expected to do in wake of Wednesday shooting?",
      "prediction": "According to the context, the answer is: \"we do not disclose or discuss our enhanced security measures and procedures that we have in place or may institute at any time, some of which are visible to the public, with others intentionally less noticeable.\"",
      "ground_truths": [
        "review their emergency plans and consider additional security measures"
      ],
      "em": 0,
      "f1": 0.12244897959183673
    },
    {
      "id": "2ff5cf3e188e4e37ac835adc32165b87",
      "question": "What did a gunman and Omaha Nebraska mall do?",
      "prediction": "The answer is that Omaha's Westroads Mall, under the command of the \"Von Maur\" store, was a mall in an area where security was suspicious.",
      "ground_truths": [
        "murderous rampage"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d8a6ddd00ff46a4a68fb56507cd76ce",
      "question": "Who opened civil rights investigation?",
      "prediction": "The answer is: the Maryland State Police.",
      "ground_truths": [
        "FBI's Baltimore field office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b7525de970c4f40865c6e5c3dc89500",
      "question": "Who is being held for the death of a police officer?",
      "prediction": "The answer is \"The Maryland State Police. The FBI.\"",
      "ground_truths": [
        "Ronnie White,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5ffedbec441477fa239e782fb628236",
      "question": "What type of investigation did the FBI open?",
      "prediction": "The FBI's Baltimore field office opened a civil rights investigation into White's death.",
      "ground_truths": [
        "civil rights"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "287c319b65094d23809ba1563ec3e70e",
      "question": "Who has been held following the death of police officer?",
      "prediction": "The answer is \"The family of Ronnie White is absolutely, unequivocally outraged, incensed, and deeply saddened that the life of their loved one could be taken so coldly, so coldly, so undeservedly,\" with \"i call the Department of Justice, the FBI, the Maryland State Police and county officials \"to conduct a thorough and exhaustive investigation into all of these tragic events, beginning with the events of Friday, June 27.\"",
      "ground_truths": [
        "Ronnie White,"
      ],
      "em": 0,
      "f1": 0.06349206349206349
    },
    {
      "id": "16210271258f4a84b1dd5f0e13449d6d",
      "question": "What did Ronnie White die of?",
      "prediction": "Answer: The answer is two.",
      "ground_truths": [
        "strangulation and asphyxiation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9da90ceaca2f41d6a8aab4bf02a599df",
      "question": "Where was Ronnie White?",
      "prediction": "The answer is \"Baltimore County Jail.\"",
      "ground_truths": [
        "Prince George's County Correctional Center,"
      ],
      "em": 0,
      "f1": 0.20000000000000004
    },
    {
      "id": "67f03659bf904f96b7c75c168e76bbc1",
      "question": "Where was White being held?",
      "prediction": "The answer is: \"Baltimore County Correctional Center,\" and the Maryland State Police.",
      "ground_truths": [
        "Prince George's County Correctional Center,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "116ec128ff1b4e0e9affb80d35b16123",
      "question": "What is the name of the defendant?",
      "prediction": "The answer is: The name of the defendant is Anthony.",
      "ground_truths": [
        "Casey Anthony,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "817ffee256de400687ad09739aec7dbd",
      "question": "What doesn't help the mother's defense?",
      "prediction": "The mother's defense likely wants to argue that the death was a homicide, not an accident. She claims that the missing, soft tissue, and the fact that the child's body was found in a wooded area, and the search by the state's medical examiner, and the presence of DNA testing, do not support this. She ultimately argues that the key to the prosecution's case is the lack of a concrete cause of death, and that the lack of a reasonable suspect was a vital, circumstantial factor.",
      "ground_truths": [
        "her alibi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8d626720877446678ac80b14af871812",
      "question": "What could hinder the prosecution?",
      "prediction": "The answer is, \"The prosecution is going to have a great deal of circumstantial evidence, and this is a physical evidence case, which will either include her or exclude Casey.\"",
      "ground_truths": [
        "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains"
      ],
      "em": 0,
      "f1": 0.0975609756097561
    },
    {
      "id": "44a83f25d8814eb99197b7348039eab0",
      "question": "When did the toddler vanish?",
      "prediction": "The answer is \"The final leg of the child's skeleton was found in woods about a half-mile from Anthony's parents' house and identified through DNA testing.  There was no soft tissue on the skeletal remains.  The cause of death is a homicide.  \"The child's mother, Casey Anthony, 22, was the result of an accidental pregnancy. Anthony made an attempt to give her up after birth. The state's investigation also identified a nanny, but the name and address was bogus, and the scent of decomposing flesh and a trace of chloroform, a powerful knockout agent, was found in the",
      "ground_truths": [
        "last summer."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f0cb5ab79e1347979a1260074cf92df4",
      "question": "What is the cause of death?",
      "prediction": "The answer is a homicide.",
      "ground_truths": [
        "no evidence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c723a72b2904344bc7a7ba2c7d6de33",
      "question": "Who vanished last summer?",
      "prediction": "The answer is: Caylee Anthony, 22.",
      "ground_truths": [
        "Caylee Anthony's"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "1b46d98652ce4436ac3994b2f43b44d5",
      "question": "What is the mothers name?",
      "prediction": "The answer is: Caylee Anthony's mothers name is **Anthony**.",
      "ground_truths": [
        "Casey Anthony,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "239cee8d0f52431bb19648a0518dc8b7",
      "question": "Who started as a tie salesman?",
      "prediction": "The answer is Ralph Lauren.",
      "ground_truths": [
        "Ralph Lauren"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d2e5c5a41f9e4b0a9784f7c5cafde0c4",
      "question": "For how long has he designed high fashion?",
      "prediction": "The answer is: \"And now, the people walk into the store and say, 'I want your clothes.'\"",
      "ground_truths": [
        "four decades"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f11480c8747745c892004397ecff2180",
      "question": "What did Lauren start as?",
      "prediction": "The answer to the question is: \"The clothes we make for the runway, not concepts. They go into stores. Our stores. And we have lots of them.\"",
      "ground_truths": [
        "tie salesman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d9017d072a945659b3a7f10694ad4d8",
      "question": "What makes clothing glamorous?",
      "prediction": "The answer is: *the ability to make clothes that are consistent and accessible.*",
      "ground_truths": [
        "Polo because \"it was the sport of kings."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bde9e44277a3414a9ebe14c35375d069",
      "question": "Lauren sold what?",
      "prediction": "The answer to the question is, \"This is the most important thing.\"",
      "ground_truths": [
        "ties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd24a6d670df4f6db5f4c1e6fec0f867",
      "question": "Who began as a tie salesman?",
      "prediction": "The answer is Ralph Lauren.",
      "ground_truths": [
        "Ralph Lauren"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "132f427910cd43b983e06b7156df6cf4",
      "question": "Where was Ralph Lauren from?",
      "prediction": "The answer to the question is \"Ralph Lauren,\" and he was born in Bronx, New York.",
      "ground_truths": [
        "Bronx."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "c6dff9d75f984f36a1ebd1fd5a3e31c1",
      "question": "What was his first design?",
      "prediction": "The answer is: Ralph Lauren's clothing, specifically his tie-dyed, affordable, and versatile tie-dyed apparel, which he calls \" Polo.\"",
      "ground_truths": [
        "ties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b85480c5b66345d685f83715df02faf6",
      "question": "What was Ralph Lauren's first job?",
      "prediction": "The answer is, Ralph Lauren has his eye on China and Japan.",
      "ground_truths": [
        "former tie salesman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65323f0418c6473882bb7078b52e8568",
      "question": "what did he design for four decades?",
      "prediction": "The answer is, Ralph Lauren's clothing is known for being handcrafted, by the way.",
      "ground_truths": [
        "way American men and women dress"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "6869198945d042f0b076c7d26debcaef",
      "question": "What did Ralph Lauren originally do?",
      "prediction": "The answer is, Ralph Lauren did go into boutique spa hotels around the globe.",
      "ground_truths": [
        "tie salesman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8409d0d4207f4d84a097738867f2b082",
      "question": "What did he design?",
      "prediction": "The answer is: **\"The clothes that go into the shoes of the people who feel it, seek it, and indulge in the Good Life. The clothes that are created by the people who have been successful and have made a difference in the world, not as concepts, but as the products that consumers are willing to buy and use in their own homes.\"**",
      "ground_truths": [
        "ties,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "147beb71e56b4756af9790dcedf83b13",
      "question": "where did Lauren began as tie salesman?",
      "prediction": "The answer is, \"Ralph Lauren is the American tailor, and he's known for creating affordable, luxurious, and fashionable clothing. He specializes in chinos and hoodies, and his clothing is designed to be versatile and accessible, featuring a \"brand-like\" feel and a focus on luxurious, often-stylish details. He's known for his signature \"polo match\" color, which is essentially a red and white striped polo. He's also known for his work at the Bronx, where he collaborated with several other designers and had a successful successful new business. He also has a strong association with the \"Golden Age\" of American",
      "ground_truths": [
        "the Bronx."
      ],
      "em": 0,
      "f1": 0.02222222222222222
    },
    {
      "id": "0be6356817ca41d59b0a75655fca24b9",
      "question": "What city has closed the airport?",
      "prediction": "Answer: The Cincinnati- Cincinnati- Atlanta (C-A) International Airport in Ohio is closed at 2:30 PM.",
      "ground_truths": [
        "Oklahoma"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5aaeceef16f34275b07bef56b93edddc",
      "question": "when close oklahoma city?",
      "prediction": "Based on the provided text, the answer is: Oklahoma City, Tennessee. The text states that \"a major winter storm with heavy snow and ice was headed from Texas and Oklahoma to points east, with 8 to 10 inches of snow possible in some locales, the National Weather Service said Friday.\"",
      "ground_truths": [
        "2:30 p.m."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20e0dcefdff242ebb51ce89c34182814",
      "question": "Where is the storm going?",
      "prediction": "The storm is going from Oklahoma to Virginia and West Virginia. It is expected to reach Virginia and West Virginia by the afternoon. The storm caused significant snow in Nashville, Tennessee, but it is expected to form ice in parts of the night before ending Saturday afternoon. A significant rain was expected to move from Oklahoma to the southeast to Tennessee and into Mississippi, with snowfall, sleet, and snow in the evening. The storm's intensity shifted to a mixture of rain and snow, with the rain coming from Atlanta, Georgia, but for the first time, it was expected to move northeastward to Virginia and West Virginia. The storm's",
      "ground_truths": [
        "Virginia, West Virginia, the Carolinas, Tennessee, Kentucky and Arkansas."
      ],
      "em": 0,
      "f1": 0.09615384615384616
    },
    {
      "id": "b47d1c59552a453ba57c2121f029242d",
      "question": "When will Oklahoma city close runways?",
      "prediction": "The answer is: \"Oklahoma City's Will Rogers World Airport closed all its runways at 2:30 p.m. local time due to \"deteriorating weather conditions.\"",
      "ground_truths": [
        "2:30 p.m."
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "3e83d3398a8a4e26a98fab7f85c8cb59",
      "question": "what is in nashville and georgia mountains?",
      "prediction": "In Nashville and Georgia mountains, the precipitation is expected to be combined with snow in the evening on bridges and overpasses, until it reaches Virginia and West Virginia. This event is expected to be a \"ice event more than a snow event, not for Nashville, but places south of there, from Memphis to Little Rock, and even over to Chattanooga, Tennessee.\"",
      "ground_truths": [
        "large accumulations of ice"
      ],
      "em": 0,
      "f1": 0.06557377049180328
    },
    {
      "id": "4a93c5ba8a9f4632b9fd8768d008ea73",
      "question": "What weather will other's get?",
      "prediction": "The weather forecast predicts that other's will receive severe storm warnings for these states. The forecast for other's will is:",
      "ground_truths": [
        "winter storm"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "fcc39f65d75847b79baf078313a37bdb",
      "question": "What was said at the university?",
      "prediction": "The context provides key information about President Obama's statement:",
      "ground_truths": [
        "\"I'm just getting started.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e44335869d29465f972c14a36ada5ad1",
      "question": "What did he say at the univerisity?",
      "prediction": "The answer is, \"I'm just getting started.\"",
      "ground_truths": [
        "\"I never thought any of this was going to be easy,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a72a835834ab424e9ecc30a0c106d0a2",
      "question": "Where did the 4 hijackers board the Maersk Alabama?",
      "prediction": "The answer is the U.S. Navy.",
      "ground_truths": [
        "Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bbae459788f3428fb4d9941e806c07e2",
      "question": "What did the crew say?",
      "prediction": "The crew said that the crew had captured four pirates, and they released the captain.",
      "ground_truths": [
        "had captured one of four pirates who hijacked their vessel, but they released him in a failed hostage trade."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "46bcda0c67f849ca91cbd6707073d260",
      "question": "Where was the Alabam located?",
      "prediction": "The answer is Massachusetts, Massachusetts. The context only provides information about the crew's location, and the CNN's information is not directly attributable to the Alabam.",
      "ground_truths": [
        "off Somalia's coast."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc05642a8ec84226979d1a3aa29edea6",
      "question": "Who did Alabama elude?",
      "prediction": "The answer is: Four pirates.",
      "ground_truths": [
        "The ship had eluded the pirates for more than three hours off the coast of Somalia before being boarded,"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "b03261d3d4be4b90886641c1832f0083",
      "question": "What did Maersk Alabama elude for more than three hours?",
      "prediction": "The answer is: Four pirates, who had scuttled their boat when they boarded the ship armed with AK-47s, took Phillips with them aboard the ship's 28-foot lifeboat.",
      "ground_truths": [
        "eluded the pirates"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "36117da6832d406fa33aa93e477dfcb5",
      "question": "Who did a crew member call with details?",
      "prediction": "The answer is: The crew member who called with details.",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "278313c771714829ac3610569a889fbc",
      "question": "Where did the hijackers board Maersk Alabama?",
      "prediction": "The answer is Massachusetts, Massachusetts.",
      "ground_truths": [
        "off Somalia's coast."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b55c21723801443a97f7eecb8d39fdd0",
      "question": "Who boarded the Alabama?",
      "prediction": "The answer is the U.S. Navy.",
      "ground_truths": [
        "four pirates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eabc6d504d3546a3b1d46f161e8e0981",
      "question": "What happened in 1975?",
      "prediction": "The answer is, Iggy Pop played rock and roll.",
      "ground_truths": [
        "he checked himself into a Los Angeles mental institution"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2e58ec6e792484f80ff5002eab4c4f5",
      "question": "Who produced the 3 classic albums?",
      "prediction": "The answer is The Stooges.",
      "ground_truths": [
        "David Bowie,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6fd87b6e1c24d9595f8fc12a3c1be71",
      "question": "Who co-wrote and produced Iggy's The Idiot and Lust for Life albums?",
      "prediction": "The answer is **David Bowie**.",
      "ground_truths": [
        "David Bowie,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "03d55bea2f16415f8a57e00d81d30218",
      "question": "In what year did Iggy check himself into a mental institution?",
      "prediction": "The answer is 1971.",
      "ground_truths": [
        "1975"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8feaa5ac0b5746d6a145102e2da94175",
      "question": "When did the group reunite?",
      "prediction": "The answer is 1977.",
      "ground_truths": [
        "1973's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ae978d0f8154c7694f562ef5cf6b4ef",
      "question": "What was the reason for FARC to target the specific group that they did?",
      "prediction": "The answer is: FARC targeted the Awa community, not indigenous populations.",
      "ground_truths": [
        "collaborating with the Colombian government,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f04922a1c8b04513b654d344038b5ef1",
      "question": "Who said that rebels tortured some Indians before killing them?",
      "prediction": "The answer is: \"They understood the CUMAP (the Colombian military) was not interested in the Awa people's participation in the armed struggle and they did not even look out for indigenous communities. The FARC is Mexico's leader.",
      "ground_truths": [
        "The Human Rights Watch organization"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fda78fbbc7184f34b3c4dcbe111f593a",
      "question": "Who took responsibility for the slayings?",
      "prediction": "The answer is the Colombian government.",
      "ground_truths": [
        "Marxist guerrillas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33fefd93a2694bb8a3566caa9acbc60c",
      "question": "What does FARC stand for?",
      "prediction": "In Bogota, Colombia, the FARC is a Marxist guerrilla group that has killed 8 Indians in the past two weeks. The group has reportedly targeted indigenous people in Rio Bravo, a town in southwest Colombia. The FARC's communication also urged Indians not to be manipulated by government officials and said the FARC is their executioners.",
      "ground_truths": [
        "Revolutionary Armed Forces of Colombia,"
      ],
      "em": 0,
      "f1": 0.03773584905660377
    },
    {
      "id": "6810a483fad34323be263d443696b0e7",
      "question": "Number of Indians that FARC slayed?",
      "prediction": "The answer is: There are 10 Awa Indians killed.",
      "ground_truths": [
        "at least 27"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "15b6eb5fb08b4a7da535ea7047d26794",
      "question": "How many people did FARC kill?",
      "prediction": "The answer is 120.",
      "ground_truths": [
        "at least 27"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85c04b3d7724447880a73aad3a81da3b",
      "question": "Why were the victims executed?",
      "prediction": "The answer is that FARC (the Colombian Revolutionary Armed Forces) was executed in Bogota, Colombia on February 6, 2023, because they were gathering information about the rebels to give to the Colombian military.",
      "ground_truths": [
        "collaborating with the Colombian government,"
      ],
      "em": 0,
      "f1": 0.0606060606060606
    },
    {
      "id": "b6614f09b982496697e987427eaaa1ae",
      "question": "Who was tortured?",
      "prediction": "The answer is the FARC.",
      "ground_truths": [
        "some of the Awa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52748255a17b442c8ee693520c697af1",
      "question": "What did FARC cite as the cause for executions?",
      "prediction": "FARC's actions in Bogota, Colombia, resulted in the execution of 8 Awa Indians in February 6. The FARC's statement, posted on Tuesday, stated that the guerrillas \"were not targeting indigenous populations but taken the action 'against people who independent of their race, religion, ethnicity, social condition etc. accepted money and put themselves at the service of the army in an area that is the object of military operations.\"",
      "ground_truths": [
        "because the Indians were gathering information about the rebels to give to the Colombian military."
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "6039c005852044ee8cd9760fe3d91e1b",
      "question": "What is the cost of the conveyor belts?",
      "prediction": "The cost of the conveyor belts is estimated to be around $5.5 billion, which is roughly $20 million per year.",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "e0b6b807c35b498b9c57d7c05813c554",
      "question": "How much did the plant cost to build?",
      "prediction": "The answer is: Hyundai Steel's Dangjin plant, in South Korea, cost the steel manufacturer approximately $5.5 billion, which is approximately $20 million per year.",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "42923549ccb94a399374e526f6651aa7",
      "question": "Which company is reducing emissions at its steel plant in Dangjin, South Korea?",
      "prediction": "The answer is Hyundai Steel. The answer directly addresses the context, providing the answer based on the information provided in the text:",
      "ground_truths": [
        "Hyundai"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "39cb589172e84effa1e2d22eb157da0f",
      "question": "Does the company recycle?",
      "prediction": "The company's answer is \"Yes, the company recycled 100% of its byproducts. The carbon monoxide and hydrogen produced from the furnace was used for heating the furnaces, which reduced the consumption of coal.",
      "ground_truths": [
        "100% of its byproducts"
      ],
      "em": 0,
      "f1": 0.25806451612903225
    },
    {
      "id": "908f4a95e90647a78fd238a2d96f1d2e",
      "question": "Who says it recycles all of its byproducts and monitors its emissions 24 hours a day?",
      "prediction": "The answer is **Hyundai Steel**.",
      "ground_truths": [
        "Dangjin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ea575fda1ae4b3785ebcb9cc2ebc1ad",
      "question": "Where is Hyundai reducing their emissions at?",
      "prediction": "The answer is: Hyundai Steel reduces its emissions at, according to its technical research center.",
      "ground_truths": [
        "Dangjin plant,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "791f7cd96b1b452a90b44d16904a868c",
      "question": "How much did the conveyor belts cost to build?",
      "prediction": "The conveyor belts cost Hyundai Steel approximately $5.5 billion, and the steel production was about 8 million tons per year.",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "efacc04e1812429f83fd4409ea5093de",
      "question": "The company recycles what 24 hours a day?",
      "prediction": "The company's recycling of byproducts, which contain carbon monoxide, hydrogen, and methane, is a key component of its \"green steel production\" efforts.",
      "ground_truths": [
        "100% of its byproducts"
      ],
      "em": 0,
      "f1": 0.24999999999999997
    },
    {
      "id": "47ec654cee204c2ca4aabd88438e35e2",
      "question": "Who has a steel plant in Dangjin?",
      "prediction": "The answer is Hyundai Steel.",
      "ground_truths": [
        "Hyundai"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "36c255f124334df8baabb2e913aefc6f",
      "question": "What is a test of obamas policy?",
      "prediction": "The answer is, \"Change Has Come: Barack Obama and the Meaning of Progress.\"",
      "ground_truths": [
        "the piracy incident"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0412b5c73f744608255bd12ebf67d3b",
      "question": "Who cared about Somalia?",
      "prediction": "The answer is, \"We care about Somalia.\"",
      "ground_truths": [
        "William Jelani Cobb"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38af62bfb6eb48a8b4937f1d7ca7ac09",
      "question": "What did Cobb say?",
      "prediction": "The answer is: C-21.",
      "ground_truths": [
        "Somalia's piracy problem was fueled by environmental and political events."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "283449da34f24389b1967779d0ed4dfd",
      "question": "What is said to have caused piracy?",
      "prediction": "The answer is: \"This escalating pattern of pirate attacks may soon constitute an international crisis. But what this situation actually reveals is how little we've learned in the nearly eight years since George W. Bush declared war on terror.",
      "ground_truths": [
        "environmental and political events."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02236d41505743b7995ad3ffa3fd1318",
      "question": "in what year its revolution?",
      "prediction": "The answer is **680**.",
      "ground_truths": [
        "1979"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c90c0b26990c48c3b78097fd570a807b",
      "question": "What may Iran find difficult this year?",
      "prediction": "Iran may find it difficult to put a lid on the marking of Ashura this year, as it is a week to the day since the death of Grand Ayatollah Hussein Ali Montazeri.",
      "ground_truths": [
        "put a lid on the marking of Ashura"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "cbef062d89b944c5acd16f892a79611c",
      "question": "What falls one week to the day after the death of Grand Ayatollah?",
      "prediction": "The answer is: **Sunday**.",
      "ground_truths": [
        "Ashura."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0811fa0aee1141f0a91351c897beb555",
      "question": "What drugs are marketed without approval?",
      "prediction": "The answer is, the morphine sulfate elixir is not marketed without approval. This was announced by the FDA on Thursday, November 19, 1976.",
      "ground_truths": [
        "A form of liquid morphine"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "6a85d330e9b948fda0c33536760ff0b9",
      "question": "What drug had its usage extended?",
      "prediction": "The drug that had its usage extended was morphine sulfate elixir 20mg/ml.",
      "ground_truths": [
        "morphine sulfate oral solution 20 mg/ml."
      ],
      "em": 0,
      "f1": 0.23529411764705885
    },
    {
      "id": "96ec04ebdb2b47d58ed22d148137caf1",
      "question": "What is the top priority until alternatives are developed?",
      "prediction": "The answer is: Until there are, and the FDA says it will allow companies to continue and until 180 days after receiving approval.",
      "ground_truths": [
        "ensure there is no shortage of the drug while patients wait for an approved product to take its place."
      ],
      "em": 0,
      "f1": 0.15789473684210528
    },
    {
      "id": "5ffa044924924886bb08d1d71a406cac",
      "question": "What would pulling the drug cause?",
      "prediction": "The answer is: *Until there is a new morphine replacement drug of the same dosage, the FDA will continue to allow them to continue the drug.*",
      "ground_truths": [
        "hardship for terminally ill patients and their caregivers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3692805baa804755bab5c37bc3240fd4",
      "question": "what was the estimation",
      "prediction": "The answer is: the FDA estimates there are several thousand drugs, mostly older products, marketed illegally without FDA approval, and new morphine replacement drugs are being offered until 180 days after the drug is approved for manufacture.",
      "ground_truths": [
        "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country."
      ],
      "em": 0,
      "f1": 0.52
    },
    {
      "id": "7dc45360651d4f84a2648e9b0993c895",
      "question": "what did hospital groups say",
      "prediction": "The answer is: The FDA said it had to balance the goal of ensuring all prescription drugs on the market are FDA-approved, with the flexibility and compassion it offers to patients who have access to alternatives for pain relief.",
      "ground_truths": [
        "expressed concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers,"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "f4e04bf1629c41f38e15786299a33202",
      "question": "What do treasure hunters hope to find?",
      "prediction": "Treasure hunters hope to find:",
      "ground_truths": [
        "lost Nazi gold."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f13af4c8e3574745a176421c2ada029d",
      "question": "What is called off?",
      "prediction": "The answer is: \"German mountain\"",
      "ground_truths": [
        "the hunt for Nazi Gold"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a463b547f86240248cbeca805df114fb",
      "question": "Where is the dig?",
      "prediction": "The answer is: Germany's mountain in Ore Mountains.",
      "ground_truths": [
        "Deutschneudorf,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1fdd740b9bb490ebdabb2590fbeb38d",
      "question": "What may resume in two weeks?",
      "prediction": "The answer is: *the search for Nazi gold and the amber room at the Amber Room, a gold-filled chamber of former storage, at German Neudorf, is not yet a week away.*",
      "ground_truths": [
        "digging"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9546b34ca734a26ba7075fd2f064404",
      "question": "Where is the location of the dig?",
      "prediction": "The answer is **Germanneudorf**, Germany's mountain in Ore Mountains. The answer is that the hunt for Nazi gold and potentially the Amber Room, a gold-filled interior, was dug at the site.",
      "ground_truths": [
        "village Deutschneudorf,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89c493d17fad4483a54447872ff7fe6d",
      "question": "What do treasures hope to find?",
      "prediction": "Treasure hunters hope to find gold, but they do not make that the next question is about the Amber Room.",
      "ground_truths": [
        "Nazi Gold"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "7def903743ac43748483019d8b62d815",
      "question": "What was the coldest spot in the US?",
      "prediction": "The answer is:  \"East through the weekend, according to the National Weather Service.\"",
      "ground_truths": [
        "Fargo, North Dakota,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57e62137793b4537bb7eb95d44481eb3",
      "question": "It was what temperature in Chicago?",
      "prediction": "The answer is 19 degrees Fahrenheit.",
      "ground_truths": [
        "(3 degrees Fahrenheit),"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e436b3b625de462c800a23badd21bf28",
      "question": "Where is the icy cold front pushing into?",
      "prediction": "The answer is in New York.",
      "ground_truths": [
        "the Southeast,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2711d5ffc59a43ecae9d0ce410db0d93",
      "question": "What was the feels-like temp in Mpls.?",
      "prediction": "The answer directly from the context is:",
      "ground_truths": [
        "40 below"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6401157d496747a092bf97a5f430d281",
      "question": "What was the temperature in Chicago?",
      "prediction": "The answer is 19 degrees below zero.",
      "ground_truths": [
        "10 below"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "ff5f25fc4c9b4354b49fc603df373ee0",
      "question": "What was the actual temp in Chicago?",
      "prediction": "The answer is New York, New Haven, Connecticut, and other Northeast cities.",
      "ground_truths": [
        "Illlinois."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a7fcc1296b646bc86439722d2a7ac81",
      "question": "The cold front is pushing where?",
      "prediction": "The answer is \"East through the weekend, according to the National Weather Service.\"",
      "ground_truths": [
        "Southeast,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4e39f9e15734b6a82739950c34aa8ee",
      "question": "How cold did it feel in Minneapolis?",
      "prediction": "The answer is \"Bitter cold weather was sweeping across the nation, putting the Midwest in a deep freeze.\"",
      "ground_truths": [
        "felt like 40 below"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e08b24a8d41544e38f2489b4e32e2764",
      "question": "How long will this cold weather last, possibly?",
      "prediction": "The answer is: The answer is: The weather will likely be bitterly cold and have a distinct gust of wind chill, potentially reaching a record-low of 19 degrees below zero in Michigan and 10 below in Chicago.",
      "ground_truths": [
        "through the weekend,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "00388805ff934f4a9f6c374dd50be557",
      "question": "What did it feel like because of the wind chill?",
      "prediction": "The answer is: Bitter cold weather was sweeping across the nation, putting the Midwest in a deep freeze.",
      "ground_truths": [
        "40 below"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1fca269781c4454a9fbcf23844c79dd",
      "question": "It felt like 40 below in Minneapolis because of what?",
      "prediction": "The answer is \"West, and the East,\" as the context indicates.",
      "ground_truths": [
        "of the wind chill,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2259f85230746369c272c79f271f791",
      "question": "Where is the sanctuary located?",
      "prediction": "The answer is Tennessee, Tennessee. The specific location is not explicitly stated.",
      "ground_truths": [
        "rural Tennessee."
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "d3ae3c08aad44cbda0ba4bf1b82fca1e",
      "question": "Where is the Elephant Sanctuary?",
      "prediction": "The Elephant Sanctuary is in Tennessee, where elephants are kept in a rural Tennessee sanctuary. It's not a zoo.",
      "ground_truths": [
        "rural Tennessee."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "3040bd06744c435a980080d34055f300",
      "question": "is the site open to the public?",
      "prediction": "The answer is: Yes, the site is open to the public.",
      "ground_truths": [
        "Visitors aren't allowed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6c2f7abf2414aa78c2c2bf6213d7d29",
      "question": "What takes in old, injured, and abused animals?",
      "prediction": "The answer is not directly stated in the context, but it is implied to be \"all of these elephants\", referring to the animals' complex, often neglected, and sometimes traumatized state. The specific names of the animals are not explicitly stated.",
      "ground_truths": [
        "The Elephant Sanctuary."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cae2fe78dae544d3a5bf2c2eb3919562",
      "question": "What size is the facility?",
      "prediction": "The answer is: \"The Elephant Sanctuary is the largest natural refuge of its kind in the United States, located in rural Tennessee. It's not where nature intended the animals to be, roaming free in the vast expanses of subtropical Africa and Asia, but a life far apart from the ones they once lived. For most of them, it's a life far apart from the ones they once lived. Tarra lived in the back of a truck at a tire store, and Shirley performed at a circus until she broke her leg. Thus, the answer is \"the largest natural refuge of its kind in the United States, and not",
      "ground_truths": [
        "2,700-acre"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2fec7feaca844cc8b0c63711b785de5",
      "question": "When was the facility opened?",
      "prediction": "The answer is found in the context provided.",
      "ground_truths": [
        "1995"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88cb2f147775410ab3e7470896b76c80",
      "question": "Where did the new face come from",
      "prediction": "The new face was from a donor cadaver.",
      "ground_truths": [
        "a donor cadaver."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "465e12aff2ba4eac863fa56232c3b64b",
      "question": "What is as important as the face transplant itself?",
      "prediction": "The answer is that the face transplant is considered a life-changing procedure.",
      "ground_truths": [
        "Social reincorporation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "982a9f6440124bf585f35e72281200d5",
      "question": "who recivied new face?",
      "prediction": "The answer is the patient, who had previously suffered severe facial trauma, with no nose, right eye, and upper jaw.",
      "ground_truths": [
        "woman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e337e8458d6241c3a340dbd46d16208a",
      "question": "Who can now eat solid foods?",
      "prediction": "The answer is: the patient can now eat solid foods.",
      "ground_truths": [
        "The woman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "588aca70e4b94ed6a92633e34ef2c5f5",
      "question": "What can face transplant recipient do now",
      "prediction": "The answer is, \"it's not necessarily a cure for the common world, but it's a powerful tool for recovery, especially for those who have severe facial trauma and are not able to eat, drink, and get a normal life. It is a gift for the body to help its patients.\"",
      "ground_truths": [
        "breathe through her nose, smell, eat solid foods and drink out of a cup,"
      ],
      "em": 0,
      "f1": 0.10714285714285714
    },
    {
      "id": "1102fa5e20584c7ab36afa172f64709c",
      "question": "Who received her new face in one graft from a donor cadaver?",
      "prediction": "The answer is the patient, who received her new face in one graft from a donor cadaver.",
      "ground_truths": [
        "The patient,"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "ab72decc14e541329832a35eb36c9fa4",
      "question": "What did the police chief say?",
      "prediction": "The police chief said that Pakistan security officials showed seized weapons and ammunition in Karachi on Sunday after the arrest of seven alleged militants.",
      "ground_truths": [
        "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "8e083432032f4c0c9df404f7db7478dd",
      "question": "What did the arrest preventing happening in Karachi",
      "prediction": "The arrest of seven alleged militants in Karachi, along with the Taliban in Punjab, resulted in the seizure of weapons and ammunition, as well as the arrest of three individuals carrying suicide jackets and explosives inside a bus station.",
      "ground_truths": [
        "numerous suicide attacks,"
      ],
      "em": 0,
      "f1": 0.05405405405405406
    },
    {
      "id": "d2e9097aa9a8438c820acdc165cc2785",
      "question": "How many suspects are there?",
      "prediction": "The answer is 13.",
      "ground_truths": [
        "At least 13"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "14f15d96977a488fbc42bccc615d586a",
      "question": "Who was assassinated?",
      "prediction": "The answer is: Pakistan's national security forces (Armed Forces of Pakistan, F-A-S) assassinaded the former Pakistani Prime Minister Nawaz Sharif in 1999.",
      "ground_truths": [
        "Mehsud"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9042f11cbc7409aaf5e7364dd1358fc",
      "question": "What sports team was attacked?",
      "prediction": "The answer is Pakistan's National Hockey League (NGL), known as the Pakistan Avengers.",
      "ground_truths": [
        "Sri Lankan cricket"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "59f797b748f94ba48ef4d452bc78310b",
      "question": "Three Pakistan Tailban members where arrested wearing what",
      "prediction": "The answer is Pakistan's National Defence Authority (NDA).",
      "ground_truths": [
        "people carrying suicide jackets and explosives"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83740e4a867446d486f86cba14a2e433",
      "question": "Where were attacks averted?",
      "prediction": "The answer is Pakistan's capital city of Karachi, Pakistan.",
      "ground_truths": [
        "southern port city of Karachi,"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "e6f16711704b482a82373c1fdb384783",
      "question": "Who attached the cricket team?",
      "prediction": "The answer is: Pakistani security officials.",
      "ground_truths": [
        "Lashkar-e-Jhangvi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5e5d9c40e1545c4b6ba1881756b297f",
      "question": "who testifies about exposure in Iraq?",
      "prediction": "The answer is: The T-1300, a type of American-made, 1960s-1980s US Marine diesel engine, that the Committee on Veterans' Affairs said it contained hazardous materials.",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c101f1e6013417689d6efbc66d716f2",
      "question": "What was stationed at base when he was born?",
      "prediction": "The answer is that Michael Partain, a former U.S. Marine Corps (M-63) who was diagnosed with cancer, told the Senate Committee on Veterans' Affairs that his parents were stationed at Camp Lejeune in North Carolina when he was born.",
      "ground_truths": [
        "his parents"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "08e3cb915245438d8034ea7139799ab6",
      "question": "who was exposed to chemicals?",
      "prediction": "The answer is: The federal government.",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5f99cc82a7b48289ba83b2ed51ffc4e",
      "question": "What type of cancer did the man experience after the exposure?",
      "prediction": "The answer is:  \"The man experienced breast cancer.\"",
      "ground_truths": [
        "breast"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "4074ae28c4a74172ae8d9d74a8877e8a",
      "question": "What type of exposure occured in Iraq?",
      "prediction": "Based on the text, the answer to the question is: **Iraq.**",
      "ground_truths": [
        "by military personnel to hazardous materials"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "ab324f250d0b4f4e8dfa7c59b237a270",
      "question": "Who was exposed to chemicals in tap water?",
      "prediction": "The answer is the Marines (50, with 30 sons).",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce887cd5e83841a18f14a0a15043df12",
      "question": "who was diagnosed with breast cancer?",
      "prediction": "The context tells us that Michael Partain was diagnosed with breast cancer, and the answer is **200,000 men a year**. This is a significant amount, and it's important to note that the U.S. Marine Corps has been known for its efforts to protect its soldiers by not dumping water-contaminated water into the streets.",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0.08163265306122448
    },
    {
      "id": "afbbc141253142fab6d2b697ae4ee6fe",
      "question": "What weakened us security?",
      "prediction": "Based on the context, the answer is: **\"tarnity.\"**",
      "ground_truths": [
        "the use of torture and indefinite detention"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "766f91b668a7407690874162a7f1bf7c",
      "question": "What did John Kerry say?",
      "prediction": "John Kerry said that the US must respect its values and the rule of law. He expressed a strong commitment to the country's moral leadership. He also warned about the use of torture and detention, which he argued was a significant threat to the American people. He also stated that the use of torture and the lack of a legal system for detainees, including those who were cleared for departure, has left lasting scars in the Muslim world. He did so in a letter to the president, and this was ultimately ignored, due to the absence of evidence of improper coerced confessions.",
      "ground_truths": [
        "Barack Obama sent a message that fight against terror will respect America's values."
      ],
      "em": 0,
      "f1": 0.060000000000000005
    },
    {
      "id": "4ca71c2aabe54b088ede58a422a56ec6",
      "question": "what was the law",
      "prediction": "The answer is, yes, the American government was always committed to protecting Americans' values, and the use of torture and indefinite detention, regardless of whether it was in the name of national security, is a deeply ingrained and necessary ethical decision.",
      "ground_truths": [
        "the rule of"
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "aa17174e9fd54409b297cb9ea0371333",
      "question": "What was president Obama's message?",
      "prediction": "The answer is, \"We should all agree that it's important to protect the American people.\"",
      "ground_truths": [
        "that fight against terror will respect America's values."
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "c1ade6f519404356a8aa9a474c20374e",
      "question": "what was the decision",
      "prediction": "The answer is: I acknowledge the risk of torture and detention, but I also believe that the American people's moral authority, born from our common law, our legal system, and our unwavering commitment to upholding the rule of law, is crucial for the protection of our American people. I also believe that the use of torture and indefinite detention, as described by former Vice President Dick Cheney, is a form of institutionalized torture, and those who seek it, and the resulting dehumanization, are ill-equipped to handle it. The international system, like the Guantanamo camp, is not easily contained, and the consequences of torture, particularly",
      "ground_truths": [
        "eventual closure of Guantanamo Bay prison"
      ],
      "em": 0,
      "f1": 0.04040404040404041
    },
    {
      "id": "a58732fa10294a8fb02303af8a2b4553",
      "question": "Who highlights public spirited people?",
      "prediction": "The answer is: \"Awearness: Inspiring Stories About How to Make a Difference\"",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "15a23c40a7ff4d308287523820b86539",
      "question": "What is the name of the fashion designer?",
      "prediction": "The answer to the question is \"Awearness: Inspiring Stories About How to Make a Difference.\"",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "61fd162dc6174559989603efdfadb330",
      "question": "What use of humor leavens passion?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "And I think we also have these walls around us, and when people disagree, we're inclined not to listen, but to a degree you can break through that wall often"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2819a55322f74b0d9bbaaca2e4bf5671",
      "question": "What does Kenneth Cole highlight?",
      "prediction": "The answer is \"Awearness: Inspiring Stories About How to Make a Difference.\"",
      "ground_truths": [
        "social issues"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "993381382d96468e85b18ca37eada8f3",
      "question": "What fashion designer admires use of humor?",
      "prediction": "The answer is: Cole admires people who inspire him.",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "b76d9a4d357146cf8761f216db62cf32",
      "question": "What level is the Mongolian?",
      "prediction": "The answer is based on the context provided.",
      "ground_truths": [
        "most successful ever grand champion,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8606273fab14a929fde3fa46e58300c",
      "question": "Why was he banned?",
      "prediction": "The answer is: Asashoryu was banned from sumo wrestling in Japan for allegedly bribing wrestlers in a match-fixing incident in 2007. He was also penalized for faking a doctor's note.",
      "ground_truths": [
        "using recreational drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be35f44aabff441fa8c79147a8c67371",
      "question": "Who feigned illness?",
      "prediction": "The answer is Asashoryu, the 330-lb Mongolian sumo wrestler.",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5c79509d21e1437a949f0a5b481b26ad",
      "question": "What is the wrestler accused of?",
      "prediction": "The wrestler accused of being the child-terrible of sumo is *Kitanoumi*.",
      "ground_truths": [
        "to lose bouts,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "24eee9ea8c97417fa7655ea0363e789c",
      "question": "What is he accused of?",
      "prediction": "The answer is: Asashoryu is accused of bribing wrestlers to lose bouts, which led to a downfall in sumo's reputation and the introduction of a new, stricter, and less-honored form of \"hinkaku.\"",
      "ground_truths": [
        "bribing other wrestlers to lose bouts,"
      ],
      "em": 0,
      "f1": 0.29411764705882354
    },
    {
      "id": "41ee31948eb0428294b160f5ee76e85c",
      "question": "He is a grand champion what?",
      "prediction": "The answer is: Asashoryu is a grand champion of sumo.",
      "ground_truths": [
        "sumo wrestling"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "d6152a8e8d0a480681f85f240c9bf762",
      "question": "Who reinvigorated the sport?",
      "prediction": "The answer is Asashoryu.",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3c63a50e5cf94374a3f70b850a9c90e1",
      "question": "Who is one of the sumo's most successful ever?",
      "prediction": "The answer is Asashoryu.",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8467d64414a246cdbee345b883085de2",
      "question": "What is the name of the Mongolian sumo champ?",
      "prediction": "The answer is **Mongo**.",
      "ground_truths": [
        "Dolgorsuren Dagvadorj,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8023d1f9e86e4b7c8a862887e3d021d4",
      "question": "How old is Robert Mugabe?",
      "prediction": "The answer is 62 years old.",
      "ground_truths": [
        "84-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e1681c57e4347889007a5d3bb1bcf87",
      "question": "Millions of extra what were printed?",
      "prediction": "The answer is 9 million.",
      "ground_truths": [
        "ballots"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b532cee5e3c849fe998c2889e1bfab8f",
      "question": "How long has Mugabe ruled the country?",
      "prediction": "The answer is 28 years.",
      "ground_truths": [
        "28 years"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "00a9927209044872a29d9e1379e3e241",
      "question": "Who is the president?",
      "prediction": "The answer is: **Robert Mugabe**.",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9123f89740f0448f9249410770e1780c",
      "question": "What will Mugabe face on Saturday?",
      "prediction": "The answer is: The International Community may never find out whether the vote was free and fair.",
      "ground_truths": [
        "elections"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbb08647ed16423eb66dc10023c88798",
      "question": "What official is suspicious of rigged elections?",
      "prediction": "The official suspicious of rigged elections is Mabe.",
      "ground_truths": [
        "President Robert Mugabe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6337c3b3858489ca8604ee85b5c11dc",
      "question": "How old is Mugabe?",
      "prediction": "The answer is: The document states that \"The report brought derision from a Zimbabwean official. ...\"",
      "ground_truths": [
        "84-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cac535b4f8a844b48303a5a809a5c76e",
      "question": "Who suspects elections will be rigged?",
      "prediction": "The answer is: Zimbabwe's Electoral Commission.",
      "ground_truths": [
        "Human Rights Watch"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16be475fe48941c3b9848826b416940b",
      "question": "Where did the Movement for Democratic Change secretary suspect will happen in the elections?",
      "prediction": "The Movement for Democratic Change suspected that the African country's electorate of 5.7 million would be 5.7 million.",
      "ground_truths": [
        "President Robert Mugabe intends to rig"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca30d55d5a03439a985864d0da100ef6",
      "question": "How long has Robert Mugabe ruled Zimbabwe?",
      "prediction": "The answer is 28 years.",
      "ground_truths": [
        "28 years"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "506ee84329dd45168c9ad80ccb2c4890",
      "question": "What year is the World Championships?",
      "prediction": "The answer is 2010.",
      "ground_truths": [
        "2009"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c0731c5f9444bef8b93d3b5e81579ea",
      "question": "Where were the 2009 World Championships held?",
      "prediction": "The answer is London, England.",
      "ground_truths": [
        "Scotland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0476dedf5ec040b1b0ff875bb6891391",
      "question": "Who started Swamp Soccer?",
      "prediction": "The answer is: Glasgow Rangers.",
      "ground_truths": [
        "cross-country skiers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b07600ccc1b34b68af3d83f3e558516e",
      "question": "What was started by cross-country skiers?",
      "prediction": "The answer is \"local bogw\", which means muddy.",
      "ground_truths": [
        "\"Swamp Soccer\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c67aaa46c3bf4d2c81e972b4b41905a9",
      "question": "Who makes wheelchairs?",
      "prediction": "The answer is \"The Iraqi boys, moms, and fathers.\"",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f106c4d596a41a393aeb1a49f3d396b",
      "question": "Where are wheelchairs made that are delivered by the U.S. military?",
      "prediction": "The answer is \"Iraq\" (Iraq).",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65669ddf87c8475386a72370395b8a22",
      "question": "What did the Dad of three disabled children say?",
      "prediction": "The father of three children, Khaled, said, \"I am sick of life ‚Äì what can I say to you?\"",
      "ground_truths": [
        "can I"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "569caf1ecad34c36b3aa62a28faffb74",
      "question": "What was the vision of Brad Blauser?",
      "prediction": "The answer is, Brad Blauser's vision of helping Iraqi children was to make life a little easier for the families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchairs for Iraqi Kids, and the U.S. military.",
      "ground_truths": [
        "organizing the distribution of wheelchairs,"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "adf40d194a474f1881d407c00bdb62db",
      "question": "What was the vision of American contractor Brad Blauser?",
      "prediction": "The answer is: Brad Blauser's vision of American contractor was to make life a little easier for these families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchairs for Iraqi Kids.",
      "ground_truths": [
        "make life a little easier"
      ],
      "em": 0,
      "f1": 0.21621621621621626
    },
    {
      "id": "1e851401e19a42e1a155231b92e63312",
      "question": "Who make the wheelchairs in South Dakota?",
      "prediction": "The answer is: The U.S. military.",
      "ground_truths": [
        "prisoners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef6f214df0ae42cc83130d61fb329036",
      "question": "Who makes the wheelchairs distributed by the U.S. military?",
      "prediction": "The answer is the U.S. military. (The CNN report)",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e17e9f25a202485c8d14b354a92e101d",
      "question": "Who beat Federer in the match?",
      "prediction": "The answer is **Juan Martin Del Potro**.",
      "ground_truths": [
        "Juan Martin Del Potro."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2a1d9283cc9e4b2c8254c54dc6ca538d",
      "question": "In what city did the tournament take place?",
      "prediction": "The answer is **Genoa**.",
      "ground_truths": [
        "Genoa."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fcaa65142b5e4b9f9131b1ff61bccbcc",
      "question": "How much was Federer fined?",
      "prediction": "The answer is $1,500.",
      "ground_truths": [
        "$1,500"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ddf8070fbf124b54bfdca93791ef2056",
      "question": "Who did Federer confront?",
      "prediction": "The answer is: The World Number One.",
      "ground_truths": [
        "umpire Jake Garner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32e5e53144904525bdd91c2346ecdc21",
      "question": "what did Del Potro won?",
      "prediction": "The answer is **$1,500**.",
      "ground_truths": [
        "his first grand slam,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "75d472336f5146239acd2998b0eb0bd3",
      "question": "How many sets was the final match?",
      "prediction": "The answer is 5. The context states \"Roger Federer has been hit with a $1,500 fine for swearing at the umpire during his shock U.S. Open final defeat to Juan Martin Del Potro.",
      "ground_truths": [
        "five-set"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62747a42a9074e7da75c495cc4daca38",
      "question": "how much money did Roger Federed fined for his argument with umpire?",
      "prediction": "The answer is $1,500.",
      "ground_truths": [
        "$1,500 fine"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "574519a5dbfc4727bd68c1f4f0ad92b0",
      "question": "who confronted Federer?",
      "prediction": "The answer is \"Clijsters.\"",
      "ground_truths": [
        "Jake Garner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30e3992d09454ab9ab96ec7a3ded0a29",
      "question": "Where was the Vancouver Olympic Torch lit?",
      "prediction": "The Vancouver Olympic Torch was lit in a ceremony at the ancient Greek site of Olympia on Thursday, less than four months ahead of the Olympic Games' opening ceremony.",
      "ground_truths": [
        "of Olympia"
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "8ef8fd98f839468294d2d3ee05b6ac8e",
      "question": "Where will it be lit",
      "prediction": "The answer is Canada.",
      "ground_truths": [
        "of Olympia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16ee96b324b84dbb817b18b4861a48fa",
      "question": "When is the Olympic Flame due to arrive in Vancouver?",
      "prediction": "The Olympic Flame is due to arrive in Vancouver, British Columbia, on February 12, 2010, at the domed BC Place Stadium.",
      "ground_truths": [
        "February 12"
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "118155e6c91348bb855638c1150b4cfb",
      "question": "What makes history",
      "prediction": "The answer is: the torch is carried on an eight-day trip through Greece, the birthplace of the Olympics, before being transported to Canada for the longest domestic torch relay in the Games' history.",
      "ground_truths": [
        "the longest domestic torch relay in the games'"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "ecb6fdce8c384ec4ab6816743369a33f",
      "question": "who was exposed to cancer-causing chemical at Iraq plant?",
      "prediction": "The answer is \"the Army Corps of Engineers of Iraq,\" and the Chicago-based Houston-based contractor KBR.",
      "ground_truths": [
        "16 Indiana National Guard soldiers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f6cc70366444535856b1723cb8a412f",
      "question": "What did the suit allege about KBR",
      "prediction": "The suit alleges that the U.S. military has exposed the soldiers to a cancer-causing toxic chemical in the water plant, in accordance with the 2003 suit against KBR.",
      "ground_truths": [
        "company knowingly exposed the soldiers to a cancer-causing toxic chemical."
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "7e1c7da2d759475a884c617a527cb3a9",
      "question": "what company says it's not to blame for creating unsafe conditions at plant?",
      "prediction": "The company says it is \"not to blame for creating unsafe conditions at the plant.\"",
      "ground_truths": [
        "Kellogg Brown and Root,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d90f411d7d14ea6997a41d9a95f785a",
      "question": "who knew of contamination threat and did nothing to protect soldiers?",
      "prediction": "The answer is KBR.",
      "ground_truths": [
        "Houston-based company Kellogg Brown"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d0d71a5c8444f8f9af90158cc607d45",
      "question": "The suit says Guardsman wsa exposed to what",
      "prediction": "The answer is: KBR's lawsuit against Houston-based KBR, alleging the company knowingly exposed the soldiers to a toxic chemical, does not contain a specific number of toxic chemical compounds.",
      "ground_truths": [
        "cancer-causing toxic chemical."
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "dde0fd30f93b4eb8977287881411a2a2",
      "question": "Who did Condoleeza Rice say was used by non-state actors?",
      "prediction": "The answer is \"No, Condoleeza Rice did not say that the deadly attack on India's financial capital last month was planned inside Pakistan. She said, \"It's not compelling evidence of involvement of Pakistani officials. But I do think Pakistan has a responsibility to act.\"",
      "ground_truths": [
        "Pakistani territory"
      ],
      "em": 0,
      "f1": 0.046511627906976744
    },
    {
      "id": "e4063c87f2db432da6bea32bdbd9b4ca",
      "question": "What is Pakistan obligated to do according to Rice?",
      "prediction": "The answer is: Pakistan is obligated to root out terrorist attacks within its borders.",
      "ground_truths": [
        "root out terrorists within its borders."
      ],
      "em": 0,
      "f1": 0.5263157894736842
    },
    {
      "id": "727e9e359e494729b6230a54f0a0e54d",
      "question": "What did Rice say about Pakistan's obligations?",
      "prediction": "The answer to the question is: \"No, Rice did not say about Pakistan's obligations.\"",
      "ground_truths": [
        "is obligated to root out terrorists within its borders."
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "4c2b980aefd442bfaf443a14dfc548cf",
      "question": "Who did India blame for the Mumbai attacks?",
      "prediction": "The answer is \"Pakistan.\"",
      "ground_truths": [
        "Lashkar-e-Tayyiba (LeT),"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fa48dfd60ff499391b05561646ddf11",
      "question": "How many people died in the Mumbai attacks?",
      "prediction": "The answer is 10.",
      "ground_truths": [
        "170"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e59c51ab66a74987991669ef16dec043",
      "question": "What position does Condoleezza Rice hold?",
      "prediction": "Answer: Condoleezza Rice holds a position of Condoleezza Rice is not a policy of the U.S. (and is not a \"department of the US)",
      "ground_truths": [
        "Secretary of State"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "b25284d251ef491d9c0b6273f654d1c8",
      "question": "What did India allege?",
      "prediction": "The answer is: \"We have no concrete evidence to prove that Pakistan was not involved in the Mumbai attacks. We have a responsibility to cooperate with the United States and other countries.\"",
      "ground_truths": [
        "terrorists operating within its borders."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2d80efa9b7b479180a74423dfc26450",
      "question": "What does he say about humans?",
      "prediction": "The answer to the question is: \"The gift of fear.\"",
      "ground_truths": [
        "will sense danger yet still walk right into it."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbe0a747cc2143b493b0cb079d9364dd",
      "question": "When did the victims recall feeling uneasy?",
      "prediction": "The victims recalled feelings of \"uneakness\" and \"panic attacks.\"",
      "ground_truths": [
        "noticed a UPS delivery box where it shouldn't be."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0924af12440f4773bc02e167ab91616f",
      "question": "What is the only animal who senses danger and walks into it?",
      "prediction": "The answer is \"Fear,\" but the \"fear\" is not a particular animal in the context.",
      "ground_truths": [
        "humans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a42c52547f04e958698f5040284ad01",
      "question": "Who are the only animals who can sense danger?",
      "prediction": "The answer is \"Humans,\" and the video's \"story of a woman named Kelly begins with a warning sign.\"",
      "ground_truths": [
        "humans"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "a316b27e5c304b808e861d89d8d72238",
      "question": "Who says fear can be a gift?",
      "prediction": "The answer is \"Fear,\" as the context implies.",
      "ground_truths": [
        "Gavin de Becker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e58fb5569ab14dc18252ec9ebe19a0e4",
      "question": "Who felt uneasy?",
      "prediction": "The question asked: Who felt uneasy?",
      "ground_truths": [
        "Nicole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bfbcdab7351542ffb580b819b8be0db7",
      "question": "What should you tell his mom?",
      "prediction": "The correct answer is:",
      "ground_truths": [
        "mention how"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8288e34202a43ba912afef8cf20ca42",
      "question": "Post what to facebook to get back at your ex?",
      "prediction": "Here's the answer, based on the context:",
      "ground_truths": [
        "photo album full of pics of you looking smiley."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "edf07fccef7d409596c9a49cf47e207e",
      "question": "Where should you post \"pix\"?",
      "prediction": "The answer is the following:",
      "ground_truths": [
        "Facebook"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "42b41c9d7daa4fb89b93e94932183fa5",
      "question": "Where should you post pix?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "Facebook"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4509a472ffd6415aad18c8dc2da90e69",
      "question": "When should you tell he cheated on you?",
      "prediction": "Here is the answer: \"When you should tell him cheat on you is often a complex question. It depends on the specific context. Here are some common responses and considerations to ask:",
      "ground_truths": [
        "your ex's loved ones ask why"
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "72d5f68909b64c34894c5f1bd6766b19",
      "question": "What impeded rescue efforts?",
      "prediction": "The answer is, Galveston, Texas.",
      "ground_truths": [
        "flooding and debris"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbec4bf4aa824ceaa082b7cf6640011e",
      "question": "What was impeding rescue efforts?",
      "prediction": "The answer is, there is no answer. The context provides no information about the speed or nature of the rescue efforts.",
      "ground_truths": [
        "heavy flooding and scattered debris."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "51a87a55365340bc864d259f036bd38d",
      "question": "What's the name of the tropical storm?",
      "prediction": "The answer is \"Galveston, Texas.\"",
      "ground_truths": [
        "Hurricane Ike,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a6a960ca31344e69c202561b9687db7",
      "question": "Who rode out the storm at home?",
      "prediction": "The answer is, \"We are in a recovery mode,\" and the text doesn's explicitly state who. The information on Houston residents chose to stay and is provided at the end of the text.",
      "ground_truths": [
        "D.J. Knight of Pearlman, Texas, decided to ride"
      ],
      "em": 0,
      "f1": 0.11428571428571428
    },
    {
      "id": "17b6b8ced336420da3ab95bd9da12e60",
      "question": "How many were affected by the power outages?",
      "prediction": "The answer is 27.",
      "ground_truths": [
        "20,000 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8ae61251f0d48168e7974d197c2353d",
      "question": "How many deaths were attributed to Ike?",
      "prediction": "The answer is 27.",
      "ground_truths": [
        "four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f5fb5d4b20344b0971b7a250d8e452c",
      "question": "Where was the wereckage?",
      "prediction": "The answer is **Galveston, Texas.**",
      "ground_truths": [
        "Galveston, Texas,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1d4fdb8572534e2e9f138fb02278083e",
      "question": "What storm caused four deaths in Texas?",
      "prediction": "The answer is: The Galveston, Texas storm caused four deaths.",
      "ground_truths": [
        "Ike"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc9a1c8de6ba491aa2161ec2236f2830",
      "question": "when was the attack?",
      "prediction": "The answer is India's Foreign Ministry. The Ministry's response says that Kasab, 21, was a member of the 10 gunmen who attacked several targets in Mumbai on November 26, 2023.",
      "ground_truths": [
        "November 26,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "b9570d5dc5754c3da865896d3c701553",
      "question": "What did Kasak admit to?",
      "prediction": "Indian authorities said Kasab admitted to writing a letter to Pakistan's High Commission.",
      "ground_truths": [
        "he was one of 10 gunmen who attacked several targets in Mumbai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28c201d7c1a049e3a2f1b1d2058acf03",
      "question": "what did officials say",
      "prediction": "The officials said they received no information or evidence regarding the Mumbai incident, and the Pakistan's High Commission did not provide any information to India's foreign ministry.",
      "ground_truths": [
        "Kasab had admitted he was one of 10 gunmen who attacked several targets in Mumbai on November 26,"
      ],
      "em": 0,
      "f1": 0.04761904761904762
    },
    {
      "id": "11c7856a8f4d4bb89c20bb2d212c120e",
      "question": "Who does the surviving attacker seek help from",
      "prediction": "The surviving attacker seeks help from Pakistan's High Commission.",
      "ground_truths": [
        "Pakistani officials,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d59b5a2e398a4687b6ea3404b7ffa789",
      "question": "who wrote a letter",
      "prediction": "The answer is \"Pakistan's Foreign Ministry.\"",
      "ground_truths": [
        "The suspect, Mohammed Ajmal Amir Kasab,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4141a787f20a49ff845742502d21baba",
      "question": "what did kasab admit",
      "prediction": "The question asks for the answer to be directly based on the provided context.",
      "ground_truths": [
        "he was one of 10 gunmen who attacked several targets in Mumbai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01110d58a77142f9a89705de4d78dbac",
      "question": "Who did  Kasak write a letter to?",
      "prediction": "Answer: Pakistan's High Commission in New Delhi.",
      "ground_truths": [
        "Pakistan's High Commission in India"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "af6bf271717f4e8a8b0fec2b8a4120ef",
      "question": "who joins the attorneys",
      "prediction": "The answer is \"the prosecution delivered a 90-minute opening statement at the U.S. District Court for the Eastern District of Michigan,\" and \"AbdulMutallab was indicted on charges including attempted use of a weapon of mass destruction, conspiracy to commit an act of terrorism, and possession of a firearm or destructive device in furtherance of an act of violence.\"",
      "ground_truths": [
        "AbdulMutallab"
      ],
      "em": 0,
      "f1": 0.039999999999999994
    },
    {
      "id": "7739bb07f2e641089a5f30a0c540234f",
      "question": "What witness said a passenger yelled something",
      "prediction": "The answer is: Attendees who were on the flight.",
      "ground_truths": [
        "Michael Zantow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37b9ed4b33a7417593525ba9f53b5742",
      "question": "what does the prosecutor say of Abdulmutallab?",
      "prediction": "The prosecutor's statement to the U.S. District Court for the Eastern District of Michigan's opening statement on Tuesday, at a later time, states that the prosecution will deny the motion to exclude the photo.",
      "ground_truths": [
        "\"His sole reason for being on Flight 253 was to kill all of the passengers and himself. He thought he'd end up in heaven.\""
      ],
      "em": 0,
      "f1": 0.15999999999999998
    },
    {
      "id": "afdfa4bf455643d9b82c0305fef02b9f",
      "question": "what did the witness says?",
      "prediction": "The answer is, the witness said, \"Hey man, your pants are on fire!\":",
      "ground_truths": [
        "that AbdulMutallab was in the bathroom for about 15 to 20 minutes,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "79918fdc580d45b9a26512ae87fe4fca",
      "question": "What person thought they would end up in heaven",
      "prediction": "The answer is, the prosecution denies that AbdulMutallab was carrying a bomb. The judge in the federal trial of alleged \"underwear bomber\" Umar Faruk AbdulMutallab refused to allow the prosecution to use the words \"explosive device\" and \"bomb\" during the trial.",
      "ground_truths": [
        "AbdulMutallab,"
      ],
      "em": 0,
      "f1": 0.05882352941176471
    },
    {
      "id": "26d739c555714488a425a4675291dcb9",
      "question": "What adult remained calm throughout",
      "prediction": "The answer is: **Abigail (U.S. District Court for the Eastern District of Michigan) Judge Nancy Edmunds.**",
      "ground_truths": [
        "AbdulMutallab"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3997d6350e36484e8157c6fcee5d9523",
      "question": "What did Magee apologize for?",
      "prediction": "Magee apologized for any abuse he had committed, and he offered his sincere apologies to those who had been abused by any priest of the Diocese of Cloyne.",
      "ground_truths": [
        "Catholic church sex abuse scandal,"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "3df1cf455a204c7ea4d7e9d3510f2073",
      "question": "Does Vatican accept Bishop John Magee's resignation ?",
      "prediction": "The answer is yes, Vatican does accept Bishop John Magee's resignation.",
      "ground_truths": [
        "it has been accepted,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6dd8ec6ba0d94b7b9b84f549ab000ca9",
      "question": "Who's resignation does the Vatican accept?",
      "prediction": "The Vatican accepts the resignation of Bishop John Magee of Cloyne.",
      "ground_truths": [
        "Bishop John Magee"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a62b164f214c4f1a829a822730275b14",
      "question": "How many irish bishops resigned since December ?",
      "prediction": "The answer is 4.",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18f073004adb4b668d69638bb2c2ede6",
      "question": "What will happen Sunday morning?",
      "prediction": "The answer is, Thailand is still facing two separate threats.",
      "ground_truths": [
        "Another high tide"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f5dc60b23b934c858913fdb568523b0d",
      "question": "what does Thailand is dealing with ?",
      "prediction": "Bangkok is dealing with two converging threats:",
      "ground_truths": [
        "severe flooding"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afbc7ecd775e4a99b99f7102082cbb8e",
      "question": "Where is the flood?",
      "prediction": "The answer is Bangkok, Thailand.",
      "ground_truths": [
        "Bangkok"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "70f8bc2a7e5749c4a8a759c2ee7c2cee",
      "question": "What country is suffering floods?",
      "prediction": "The answer is Thailand.",
      "ground_truths": [
        "Thailand"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "558a58525a3c4d32a0a0ec255883368c",
      "question": "what is the forecast for Sunday morning?",
      "prediction": "The answer is: Bangkok's outer suburbs were already submerged, but the central city has been largely spared the misery Thailand has been suffering for months.",
      "ground_truths": [
        "Another high tide"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "56d8cd1f1cc14c38b975f03fee9d985c",
      "question": "What is the high tide forecast?",
      "prediction": "The high tide forecast for Bangkok is expected to be about 1.5 to 2 meters (5 to 6 feet), with a significant, potentially significant, and potentially major, high tide approaching Sunday morning.",
      "ground_truths": [
        "expected to reach about 4 meters (13 feet)"
      ],
      "em": 0,
      "f1": 0.2631578947368421
    },
    {
      "id": "d1dfcaa42fb54127ac19e50f2c2adc21",
      "question": "Where is the largest center for breeding and exporting terrorism?",
      "prediction": "The answer is Afghanistan. The context provides no information on the largest center for breeding and exporting terrorism.",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10a96cc02bef41ff853195cc0c11ac24",
      "question": "Who makes allegations without proof?",
      "prediction": "The answer is: **Pakistan's intelligence agency.**",
      "ground_truths": [
        "Afghan lawmakers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a8456d57d4f496e8d4ee674693d34a4",
      "question": "What country is \"largest center for breeding and exporting terrorism\"?",
      "prediction": "The answer is \"Pakistan,\" as the provided text states.",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a0471461fcf40ee98f9639a39a73de6",
      "question": "What kind of militants are attacking?",
      "prediction": "The answer is: \"The people of Afghanistan.\"",
      "ground_truths": [
        "Islamic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd12c78d2b504fae811d89ba33fa9a61",
      "question": "Is Pakistan the leading terrorist country?",
      "prediction": "The answer is unequivocally, **Pakistan's intelligence agency, the Afghan Cabinet, made the scathing indictment in a resolution on Monday, stating that Afghanistan would boycott a series of meetings with Pakistan unless \"bilateral trust\" is restored.**",
      "ground_truths": [
        "\"the largest center for breeding and exporting terrorism.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "753c70ba368244869149d377ee86e4af",
      "question": "What has killed scores of people in Afghanistan?",
      "prediction": "The answer is \"The two incidents, along with an assassination attempt against Afghan President Hamid Karzai at a military ceremony on April 27, are \"indicative of the attempts by the ISI (Inter-Services Intelligence) to once again occupy Afghanistan and to perish the true right of the people of Afghanistan for national sovereignty.\"",
      "ground_truths": [
        "suicide bomber"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21f10f73f5d94cce841b6f93f68bb9bf",
      "question": "Who accused of orchestrating attacks?",
      "prediction": "The answer is: Pakistan.",
      "ground_truths": [
        "Pakistan's intelligence service"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db16711529134ff3ba582b3aa4971373",
      "question": "Where is the killing the worse?",
      "prediction": "The answer is Afghanistan. The context explicitly states that Afghan lawmakers \"directly accused Pakistan's intelligence agency of involvement in a string of deadly attacks in Afghanistan, blasting their neighbor as the largest center for breeding and exporting terrorism.\"",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "dc0659b5894a487fa98b941caa1486b1",
      "question": "Who suffered first defeat of the tournament in New Zealand?",
      "prediction": "The answer is France. The team lost to New Zealand in the group stage, and in the final, they suffered a 16-0 defeat to Argentina.",
      "ground_truths": [
        "Ireland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "950471917d1b470d8bdb484f1a4ec3ab",
      "question": "Where did the tournament take place?",
      "prediction": "The answer is France, after a 19-12 loss to England in Auckland, then a 16-0 halftime lead at Eden Park, where winger Vincent Clerc scored a try, before England equalled with a penalty.  The tournament took place in New Zealand against Argentina, South Africa against Australia, and then in Wales against France.  The National Football Teams then went on to lose to New Zealand and Tonga. The following match was a 16-0 score for France in a late-for England goal, when Frenchman substitute Mathieu Colladdition converted a penalty.  The Cup final was in Wellington, with",
      "ground_truths": [
        "New Zealand"
      ],
      "em": 0,
      "f1": 0.047058823529411764
    },
    {
      "id": "a40f6724f3804b098d5497661da35619",
      "question": "When did Les Bleus last lose to England?",
      "prediction": "The answer is: France's reputation as rugby's Jekyll and Hyde team was reaffirmed on Saturday as Marc Lievremont's inconsistency in a loss to England.",
      "ground_truths": [
        "2007"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a0d5463f68f9491a9266e492a057d428",
      "question": "Who won the game 19-12 in the World Cup quarterfinals?",
      "prediction": "The answer is France.",
      "ground_truths": [
        "Les Bleus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "815441b0e2b74f1b89e8d877d64e842a",
      "question": "What was the score in semifinal game of Wales-Ireland?",
      "prediction": "The answer is: Argentina scored a try in the final.",
      "ground_truths": [
        "22-10."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c2744f9a22d4fcb8921360fc515824d",
      "question": "What country defeated England in quarterfinals?",
      "prediction": "The answer is France.",
      "ground_truths": [
        "France's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fda9c59e8374ccda356e7822945c769",
      "question": "when was she found",
      "prediction": "The answer is **November 1, 2002**.",
      "ground_truths": [
        "nine months later,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b65b25205b634394ac78d0413098bd39",
      "question": "Who was charged with kidnapping Elizabeth Smart from her bedroom?",
      "prediction": "The answer is **U.S. District Judge Dale Kimball**.",
      "ground_truths": [
        "Brian David Mitchell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "00d92a1afce34a8196d1c7e27038014b",
      "question": "what age was smart when she was kidnapped",
      "prediction": "Based on the context, the answer is 21.",
      "ground_truths": [
        "14,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b2f02c184e7f4d4f91bc1f7b87260e56",
      "question": "Who agreed to cooperate against her husband?",
      "prediction": "The answer is **Wanda Eileen Barzee**.",
      "ground_truths": [
        "Barzee,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "60aaf4fca83f4fd9ac3dd152fff0de6d",
      "question": "What age was Smart?",
      "prediction": "The answer is 21.",
      "ground_truths": [
        "14,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7586c657762a4899b2a03fa3a483f26c",
      "question": "who was charged",
      "prediction": "The answer is \"New York City.\"",
      "ground_truths": [
        "Brian David Mitchell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1272079830e045288cf3d0752a6fa3d7",
      "question": "when was T.I arrested?",
      "prediction": "The answer to the question is T.I. was arrested in federal custody, after his weekend arrest on gun charges.",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "391a7fdd51944e06afa3c7fe338dea81",
      "question": "Where was T.I. arrested?",
      "prediction": "The answer is, T.I. was arrested in Atlanta, Georgia.",
      "ground_truths": [
        "midtown Atlanta."
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "ed0bb059347f4fb38dcec73cb6fc8553",
      "question": "Who won awards?",
      "prediction": "The answer is: T.I.",
      "ground_truths": [
        "Harris"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e983455b5be4ee788c28ee5b889972c",
      "question": "When will the bond hearing take place?",
      "prediction": "The answer is, after a bond hearing on Friday.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "3af5fd79dfe743be9e29cec27572fe13",
      "question": "What was the rapper T.I. arrested for on Saturday?",
      "prediction": "The rapper T.I. was arrested in federal custody over the weekend.",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e0906b93e02441783d5b68582e59d81",
      "question": "What weapons was T.I carrying?",
      "prediction": "Answer: T.I. was carrying three machine guns and one loaded gun, with one unloaded.",
      "ground_truths": [
        "three machine guns"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "d571d248d30b4ecaba93f1eb28f88abd",
      "question": "what did TI do?",
      "prediction": "Answer: TI did not do anything. He was arrested in Atlanta, Georgia, on charge of gun charges.",
      "ground_truths": [
        "Harris provided the bodyguard $12,000 to buy the weapons,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b3812b214fc04fad833518e375f34e58",
      "question": "When was T.I. arrested",
      "prediction": "The answer is yes, T.I. was arrested in federal custody.",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6a3831e65bd4a06aa984a16c845edd5",
      "question": "What is the rapper charged with?",
      "prediction": "The rapper was charged with felony drug charges in 1998 and a federal felony charge for firearm possession. He was also charged with gun possession. He is also charged with a state felony charge for his arrest in 1998 for his body-changer-informant. He was captured in Atlanta, South Georgia, in federal custody.",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 0,
      "f1": 0.07843137254901959
    },
    {
      "id": "49e37f2ab89a42948fbd2249c10c2ddb",
      "question": "What is T.I. in custody for?",
      "prediction": "T.I. is in custody for three felony drug charges, including one found in Cobb County, Georgia. The affidavit states he has also been arrested on gun charges in the past.",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "0d0a9395b6cb46db9f64e6acad197fd6",
      "question": "What did T.I. got arrested for",
      "prediction": "T.I. received $12,000 in cash from a bank to buy the guns.",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af1df0d16de14e6187d0a6906505ac65",
      "question": "Who was arrested on weapons charges?",
      "prediction": "The answer is \"Central Georgia\" (Georgia).",
      "ground_truths": [
        "Clifford Harris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13990b7394164f97b4c35cc400d63d5e",
      "question": "Did the rapper win any awards at the BET show?",
      "prediction": "The answer is yes, the rapper won two awards at the BET Hip Hop Awards.",
      "ground_truths": [
        "Harris won two"
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "2cf921f1a4854045bd615640a37b2bad",
      "question": "What is happening to T.I?",
      "prediction": "AtLANTA, Georgia (CNN)",
      "ground_truths": [
        "will remain in custody"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "48cc49890e394c47ab4443d4f2e65e46",
      "question": "Who remains in custoday until Friday?",
      "prediction": "The answer is: The entertainer, whose real name is Clifford Harris, is in custody until a bond hearing Friday.",
      "ground_truths": [
        "T.I., whose real name is Clifford Harris,"
      ],
      "em": 0,
      "f1": 0.5217391304347825
    },
    {
      "id": "3afe86583cc04c64ad908b3b2ae889e1",
      "question": "When is T.I.s bond hearing scheduled for",
      "prediction": "The answer is \"until a bond hearing Friday,\"",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "ae2b7dd03781453d9e0ef25d421d808d",
      "question": "when was arrested the raper?",
      "prediction": "The answer is Wednesday, November 19, 1998.",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c46fbc04a814abc9146acfa990a1b4e",
      "question": "When is the bond hearing?",
      "prediction": "The answer is Wednesday, October 26, 2023.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cff10f8297c64447bf388d45ed577e95",
      "question": "When does the agreement expire?",
      "prediction": "The answer is: \"The withdrawal marks the first time since the invasion of Iraq.\"",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4790b60660da497bb390ed9bc5996973",
      "question": "What number of troops will remain?",
      "prediction": "The answer is 100 to 150.",
      "ground_truths": [
        "a dozen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dcd06efd15d8483f9ae99008aae4b8e6",
      "question": "Who are being pulled out of Iraq?",
      "prediction": "The answer is: \"Iraq's remaining contingent of 100 to 150 troops is leaving Iraq for Kuwait, a Ministry of Defence spokesman told CNN.",
      "ground_truths": [
        "Almost all British troops"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "2b41e95bcc554f69b8d594bc3d05bf1a",
      "question": "Which troops are being pulled out",
      "prediction": "The answer is: The United Kingdom's remaining contingent of 100 to 150 British troops, and a few Iraqi army troops, is being withdrawn for Kuwait's parliament's withdrawal.",
      "ground_truths": [
        "British"
      ],
      "em": 0,
      "f1": 0.07999999999999999
    },
    {
      "id": "1c3202898a2340d8b9ee33b3d2feb483",
      "question": "What day does the agreement for them to be there expire?",
      "prediction": "The answer is Thursday. The context states that \"almost all British troops in Iraq will be removed by the week's end,\" and the answer from the Ministry of Defence says the withdrawal is the first, and no British troops are there.",
      "ground_truths": [
        "Friday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f5b7236c16734b2388cf37cfd5cbf27b",
      "question": "Which countries troops are being pulled out?",
      "prediction": "The countries being pulled out are:",
      "ground_truths": [
        "Britain's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f86df4ac102d4a058ac5a346fed44de9",
      "question": "What team is associated with Diego Maradona?",
      "prediction": "The answer is Italy.",
      "ground_truths": [
        "Argentine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "435fd88dd9034cf1bd211e03bab2bed5",
      "question": "What position does Carlos Tevez play?",
      "prediction": "The answer is **Argentina**.",
      "ground_truths": [
        "striker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a14a2ea82a1544a19f7c45cbf7f82f6e",
      "question": "Who urges Carlos Tevez to quit Manchester Utd at end of season?",
      "prediction": "The answer is **Argentina**. Diego Maradona urged Carlos Tevez to quit Manchester United at the end of the season and head for Italy, which Coach Maradona advised Tevez to leave at the end of the season.",
      "ground_truths": [
        "coach Diego Maradona"
      ],
      "em": 0,
      "f1": 0.17647058823529413
    },
    {
      "id": "48685e35e9f24e6f94468729a1571a18",
      "question": "What bench was Tevez on?",
      "prediction": "The answer is Italy.",
      "ground_truths": [
        "Manchester United"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f64621c513a84c33a48b8f024d190371",
      "question": "Who is the Argentine coach?",
      "prediction": "The Argentina coach is Diego Maradona.",
      "ground_truths": [
        "Diego Maradona"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "308816ab37144748816002e1b38d2e0b",
      "question": "Where was Tevez when Maradona watched United beat Chelsea ?",
      "prediction": "The answer is Argentina. Maradona urged Carlos Tevez to quit at the end of the season.",
      "ground_truths": [
        "on the bench"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8bf4e9d3157446bd95bbea12227c100b",
      "question": "Who believes striker would be better off joining Italy's Inter Milan?",
      "prediction": "The answer is Argentina. Diego Maradona, under the tutelage of Argentina's coach, Diego Rio de Janeiro (also known as Rio de Janeiro (December 2006) under Diego's direct leadership), urged Tevez to quit at the end of the season.",
      "ground_truths": [
        "Argentine coach Maradona"
      ],
      "em": 0,
      "f1": 0.10810810810810811
    },
    {
      "id": "06631314756e4d9fa7eb5f040d4d9da4",
      "question": "When will Tevez quit?",
      "prediction": "The answer is Argentina.",
      "ground_truths": [
        "end of the season."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4cedc34c6b2744e5b9385e0a2c2e7208",
      "question": "Who is at the top of Isreal's threat list?",
      "prediction": "Based on the context, the answer is **Israel.**",
      "ground_truths": [
        "Iran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2df5a7b010e644369ace3fe64a1d8fa8",
      "question": "What does an analyst think of Iran developing a nuclear bomb?",
      "prediction": "Based on the context, the answer is: **Israel makes no secret. The scale of the threat is less clear cut.**",
      "ground_truths": [
        "could develop a"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b825f11fc8544f39b67d9e0e276ed3a",
      "question": "Who believes Iran still has secret sites?",
      "prediction": "Based on the context, the answer is **Israel.**",
      "ground_truths": [
        "Israel"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "c74ea36d83af4441be3167fda8bee5c6",
      "question": "What was man accused of?",
      "prediction": "The question is, what was the accused of.",
      "ground_truths": [
        "gunned down four Lakewood, Washington, police officers Sunday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb8c1477740b4bd18455b3055f5bb98a",
      "question": "When did Huckabee commute the sentence?",
      "prediction": "The answer to the question is provided in the context.",
      "ground_truths": [
        "In May 2000,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "4eabcc0952174c2aa97df657e13776b1",
      "question": "What percentage of requests were denied?",
      "prediction": "The answer is 10¬Ω.",
      "ground_truths": [
        "\"Ninety-two percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85eb4e6d469f4dc3aa73c89a9f8c13e3",
      "question": "on what date the event occurred",
      "prediction": "The event occurred on Tuesday.",
      "ground_truths": [
        "May 2000,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "956f69c5b64e48238441d49ffa32e523",
      "question": "When did the surgery occur?",
      "prediction": "The answer is on the last sentence of the context.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "646c226fc7de4bf8ab2940795db0b2c5",
      "question": "Who gave Cole a new kidney?",
      "prediction": "The answer is Cole, the daughter of Nat King Cole.",
      "ground_truths": [
        "someone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f66c93d5e36e45edbf7f8e1dda09156c",
      "question": "When did she have her surgery?",
      "prediction": "Answer: On Tuesday in Los Angeles.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "ff7c4097100d487aa28a0e5482de7448",
      "question": "Where did the kidney come from?",
      "prediction": "The kidney came from Cole's kidney donor in Los Angeles.",
      "ground_truths": [
        "a deceased organ donor,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "5a3bf32e88574c78a5ef3fcb1756502f",
      "question": "What did the singer receive when on CNN?",
      "prediction": "The singer received a kidney transplant in Los Angeles.",
      "ground_truths": [
        "new kidney"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "a2a1fb5d4f984d009cf64e937c5df55d",
      "question": "When was her surgery?",
      "prediction": "The answer is after Tuesday, October 27, 2008.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "4f014120341b4f609cf55f752f70e1ae",
      "question": "Who received a new kidney?",
      "prediction": "The answer is: Cole received a new kidney.",
      "ground_truths": [
        "Natalie Cole's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e6a38ca6c644c0e8fc56f5779009ab2",
      "question": "What newspaper says about landing?",
      "prediction": "The newspaper says the British military officials on Sunday ordered a Chinook helicopter to land in a field next to Kate Middleton's house, in a field, at 20 seconds on the ground.",
      "ground_truths": [
        "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0.17543859649122806
    },
    {
      "id": "42fb458e399b41f9b78a83d4b30d27b0",
      "question": "What Ministry of Defense says about the operation?",
      "prediction": "The Ministry of Defense says that the operation, which involved a Chinook helicopter, was part of a planned training exercise aimed to help the prince learn to fly in combat situations in Iraq and Afghanistan.",
      "ground_truths": [
        "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations."
      ],
      "em": 0,
      "f1": 0.6382978723404256
    },
    {
      "id": "bd0f2b0fc40146edb228ac6be2fd9e5a",
      "question": "What did Prince William use to visit a girlfriend?",
      "prediction": "The answer is likely a Chinook helicopter.",
      "ground_truths": [
        "Royal Air Force helicopter"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "a71f1a9b718241dd890fd8feb1be3ab8",
      "question": "What did the Ministry of Defense say about it?",
      "prediction": "The Ministry of Defense said the maneuver was part of a planned training exercise designed to help the prince learn combat situations in combat situations, which was a vital part of training, during which the \"Battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields as a key element of their training.\"",
      "ground_truths": [
        "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations."
      ],
      "em": 0,
      "f1": 0.44776119402985076
    },
    {
      "id": "d21a2587e9e64ae08a9243d3f950521b",
      "question": "who says operation was training procedure?",
      "prediction": "The answer is \"Battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields as a vital part of their training for operations.\"",
      "ground_truths": [
        "The Ministry of Defense"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "9fb85b5dcad34ba497f04a41616f6d0c",
      "question": "What did the media say about  it?",
      "prediction": "The media said the \"battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields as a vital part of their training for operations\" at Iraq and Afghanistan.",
      "ground_truths": [
        "The paper said the trip had caused fury among some in the military who saw"
      ],
      "em": 0,
      "f1": 0.0975609756097561
    },
    {
      "id": "502a0737d1ac4ce58ae0a8e0098dfa91",
      "question": "what caused the accident",
      "prediction": "The answer is: Washington (CNN).",
      "ground_truths": [
        "The train in front had stopped"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cc168d69c69444cb1abd767d8fd7e37",
      "question": "A former commanding general of what was among those killed?",
      "prediction": "The answer is: \"Mary Doolittle, 59, of Washington, Maryland, was 40, of Metro New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York.  This is not accurate.  The Washington Metropolitan Area Transit Authority (WMTTA) has not been considered to be killed.\"",
      "ground_truths": [
        "of Columbia National Guard,"
      ],
      "em": 0,
      "f1": 0.03389830508474576
    },
    {
      "id": "1e765f40aab74bffb1a8a5fc1ab4f90d",
      "question": "where was the killing",
      "prediction": "The answer is Washington, Maryland. The context states that Washington, Maryland was a \"sight that no one should have to see.\"",
      "ground_truths": [
        "Washington."
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "8e5f4f90422c425bb047dd335d4a5598",
      "question": "Who said shots were fired?",
      "prediction": "The answer is: Israeli Foreign Minister Avigdor Lieberman.",
      "ground_truths": [
        "Hanin Zoabi,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "956c96514760435e81db606781abcb67",
      "question": "When were the shots fired?",
      "prediction": "The answer is: Israel has said its forces found several weapons among the passengers on the Miva Marmara. The specific time of the shots fired is not definitively stated.",
      "ground_truths": [
        "five minutes before commandos descended"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "342f15ed9e814127b49f3340dead119c",
      "question": "How many dead bodies did Zoabi see?",
      "prediction": "The answer is: yes, Zoabi saw at least nine dead bodies.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49d05065daf847d7b985d4252b97a7cf",
      "question": "What was placed over a witness's head?",
      "prediction": "The answer is: She said she believed Israel has video footage of how the ten passengers were killed.",
      "ground_truths": [
        "bag"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e6bceecd0180492db6f7d9f4df16e546",
      "question": "How many dead bodies were there?",
      "prediction": "The answer is:",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc4dab450d5b4a6e9d009d7c5f2a9e4a",
      "question": "Whose forces started shooting?",
      "prediction": "The answer is: Israeli forces started shooting on the Miva Marmara, the Israeli Navy fired on the ships, and the passengers were unarmed.",
      "ground_truths": [
        "Israeli"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "531226f1c86f49a3bee6638692fedd34",
      "question": "How many say things are going well for them, personally?",
      "prediction": "The answer is **three out of four Americans are angry about the country's way of things.**",
      "ground_truths": [
        "three out of four"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "70ff5cd057104a24a7de546f6a0a93e2",
      "question": "What are Americans angry about?",
      "prediction": "The answer to the question is \"a tiny sliver of good news. The number of Americans who think things are going very badly has dropped from 40 percent to 32 percent. Because of that, the public is more pessimistic than in December, and those who switch to the less pessimistic category are less likely to be angry.\"",
      "ground_truths": [
        "the way things are going in the country."
      ],
      "em": 0,
      "f1": 0.14035087719298245
    },
    {
      "id": "388dcae5aa9248318cd8d493a4c3519b",
      "question": "Who ran this poll?",
      "prediction": "The answer is \"The 1,046-year-old American voter.\"",
      "ground_truths": [
        "CNN/Opinion Research Corporation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da630eb21dea4abe920e8253d4fd6a34",
      "question": "What positive sentiment did a lot of Americans echo?",
      "prediction": "The answer is: \"almost three out of four Americans are scared about the way things are going in the United States.\"",
      "ground_truths": [
        "that things are going well for them personally."
      ],
      "em": 0,
      "f1": 0.23076923076923078
    },
    {
      "id": "692084604fa74539aa32ec6abec75341",
      "question": "What out of four people say things are going well for them?",
      "prediction": "The answer is **three out of four Americans**.",
      "ground_truths": [
        "three out of four questioned say that things are going well for them personally."
      ],
      "em": 0,
      "f1": 0.38095238095238093
    },
    {
      "id": "851dc215cd4f478ea5bc5be433bb650b",
      "question": "How many say things are going badly in the country?",
      "prediction": "The answer is 3.",
      "ground_truths": [
        "Nearly eight in 10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91ef74a5a47a442aaa5af41b3d564051",
      "question": "Who says things are going well?",
      "prediction": "The answer is **Americans**.",
      "ground_truths": [
        "21 percent suggesting that"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b99c50bfd5f849afb601bd8ac95b0bf4",
      "question": "Number of people in 10 that say thing are going badly in country?",
      "prediction": "The answer is 6.",
      "ground_truths": [
        "Nearly eight"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6d8f6111e0f2484f961308f6dc5270b3",
      "question": "What proportion of Americans think things are going badly domestically?",
      "prediction": "The answer is **32%.**",
      "ground_truths": [
        "Nearly eight in 10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1bd3693226104fdeb8577e678c8560e1",
      "question": "Who celebrates Thanksgiving and an important religious occasion on the same day?",
      "prediction": "The answer is: The same night the Muslim Eid-ul-Adha was celebrated.",
      "ground_truths": [
        "Muslims"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5dee8293fd7e42a1ba9896818b5aea22",
      "question": "Who celebrates Thanksgiving?",
      "prediction": "The answer is: **The New York Times columnist David Brooks.**",
      "ground_truths": [
        "millions of Americans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dfdebae12b1448ce8bb75f3c7e8029d7",
      "question": "Who played a key role in civil rights movement?",
      "prediction": "The answer is \"The Southern Baptist genealogy of Martin Luther King Jr.\"",
      "ground_truths": [
        "Muslim revolutionary named Malcolm X"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d41593a1eb0e4edd8e8941e625eb8b8f",
      "question": "Who played roles in the civil rights movement?",
      "prediction": "The answer is: The same who played roles in the civil rights movement of the 1960s.",
      "ground_truths": [
        "Martin Luther King Jr."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c9cf44b1466b49388a43f7ded1ec0cba",
      "question": "Who does Obama hold formal talks with?",
      "prediction": "The answer is: China.",
      "ground_truths": [
        "Chinese President Hu Jintao"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ec1cc208a0d4795a63e4b5fcb1f15a2",
      "question": "Who is Obama holding formal talks with?",
      "prediction": "The answer is:  The United States President.",
      "ground_truths": [
        "Chinese President Hu Jintao"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "42bcbd0ec2d340968120e2ce262ed8a9",
      "question": "How much does the US import in Chinese products every year?",
      "prediction": "The answer is approximately $100 million per year.",
      "ground_truths": [
        "billions of dollars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "210c806253b1415999dcfd89f23c9d8e",
      "question": "What does the new poll show?",
      "prediction": "The New Poll shows 71% of Americans consider China an economic threat to the United States, with 2-third believing it a source of unfair competition for U.S. companies.",
      "ground_truths": [
        "71 percent of Americans consider China an economic threat to the United States,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "cc4d7cb20065488ea6e8440d75e36b8a",
      "question": "How much does the US import?",
      "prediction": "The answer is $100 billion.",
      "ground_truths": [
        "billions of dollars in Chinese products each year,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "baeddc270ac04d039a6f83b9ccef8627",
      "question": "how many dollars does US import in Chinese products each year?",
      "prediction": "The U.S imports billions of dollars of Chinese products each year, while China is a major purchaser of the U.S. Treasury bonds, and the United States' budget deficit.",
      "ground_truths": [
        "billions of"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "2e3e4e90adb64e86bf0e6ccf5499126b",
      "question": "Which days are declared holidays?",
      "prediction": "The answer is: Monday and Tuesday are declared holidays.",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "8283aed3c37f45439d976f9d7fd3851f",
      "question": "Who switched from driving on the left to the right?",
      "prediction": "The answer is the United States. The context states that Samoa's driving is shifted to the left.",
      "ground_truths": [
        "Samoa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f955cb3e7bda4a93a867b361b7b96327",
      "question": "Which countries switched from driving on left side to the right?",
      "prediction": "The answer is New Zealand. The context states that Samoa will switch its driving from the right side to the left side on Monday in a move that opponents have called ill-conceived and said will lead to dozens of wrecks and fatalities.",
      "ground_truths": [
        "Samoa"
      ],
      "em": 0,
      "f1": 0.052631578947368425
    },
    {
      "id": "644155df8e7446718cc1d9ca4d2f4c05",
      "question": "Who rejected constitutional challenge to change?",
      "prediction": "The answer is: **The people of Samoa.**",
      "ground_truths": [
        "People Against Switching Sides (PASS)"
      ],
      "em": 0,
      "f1": 0.20000000000000004
    },
    {
      "id": "5010c8ecc7584e2d86bbb2ab8acfa878",
      "question": "What days were declared holidays?",
      "prediction": "The answer is Monday and Tuesday.",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "c0d5e844424d4d9bb84902a8ce411cc2",
      "question": "What was declared holidays for people to adjust?",
      "prediction": "The answer is, Samoa declared Monday and Tuesday holidays for people to adjust to the change and banned alcohol sales for the next three days.",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 0,
      "f1": 0.24000000000000002
    },
    {
      "id": "808b1f032cab4da587ca8837b7ee6686",
      "question": "What types of ships are trapped?",
      "prediction": "The types of ships trapped are:",
      "ground_truths": [
        "passenger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04676862004644f18aa94ff54107047c",
      "question": "What does the spokesman expect?",
      "prediction": "The spokesman expects that the Swedish Maritime Administration will provide additional ice breakers for the Baltic Sea.",
      "ground_truths": [
        "that most of the ships would be freed by Friday."
      ],
      "em": 0,
      "f1": 0.08695652173913043
    },
    {
      "id": "31d748672420422aa724d2b6db6b1d18",
      "question": "What is being used to free the ships?",
      "prediction": "The answer is \"It has been a lot colder than normal in the southern parts of the Baltic sea, but in the north all is normal with normal levels of ice,\" and \"in the worst-affected areas, the ice breakers not being able to cope with the ice, which is why we are sending additional ice breakers.\"",
      "ground_truths": [
        "additional ice breakers.\""
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "893449002d894f7695b2a32aa7713cb5",
      "question": "few ships trapped in the ice of the Sea?",
      "prediction": "The answer is: Most of the ship trapped in the ice of the Sea.",
      "ground_truths": [
        "to 40"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e48a0f7a1c464cc2836d315091387b4f",
      "question": "What was packed?",
      "prediction": "The answer is thousands of visitors to the National Mall.",
      "ground_truths": [
        "the Mall"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "dc9ba1c066e849778a296e23ea424da0",
      "question": "What is the wait time for some?",
      "prediction": "The answer is: \"All of our stations are overcrowded,\" and \"Metro stations were crowded, making it a challenging experience for people to move.\"",
      "ground_truths": [
        "several hours"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72e756b7dce94c2e9107e7efc71ea29e",
      "question": "What city is this?",
      "prediction": "The answer is New York City.",
      "ground_truths": [
        "Washington"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fbaacdd8faf49ed89f14aabbfd62193",
      "question": "What time does the mall close off?",
      "prediction": "The mall closed off at 7 p. m. Tuesday, as the inauguration of Barack Obama.",
      "ground_truths": [
        "9 a.m.,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c13625cdb6614fe9914a3eaca30ef8cf",
      "question": "What is the change in threat level?",
      "prediction": "The shift in threat level, from 907 to 957, suggests that the new threat level is more likely to be a reaction to an incident, a failure of the inauguration, or a combination of these factors, which are not definitively established. The change in threat level is not a matter of a single, isolated event, but is a consequence of the widespread incidents in Washington, NY, and the National Park Service's closure of many of those stations.",
      "ground_truths": [
        "did not"
      ],
      "em": 0,
      "f1": 0.030303030303030304
    },
    {
      "id": "b955e270519147ad8c4ef7356d7a4011",
      "question": "when Zeiger became Larry King?",
      "prediction": "The answer is \"I went to stay with my Uncle Jack in Miami Beach.\"",
      "ground_truths": [
        "1957,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a4ee6e421be40b2afc83dd19d87ac7f",
      "question": "Who is Larry Zeiger?",
      "prediction": "The answer is Larry King.",
      "ground_truths": [
        "\"Larry King"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "74cabd6142eb4bab88050888d5f4f324",
      "question": "What name did Zeiger use?",
      "prediction": "The answer to the question is \"Swingin' Down the Lane.\"",
      "ground_truths": [
        "Larry King"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44037ae7db3d4534986eb3db347727de",
      "question": "Who became the King?",
      "prediction": "The answer is Larry King.",
      "ground_truths": [
        "Larry King?\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ef078e968b6942d18ad8d2e5f4104222",
      "question": "Where did Larry Zeiger  move?",
      "prediction": "The answer is \"Miami Beach,\" with the phrase \"I went to stay with my Uncle Jack in Miami Beach.\"",
      "ground_truths": [
        "Miami Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "716b2f16ce0645bf89407581a31ee24b",
      "question": "What happened the first time Larry was on the air?",
      "prediction": "The answer is \"Swingin' Down the Lane\" is a song written by Larry King.",
      "ground_truths": [
        "couldn't speak."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f29f74ee9dbd4a8bb20bbebd4e7aa566",
      "question": "who moved from Brooklyn to Miami?",
      "prediction": "The answer is: Larry King.",
      "ground_truths": [
        "Larry Zeiger"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "c5e69a8c19ec4fadb153de7ed8d06b9f",
      "question": "What time is the lift off scheduled for?",
      "prediction": "The lift off scheduled for is Wednesday.",
      "ground_truths": [
        "9:20 p.m."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0af880e8558a4fc3afbc9a8a25d081fd",
      "question": "When is the  space shuttle Discovery  scheduled for launch?",
      "prediction": "The space shuttle Discovery is scheduled for launch on Wednesday.",
      "ground_truths": [
        "9:20 p.m. ET Wednesday."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "3645db7e841f4242ae0b3be674e8bc8a",
      "question": "When is the launch scheduled for?",
      "prediction": "The launch date was scheduled for Wednesday.",
      "ground_truths": [
        "9:20 p.m. ET Wednesday."
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "0104a4f929b64f0e84d574262a285e30",
      "question": "What is scheduled for launch on Wednesday?",
      "prediction": "The scheduled launch on Wednesday is **Wednesday**.",
      "ground_truths": [
        "The space shuttle Discovery,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a9dae85ef8448b88947f67b7368cd84",
      "question": "How long is the mission?",
      "prediction": "The mission is scheduled to launch Wednesday.",
      "ground_truths": [
        "14-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c034e68af36c41d79e0782c060d7acbd",
      "question": "what time is the shuttle",
      "prediction": "The shuttle Discovery was scheduled to launch on Wednesday.",
      "ground_truths": [
        "9:20 p.m."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "48e5d37aaea9404a891d01584e699f18",
      "question": "What is the name of the space shuttle?",
      "prediction": "The space shuttle Discovery is scheduled for launch in five days.",
      "ground_truths": [
        "Discovery"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "a3ac333098d74ad7acf2d20bbc261c9b",
      "question": "What is set to lift off?",
      "prediction": "Answer: Wednesday.",
      "ground_truths": [
        "space shuttle Discovery"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1b9b4f3aeae403297d234cfe9a6adfc",
      "question": "what will Band partners do",
      "prediction": "Based on the context, the answer is \"we will not have a crew.\"",
      "ground_truths": [
        "partnered with Keep America Beautiful,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b483ffa8a594b72ada39a0d9adb43a2",
      "question": "What is the purpose of the band traveling  light?",
      "prediction": "The band is traveling light, meaning they are not using their own buses, equipment, or cars. They are relying on local roadies, local T-shirts, local suppliers, and local amplifiers and equipment to deliver their goods.",
      "ground_truths": [
        "reduced their carbon footprint by 132 tons."
      ],
      "em": 0,
      "f1": 0.04878048780487805
    },
    {
      "id": "83191d65ecfe4f649a4a2c61b6f99ee0",
      "question": "What did the guitarist say they wanted to do?",
      "prediction": "The guitarist in The Lost Trailers' \"Lean, Clean and Local\" tour said they wanted to do a \"local roadies, local T-shirt vendors, local suppliers for amps and equipment, and a small stimulus of their own.\"",
      "ground_truths": [
        "hire local,"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "8f8dc0c9c2264752b3f946c8e7d2cdcc",
      "question": "Where did the iReporter grow up?",
      "prediction": "The provided text is a news report from a CNN report on London's Leicester Square, England, on Friday, Thursday afternoon. It describes the activities of a crowd in Sydney, Australia, watching a TV screen, where officials say people were listening to Michael Jackson's music, and on the London's Leicester Square, where news of his death spread. This is met with confusion and shock by London's British fans.",
      "ground_truths": [
        "Middle East,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b91bec646d641fab11061917d864c78",
      "question": "who says \"I remember growing up in the Middle East ... waiting for his albums,\"?",
      "prediction": "The answer is \"I remember growing up in the Middle East, waiting for his albums,\" or \"I remember growing up in the Middle East, waiting for his albums.\"",
      "ground_truths": [
        "Rany Freeman,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5faf2cc346b4c92a7258acfc94e389f",
      "question": "what Fans across the world remark on Jackson's music?",
      "prediction": "The answer is: \"Every person's comment changes, not always in a positive or negative way, depending on the context. This is because people's memories and experiences shape our thoughts, and the way we respond to them can be as strong or as weak as the content of the comment.",
      "ground_truths": [
        "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\""
      ],
      "em": 0,
      "f1": 0.04
    },
    {
      "id": "7c027ab8bde24b2aaae43a127848342e",
      "question": "What did fans remark about?",
      "prediction": "The answer is: Fans across the world commented on pop legends Michael Jackson's death, describing it as a \"significant day for the music scene,\" and \"shocking\" and \"unimaginative memorial for the late singer.\"",
      "ground_truths": [
        "talked of an impromptu memorial for the late singer at the \"Stone Circle,\""
      ],
      "em": 0,
      "f1": 0.21052631578947364
    },
    {
      "id": "9a0ce6a0b7664deca7f3454e5520195a",
      "question": "Where do the most serious attacks come from?",
      "prediction": "The most serious attacks come from the sins of the members of the Catholic Church.",
      "ground_truths": [
        "the sins of the members of the church,"
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "d84a937c35bd44f9ba5d4952c3df1c34",
      "question": "where does the most serious attacks come from?",
      "prediction": "The most serious attacks come from the sins of the members of the Church.",
      "ground_truths": [
        "sins of the members of the church,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bd6ff486bd6d413baa8804b53f6e5382",
      "question": "Where was the Pope en route to?",
      "prediction": "The answer is \"Portugal\"",
      "ground_truths": [
        "Portugal,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a0ba1c2d49b54ca4adc7e5c38dbe942e",
      "question": "Which church does Pope Benedict lead?",
      "prediction": "The correct answer is **CNN**.",
      "ground_truths": [
        "Roman Catholic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d96314813ffe466c92d3778208871ade",
      "question": "Who says the scandal is terrifying?",
      "prediction": "The scandal is terrifying.",
      "ground_truths": [
        "Pope Benedict XVI"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ab491f01e8b47e79001da10007967fe",
      "question": "what does Pope Benedict XVI say about child abuse?",
      "prediction": "Pope Benedict XVI says that \"terrifying\" was the most important attacks on the church's \"sin\" come from the sins of the members, and that the most important attacks on the church don't come from the outside.",
      "ground_truths": [
        "the reality he has seen is \"terrifying.\""
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "4ebc2de2a65e4549893bb0d0d4dbed4e",
      "question": "he was speaking en route to where?",
      "prediction": "The answer is: Pope Benedict XVI was speaking en route to Portugal.",
      "ground_truths": [
        "Portugal,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "8fcc1e39bba146a08300eef42c641e71",
      "question": "Where is tennis player Carlo Moya from?",
      "prediction": "The answer is **Cypriot tennis player Carlo Moya.**",
      "ground_truths": [
        "Spaniard"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27ed5ffe8231443985b27f0d9d605774",
      "question": "Who won their opening match at the Kooyong Classic?",
      "prediction": "Answer: Fernando Gonzalez of Chile won their opening match at the Kooyong Classic.",
      "ground_truths": [
        "Roger Federer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30cbead889fc4c0783a1afd652cfb163",
      "question": "Who lost to Roger Federer?",
      "prediction": "The answer is Fernando Gonzalez of Chile.",
      "ground_truths": [
        "Carlos Moya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5dc68d2932bb4b0f861bd4642c674c43",
      "question": "What country is Fernando Gonzalez from?",
      "prediction": "Fernando Gonzalez of Chile is from Argentina.",
      "ground_truths": [
        "Chile"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c8c3c9f350c5469da18f367a6a250097",
      "question": "Who won his opening match?",
      "prediction": "The answer is Fernando Gonzalez of Chile.",
      "ground_truths": [
        "Federer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cc48ae35d6a4676a0823fdf2550f2ea",
      "question": "Who is the worlds's number two tennis player?",
      "prediction": "The worlds's number two tennis player is Fernando Verdasco.",
      "ground_truths": [
        "Roger Federer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b6a4b25c0cf41d89f69fab7e636d836",
      "question": "Where is the Kooyong Classic Held?",
      "prediction": "The Kooyong Classic was held at the Melbourne Cricket Ground in Australia.",
      "ground_truths": [
        "Melbourne"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "a3350d80e3c94368aed1d9a34577d519",
      "question": "Which competitor from Chile went through?",
      "prediction": "The answer is Fernando Gonzalez.",
      "ground_truths": [
        "Fernando Gonzalez"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b94b8e734c3b40eaba7291c227e2edb7",
      "question": "Where is Carlo Moya from?",
      "prediction": "The answer is *Spanish* from Argentina.",
      "ground_truths": [
        "Spaniard"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6270ad7fc144895abf4552a2e0f8fc4",
      "question": "Who did Federer beat?",
      "prediction": "Answer: Fernando Verdasco",
      "ground_truths": [
        "Spaniard Carlos Moya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a78b494b73574ef89bc015078c11cafc",
      "question": "what was approved for use",
      "prediction": "The answer is, I tiptoe through hormonally laced minefields hoping to avoid emotional carnage in response to any of my random comments or actions.",
      "ground_truths": [
        "cervical cancer vaccine,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c7c4dbc9ee8445f9b784291cd63d7f6e",
      "question": "Who should get the vaccine?",
      "prediction": "The answer is the New York-type HPV vaccine.",
      "ground_truths": [
        "girls around 11 or 12."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "308fd3471884450dbcfc690ab8dc5398",
      "question": "What can cause cervical cancer",
      "prediction": "The answer is, \"I want to get the HPV vaccine,\" which translates to \"I want to be vaccinated against cervical cancer.\"",
      "ground_truths": [
        "HPV (human papillomavirus)"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "3f0e6f73fe4c4e79b1f6c2ab05ae44e2",
      "question": "What position does Kgalema Motlanthe have?",
      "prediction": "The answer is: Kgalema Motlanthe is the deputy president of the African National Congress.",
      "ground_truths": [
        "African National Congress Deputy President"
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "74fe96e647704cca90c6b1e351132a0d",
      "question": "Who resigned in South Africa?",
      "prediction": "The answer is: Ten South African ministers and the deputy president.",
      "ground_truths": [
        "Ten"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "e45a17436bc24b3880cd99d05da8d4ca",
      "question": "What country is being discussed?",
      "prediction": "The answer is South Africa. The context explicitly states that \"South African President Thabo Mbeki announced his resignation in a televised address on Sunday.\"",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "728a6e1ff8d74b919e2cb64333781ad0",
      "question": "Who was sworn in on Thursday?",
      "prediction": "The answer is the Deputy President of African National Congress.",
      "ground_truths": [
        "African National Congress Deputy President Kgalema Motlanthe,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d299bbb3cb8c4cc586d87e04eedef293",
      "question": "What longtime rival did Mbeki's court case involve?",
      "prediction": "The answer is: A long-time rival of President Mbeki.",
      "ground_truths": [
        "Zuma"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac119b06a9954a51ab96864361e5f831",
      "question": "Who resigned with President Thabo Mbeki?",
      "prediction": "The resigned with President Thabo Mbeki as South Africa's president.",
      "ground_truths": [
        "Ten South African ministers and the deputy"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "ef88996852174dd8971704d9556057e9",
      "question": "Who was sworn in Thursday?",
      "prediction": "The answer is the South African President, Thabo Mbeki.",
      "ground_truths": [
        "African National Congress Deputy President Kgalema Motlanthe,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "08cc57b5d9f342b489a0907d02617ab6",
      "question": "Who quit after court case?",
      "prediction": "The answer is: The deputy president of the African National Congress (ANC).",
      "ground_truths": [
        "Ten South African ministers and the deputy president"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "f70ba795250b478498351d4a7e8c5154",
      "question": "Who is the rival of Mbeki?",
      "prediction": "The answer is: The deputy president, Kgalema Motlanthe.",
      "ground_truths": [
        "Zuma"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f7cfe3979d14e35acb5cf8337396d2d",
      "question": "Name of the replacement that was sworn in Thursday?",
      "prediction": "The replacement at South Africa's presidential office is Kgalema Motlanthe, and he is sworn in as South Africa's president on Thursday.",
      "ground_truths": [
        "Kgalema Motlanthe,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "da4803aad9bd412f97dc1d468eb6fd52",
      "question": "what people used a surrogate to have a baby",
      "prediction": "The answer is: Kim Kardashian's marriage to her husband, Bill Klein, was mistreated.",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "aec05463fc614cf787003f33ba3101f7",
      "question": "what family was happy to visit the doctor",
      "prediction": "The answer is: Kim Kardashian's marriage.",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b481a8d55f2d45029107048c524decc5",
      "question": "Who suffer from skeletal dysplasia?",
      "prediction": "The answer is Kim Kardashian.",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ca166bde82644d2937a20a6c4eb3122",
      "question": "what men have skeletal dysplasia",
      "prediction": "The answer is, Kim Kardashian.",
      "ground_truths": [
        "Bill Klein,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ee12be811354cf89be9edefeaad7dd0",
      "question": "What can pose health problems?",
      "prediction": "The answer is, \"Unfortunately, as the most difficult part of the process is pregnancy, the couple's surrogate was likely lost the pregnancy, which, as a result, led to a miscarriage.",
      "ground_truths": [
        "the challenges a pregnancy"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "531b2b755eb7418b91795b6546663372",
      "question": "What lost the couple?",
      "prediction": "The answer is Kim Kardashian's marriage.",
      "ground_truths": [
        "the pregnancy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b835ba9d0b7463589f3a5a140fd372c",
      "question": "Where is Natalia Vodianova from?",
      "prediction": "The answer to the question is Natalia Vodianova, born in Nizhny Novgorod-Sodny, Russia.",
      "ground_truths": [
        "I was born in Nizhny Novgorod"
      ],
      "em": 0,
      "f1": 0.3529411764705882
    },
    {
      "id": "9b4b8e59f2ae42309c527a39ee3fb62e",
      "question": "What is Natalia Vodianova's occupation?",
      "prediction": "Natalia Vodianova is the Russian supermodel and philanthropist, described as the \"most beautiful woman in the world.\"",
      "ground_truths": [
        "supermodel"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "8b02e7e972a048cdb2835c247225be55",
      "question": "Where were authorities were called to?",
      "prediction": "Answer: The answers to the questions \"Fayetteville, North Carolina, North Carolina, Los Angeles, California, and Camp Lejeune\" and \"Holley Wimunc's husband, John Wimunc, was not residing at Camp Lejeune.\"",
      "ground_truths": [
        "Fort Bragg in North Carolina."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "ed8e566b72f247d5a7f9838b76acd5a7",
      "question": "Who was still missing after the fire?",
      "prediction": "Based on the provided context, the answer is **Holley Wimunc**, a Marine stationed at Camp Lejeune, North Carolina, who was missing after a fire at his apartment near Fort Bragg.",
      "ground_truths": [
        "female soldier,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "103394fd775f4bec8e17686dfbd6c16e",
      "question": "What did she say in the court filing?",
      "prediction": "The answer is: Wimunc is a Marine, and the court filing indicates that Wimunc's husband, John Wimunc, had knocked her down, held her head, and threatened suicide.",
      "ground_truths": [
        "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,"
      ],
      "em": 0,
      "f1": 0.5238095238095238
    },
    {
      "id": "d29d882518d246d5a12a3f4e4543a54c",
      "question": "Who did she say was threatening her in court?",
      "prediction": "The answer is: The family stated they are \"still trying to absorb the impact of this week's stunning events\" and they are grateful for their friends and prayers.",
      "ground_truths": [
        "husband"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b76b4f9a9b74046ae103cc87f19639c",
      "question": "when were the authorities called",
      "prediction": "The answer is:",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02faadc509b54a07a40ecb311a448c8e",
      "question": "What statement did a missing soldier's family make?",
      "prediction": "The answer is: The family released a statement on Friday.",
      "ground_truths": [
        "they are \"still trying to absorb the impact of this week's stunning events.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9078e22fc4a74f44b0d692850bc4fe82",
      "question": "what did the family say",
      "prediction": "The answer is: The family said they would not make any further comments.",
      "ground_truths": [
        "\"still trying to absorb the impact of this week's stunning events.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10dc12a62b1048f286bfc224f79ec612",
      "question": "What happened to the fire?",
      "prediction": "Based on the provided context, the answer is: **C.  Camps Lejeune, North Carolina, has no information. The investigation is ongoing.**",
      "ground_truths": [
        "No one was inside the apartment at the time of the fire, police said."
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "7c46db8111bf4306ad1afe8eec0e8d4d",
      "question": "Who is missing?",
      "prediction": "The answer is: **Holley Wimunc, a Marine in Camp Lejeune, North Carolina.**",
      "ground_truths": [
        "Lt. Holley Wimunc."
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "67b6f116f59b45d7b224fad351757029",
      "question": "Where were authorities called to fire at on Thursday?",
      "prediction": "Answer: Authorities are searching for a female soldier, missing after a fire at her apartment near Fort Bragg in North Carolina.",
      "ground_truths": [
        "Fort Bragg in North Carolina."
      ],
      "em": 0,
      "f1": 0.4166666666666667
    },
    {
      "id": "1905d03db9fb4f42a537ba45c34f1301",
      "question": "Who says two soldiers likely not alive?",
      "prediction": "The answer is: \"We have an obligation, a moral obligation to their families to bring finality,\"",
      "ground_truths": [
        "Israeli Prime Minister Ehud Olmert"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95d3252b70fc4d5aa8082d3a85433fa1",
      "question": "Who agreed to swap prisoners?",
      "prediction": "The answer is: Israel.",
      "ground_truths": [
        "Israel"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ebbc94f432924b7c9934aec7ca367caf",
      "question": "What will Israel swap for captured soldiers?",
      "prediction": "The answer is: \"We will also provide information to the United Nations about four Iranian diplomats who disappeared in Beirut during the 1982 Israeli invasion, the statement said.",
      "ground_truths": [
        "five Lebanese prisoners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8cc47e7023d4379a08aa2f18ba94cb2",
      "question": "Who are the two soldiers thought to be dead?",
      "prediction": "The answer is:  The two soldiers thought to be dead are *Samer Kuntar* and *Eudreva*.",
      "ground_truths": [
        "Ehud \"Udi\" Goldwasser and Eldad Regev."
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "530b0b3b0802450d9e7ebda6283e6b6c",
      "question": "What is the number of Lebanese prisoners that will be released?",
      "prediction": "The answer is 13.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e08c7860a95e47bb97a9dfa2fd2a6713",
      "question": "Who will release five Lebanese prisoners?",
      "prediction": "The answer is: Israel will release five Lebanese prisoners.",
      "ground_truths": [
        "Israel"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "77b194a548d2400a96ecd5709b8b1d64",
      "question": "How many prisoners will be released?",
      "prediction": "Answer: 4",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4888bc9c3cf5402e8ccd794fbac2588e",
      "question": "What is the Israeli Cabinet willing to swap prisoners for?",
      "prediction": "The Israeli Cabinet would be willing to swap prisoners for the following:",
      "ground_truths": [
        "soldiers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96a893350b4a47a2b0d56d5b007cf6e9",
      "question": "What nationality are the prisoners about to be released?",
      "prediction": "The prisoners are described as being about to be released to the United States, which is a fact.",
      "ground_truths": [
        "Lebanese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e33f9ac0e542439281cdf842b17cb53e",
      "question": "What does the prime minister say?",
      "prediction": "The prime minister's statement, \"It's not an easy decision,\" confirms that the prime minister says:",
      "ground_truths": [
        "told his Cabinet that the two soldiers are not alive, \"as far as we know.\""
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "782be7eadeb040b6ae37b1cfc7918c5c",
      "question": "What does the Israeli Cabinet agree to do?",
      "prediction": "The answer is: \"We have an obligation, a moral obligation to their families to bring finality to the loss of their families.\"",
      "ground_truths": [
        "release a notorious killer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "689aed65128147c2948d8fc665e6ef0d",
      "question": "What background is her family from?",
      "prediction": "The answer is: I am a Muslim.",
      "ground_truths": [
        "many different"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "df8fc2dc6ff34ade9ae6b4242632e8b1",
      "question": "What was a part of radio station promotion?",
      "prediction": "Answer: \"Nothing at all. I've always been very respectful to my family and my reputation. I'm known to be very respectful to my family and my reputation.\"",
      "ground_truths": [
        "\"learn how to dance and feel sexy,\""
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "fdc90f6593464af6b65c2f38b2c814c6",
      "question": "Who is Miss USA?",
      "prediction": "The answer is \"I'm the number one person on Yahoo or Google, I heard,\" and \"It's nothing at all, I've always been known to be very respectful to my family and my reputation.\"",
      "ground_truths": [
        "Rima Fakih"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bc44917bc70242a7b01f7b024c5fa3ea",
      "question": "who said that pole-dancing pictures were part of radio station promotion?",
      "prediction": "The answer is, \"I'm an American girl,\" and the Detroit, Michigan radio station promotion was a class to help women learn to dance and feel sexy.",
      "ground_truths": [
        "Rima Fakih"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c411868fbe054e80b90a12463b2beddd",
      "question": "who downplays downplays significance of photographs that emerged online this week?",
      "prediction": "Answer: The answer is \"Nothing at all,\" as Fakih did not, and she did not decline the question.",
      "ground_truths": [
        "Miss USA Rima Fakih"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "9a83f32bddac46a19097e899dd1e1d65",
      "question": "Which background does she say her family is from?",
      "prediction": "The answer is: \"I'm a Muslim with Lebanese heritage.\"",
      "ground_truths": [
        "many different"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90f5064a214f4e05b9a65eb206d773ea",
      "question": "Who downplayed the significance of photographs?",
      "prediction": "The answer is \"Nothing at all,\" referring to the Detroit, Michigan, radio station promotion of the Miss USA, which is a class to help women learn how to dance and feel sexy, and they helped her name a top search term on the internet.",
      "ground_truths": [
        "Fakih"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b74bd4ddf00e4e59bd280742fa82144d",
      "question": "What were the pictures of?",
      "prediction": "The answers to the question are based on the provided context:",
      "ground_truths": [
        "dancing against a stripper's pole."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46e09c1bc9304e71a144d33bf875c619",
      "question": "is she from a muslim background?",
      "prediction": "The answer is: Yes, she is from a Muslim background.",
      "ground_truths": [
        "a"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf138cf693f0444e960d28a85e72f955",
      "question": "Who can not afford to pay for cable?",
      "prediction": "The answer is \"I am not able to provide a definitive answer on whether I am able to pay for cable. My personal financial situation is limited, and I am not able to provide a final answer.\"",
      "ground_truths": [
        "James Richter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "734ca4956e434fc8b6712a684fcb7bdc",
      "question": "who delayed the switch?",
      "prediction": "The answer is: \"With the economy, I don't have enough cash to do cable, so I'm just really debating whether I really want to make that switch to digital,\" reads Richter.",
      "ground_truths": [
        "Congress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8fc9240c389d407db9a7410d11f3352e",
      "question": "Who delayed a nationwide switch?",
      "prediction": "The answer is: \"With the economy, I don't have enough cash to do cable, so I'm just really debating whether I really want to make that switch to digital,\" which translates to \"I'm really debating whether I really want to make that switch to digital.\"",
      "ground_truths": [
        "Congress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e5d9591150248d8b366c339f50b3aad",
      "question": "What was delayed?",
      "prediction": "The answer is \"with the economy, I don't have enough cash to do cable, so I'm just really debating whether or not I really want to make that switch to digital,\"",
      "ground_truths": [
        "the end of TV's rabbit-ears era."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44aa6bf4f6a74816940787187cfe1bac",
      "question": "what is the switch?",
      "prediction": "The switch was scheduled for February 17, but Congress delayed the conversion.",
      "ground_truths": [
        "to digital."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e4899d00d6a44a69ed4715342726352",
      "question": "The criticism over what act caused the move?",
      "prediction": "The answer is \"Subject to the whim of a supervisor.\"",
      "ground_truths": [
        "Defense of Marriage"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "bf25dbd17a734546aa9207995fedf187",
      "question": "what is the criticism?",
      "prediction": "The answer is: \"Subject to the whim of a supervisor.\"",
      "ground_truths": [
        "over a Justice Department motion filed last week in support of the Defense of Marriage Act"
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "4d699f52ba254a96b8622bb286e28072",
      "question": "Who spoke in favor of gay and lesbian rights?",
      "prediction": "The answer is \"The White House.\"",
      "ground_truths": [
        "Carisa Cunningham,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbf2b2f61b4d40ad91f086f4dc927b97",
      "question": "what does the coverage have to do with gay marriage?",
      "prediction": "The answer is, it is mandatory.",
      "ground_truths": [
        "granting some benefits"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1354074638a34aac988882a24671fb15",
      "question": "Does executive order grant full health coverage?",
      "prediction": "The provided text does not contain a specific answer to the question at the time of the report. It only states that President Obama signed an executive order granting some benefits to same-sex partners of federal employees. It does not include the specific details of the memorandum. The text doesn's say what the president is doing.",
      "ground_truths": [
        "health-care"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37b3b28e375e4216888e4da3c16a3f4c",
      "question": "Who told fire victims they won't be forgotten in Washington?",
      "prediction": "The provided text doesn't contain any further context. Therefore, I cannot answer the question based on the context. I need more information to provide a complete and accurate answer.",
      "ground_truths": [
        "the president"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9bd82e4e8474f5aa64203fb5ebb742f",
      "question": "What caused the bodies to be charred?",
      "prediction": "The answer is, \"We see pictures from online and on TV, but it's just, it's much different when you see it up front.\"",
      "ground_truths": [
        "the fires"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe3dc6c9564e490f8e5b857613093186",
      "question": "Who spoke to fire victims?",
      "prediction": "The answer is, \"We are just all grateful that everybody is safe.\"",
      "ground_truths": [
        "President Bush"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e199868b2fa47cea32382c96ab8a91e",
      "question": "Where is the Qualcomm Stadium?",
      "prediction": "The context provides a summary of the San Diego fire crisis, including:",
      "ground_truths": [
        "San Diego,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "f19f066e81ec40d7aae87630df542d27",
      "question": "What has been found near San Diego?",
      "prediction": "The answer is:  California (Los Angeles, California)",
      "ground_truths": [
        "Four bodies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4a02dd17a70249c79c782040d1ff267a",
      "question": "where were the bodies found?",
      "prediction": "The answer is: \"We are just all grateful that everyone is safe,\" and \"that everybody is safe,\" and \"then we will rebuild again,\" and \"those things you can replace. We are just all grateful that everybody is safe.\"",
      "ground_truths": [
        "in a canyon in the path of the blaze Thursday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "891aa475e0f64276bbcec79f79a8d50e",
      "question": "what was found near san diego?",
      "prediction": "The answer is: California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California, California,",
      "ground_truths": [
        "Four bodies were"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cfd40ade7d554772833d0880f051d043",
      "question": "What stadium is closing at noon on Friday?",
      "prediction": "The answer is: San Diego, California.",
      "ground_truths": [
        "Qualcomm"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7693a75ee18f4901a997aebeb4b22fcb",
      "question": "what did Bush say to victims?",
      "prediction": "The answer is: \"We are not going to forget you in Washington, D.C.,\" and the fire victims are described as \"like \"those who can replace things \"you can't replace.\"",
      "ground_truths": [
        "\"We're not going"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "0ec6bb1684c543b09f6617fe46af7f85",
      "question": "Who says border has never been more protected?",
      "prediction": "The answer is: A border patrol agent, not an immigration officer.",
      "ground_truths": [
        "Arizona Gov. Jan Brewer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22f4ef32a6a84b5681374e3a2aa13b64",
      "question": "What did President Obama send the National Guard for?",
      "prediction": "The President sent the National Guard for reconnaissance helicopters and robotic surveillance craft to the Border States to patrol the border region.",
      "ground_truths": [
        "reallocate reconnaissance helicopters and robotic surveillance craft"
      ],
      "em": 0,
      "f1": 0.48
    },
    {
      "id": "5f54b72f33ee4bc885764e1c0674eb2b",
      "question": "What has Arizona asked?",
      "prediction": "The question is: What has Arizona asked?",
      "ground_truths": [
        "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f40fa2cb594a4b1abf731b8c04c9ed95",
      "question": "What did Arizona ask for?",
      "prediction": "Arizona's request for an alien invasion is a short-lived, short-term response to a problem. The question is, what did Arizona ask for?",
      "ground_truths": [
        "air support."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2d500e47e4d4a18bd068db4ff12063c",
      "question": "What causes people to hire illegals?",
      "prediction": "The answer to the question is that it is not inherently an \"evil\" or \"desirable\" thing to think of it as. The answer is: \"It's not inherently evil\" because it is a law and a way for the state to work.",
      "ground_truths": [
        "to do jobs that Arizonans wouldn't do."
      ],
      "em": 0,
      "f1": 0.0975609756097561
    },
    {
      "id": "459a9cdc3e864194b24734d08893c10a",
      "question": "What did Arizona asked for to beef up border security?",
      "prediction": "The question asks what did Arizona ask for to beef up border security.",
      "ground_truths": [
        "helicopters and unmanned aerial vehicles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "708b1af75c064fbbbb9101748af472db",
      "question": "What is his sport?",
      "prediction": "The answer is: French trimaran l'Hydroptere, with an average speed of 50.57 knots (60 miles per hour) over 500 meters, achieves its world sailing speed record for a break-over 11 years after 1993 with a cart speed of 50.57 knots.",
      "ground_truths": [
        "sailing"
      ],
      "em": 0,
      "f1": 0.05405405405405406
    },
    {
      "id": "90fe0d6a432c429b8015817c244c5a1e",
      "question": "Who is Alexandre Caizergues?",
      "prediction": "The context provides information about the high-speed boat races in England, with the following:",
      "ground_truths": [
        "kite surfers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8be75204345642ec87b18f1f7e8d3513",
      "question": "who holds the record",
      "prediction": "The answer is: **L'Hydroptere**",
      "ground_truths": [
        "Alexandre Caizergues,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dae18363ce314d5e9dfe606d1c1a8f9e",
      "question": "how many formal applications",
      "prediction": "The context provides information about the international race in speed sailing, including:",
      "ground_truths": [
        "50"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70c751e856e1426091722028754d12be",
      "question": "what year had 5o applications",
      "prediction": "The answer is: The WSSRC had received about 50 formal applications for speed attempts during 2008.",
      "ground_truths": [
        "2008."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "be4a1b1eb4d44d628dbaefa4ede886a3",
      "question": "Who holds the world sailing speed record?",
      "prediction": "The answer is: The International Race among sailors, kite surfers, and wind surfers to be the fastest wind-powered boat on the planet.",
      "ground_truths": [
        "Alexandre Caizergues,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a4384bdb549450fb8a8a634bb71db5b",
      "question": "What happened in 2008?",
      "prediction": "The answer is: The WSSRC received about 50 formal applications for speed attempts during 2008.",
      "ground_truths": [
        "WSSRC had received about 50 formal applications for speed attempts"
      ],
      "em": 0,
      "f1": 0.7826086956521738
    },
    {
      "id": "d150b928a5c9471399cc5faec90e8b16",
      "question": "What does the Council oversees?",
      "prediction": "The Council oversees the World Sailing Speed Record Council (WSSRC) to ensure the record is upheld and the record is accurately maintained.",
      "ground_truths": [
        "speed attempts"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "11701192341248d787b000a454c9792b",
      "question": "Hoe many were left behind in Japan?",
      "prediction": "From the context, the answer is:",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83da903b59c94d51b07757675a1cd160",
      "question": "How many Oka brothers served in the military?",
      "prediction": "The answer is \"I do!\"",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1c1e7a07b394af3975a151f1ff02f54",
      "question": "How many brothers were too young to fight?",
      "prediction": "The answer is \"I do!\".",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b82eb60ec4c476e9025892dcfb76873",
      "question": "When did the two youngest serve on the U.S. side?",
      "prediction": "The answer is: \"I do!\"",
      "ground_truths": [
        "the Korean War"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "17734567580e41fda19424773eb5ad5a",
      "question": "How many were in the U.S. army?",
      "prediction": "The answer is, \"I do!\"",
      "ground_truths": [
        "\"Five of us for the United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73b5f1567c354aa7bc37e3664f61d0e6",
      "question": "What did all seven brothers do?",
      "prediction": "The context provides information about the Oka boys, including the 1920s migrant labor hotel in Castroville, their American roots, and their Chinese ancestry.",
      "ground_truths": [
        "served in the military,"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "4ef90843df6b46609bb78fe1748c188e",
      "question": "What is the club about?",
      "prediction": "The answer is: \"The club is \"very diverse\" and \"convenient camps in the Philadelphia area.\"",
      "ground_truths": [
        "Swim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99e20c89ab1848fab5ad31ee18d07ab5",
      "question": "What claims to be \"very diverse\"?",
      "prediction": "The answer to the question is \"It was never our intention to offend anyone.\"",
      "ground_truths": [
        "The Valley Swim Club"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc4b8826a9a041699442a8e89f061aa3",
      "question": "What did the president say?",
      "prediction": "The president of The Valley Swim Club stated that the club's racism is not at play.",
      "ground_truths": [
        "\"It was never our intention to offend anyone,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "937c40c196d0426591642dd8137cf223",
      "question": "What did John Duesler say?",
      "prediction": "The answer is: John Duesler said he underestimated the number of swimmers.",
      "ground_truths": [
        "underestimated the number of swimmers who would come to swim at the club."
      ],
      "em": 0,
      "f1": 0.380952380952381
    },
    {
      "id": "6cf873665ae247898bae6330c367c09f",
      "question": "What is the club",
      "prediction": "The answer is: Duesler's comments, as described by his wife, John Duesler.",
      "ground_truths": [
        "The Valley Swim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74ff309b62674179b7c7ee1d6f202157",
      "question": "Who has to decide to reinstate the center's contract?",
      "prediction": "The answer is \"We need to make sure all the camps will be treated with the same standards.\"",
      "ground_truths": [
        "the club's board"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1dbd79c72724f15bb814757570cfaeb",
      "question": "What is theboard deciding",
      "prediction": "Based on the provided text, the answer is:",
      "ground_truths": [
        "how it will proceed."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e1eb993a1ca4265978c2db209a671b7",
      "question": "Who owned the helicopter that crashed?",
      "prediction": "The answer is the U.S. Marines.",
      "ground_truths": [
        "Russian air company Vertikal-T,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f24d26d866b410da298cec18f321605",
      "question": "Who was the chopper owned by?",
      "prediction": "The helicopter was owned by the Russian air company, Vertikal-T, which is classified as a \"civilian contracted\" helicopter.",
      "ground_truths": [
        "the Russian air company Vertikal-T,"
      ],
      "em": 0,
      "f1": 0.4210526315789474
    },
    {
      "id": "a07201b5d4514f918eb547a2825af689",
      "question": "where did the helicopter crash",
      "prediction": "The helicopter crash at Kandahar airfield in southern Afghanistan, with 16 people killed, is not a military one. The International Security Assistance Force (ISAF) stated the cause was unknown, and the exact cause was not known. This was officially confirmed by the U.S. military. The American military helicopter was a \"civilian contracted\" helicopter, not a military one, and was only a \"Mil Mi-8\" helicopter, owned by the Russian Air Force, which is part of the Rosaviatsia (the Federal Air Transportation Agency) and is not a military one. The U.S. military confirmed the crash",
      "ground_truths": [
        "Kandahar airfield in southern Afghanistan"
      ],
      "em": 0,
      "f1": 0.12195121951219512
    },
    {
      "id": "f58b2227f5a54648811bf0959527bf7d",
      "question": "who owned the helicopter",
      "prediction": "The answer is the U.S. Marines.",
      "ground_truths": [
        "by the Russian air company Vertikal-T,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8d21ecd07db4b0888ea7c2c91f9135c",
      "question": "What were all of the dead?",
      "prediction": "The dead were civilians, and five casualties whose condition was not known.",
      "ground_truths": [
        "civilians,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "bae286302f3748f9bc00ffc4b45bfab7",
      "question": "Where did the helicopter crash take place?",
      "prediction": "The helicopter crash occurred in Eastern Afghanistan.",
      "ground_truths": [
        "Kandahar airfield in southern Afghanistan"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "8907b0e8c2e142a395a35b0c5560fe6a",
      "question": "What also makes a hard landing?",
      "prediction": "The question asks what makes a hard landing in Afghanistan.",
      "ground_truths": [
        "U.S. military helicopter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6d1b542705c74dfcb9ca889be8fc5c82",
      "question": "what is the condition of the five casualties",
      "prediction": "The answer is \"No, the five casualties were not known. Watch report on the crash ¬ª",
      "ground_truths": [
        "was not known,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "3b6c10cafc3944a098e4ee170fbfeb9e",
      "question": "What happened in southeastern U.S?",
      "prediction": "The answer is: The U.N. General Assembly in New York City addresses the world's climate change issues.",
      "ground_truths": [
        "severe flooding"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ba16cbd6e7b4c3193e234e5c68ae150",
      "question": "Where is the war being fought?",
      "prediction": "The answer is: Southeast Flooding.",
      "ground_truths": [
        "Afghanistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76f80eb5f49e41c7b8c76f093e985b88",
      "question": "Which international airports were badly affected?",
      "prediction": "The answer is London's other two airports, Stansted and Gatwick, were also heavily affected by the snow storm.",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "4c0d30ddd6da4ba9a174843ca4669dc8",
      "question": "Who said that the snow was worst in southeastern England in 18 years?",
      "prediction": "The answer is: \"It was the first time all city bus service had been suspended, including when London was bombed during World War II,\"",
      "ground_truths": [
        "(CNN)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91fb0d81f0f94b89a6a60783d5742723",
      "question": "What was one of the airports affected by the snow?",
      "prediction": "The answer is, the British capital London, with its major transport hubs, is also closed.",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f0e807c9422490e8fed93d5bbd37826",
      "question": "Which airports are affected?",
      "prediction": "The most important information from the context is:",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "663600c74e02454896eb1494fb334bc1",
      "question": "What kind of weather was England experiencing?",
      "prediction": "The answer is \"London's worst snowstorm to hit Britain in 18 years.\"",
      "ground_truths": [
        "snowstorm"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "64a570a28516471bbb028a25beec8028",
      "question": "What is the worst in 18 years?",
      "prediction": "The worst snowstorm in 18 years hit Britain's London, resulting in the cancellation of over 650 flights at Heathrow Airport, and the closure of the British city's bus network. The British capital's other airports, Stansted and Gatwick were also closed.",
      "ground_truths": [
        "snowstorm"
      ],
      "em": 0,
      "f1": 0.05555555555555556
    },
    {
      "id": "bec5942b28c94983a86a4b16adc0b4e7",
      "question": "Where is Gatwick?",
      "prediction": "The answer is Gatwick, England, UK.",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11682b8601734e57bd4ba82eb9f20e77",
      "question": "what has been a local favorite for 68 years?",
      "prediction": "The answer is the Ski Train.",
      "ground_truths": [
        "The Ski Train"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e8c205f129ea4cff9a5204db9b89ddf2",
      "question": "where does the train travel from?",
      "prediction": "The answer is Denver, Colorado. The Ski Train departs from Union Station in Denver, Colorado, and travels to Winter Park in Denver, Colorado.",
      "ground_truths": [
        "Denver, Colorado."
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "cc143e8060be487db153d91159e40e82",
      "question": "where does the train pass through",
      "prediction": "The train passes through Colorado.",
      "ground_truths": [
        "the Rockies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8adf6db963574dad935d7158c4c5d364",
      "question": "where does the train take skiers to",
      "prediction": "The answer is Denver, Colorado.",
      "ground_truths": [
        "between Denver and Winter Park."
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "d67b0638b8c943c596db2bb2795e4edb",
      "question": "what do residents say about the train ride",
      "prediction": "The Ski Train, with its 68-year-old local train, delivers passengers to Winter Park, Colorado. It's a round-trip trip, with a cost of $49 to $85, with additional expenses like parking at the train station in Denver and lift tickets for skiers.",
      "ground_truths": [
        "riders love the trip route, which winds through the Rockies and climbs to 9,000 feet."
      ],
      "em": 0,
      "f1": 0.12
    },
    {
      "id": "d72a1897a39b4a289c85a26e68a970b2",
      "question": "what is the name of the tunnel underneath the Continental Divide?",
      "prediction": "The Tunnel underneath the Continental Divide is the tunnel under the Continental Divide.",
      "ground_truths": [
        "Moffat"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d406698c75a144f88c496b4e90f26ca7",
      "question": "Where has the Saudi militant been hiding?",
      "prediction": "The answer is, Saudi Arabia's Interior Ministry has identified the suicide bomber.",
      "ground_truths": [
        "in Yemen,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f06f7d84a78f412282d3bc847fcca401",
      "question": "Who is wanted?",
      "prediction": "The answer is Saudi Arabia's Interior Ministry. The text provides information about the Ministry's identification of the suicide bomber and the reports of the country's official news agency's disclosures.",
      "ground_truths": [
        "Abdullah Hassan Talea' Asiri,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b3a6721fd03c4610b1485c6b394d885a",
      "question": "W\\Who did Asiri meet with?",
      "prediction": "The answer is **the prince**.",
      "ground_truths": [
        "Saudi Prince Mohammed bin Naif, the assistant minister of Interior for Security Affairs."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "2192ae48715343f7b729bfebfe062bed",
      "question": "Where had Asiri been hiding?",
      "prediction": "The answer is Saudi Arabia's Interior Ministry.",
      "ground_truths": [
        "Yemen,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5075af622c6440fe9b693341c37a0b73",
      "question": "Who fell at her California home?",
      "prediction": "Nancy Reagan falls at her California home.",
      "ground_truths": [
        "Reagan,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5b20d1dccf164c7f9e35acc6d37599b1",
      "question": "From her hospital bed, How is Nancy Regan reportedly feeling?",
      "prediction": "From her hospital bed, Nancy Reagan is reportedly feeling good, especially comforted to be received by talented doctors in a world-class hospital in Chicago.",
      "ground_truths": [
        "Reagan is in good spirits, especially comforted to be receiving care"
      ],
      "em": 0,
      "f1": 0.4705882352941176
    },
    {
      "id": "e3d3f69a45554550ad9a8f8efb0752a9",
      "question": "what was the fracture",
      "prediction": "The fracture was in the pelvis.",
      "ground_truths": [
        "fractured pelvis and sacrum"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "0641421ccb1349ff8a9131e411e0a878",
      "question": "Where is nancy reagan being treated?",
      "prediction": "Nancy Reagan is being treated in the Ronald Reagan UCLA Medical Center in California.",
      "ground_truths": [
        "Ronald"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "06bff66d3af04edd808a81c4b0a106a7",
      "question": "What other organization is TAK affiliated with?",
      "prediction": "The answer is \"The Kurdistan Freedom Falcons\" (TAK).",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "64fee0d345024f42864a361022475420",
      "question": "What does PKK stand for?",
      "prediction": "The PKK stands for \"Kurdistan Workers' Party.\"",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fc7d87cd43814b8db5d9bcc43cba1da3",
      "question": "Who is TAK affiliated with?",
      "prediction": "The United States has designated a Kurdish militant group, the Kurdistan Freedom Falcons, as a terrorist organization.",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "4c20cfad09754954a40daa931403ff60",
      "question": "Who is the Turkish President?",
      "prediction": "The Turkish President is Abdullah Gul.",
      "ground_truths": [
        "Abdullah Gul,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "ba68aac128784f47a20eafdb4fa84627",
      "question": "Where did PKK attack Turkey from?",
      "prediction": "The answer is United States. The Turkish government stated that the group is responsible for multiple terrorist attacks in Turkey, which targeted tourist areas, military sites, and government buildings.",
      "ground_truths": [
        "inside northern Iraq."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb776aaae451464692b974373b39e4c5",
      "question": "What country has PKK been attacking?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Turkey"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7b190c41b59249bcab29c887d579bb26",
      "question": "What nation has the PKK been attacking?",
      "prediction": "The United States has designated the PKK as a terrorist organization, and the United States reaffirms its commitment to fighting terrorism in cooperation with Turkey, Iraq, and Europe.",
      "ground_truths": [
        "Turkey"
      ],
      "em": 0,
      "f1": 0.07999999999999999
    },
    {
      "id": "79d983f421ab4c3e92364b83b7186e99",
      "question": "Who met with President Bush?",
      "prediction": "The answer is: Turkish President Abdullah Gul.",
      "ground_truths": [
        "Turkish President Abdullah Gul,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "af41b3be86a24a4ca131ec6768beab58",
      "question": "Who is on the list of terrorist groups?",
      "prediction": "The answer is \"Turkey.\"",
      "ground_truths": [
        "Kurdistan Freedom Falcons, known as TAK,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8bff718944c84becbf657952138356fd",
      "question": "Who has been attacking Turkey?",
      "prediction": "The answer is \"Turkey.\"",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e772ff7304f54b78a3280763cf427221",
      "question": "What group did the US recently put on the list of terror groups?",
      "prediction": "The US recently put on the list of terror groups, with the PKK as a category.",
      "ground_truths": [
        "Kurdistan Freedom Falcons,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62a5565f76544eaf979a8fb42db7a3df",
      "question": "What group did US put on list of terrorist groups?",
      "prediction": "The US placed on its list of terrorist groups, based on the text.",
      "ground_truths": [
        "Kurdistan Freedom Falcons, known as TAK,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f621718fc1bf4d0685f06afc881ef447",
      "question": "What was found at an LA construction site?",
      "prediction": "The answer is \"The skull was hit and shaved off...\"",
      "ground_truths": [
        "Zed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd682a62e0f0404aa2c058c8dd6f2a1a",
      "question": "What animal was found?",
      "prediction": "The animal was found at a construction site in Los Angeles, in the heart of the area.",
      "ground_truths": [
        "Columbian mammoth"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7bb016eec4214226bfb492489fca9cfe",
      "question": "What was found at a construction site?",
      "prediction": "The answer is \"Zed was found at a construction site in Los Angeles, the heart of the L.A. County Museum of Art. It was covered in a plaster jacket, and the remains were found along with 16 other deposits at the site, which contained 23 massive crates weighing 5 to 53 tons, which were then lifted out intact. This discovery was a major finding for the museum, with the most important discovery of the last 90 years.",
      "ground_truths": [
        "\"Zed,\" a Columbian mammoth"
      ],
      "em": 0,
      "f1": 0.028985507246376812
    },
    {
      "id": "48f4f88751c04d0c9809e86c21902c91",
      "question": "What part of the animal was especially interesting?",
      "prediction": "The most interesting part of Zed's remains is the skeleton, which was covered in a plaster jacket. The construction of the parking garage was being monitored by a consulting firm, which likely involved the Paleontological Services at the Page Museum. The fossils were carefully removed, and only the skull and the lower jaw were excavated, including the vertebrae and ribs. The discovery of the tusk material was also very exciting for the museum.",
      "ground_truths": [
        "two complete, beautifully preserved tusks.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "866d127d05bf4f6981065c7d4a8142e2",
      "question": "Where can you find the La Brea tar pits?",
      "prediction": "The answer is Los Angeles.",
      "ground_truths": [
        "Los Angeles"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6a57629ec5b2439594ebb8e1f89ac3ae",
      "question": "What was found in Los Angeles?",
      "prediction": "Based on the context, the answer is \"Los Angeles's heart of the city.\"",
      "ground_truths": [
        "Columbian mammoth fossil \"Zed.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a117ca4fb504f389b8888df250f21eb",
      "question": "Where was the skeleton unearthed?",
      "prediction": "The answer is in Los Angeles, California.",
      "ground_truths": [
        "a construction site in the heart of Los Angeles."
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "44f0cff0d54445d9888cefb0752b4d3f",
      "question": "Where was it found?",
      "prediction": "The answer is \"Los Angeles' heart of Los Angeles.\"",
      "ground_truths": [
        "Los Angeles."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b636f32bd23e46cd840fe8725a8e32c0",
      "question": "What was a rare occurrence?",
      "prediction": "The answer is, \"the clearing of the plaster jacket of four sections, including vertebrae and ribs and pelvis, one tusk and the lower jaw,\" and the efforts to unlock and analyze the fossils.",
      "ground_truths": [
        "Zed's tusks were found intact,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09397d660e8d46119fac2f6ad902a6e6",
      "question": "What did they call the most important?",
      "prediction": "The most important discovery at Los Angeles' George C. Page Museum is the finding of \"a whole new treasure trove of fossils,\" including four sections of vertebrae and ribs and the lower jaw, of a Columbian mammoth, Zed.",
      "ground_truths": [
        "\"a whole new treasure trove of fossils\""
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "2d1cd230a1d9409a9231d27d442b34c2",
      "question": "What is the disconvery called?",
      "prediction": "The answer is \"the plaster jacket of four sections, including vertebrae and ribs and pelvis\", and the excitement is generated by the discovery of \"a whole new treasure trove of fossils\" for the museum's attendance and funding.",
      "ground_truths": [
        "\"Zed.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e33b52e8485c41518aaf76f1048c35e4",
      "question": "What is the name of the movie that Crowe starred in?",
      "prediction": "The movie that Crowe starred in was \"State of Play.\"",
      "ground_truths": [
        "\"State of Play\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "198f769edd5c4bd7b829ffeaa6d595c1",
      "question": "What was the box office debut for \"17 Again\"?",
      "prediction": "The box office debut for \"17 Again\" was in No. 1, with a $24.1 million total.",
      "ground_truths": [
        "$24.1 million,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "2f82125c096845fc8b5fc65418c486aa",
      "question": "What were the box office earnings?",
      "prediction": "The box office earnings were $12.9 million for \"17 Again,\" and the final tally for this weekend's box office sweepstakes is below.",
      "ground_truths": [
        "$12.3 million"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "2cda8c267a624be184e87b1d2cd0b05c",
      "question": "Who stars in the movie?",
      "prediction": "The answer is \"Cacaw.\"",
      "ground_truths": [
        "Zac Efron"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe0d714e8be546d994f3469fbc3bd068",
      "question": "What movie debuted at No. 1?",
      "prediction": "The answer is \"17 Again\"",
      "ground_truths": [
        "\"17 Again,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "74518b730d2941859c8fc206f74de46a",
      "question": "Which film was at number two?",
      "prediction": "The film was at No. 2 in the box office.",
      "ground_truths": [
        "\"State of Play\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce5d0f11b9664510a27bf3c483792f13",
      "question": "What film debuted at number one at the box office?",
      "prediction": "The answer is \"17 Again.\"",
      "ground_truths": [
        "\"17 Again,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "33313f986ec740f3b824bc3accb9111e",
      "question": "How much money did \"17 Again\"pull in?",
      "prediction": "The answer is $24.1 million.",
      "ground_truths": [
        "$24.1 million,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "29544317d3ab466d8b13e1eac4463c10",
      "question": "What were the box office earnings of the film at the top of the charts?",
      "prediction": "The box office earnings of the film at the top of the charts were $12.7 million.",
      "ground_truths": [
        "$24.1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a4c2d78a44cb4c65b98f86adebf8e4b5",
      "question": "Where did the bombing occur?",
      "prediction": "The bombing occurred in Peshawar, Pakistan, capital of North West Frontier Province.",
      "ground_truths": [
        "Pakistani city of Peshawar, capital of North West Frontier Province."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4d010f61af224f21bc9f870d5935aaf6",
      "question": "who is responsible for the bombing",
      "prediction": "The answer is the Interior Minister, Rehman Malik.",
      "ground_truths": [
        "militants"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d31f21644c548788fef6f077ae10d1f",
      "question": "What was the number of men involved?",
      "prediction": "The answer is: **2**",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bc591d2c425c47d2914bcf642d6e412b",
      "question": "According to the Interior Minister, how many men plotted to carry out the attacks?",
      "prediction": "The answer is **3**.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a4dc0db408144d23a01a2b5e6d6f1b44",
      "question": "What happened to the suspect?",
      "prediction": "The answer is that the suspect was killed in a gunfight.",
      "ground_truths": [
        "blew himself up."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64a62c4af43e49438c5f0c1c8a993ac1",
      "question": "How many people did the suicide bomber kill?",
      "prediction": "The suicide bomber killed at three people in the city of Peshawar.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "aa8d8e1df27b48e187d9a8c95f42bf36",
      "question": "how many were involved",
      "prediction": "The answer is 3.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "61c0ad1bdfa54b6ca64936e0b1162737",
      "question": "Had the suspect blown himself up after running out of bullets?",
      "prediction": "Based on the provided text, the answer is **Yes, the suspect blew himself up after running out of bullets.**",
      "ground_truths": [
        "of"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "4973d06104ff42a48ec95b6d8266cffa",
      "question": "How many military officers were sentenced?",
      "prediction": "The answer is 16.",
      "ground_truths": [
        "Sixteen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9206f18617d749b09fc00c5f949e4a54",
      "question": "Who was known as the blonde angel of death?",
      "prediction": "The answer is, Marianela Galli was known as the blonde angel of death.",
      "ground_truths": [
        "Alfredo Astiz,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a80717a88e74b44856439a7cc7e8eac",
      "question": "Where did the victims wait to hear the judge read the sentences?",
      "prediction": "The victims waited for the judge to read the sentences on a large-screen television in the Andes.",
      "ground_truths": [
        "courtroom in downtown Buenos Aires,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "727106ef76a0404cbf006524c2e6529f",
      "question": "What number of military officers was sentenced for abuses?",
      "prediction": "The answer is 18.",
      "ground_truths": [
        "Sixteen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dde94541f9794aa4a55a19e0b20cf54f",
      "question": "Who is known as the Blonde Angel of Death?",
      "prediction": "The answer is \"Marianela Galli,\" and her parents and grandmother were all victims of the 1976-1983 dictatorship in Argentina.",
      "ground_truths": [
        "Alfredo Astiz,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d0acdc75fd74611860f42cbd35c0e91",
      "question": "Who waited outside the courthouse?",
      "prediction": "The answer is \"Argentine\", as CNN noted.",
      "ground_truths": [
        "victims' family members and friends,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c01e2730fd0544988a6e4c5dc11e0734",
      "question": "Which party opposed the surtax?",
      "prediction": "The answer is: Republicans.",
      "ground_truths": [
        "Republicans"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c4eb8e9e88a64fa49128ad4e99776c91",
      "question": "what would the bill provide",
      "prediction": "The bill provides tax credits for businesses hiring veterans, up to $2,400 for $1 million in employment, and $5,600 for hiring veterans out of work for more than six months.",
      "ground_truths": [
        "a remedy to unemployment among veterans."
      ],
      "em": 0,
      "f1": 0.1176470588235294
    },
    {
      "id": "0c9774b9297641e39224f703c4e13ec6",
      "question": "who removed the surtax",
      "prediction": "The answer is: The bill was proposed as the next piece in President Obama's jobs bill and as a remedy to unemployment among veterans.",
      "ground_truths": [
        "Republicans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "144e38b6eea047c99e7ec1d8dbb6cbbc",
      "question": "What is controversial?",
      "prediction": "The controversial bill, enacted as the next piece in President Obama's jobs bill, is a tax incentive for businesses hiring veterans, and Congress and the President have rejected it. The bill would increase taxes on former military personnel and provide tax credits to unemployed veterans for spending more money in other areas. It also makes mandatory jobs training for all service members available to all veterans and offers Medicare for seniors to work for at least 6 months. The bill also makes mandatory job skills training available to all service members and provides funding for community colleges and technical schools.",
      "ground_truths": [
        "millionaire's surtax,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b256cf4c08f4507b8b2f96d7289b585",
      "question": "What criteria would be used to give tax incentives?",
      "prediction": "The answer is: \"It would make sure that our veterans are not standing at the back of any unemployment line.\"",
      "ground_truths": [
        "businesses hiring veterans as"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "b33e0d818b364b87878b747b09987f28",
      "question": "What incentives are offered to hire veterans?",
      "prediction": "The answer is: Tax incentives for businesses hiring veterans.",
      "ground_truths": [
        "tax"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "e6a17a43379a40d1a0388edd679e1fc3",
      "question": "Who opposes the surtax?",
      "prediction": "The answer is Democrats. The context states the Senate Democrats proposed a tax incentive for businesses hiring veterans, and the Washington (CNN) provided information about the bill's provisions.",
      "ground_truths": [
        "Republicans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13c7ec87c0324be5bee05d51772027c5",
      "question": "What would be provided for those leaving the military?",
      "prediction": "The answer is: C. President Obama's jobs bill.",
      "ground_truths": [
        "job training"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "937709a34e1b44b487be29fdbd96c709",
      "question": "What are they waking up to?",
      "prediction": "The answer is \"it's a trend because, what little girl doesn't want to emulate the first kids?\"",
      "ground_truths": [
        "black is beautiful,\""
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "ccdaabb0544c42e4b61b3efc59a1ca47",
      "question": "What brand did the girls wear?",
      "prediction": "The answer is \"marketers are finally waking up to it -- you know -- black is beautiful,\" and in this context, the answer is \"marketers are finally waking up to it -- you know -- black is beautiful.\"",
      "ground_truths": [
        "J.Crew"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5ce8d7d648dd46d3ac228d1002fc64ac",
      "question": "Which website crashed?",
      "prediction": "The website crashed at Wilhelmina Kids.",
      "ground_truths": [
        "J.Crew"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3164fc40406b46299e4ae8f1c7d9bcf8",
      "question": "Like whose daughter does she look?",
      "prediction": "Answer: Sasha Obama",
      "ground_truths": [
        "Obama."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7c41bc4cbdea419fb9fcebfbb4615ab8",
      "question": "Who does one young model look like?",
      "prediction": "The answer is \"I think it will, and it's a trend because it makes the fashion industry's image of the office look appealing, and it can help people find similar products and brands.\"",
      "ground_truths": [
        "Sasha"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9d1e57a0992485f93d0d153df752959",
      "question": "What are marketers waking up to?",
      "prediction": "The answer is \"it's just going to become part of the fashion imagery of pop culture, which is a great thing,\" with a brief note about the impact of brands on marketing.",
      "ground_truths": [
        "black is beautiful,\""
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "67580c657c3e42ae928091f7fdb6940a",
      "question": "What is beautiful?",
      "prediction": "The answer is \"beautiful.\"",
      "ground_truths": [
        "black family"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "31e73b41abf14323a275438fa89ec36e",
      "question": "What wil the federal government do?",
      "prediction": "The federal government's response to the U.S. judge's order is:",
      "ground_truths": [
        "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "08655f61d8a04962866a2cfd76710d6c",
      "question": "A judge ruled that how many Chinese Muslims must be released?",
      "prediction": "The answer is 17.",
      "ground_truths": [
        "17"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0168fac7c43d42b59e77d2c1f6a0c1e9",
      "question": "What is the federal government going to do?",
      "prediction": "The answer is: The federal government is going to do nothing.",
      "ground_truths": [
        "immediate release"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da24c5186346416db01377a9fb9fdb96",
      "question": "What part of China are the detainees mostly from?",
      "prediction": "The detainees are from a mostly Muslim autonomous region in western China.",
      "ground_truths": [
        "western"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "41771f0c93994375956cde79ea9c4136",
      "question": "Who ruled they must be released?",
      "prediction": "The answer is a guard tower.",
      "ground_truths": [
        "A federal judge"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08b1f43797d5403baa51de8c8b298d6a",
      "question": "What action did the Federal government take?",
      "prediction": "The answer is, the U.S. federal government took the following actions:",
      "ground_truths": [
        "immediate release into the United States of 17 Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73dc231d896a4e13b9d6acaa98dd5e8f",
      "question": "What is the ethnicity of the 17 detainees?",
      "prediction": "The context provides information about the 17 Uighur Chinese Muslims who have been held in the U.S. military facility at Guantanamo Bay. This information is presented in a concise format.",
      "ground_truths": [
        "Chinese"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "40700ca8ac7f425bbd23ab9943f843eb",
      "question": "what is your latest novel?",
      "prediction": "Answer: \"Rin Tin Tin: The Life and the Legend\" is a biography of the German shepherd who played a key role in the 1998 best-seller \"The Orchid Thief\" (and later \"Adaptation\"). It's about the dog's life, his memories, and the dogs that played him, including his mother, his wife, and the orchids, all of which are distinct and exotic.",
      "ground_truths": [
        "\"Rin Tin Tin: The Life and the Legend\""
      ],
      "em": 0,
      "f1": 0.2181818181818182
    },
    {
      "id": "d19cfaa002124609a6dbed47adea985b",
      "question": "What is she drawn to?",
      "prediction": "The answer is \"the orchids are both things that very disparate and sometimes unlikely people come together over.",
      "ground_truths": [
        "to the idea of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "835a3a2903524ba1a9d9eea52f339d73",
      "question": "where is Her latest novel is a biography?",
      "prediction": "The answer is: \"Rin Tin Tin: The Life and the Legend\" is a biography of the German shepherd, Rin Tin Tin.",
      "ground_truths": [
        "Rin Tin Tin,"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "b7ae02f7a3e84e70b7522d91392dc247",
      "question": "What is Susan Orlean known for?",
      "prediction": "The answer is: Susan Orlean is known for her work as a well-known American author, known for her stories about the German shepherd, her attempts to create a biography of Rin Tin Tin, and her role in the film adaptation of \"Adaptation,\" where her friends were puzzled.",
      "ground_truths": [
        "1998 best-seller \"The Orchid Thief\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad9c3f85a2a546998c8e27ac3395eca8",
      "question": "What is her latest novel?",
      "prediction": "The answer to the question is: \"Rin Tin Tin: The Life and the Legend\" (Simmany & Schuster).",
      "ground_truths": [
        "\"Rin Tin Tin: The Life and the Legend\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "205530b4bd49448ab55e7cce2fcf6b00",
      "question": "What was Olson a member of?",
      "prediction": "Answer: Jon Opsahl was a member of the California Department of Corrections and Rehabilitation, and was released to her husband after seven years.",
      "ground_truths": [
        "the self-styled revolutionary Symbionese Liberation Army"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff9e7d0ab1b046a99f617df0ad8c16b5",
      "question": "What did Olson belong to?",
      "prediction": "The answer is **Kathar Olson**.",
      "ground_truths": [
        "Symbionese Liberation Army"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22ea755508484f639d641a28f1bc01af",
      "question": "What did Olson do during the bank robbery?",
      "prediction": "The answer is: \"She did good deeds everywhere. She raised three wonderful daughters. It was always a shock to all of us that the Sara we know had that past,\" Dawkins said.",
      "ground_truths": [
        "with a firearm and kicked a nonresisting pregnant teller in the stomach."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b344f9c87e2d4e5498338ea56023892c",
      "question": "Which King's palace?",
      "prediction": "The answer is \"Narayanthi Royal Palace.\"",
      "ground_truths": [
        "Narayanthi Royal"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "d1f6974f4b8f4a189e64cdf36c986df7",
      "question": "What is being converted into museum?",
      "prediction": "The answer is the national flag.",
      "ground_truths": [
        "deposed king's opulent palace"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68ac44d7d9e24aee8fcaa91e0f364062",
      "question": "When did the monarchy begin?",
      "prediction": "The answer is: When did the monarchy begin? 239 years after the 2001 Kathmandu Royal Palace (Narayanhiti Royal Palace) was converted into a museum.",
      "ground_truths": [
        "239 years of rule"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "bb334d8819054ded95d25bd214397832",
      "question": "What kingdom was declared a republic?",
      "prediction": "The kingdom was declared a republic in Nepal.",
      "ground_truths": [
        "Nepal"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "53600e1f83ab4c559e064fcac5f821bb",
      "question": "What was being ended?",
      "prediction": "The national flag is fluttering in the hands of the people in the royal palace.",
      "ground_truths": [
        "The monarchy's end after 239 years of rule"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "6dd10f33c4f8423080bdcf84c8a01ef1",
      "question": "what happened in nepal",
      "prediction": "The answer is: the national flag fluttered in the hands of the people in the royal palace, and the prime minister at the ceremony said it was a symbolic move to signify the end of the monarchy.",
      "ground_truths": [
        "new government has converted its deposed king's opulent palace into"
      ],
      "em": 0,
      "f1": 0.05405405405405406
    },
    {
      "id": "9508601ca2724c259442daa574ec6e2f",
      "question": "Who will she play in the final?",
      "prediction": "The answer is \"Piers Morgan.\"",
      "ground_truths": [
        "Annie Duke"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "947f12b8aa8c4209a1472f10f2424a90",
      "question": "Who has won this season?",
      "prediction": "The answer is \"Rivers,\" because she won the \"Celebrity Apprentice\" competition.",
      "ground_truths": [
        "Joan Rivers"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "682c7b2e730244809d631539f9bf69da",
      "question": "Who won this season's Celebrity Apprentice?",
      "prediction": "The answer is:  The Celebrity Apprentice, which aired on Donald Trump's \"The Celebrity Apprentice\" in New York, won.",
      "ground_truths": [
        "Joan Rivers'"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be16a44a8c3f4e229442e38c9701ef74",
      "question": "What does the victory mean for the charity?",
      "prediction": "The victory of \"The Celebrity Apprentice\" is likely to mean that the series has failed to achieve its stated goals. It's a controversial outcome, and the \"Celebrity Apprentice\" logo, which is an \"N\" in a French font, is a symbol of the show's initial success. It's also a hard-fought, low-key final night that leaves the final code of conduct ambiguous.",
      "ground_truths": [
        "$250,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "455c1d1b4207431f8ff8db6ca4989d2a",
      "question": "How much does the Charity get?",
      "prediction": "The Charity received $250,000 for the series.",
      "ground_truths": [
        "$250,000"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "eec978f73e724203b8b4974255dcfa4e",
      "question": "What does the victory mean?",
      "prediction": "The victory of the \"Celebrity Apprentice\" show, hosted by Donald Trump, is equivalent to $250,000 for Rivers' charity, and it's deemed \"good for the show's success.\"",
      "ground_truths": [
        "$250,000 for Rivers' charity: God's Love We Deliver."
      ],
      "em": 0,
      "f1": 0.25806451612903225
    },
    {
      "id": "8efa4e3802454905a846335425e82876",
      "question": "Who wins this season's \"Celebrity Apprentice\"?",
      "prediction": "The answer is, \"Rivers' winner of the competition.\"",
      "ground_truths": [
        "Joan Rivers'"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5190259794744c2bb57eb2ec83a7044b",
      "question": "Who goes up against poker champion Annie Duke in finale?",
      "prediction": "The answer is **Nancy Duke**.",
      "ground_truths": [
        "Joan Rivers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "059ba875878347a6a578f79b78f09c28",
      "question": "Who attracts more celebrities?",
      "prediction": "The answer is \"Rivers\".",
      "ground_truths": [
        "Rivers"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e45eb88379924f609da7299935497b64",
      "question": "What do they expect to raise?",
      "prediction": "The auction will also include a collection of letters from the Titanic survivor Barbara Dainton-West, with estimated to fetch ¬£40,000 to ¬£60,000 ($60,000 to $90,000).",
      "ground_truths": [
        "$50,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dbdd82848efd4be2b7afbc34e09b5576",
      "question": "Letters from whom are being auctioned?",
      "prediction": "The auction will also include a collection of letters from the estate of Titanic survivor Barbara Dainton-West, estimated to fetch ¬£40,000 to ¬£60,000.",
      "ground_truths": [
        "Barbara Dainton-West,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "6505e3709d6a4b3eb880d1ea9247377c",
      "question": "What age was the 97 year old when the Titanic sank?",
      "prediction": "The 97-year-old, 97-year-old, and 10-year-old Dean, was auctioned off to pay nursing home bills.",
      "ground_truths": [
        "9-week-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef6478513dd74b0a91e11ba14a4952ab",
      "question": "What year did the Titanic sink?",
      "prediction": "The Titanic sank in 1912.",
      "ground_truths": [
        "1912."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c9b5f70b74384cdb89a8f83b5d82a275",
      "question": "Whom are the letters from?",
      "prediction": "The letters from the auction will include notes of family trips to the Titanic, and the death of Barbara Dainton-West.",
      "ground_truths": [
        "Barbara Dainton-West,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "14702b4909444cf990223aebd601eed3",
      "question": "what is her survival story?",
      "prediction": "The answer is: **Dean's survival story is a story about her continued struggle, her efforts to maintain her own life, and her eventual recovery from the sinking of the Titanic.**",
      "ground_truths": [
        "lifted from the lifeboat onto Carpathia, the rescue ship, in a mail sack,\""
      ],
      "em": 0,
      "f1": 0.05555555555555555
    },
    {
      "id": "b904c9a67823413d910dc3245aa38164",
      "question": "What is the auction supposed to raise?",
      "prediction": "The auction's supposed raise is for Dean's canvas bag, which might have been used to lift the infant Dean from a lifeboat to a rescue ship.",
      "ground_truths": [
        "$50,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8244060627d4466ebb975089f7b231b5",
      "question": "What day was the American aid worker killed?",
      "prediction": "The American aid worker was killed on Wednesday.",
      "ground_truths": [
        "Wednesday."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "f7db4258609847718815614377635a9c",
      "question": "What nationality was the aid worker?",
      "prediction": "The answer is **American**.",
      "ground_truths": [
        "American"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f961611cfd2d4098a1d13b8bc086b9bb",
      "question": "Where was the diplomat kidnapped?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "near his home in Peshawar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f727854da0824a49a0125f8ce6a39de5",
      "question": "Who was kidnapped?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Heshmatollah Attarzadeh"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5ec33ded837448e79f160a1287b7b41b",
      "question": "He was traveling from his home to what destination?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Iranian consulate,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bfb29a9e3ea3412fb08f16040f6d37ce",
      "question": "Where was the American aid worker slain?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "outside the Iranian consulate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3b3caf88510477499d31a1e2081f9e3",
      "question": "Where is Peshawar?",
      "prediction": "The answer is Peshawar, Pakistan.",
      "ground_truths": [
        "is the capital of Pakistan's North West Frontier Province,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "25a75f71b62f4c9cab280dda8ac44613",
      "question": "Where was the bodyguard killed?",
      "prediction": "The answer is Pakistan. The context states \"Iran has condemned the kidnapping of one of its diplomats in northwest Pakistan on Thursday, calling it an \"act of terrorism,\" an Iranian foreign ministry spokesman said.",
      "ground_truths": [
        "home in Peshawar"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "b95ec30fdf864e6e9d204977c3426aef",
      "question": "Who is the kidnapped diplomat?",
      "prediction": "The answer is: **Iran.**",
      "ground_truths": [
        "Heshmatollah Attarzadeh"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9100743ff9db4bbd8a4a415210d2b163",
      "question": "Where was the diplomat and his bodyguard killed at?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5b28c7d93a7f4127b220b587537a53d6",
      "question": "where was this shooting?",
      "prediction": "The answer is Iraq.",
      "ground_truths": [
        "Najaf."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4100b5dc885d40898d7cf76134188ca0",
      "question": "Where was the attack?",
      "prediction": "The provided text does not contain a single, definitive answer to the question. It only reports on the following:",
      "ground_truths": [
        "City"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5d8e875e536471eb6011003ce161c39",
      "question": "Did the Iraqi patrol come under fire?",
      "prediction": "The Iraqi and U.S. forces attacked Sadr City on Friday, with the U.S. aircraft bombing the area for hours. The U.S. military's statement also mentioned the use of rockets and mortar fire. There were also reports of casualties. The Iraqi and U.S. soldiers killed two snipers, two other men firing rocket-propelled grenades, and \"multiple others from a nearby building where soldiers were taking RPG and machine gun fire.\"",
      "ground_truths": [
        "Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire"
      ],
      "em": 0,
      "f1": 0.24657534246575344
    },
    {
      "id": "4b8b1d7070fa42268d219fe74316bfa8",
      "question": "what local indian official blames?",
      "prediction": "The answer is: The Jaipur police, along with the central government.",
      "ground_truths": [
        "\"unnamed international terror group\" for the attack,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1b81a04e32e4647b7fb66a70536fdc7",
      "question": "What do police impose?",
      "prediction": "The police impose a curfew in Jaipur, India, on Wednesday, a day after near-simultaneous bomb attacks in the ancient Indian city of Jaipur.",
      "ground_truths": [
        "curfew"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "ec480559c4764db5a0ca181e89bfaaa4",
      "question": "What did police suspect were used to carry the bombs?",
      "prediction": "The police suspected that the bombs were used to carry the explosives.",
      "ground_truths": [
        "bicycles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c758cd8261847cc80c73b534f761843",
      "question": "Was anyone wounded by the bombs?",
      "prediction": "The answer is yes, there were 63 people wounded.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da9083ce238648d09164693834e78137",
      "question": "What number of people are wounded?",
      "prediction": "The answer is 200.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "c4f372d821e44cdd85dda6cb06f07d3d",
      "question": "what's the amount of peopole that wounded?",
      "prediction": "The answer is **63**.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a224f7e8f194d13b127bbd05de5e273",
      "question": "Who is responsible for the bombing that killed 63 people?",
      "prediction": "The answer is **Pakistan**.",
      "ground_truths": [
        "\"unnamed international terror group\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c423ed161c034b12b0dc03d335f8e995",
      "question": "what police impose?",
      "prediction": "The answer is \"Police impose.\"",
      "ground_truths": [
        "curfew"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "54ccee8119e04360811e2aed351e6729",
      "question": "Where did police impose a curfew?",
      "prediction": "The answer is **India**.",
      "ground_truths": [
        "Jaipur"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c7e6e2409cea4bd486b3c7821a188bd4",
      "question": "Whom does a local Indian official blame for the blasts?",
      "prediction": "The answer is the **central government**.",
      "ground_truths": [
        "\"unnamed international terror group\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2e66b663b45441ea9a9d8b3865639f7",
      "question": "What does the Bangladeshi army vow to do?",
      "prediction": "The Bangladeshi army vows to do the following:",
      "ground_truths": [
        "punish participants in this week's bloody mutiny,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8e1e606409b437eac9e6e88c9ca3470",
      "question": "How many bodies were recovered outside Dhaka?",
      "prediction": "The answer is 88.",
      "ground_truths": [
        "88"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "096419c95ada496db118b0f52228fcd8",
      "question": "What country did this take place in?",
      "prediction": "The country taken place in was Bangladesh.",
      "ground_truths": [
        "Bangladesh"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "fdb2b92b9cd74fb4a2c82bf4987657d8",
      "question": "Where were the people when the mutiny erupted?",
      "prediction": "The answer is **Bangladesh Rifles (BDR)**.",
      "ground_truths": [
        "inside the headquarters of the Bangladesh Rifles"
      ],
      "em": 0,
      "f1": 0.4000000000000001
    },
    {
      "id": "1130ea0f1e4840d889ea305956bd0c5d",
      "question": "How many were in the Rifles headquarters when the mutiny started?",
      "prediction": "The Rifles headquarters was found to be 65,000-strong paramilitary outfit, primarily responsible for guarding Bangladesh's borders.",
      "ground_truths": [
        "More than 160 army officers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "05f1edf5739f4b4ca3e0e533c1022fc3",
      "question": "How many bodies have been discovered?",
      "prediction": "The new nation's national newspaper reports that 88 bodies have been discovered from the mass graves in the outskirts of Dhaka's area.",
      "ground_truths": [
        "88"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "d30021dabd0a415b9ea1e2cbe57725f4",
      "question": "When did the mutiny occur?",
      "prediction": "The answer is: When did the mutiny occur.",
      "ground_truths": [
        "Wednesday morning,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c930dbb4f888467398ea923ccbae92d7",
      "question": "what river is rising",
      "prediction": "The answer is: Fargo, North Dakota.",
      "ground_truths": [
        "Red"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81b540cb16e540b3a95faba2f7d2857f",
      "question": "What is the river expected to crest at?",
      "prediction": "The river will crest at about 20 feet above flood stage.",
      "ground_truths": [
        "about 20 feet above flood stage."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "d5d038576cc64dd6bdd1b5085f907a13",
      "question": "What river is quickly rising?",
      "prediction": "The answer is Fargo, North Dakota.",
      "ground_truths": [
        "Red"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97f11d0f20674a169eec6dbd832aacfb",
      "question": "What is Fargo preparing for?",
      "prediction": "Fargo is preparing for a major flood in the next two weeks, and the city is making improvements to its dikes, haul dirt, and deliver sandbags.",
      "ground_truths": [
        "near-record floodwaters,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53c827a1b93d409c8b3f023472dc7b87",
      "question": "Who do forecasters predict?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "the river will crest Saturday about 20 feet above flood stage."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb4218e344aa4716958a782ecf3d0015",
      "question": "when will it crest",
      "prediction": "The answer is based on the context provided, and the information provided in the answer.",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a62183488d94fad91b27f15d35939fe",
      "question": "how many are without power",
      "prediction": "The answer is: One.",
      "ground_truths": [
        "Thousands of customers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "84adc9beb827473db0a3de0fde246902",
      "question": "When will the river crest?",
      "prediction": "The answer is \"We're in the full flood fight mode here.\"",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e614e0c6001c425f9eecd8d01c5da057",
      "question": "What were the deaths due to?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "the storm,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "87b53cb7f58349ef9de6f8a1033699c4",
      "question": "What may do what?",
      "prediction": "The answer is: \"It's a feast for the eyes in terms of color, texture variations -- it's amazing to see the warm water and look down just a few feet below at this amazing spread of ocean life. The warming water and the presence of many of the red, striped, and sickly-looking leaves of the large, green, and decaying trees make the beautiful displays in the Great Barrier Reef, Australia, and the Great Barrier Reef in New Orleans, Louisiana, a delicious feast for the eyes.\"",
      "ground_truths": [
        "Glaciers in the European Alps"
      ],
      "em": 0,
      "f1": 0.027027027027027025
    },
    {
      "id": "d3d3a6d45102452993e2ad22b6e261d3",
      "question": "will the glaciers melt",
      "prediction": "The provided context describes the impact of climate change on these lakes in Europe's Alps. The text suggests that some of the changes are already happening, and the \"Great Barrier Reef\" in Australia is seeing reduced bleaching, with the Great Barrier Reef Marine Park Authority expecting the problem to grow in coming decades. It also mentions that the West Coast of Australia is experiencing \"bleaching\" in recent years and is expected to become more conspicuous in coming decades. It concludes with a note of sobering data about the impact on New Orleans, Louisiana, where water levels are expected to rise in the next 10 years, highlighting the potential for more difficult",
      "ground_truths": [
        "may"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a24064f15c0c4365a7358f2816906914",
      "question": "What bugs are ruining the forest?",
      "prediction": "The answer is, \"It's a feast for the eyes in terms of color, texture variations -- it's amazing to see the warm water and the ocean life. It‚Äôs wonderful to be enveloped in the warm water and look down just a few feet below at this amazing spread of ocean life.\"",
      "ground_truths": [
        "pine beetles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "05c9c5f852f14a30ad866b3d517b6091",
      "question": "What is happening in Colorado?",
      "prediction": "The answer is, \"It's sobering to be in New Orleans and look up at the levees that sit above you and see the river,\" while the answer is \"It's sobering to be in New Orleans and look up at the levees that sit above you and keep out the Mississippi River.\"",
      "ground_truths": [
        "Herds of tiny pine beetles are munching away at"
      ],
      "em": 0,
      "f1": 0.037037037037037035
    },
    {
      "id": "5e12a545101d40f58842b623e28df786",
      "question": "When will the glaciers melt?",
      "prediction": "The answer is \"Going to see the hot spots you just can't miss, hoping to learn the heat from the new, changing environments and not to be afraid of the risks of the unknown.\"",
      "ground_truths": [
        "as soon as 2050,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec7b492ce5d54f9f9b7dbf41e149313c",
      "question": "What punishment does she want her attacker to receive?",
      "prediction": "The answer is \"an eye for an eye,\" as it is a legal requirement in Iran.",
      "ground_truths": [
        "an eye for an eye,\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "73897fac4ef14479b8efb8bcaad56eab",
      "question": "What happened to the woman?",
      "prediction": "The answer is, if her husband won't see her eyes, and she won't see her husband, then she will be blinded with drops of acid in each eye.",
      "ground_truths": [
        "acid attack"
      ],
      "em": 0,
      "f1": 0.06896551724137931
    },
    {
      "id": "451089bab6df4914a385a9ceacb60f6d",
      "question": "Who wants \"eve for an eye\" punishment?",
      "prediction": "The answer is \"I want an eye for an eye,\" and she is blind.",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95c846a744a244d09a9e6440113637a8",
      "question": "Who disfigured by acid wins?",
      "prediction": "The answer is *Movahedi*, the man who blinded her.",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b209472f93cb447286295096a3f0ed6f",
      "question": "Who ordered blinding of Majid Movahedi?",
      "prediction": "The answer is, \"I don't want to blind him for revenge.\"",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f692b91ee3dc4bc0bbcc27b0814c1919",
      "question": "What was this woman blinded by?",
      "prediction": "The answer is \"Majid Movahedi.\"",
      "ground_truths": [
        "acid attack"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1adadbe9d04843a483266128f2af67cb",
      "question": "Who was blinded?",
      "prediction": "The answer is:  Ameneh Bahrami.",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "60fb4646b8694ce49cf02c1eda1fb242",
      "question": "What is the name of her attacker?",
      "prediction": "The answer is \" Majid Movahedi.\"",
      "ground_truths": [
        "Majid Movahedi,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "dadbf5853ac746239f484a0c7ff02245",
      "question": "What did Cannavaro test positive for?",
      "prediction": "Fabio Cannavaro tested positive for cortisone in a medical test after a medicine in an emergency.",
      "ground_truths": [
        "cortisone."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "a4c31c16dd1a4cf493b8c20500a88539",
      "question": "Who is the match against?",
      "prediction": "The match against Cyprus is against Juventus.",
      "ground_truths": [
        "Cyprus"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "1329f988ad3845129c304b54501c934c",
      "question": "Who will join the Italian Squad?",
      "prediction": "The answer is Juventus.",
      "ground_truths": [
        "Fabio"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "427c5359e1bd4e2ca065c379a9c5d227",
      "question": "Cannavaro will join what squad?",
      "prediction": "Answer: Cannavaro will join the Italian national squad on Sunday for their World Cup qualifier against Cyprus.",
      "ground_truths": [
        "Italian national"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "359f8256bb2b4d5589fa81885ef48257",
      "question": "Who tested positive for cortisone?",
      "prediction": "The answer is **Cannavaro.**",
      "ground_truths": [
        "Cannavaro"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "574737b3f4284419a8e2bcf43167e253",
      "question": "Who has joined the italian squad?",
      "prediction": "The answer is: Juventus.",
      "ground_truths": [
        "Cannavaro"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73d58decb36a470e91d5491834972a95",
      "question": "What did he test positive for?",
      "prediction": "The answer is that Juventus's coach, Fabio Cannavaro tested positive for cortisone, not cocaine.",
      "ground_truths": [
        "cortisone."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "bd70e61aec6e44b78f99a40ec3fc5e80",
      "question": "who has no plans to shoot rocket down?",
      "prediction": "The context provides the following:",
      "ground_truths": [
        "The United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f20e36d52924359a2d55b4d89ff5b40",
      "question": "What do North Koreans intend to launch?",
      "prediction": "The context provides information about North Korea's long-range missile launch. The answer is a satellite.",
      "ground_truths": [
        "long-range missile"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "d8474a7dd84b47e29cac3fff91ee02e2",
      "question": "what are officials doing to respond?",
      "prediction": "The answer is: \"We are doing our best to dissuade the North Koreans from going forward, because it is provocative action.\"",
      "ground_truths": [
        "dissuade the North Koreans from going forward,"
      ],
      "em": 0,
      "f1": 0.4799999999999999
    },
    {
      "id": "47996e3b9a744a8a8466cc117b02b270",
      "question": "who intend to launch a communications satellite?",
      "prediction": "The answer is South Korea. The context states that North Korea has positioned a U.N. agency, and its stated intention is to launch a satellite.",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "c91eab99addd46a1bf4bf424d84e2afd",
      "question": "Who is the U.S. Secretary of State?",
      "prediction": "The answer is: The U.S. Secretary of State.",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ed16d36f0014fc09e8b76ea26ec7fe9",
      "question": "what is making us not shoot the rockets?",
      "prediction": "The answer is, North Korea. The context indicates it's a U.S. counter-proliferation official.",
      "ground_truths": [
        "U.N. Security Council"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3fe6d86f0533409599c19272067c65f2",
      "question": "What does N. Korea put into position?",
      "prediction": "The answer is South Korea.",
      "ground_truths": [
        "missile"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7932c4f7532f4436a69d592a90f8dc78",
      "question": "what could launch either a warhead or a satellite?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "361aa33d63984db4a873b0616085fe57",
      "question": "What made him choose Cameroon?",
      "prediction": "The answer is Cameroon.",
      "ground_truths": [
        "AIDS and HIV"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d92f93c967248f2b391b8769cdc72e1",
      "question": "Who is visiting Cameroon?",
      "prediction": "The answer is Cameroon.",
      "ground_truths": [
        "Pope"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4917dd8c60ac45e7a7ed07f84af0032b",
      "question": "What did he do in Africa?",
      "prediction": "The answer is: **Pope Benedict XVI refused to soften the Vatican's ban on condom use as he arrived in Cameroon for his first visit to Africa, and he walked with Pope John F. Paulson in Yaounde, the capital of Cameroon, in Tuesday.**",
      "ground_truths": [
        "reiterated the Vatican's policy on condom use"
      ],
      "em": 0,
      "f1": 0.17777777777777778
    },
    {
      "id": "6499d7053acd45a7a8ca79b44e0c0bec",
      "question": "What is he visiting?",
      "prediction": "The answer is Cameroon, the capital of Cameroon.",
      "ground_truths": [
        "Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c0ea03c08ec4af9a3cb897c11f0e0e4",
      "question": "Where did AIDS and HIV hit the hardest?",
      "prediction": "The answer is: The Catholic Church has been on the front line of HIV care in sub-Saharan Africa, with 11.4 million people infected with HIV in 2007, in 11.5 million orphans in the region.",
      "ground_truths": [
        "Sub-Saharan Africa"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "bee6bd27c2e644d9b8c491d08fccbb3a",
      "question": "What is the current AIDS and HIV rate?",
      "prediction": "The current AIDS and HIV rate is 11.4 million people in sub-Saharan Africa.",
      "ground_truths": [
        "Nine out of 10 children"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc7bb4c7ce2b4e07a85ed077fea5f775",
      "question": "What is the name of the Pope?",
      "prediction": "The answer is: Pope Benedict XVI.",
      "ground_truths": [
        "Benedict XVI"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "cd7d9d4801f34e1495ceb2575429fd7d",
      "question": "Where did he arrive?",
      "prediction": "The answer is Cameroon.",
      "ground_truths": [
        "Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5502926815147f38b7ceb7dd0ca039a",
      "question": "How many extra people has Florida asked for Federal funding to house",
      "prediction": "The answer is:",
      "ground_truths": [
        "10,000 refugees,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0327e130d14548a08ba2764687c1743a",
      "question": "Are Haitians trying to cross into the U.S.?",
      "prediction": "The answer is yes, Haitians are trying to cross into the U.S.",
      "ground_truths": [
        "attempting illegal crossings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81cc91fcae584458a6c7e62c4261a9a6",
      "question": "What is the maritine traffic around Haiti like according to the USCG",
      "prediction": "The answer is: USCG's reports on the USCG's communication with Haitians regarding the influx of refugees.",
      "ground_truths": [
        "normal maritime"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a3512558e3a4fcc9cb10e2e76802eee",
      "question": "Where are Coast Guard?",
      "prediction": "The answer is Florida. The Coast Guard's Florida Emergency Operations Center (EDOC) is the official center for maritime traffic. They are stationed in Haiti, and they are also working on sending a message to people who would be able to find food and aid.",
      "ground_truths": [
        "Haiti,"
      ],
      "em": 0,
      "f1": 0.04878048780487806
    },
    {
      "id": "fb4f693849d444c999342dfa7bc25764",
      "question": "Has the coast guard intercepted Haitians trying to cross illegally into US waters?",
      "prediction": "The answer is: No, the Coast Guard did not have any reports of Haitians attempting to cross into the U.S. waters.",
      "ground_truths": [
        "not"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "5017529885c2433ba38ad514834d5adc",
      "question": "Since when did she have had clearance?",
      "prediction": "The answer directly from the context provides the answer: \"Our hope is she should be able to smile again,\" meaning that the transplant recipient is determined to regain her ability to smile and maintain her functional abilities.",
      "ground_truths": [
        "2004."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4bec4c857534e4a8a100e3c52e6e040",
      "question": "who led face transplant",
      "prediction": "The answer is: **Dr. Eric Kodish, professor and chairman of the Cleveland Clinic's Department of Bioethics.**",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "29e2b15ab5c243208dcf0cacd969290f",
      "question": "Who led the face transplant team?",
      "prediction": "The answer is: The Cleveland Clinic's face transplant team.",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2944a1f990b4c85ac818037f926742b",
      "question": "Who has had clearance to perform full facial transplant since 2004?",
      "prediction": "The answer is: The recipient has had a full facial transplant since 2004.",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a93ae6ded2e4413baabb2502e0218a35",
      "question": "What kind of team did she lead?",
      "prediction": "The answer is: Dr. Eric Kodish, the professor and chairman of the Cleveland Clinic's Department of Bioethics.",
      "ground_truths": [
        "plastic surgery"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cd81079cfdd843c2b3bec6ee865747f9",
      "question": "What was reportedly transplanted?",
      "prediction": "The answer is tissue from the donor's face, shaped and fitted into position, and connected to the neural pathways in the recipient's brain.",
      "ground_truths": [
        "nose, cheeks, upper jaw and facial tissue from a female"
      ],
      "em": 0,
      "f1": 0.21428571428571427
    },
    {
      "id": "1cdabac233b94436b51ba5e1cee7affd",
      "question": "How many percent of trauma patient's face was reportedly transplanted?",
      "prediction": "The answer to the question is: **20%.**",
      "ground_truths": [
        "80"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b27ebf1694c348eea8d7f2b9cb5dfbf1",
      "question": "how much was transplanted",
      "prediction": "The answer directly translates to:",
      "ground_truths": [
        "80 percent of a woman's face"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e0e203984c14621a53fabe63c820df8",
      "question": "who got clearance",
      "prediction": "The answer is: **Dr. Eric Kodish, the professor and chairman of the Cleveland Clinic‚Äôs Department of Bioethics,**",
      "ground_truths": [
        "Siemionow"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73134003873744b3a6e20f3620eed515",
      "question": "What was Miami's reputation in the 80's?",
      "prediction": "The answer is: Miami was not a truly \"new\" city as the 1980s show showed. It was a tourist destination, almost from its inception.",
      "ground_truths": [
        "for glamour and hedonism came with a side order of carjackings and gangland shootouts."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fb53cfd5f7d4d2c8fec50ffecd35a06",
      "question": "What reputation did Miami shed?",
      "prediction": "The reputation of Miami's TV series, \"Miami Vice\" has reduced to something more substantial.",
      "ground_truths": [
        "much of its unwanted baggage from the 80s"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "a01a2fedcb1b4e23889682462a6780d3",
      "question": "What reputation has Miami shed?",
      "prediction": "The answer is \"not just because Don Johnson's white Armani suits and slip-on shoes belong to a mercifully bygone era.\"",
      "ground_truths": [
        "glamour and hedonism"
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "d113fa2d3c2345558c1c57e8956bf36e",
      "question": "What is a part of modern Miami?",
      "prediction": "The answer is \"Miami is a city that has become a more substantial, more sophisticated destination, a city that is not just associated with the 80s' excesses. With the 1980s' success in carjackings and gangland shootouts, Miami has become a more substantive town with a reputation for glamour, carjackings, and a dedication to the American Dream. It's not just a beach paradise; Miami has become a more substantial destination, not just for holiday makers, but for those seeking a taste of America's luxury. It's a destination known for its \"cocaine cowboy\"",
      "ground_truths": [
        "food, music, culture and language of Latin America"
      ],
      "em": 0,
      "f1": 0.047058823529411764
    },
    {
      "id": "9c3707a187594231945d865cbffb4fa8",
      "question": "How many of the victims were prostitutes?",
      "prediction": "The answer is: 3.",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962be7c101cc435eb0f1851dd0e73f07",
      "question": "How long has the suspect has avoided arrest ?",
      "prediction": "The answer is \"In the last two years.\"",
      "ground_truths": [
        "23 years."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "3b9a8e9ebfeb4a76931188f5fd249cc2",
      "question": "What police chief said,Investigation will require tips from the community ?",
      "prediction": "The police chief's response to the information provided is:",
      "ground_truths": [
        "Edward Flynn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55268af5e30a4c1e87552207e7005b73",
      "question": "Is the suspect in the DNA database?",
      "prediction": "The answer is yes, the suspect is in the DNA database.",
      "ground_truths": [
        "database,\""
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "e0eb1f767c8a4ed094393acc6901f990",
      "question": "how many people did he kill?",
      "prediction": "The question asks for the number of people who died.",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd70bd2ed231441798e1118384f67db2",
      "question": "Who matched  unknown man's DNA with seven slain women ?",
      "prediction": "The answer is \"Six of the victims were women who were working as prostitutes, high-risk behavior making them vulnerable to crime.\"",
      "ground_truths": [
        "Law enforcement officials in the Wisconsin city"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cb379cc531f4a468e254cadcb629ba5",
      "question": "what is the victims name?",
      "prediction": "The victims were Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.",
      "ground_truths": [
        "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior."
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "45769852c71b4d0498f41be9a1d7eb62",
      "question": "How many women were slain?",
      "prediction": "The answer is 11.",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55bc45f2e63149869e4d0e52da6cb421",
      "question": "who designed the vessel",
      "prediction": "The answer is: The designers designed Wally Island, a super-yacht yacht that has the comfort of home and the freedom of living alone.",
      "ground_truths": [
        "super-yacht designers Wally"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "f61206d61cf94401b8fbb566b0b67ec7",
      "question": "Who designed the 99-meter vessel?",
      "prediction": "The answer is Wally Island.",
      "ground_truths": [
        "super-yacht designers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb0bceec8bca491db5c90c3962960eea",
      "question": "what is still in the design stage",
      "prediction": "The answer is: The design stage is not yet fully completed, and the company has not been able to sell the concept to a buyer.",
      "ground_truths": [
        "Wally Island:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e23472157c0d4ebe81a0fa5c027b3359",
      "question": "What can the deck be converted into?",
      "prediction": "The deck can be converted into a private suite and six more double-king sized suites. The interior spaces are pushed towards the aft of the vessel, with the addition of a separate, front-of-the-oven suite and six further double-king sized suites. The deck is also intended for entertaining guests.",
      "ground_truths": [
        "a full garden and pool, a tennis court, or several heli-pads."
      ],
      "em": 0,
      "f1": 0.04081632653061225
    },
    {
      "id": "0af0869096df4dff8325c3f2ff584087",
      "question": "What does the deck contain?",
      "prediction": "The deck contains",
      "ground_truths": [
        "full garden and pool, a tennis court,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c754c326486641409687edae575f81ee",
      "question": "How long is the yacht?",
      "prediction": "The answer is, \"The yacht, the designers said, could make life just like at home on a personal estate for its owner.\"",
      "ground_truths": [
        "100 meter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a4e339480574584b91ba856c053fced",
      "question": "what told german chancellor",
      "prediction": "The answer is, \"The Holocaust fills us with shame,\" and \"I bow my head before the survivors and I bow my head before you in tribute to the fact that you were able to survive.\"",
      "ground_truths": [
        "Israel of Germany's \"Holocaust shame,\""
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "5ca535efd2994e87a2a3d6e63d00b1f1",
      "question": "who is angela merkel",
      "prediction": "According to the context, the answer is: **Germany and Israel are linked \"in a very special way\" by the memory of the Holocaust, in which 6 million Jews were killed under Nazi Germany and its allies in World War II. A German Chancellor has ever made a statement to the Knesset.**",
      "ground_truths": [
        "German Chancellor"
      ],
      "em": 0,
      "f1": 0.08888888888888888
    },
    {
      "id": "3cc3e6da95314c41b64f84c32602f347",
      "question": "Merkel said she supports what solution to the Mideast conflict?",
      "prediction": "The answer is: Merkel stated she supports the two-state solution to the Mideast conflict.",
      "ground_truths": [
        "further sanctions on Iran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3349542eaf3543508ecc4b365b866099",
      "question": "Who told the Knesset of Germany's \"Holocaust shame\"?",
      "prediction": "The answer is: **Germany.**",
      "ground_truths": [
        "German Chancellor Angela Merkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4066ef1e0dba4abd92bf1b2a98754d63",
      "question": "Angela called on Iran to prove what?",
      "prediction": "From the context, the answer is: \"It is not the world that has to prove that Iran is building a bomb,\" but it is more of a note about the importance of the problem to the world and how Germany will defend its own.",
      "ground_truths": [
        "that it does not want the nuclear bomb.\""
      ],
      "em": 0,
      "f1": 0.18604651162790697
    },
    {
      "id": "aaf15d0aa8364c8ebbce91a2d699ef08",
      "question": "Who became the first German chancellor to address Israeli parliament?",
      "prediction": "The answer is Germany.  The German Chancellor, Angela Merkel, stated that the Holocaust is a shame that the Jews were killed under Nazi Germany, and she expressed her commitment to Israel's defense.  She also spoke of Iran's nuclear program as a major danger to the world.",
      "ground_truths": [
        "Angela Merkel"
      ],
      "em": 0,
      "f1": 0.09756097560975609
    },
    {
      "id": "1af22045cbd249e99d4961fe2ec40c6b",
      "question": "Merkel said she supports what in the Middle East conflict?",
      "prediction": "According to the text, Merkel's answer to the Knesset's question about Germany's support for Israel's Jewish state during an unprecedented speech to the Knesset on Tuesday, is:",
      "ground_truths": [
        "the two-state solution to the Mideast"
      ],
      "em": 0,
      "f1": 0.07407407407407408
    },
    {
      "id": "21e3cc9a483347919e816f0bacf6aec1",
      "question": "what merkel said",
      "prediction": "The answer is: Merkel did not say that she would support the Jewish state during the Holocaust.",
      "ground_truths": [
        "\"The Holocaust fills us with shame,\""
      ],
      "em": 0,
      "f1": 0.10526315789473682
    },
    {
      "id": "ff093c58236e4045aadf6741032cdbf7",
      "question": "For what reason did police visit the actor's home?",
      "prediction": "The police visited Coleman's home, and the answer is: \"no criminal charges were filed, a police staff statement.",
      "ground_truths": [
        "civil disturbance call,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3464ea46cfa4e228dbc3acecd3c6fb2",
      "question": "Where was Gary Coleman arrested?",
      "prediction": "The answer is \"Utah County Jail,\" but the CNN's article specifically states that Coleman was \"not too happy.\"",
      "ground_truths": [
        "Santaquin City, Utah, home"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "4e4c75ad37f14cdfb607fb967fdafd7e",
      "question": "What did he do to get in jail if no charges were filed?",
      "prediction": "He did not file a criminal charge.",
      "ground_truths": [
        "outstanding"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c5b4234bb04478e8ffc466b63c6c641",
      "question": "Is it a felony or misdemeanour?",
      "prediction": "True. The answer is yes, it is a felony.",
      "ground_truths": [
        "misdemeanor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f29a05ad0c5444b8a6a88ff73dfe84aa",
      "question": "Warrant from what incident?",
      "prediction": "The warrant from which incident was the one given was the arrest for domestic violence in Colorado.",
      "ground_truths": [
        "a domestic violence case,"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "4d31b962e2aa4ff194eb5234b796dff0",
      "question": "What amount bail must Coleman post?",
      "prediction": "The amount of bail must be paid to prevent the arrested criminal offense from being carried out.",
      "ground_truths": [
        "$1,725"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b189954307e7414b9d81ad99c7b730bf",
      "question": "What did India estimate?",
      "prediction": "India estimated 70,000 civilians lost in Sri Lanka's war zone.",
      "ground_truths": [
        "\"Estimates on the number of civilians trapped vary, but 70,000 or so are estimated to be there now."
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "64215d84427e4d709e2914e76df055c5",
      "question": "What do the Tamil Tigers want?",
      "prediction": "The Tamil Tigers are asking for India to release all civilians, and to lay down arms.",
      "ground_truths": [
        "an independent homeland for the country's ethnic"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "8d3d4915ed104234bca2e7e8eb591d6e",
      "question": "What is India urging Tamil Tiger rebels to do?",
      "prediction": "India is urging Tamil Tigers rebels to release all civilians in Sri Lanka's war zone.",
      "ground_truths": [
        "\"release\" civilians,"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "f20bdc9681624087af1dda063b3a6945",
      "question": "Number India estimates are trapped in the region by the conflict?",
      "prediction": "India estimates that 70,000 civilians are trapped in the region.",
      "ground_truths": [
        "70,000"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "f22c94617c404f388ea469654e9a580f",
      "question": "How many are estimated to be trapped?",
      "prediction": "Estimates on the number of civilians trapped vary, but 70,000 or so are estimated to be there now. The LTTE were reportedly using them as human shields.",
      "ground_truths": [
        "vary, but 70,000 or so are"
      ],
      "em": 0,
      "f1": 0.3870967741935484
    },
    {
      "id": "65e2cb72034947f0ad2e4fbae41a103e",
      "question": "What does India urge?",
      "prediction": "India urges Sri Lanka's Tamil rebels to \"release\" civilians, which it said numbered 70,000.",
      "ground_truths": [
        "urged Sri Lanka's Tamil rebels to \"release\" civilians,"
      ],
      "em": 0,
      "f1": 0.6363636363636364
    },
    {
      "id": "14c07142ff80440483ee9fdc0f2d8b06",
      "question": "What is happening to 70,000 people?",
      "prediction": "70,000 people are being evicted from the LTTE's war zone in South India.",
      "ground_truths": [
        "trapped"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4aa0f16d9604c36980b576421ac9df5",
      "question": "Who did India urge to allow civilians to leave?",
      "prediction": "India urged Sri Lanka's Tamil rebels to \"release\" civilians, and said that 70,000 or so are considered \"estimated at 20,000.\"",
      "ground_truths": [
        "Sri Lanka's Tamil rebels"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "dc27edf1253345dabb1ae19ef7d8a829",
      "question": "What other country is communicating with the Tamil Tigers?",
      "prediction": "The answer is: India is ready to help evacuate the Tamil rebels, and External Affairs Minister Pranab Mukherjee said that \"Estimates on the number of civilians trapped vary, but 70,000 or so are estimated to be there now. The LTTE were reportedly using them as human shields,\" he stated.",
      "ground_truths": [
        "India"
      ],
      "em": 0,
      "f1": 0.04347826086956522
    },
    {
      "id": "a77bad2720ec43aba758a5c99624b6c3",
      "question": "Who locked in battle?",
      "prediction": "The answer is \"The LTTE were reportedly using them as human shields.\"",
      "ground_truths": [
        "Government troops and the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac318d10625648128b8eb68051609f14",
      "question": "When did the crash happen",
      "prediction": "The answer is: \"Vonn suffered a pre-Olympic scare after taking a crashing fall in the World Cup giant slalom in Lienz on Monday.  She received lengthy medical treatment on the slope with initial fears that she had broken the arm.  Her arm has been placed in a splint to speed recovery and she was said to have been in considerable pain.  Her car was also damaged.  Her condition was noted as being \"very better than hurting one of my legs,\" and she was quoted on her Web site www.lindseyvonn.com.  Her crash overshadowed a fine victory for Kathrin Hoelzl of",
      "ground_truths": [
        "Monday."
      ],
      "em": 0,
      "f1": 0.02173913043478261
    },
    {
      "id": "4054abd26fcb4958b130fe94dda895ee",
      "question": "Who severely bruised her arm?",
      "prediction": "The answer is **Taina Barioz of France**.",
      "ground_truths": [
        "Lindsey Vonn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eaeb8739f0bc4e37af026660abd61cd1",
      "question": "What is the point lead",
      "prediction": "The answer is: **Vonn's first Olympic medal was in Salt Lake City, Canada, followed by Karin Zettel of Austria, who finished fifth.**",
      "ground_truths": [
        "581"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb2fd89fd658434d97ecf961b12aed59",
      "question": "Who retains the lead?",
      "prediction": "The answer is **Austria**.",
      "ground_truths": [
        "Vonn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9ed3661a6d2945fbb92425ece8bc177e",
      "question": "where this world cup happened",
      "prediction": "The answer is:  Vonn is in Canada.",
      "ground_truths": [
        "Lienz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bc947534038744198b74eab0dc89a6fe",
      "question": "Who is the overall world cup leader?",
      "prediction": "The answer is **Austria**.",
      "ground_truths": [
        "Vonn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90df72b6413b403f9be47b068768d92d",
      "question": "When was the crash",
      "prediction": "The answer is: Michael Schumacher is an American Formula One driver, and he was in intensive care at the AEK military hospital in Budapest after a freak accident in qualifying for Sunday's Hungarian Grand Prix.",
      "ground_truths": [
        "in a freak accident in qualifying for Saturday's Hungarian Grand Prix."
      ],
      "em": 0,
      "f1": 0.4390243902439024
    },
    {
      "id": "dba999234e0a45f5b4c1cf9f5a0d8711",
      "question": "what is massa's condition",
      "prediction": "The answer is:",
      "ground_truths": [
        "has improved in the past 24 hours with doctors taking him out of a medically-induced coma"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa3c208ecd8b4ae9bddfdae7ac243278",
      "question": "Name the short-term replacement for Felipe Massa",
      "prediction": "The answer is:",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5734b464d1a1441bb4fa7071bd511c3f",
      "question": "What champion may return to Formula One",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52055b058df944c1b0d75a699cc5aa20",
      "question": "what is the name of the injured driver?",
      "prediction": "The answer is: Michael Schumacher was an advisor for the Formula One team.",
      "ground_truths": [
        "Felipe Massa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f64d4bcaf1b4c17a25ee8a58fbd3e34",
      "question": "who could replace massa",
      "prediction": "The provided text is a news report from a CNN interview with Schumacher's representative, Sabine Kehm, discussing the possibility of a replacement for Felipe Massa.  Here is the answer, extracted directly from the text:",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "068ff9a7ec8f43959e260f6986ad7059",
      "question": "what league is Shumacher returning to?",
      "prediction": "The answer is:",
      "ground_truths": [
        "Formula"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e305beef690942e68bef0f6d6cca79cd",
      "question": "What did Williams do?",
      "prediction": "The answer is: he lost his wife and, along with his children, died in a car accident.",
      "ground_truths": [
        "fatally shooting a limo driver"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca7e95d6dba546ff8507c9b08363d65e",
      "question": "Williams has been sentenced for how many years?",
      "prediction": "The answer is: Williams has been sentenced to five years in jail and eligible for parole in 18 months.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "4202629286034d6ea50274ca3efdb911",
      "question": "How many years was Williams sentenced for?",
      "prediction": "The answer is 18 years.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f713617b12d4ad3a9a5478775900cbf",
      "question": "How many were legally wrong?",
      "prediction": "The answer is 11. The commentator, Roland S. Martin, concludes by stating that it is the tragic outcome of Jayson Williams's death, and that the people of the United States should consider the following:",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03a537e991514cb98dee6d3aeaf4ba04",
      "question": "What was Williams' crime?",
      "prediction": "The answer is, \"Grief,\" or \"the loss of a player, a marriage, and a wife.\"",
      "ground_truths": [
        "fatally shooting a limo driver"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "848a7c4fbc474aeca0cfe9b00b765f0c",
      "question": "How many men are  legally wrong?",
      "prediction": "The answer is 17. The commentary is a collection of statements and phrases that are intended to make a specific statement about a particular event, such as a falling athlete's apology, a being's confession, or a judgment. The answer is \"17.\"",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e77e1fe488745f8bc371cb83fa07bc4",
      "question": "How many men are morally wrong?",
      "prediction": "The correct answer is 11. Jayson Williams's wife filed for divorce.",
      "ground_truths": [
        "Both"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "48b0344f035e40e793cb0fdf88ae94e1",
      "question": "Who did the U.N. accuse of actions amounting to war crimes?",
      "prediction": "The United Nations did not accuse any of the Israeli military of war crimes.",
      "ground_truths": [
        "Jewish state and the Palestinian Hamas movement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ae2e3779eb6406da8b25d8e62037b5c",
      "question": "Which body has written a report?",
      "prediction": "The answer is: The United Nations.",
      "ground_truths": [
        "commission chaired by South African Judge Richard Goldstone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "987810b275d34e78a80f1bf73e0619c9",
      "question": "What justified military actions in the 2009 Gaza offensive?",
      "prediction": "The provided text does not contain the answer to the question directly. It simply states that \"Israel's 46-page response, written in response to the UN's call for independent inquiries, outlines the steps the nation has taken to investigate its military actions in Gaza.\"",
      "ground_truths": [
        "stopping militant rocket fire into Israel."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e003871051e146b68946ad760e6987c3",
      "question": "When was the Gaza offenswive?",
      "prediction": "The answer is directly from the context provided in the text.",
      "ground_truths": [
        "just over a year ago."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "69edaef722274f1ea6eb6885e57ffd20",
      "question": "What did UN call for both sides?",
      "prediction": "The answer is: The UN called for both Israel and Hamas to initiate independent investigations into the charges of war crimes and possible crimes against humanity during the Gaza operation.",
      "ground_truths": [
        "to conduct independent inquiries to examine charges that"
      ],
      "em": 0,
      "f1": 0.1764705882352941
    },
    {
      "id": "0c64ef2bbeb543bf87efbb0b2617f2b1",
      "question": "How many lashes will the Nigeria footballer receive?",
      "prediction": "The Nigeria footballer will receive 40 lashes.",
      "ground_truths": [
        "40"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "e3fe3ef5daa6485a8a3f91dfccc9d797",
      "question": "What could Stephen Worgu receive?",
      "prediction": "The answer is $20 for the alcohol consumption, $80 for driving under the influence, and $40 for the 40 lashes.",
      "ground_truths": [
        "40 lashes"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "60438978506949e79645c14afd3a21dd",
      "question": "What is illegal in the Muslim north of Sudan?",
      "prediction": "The illegal in the Muslim north of Sudan is Article 78 of the Penal Code. It is not in the semi-autonomous South Sudanese and predominantly non-Muslim South Sudanese.",
      "ground_truths": [
        "drinking alcohol"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "161fd2e6343d49b7a3611066d7a8a805",
      "question": "Where is alcohol illegal?",
      "prediction": "The answer is **Nigeria's semi-autonomous South Sudanese football team, Al-Merreikh**.",
      "ground_truths": [
        "Muslim north of Sudan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f8f3d49dd8845de8307ad2d9fabf269",
      "question": "Who was appealed against punishing handed out to Worgu?",
      "prediction": "The question is, \"Who was appealed against punishing handed out to Worgu?\"",
      "ground_truths": [
        "The forward's lawyer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99429dc4d1044cb49e70ceb6f3914e07",
      "question": "What has the lawyer for Al Merreikh appelaed?",
      "prediction": "The lawyer for Al-Merreikh has appealed against the punishment for the player who had previously admitted to footballing in Nigeria.",
      "ground_truths": [
        "the player"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "f86d2b3d690a47bea80091f88a1758b3",
      "question": "Nigeria footballer Stephen Worgu could receive what?",
      "prediction": "Nigeria footballer Stephen Worgu receives $20 for 40 lashes.",
      "ground_truths": [
        "40 lashes"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "99798459730c4bc988c9f665fa0fd7df",
      "question": "What has the lawyer appealed against?",
      "prediction": "The lawyer has appealed against the 40 lashes for the alcohol consumption and driving under the influence.",
      "ground_truths": [
        "the punishment for the player"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "3e46e0220a18483fb0fd448bcc36815e",
      "question": "What liquid is illegal North of Sudan",
      "prediction": "The liquid is **alcohol**.",
      "ground_truths": [
        "Alcohol"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d37cc022c8344772b7d7261033a37c29",
      "question": "what did the doodle show",
      "prediction": "The doodle showed the horrifying transformation of the giant pumpkins in the heart of the mine, where they were carved with a timelapse video of the roughly eight hours of carving.",
      "ground_truths": [
        "some of the most gigantic pumpkins in the world,"
      ],
      "em": 0,
      "f1": 0.19354838709677416
    },
    {
      "id": "b83b50ae80c3463aa27b05bb0bf4b493",
      "question": "how much did they weigh",
      "prediction": "The answer is: \"To celebrate Halloween this year, the Googlers wanted to capture that fascinating transformation that takes place when carving a pumpkin, instead of picking up a few pumpkins from the grocery store. They did this by working on six giant pumpkins, specially delivered from Half Moon Bay (some weighing over 1,000 pounds), and a timelapse video of their carving.\"",
      "ground_truths": [
        "well over 1,000 pounds)."
      ],
      "em": 0,
      "f1": 0.10169491525423728
    },
    {
      "id": "76f360f25ece4e449c4682de553cee3f",
      "question": "What does the video show the emplees doing?",
      "prediction": "The video shows the teams working on giant pumpkins, specially delivered from Half Moon Bay, in the Mountain View, California, during a Halloween doodle.",
      "ground_truths": [
        "carving a pumpkin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d531ea0078248ebb3dd503bb338071f",
      "question": "What's the doodle of?",
      "prediction": "The doodle of Google's Halloween doodle is a timelapse video of the approximately eight hours of carving a giant pumpkin in a mountainview, California, campus.",
      "ground_truths": [
        "fascinating transformation that takes place when carving a pumpkin."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "319e4521a8454f6a908b91d874ce2e08",
      "question": "What does the video show?",
      "prediction": "The video shows the at the time-lapse video of carving a giant pumpkin in the formation of a mass of vibrant orange, a continuous metamorphosis from blobs of orange to art as night falls.",
      "ground_truths": [
        "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls."
      ],
      "em": 0,
      "f1": 0.5217391304347826
    },
    {
      "id": "95a04d9fc49441cd901191b2390f05cd",
      "question": "What weight are the pumpkins?",
      "prediction": "The answer is \"six giant pumpkins, specially delivered from Half Moon Bay (some weighing over 1,000 pounds).\"",
      "ground_truths": [
        "(some weighing well over 1,000 pounds)."
      ],
      "em": 0,
      "f1": 0.45454545454545453
    },
    {
      "id": "c2bf5fa95d404050b8f86692d9100be0",
      "question": "What is the weight of some of the pumpkins?",
      "prediction": "The weight of some of the pumpkins in the video is approximately 1,000 pounds.",
      "ground_truths": [
        "over 1,000 pounds)."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "581d677951c7404d922d56e144504b82",
      "question": "What is the theme on Google?",
      "prediction": "The theme on Google's \"doodles\" on its home page is \"Halloween\" in some countries, so the answer is \"Halloween\".",
      "ground_truths": [
        "Halloween"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "89971aa59e4746938e2ceb7b1ee84c0a",
      "question": "Where did the flight originate?",
      "prediction": "The answer is Iranian.",
      "ground_truths": [
        "Tehran, IRNA"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2b2c9929c364bf6b0ed1567c4a73534",
      "question": "in which city fire happened?",
      "prediction": "The answer is: In which city fire happened.",
      "ground_truths": [
        "Mashhad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96cd08874f4d48c09e4ea6c90e16349d",
      "question": "where was the flight coming from",
      "prediction": "The answer is *The Iranian fly-in/out-of-country flight 1625, from Tehran, IRNA reported.",
      "ground_truths": [
        "Tehran,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "0846ed359a9f4778ad82790f63e8de93",
      "question": "how many people killed?",
      "prediction": "The number of people who killed is **20**.",
      "ground_truths": [
        "at least 17"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "453a4ebc18724109b77caf26eb14465b",
      "question": "What skid off the runway?",
      "prediction": "The skid off the runway is identified as a Russian-made Ilyushin plane.",
      "ground_truths": [
        "Aryan Airlines Flight 1625"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e576cca2f10440cadcf63d44340d580",
      "question": "how many passengers on board in flight?",
      "prediction": "17",
      "ground_truths": [
        "150"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ea341dc6868e4322b279688f0eff7ac7",
      "question": "Where did the plane catch on fire?",
      "prediction": "The plane caught on fire in Mashhad, Iran.",
      "ground_truths": [
        "Iranian city of Mashhad"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "8acc6ebd692d495eb4ad37b3045f257f",
      "question": "How many passengers were on board?",
      "prediction": "The answer is 150.",
      "ground_truths": [
        "150"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8f0cfcb0d46d43eeb6c7ee04cd29c027",
      "question": "Number of Blackhawk helicopters that crashed?",
      "prediction": "Two U.S. helicopters, similar to this, crashed in northeastern Baghdad.",
      "ground_truths": [
        "Two"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "f1cd28d437e44ac586e3494371c55ff1",
      "question": "What did the Interior Ministry official say?",
      "prediction": "The Interior Ministry official said that a U.S. helicopter crashed in northeastern Baghdad, resulting in the deaths of two Iraqi soldiers and one Iraqi soldier.",
      "ground_truths": [
        "a U.S. helicopter crashed in northeastern Baghdad as"
      ],
      "em": 0,
      "f1": 0.41379310344827586
    },
    {
      "id": "9d3dd78635084be1bb29a74d6ae963f6",
      "question": "In what city were the helicopters landing?",
      "prediction": "The answer is Iraq.",
      "ground_truths": [
        "Baghdad,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "beefda6210c047899b54a61290a9837c",
      "question": "What kind of helicopters crashed?",
      "prediction": "Two UH-60 Blackhawk helicopters crashed in northern Baghdad, the U.S. military says.",
      "ground_truths": [
        "UH-60 Blackhawk"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "4e8ac0b2d2c448099dea09b0b88032b6",
      "question": "What crashed while landing in Baghdad?",
      "prediction": "Two U.S. helicopters crashed in northern Baghdad, the U.S. military says.",
      "ground_truths": [
        "Two UH-60 Blackhawk helicopters"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "2d006592676144faac44e16423bfb178",
      "question": "What type of helicopters crashed?",
      "prediction": "The answer is \"North Baghdad.\"",
      "ground_truths": [
        "UH-60 Blackhawk"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1028f19198147868af85aa60943519d",
      "question": "Who was among the injured?",
      "prediction": "The answer is: Two Iraqi soldiers.",
      "ground_truths": [
        "Two U.S. troops and two Iraqi soldiers"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "787d235e4c2b4d89b4d9aebff5779cf6",
      "question": "Which country's Interior Ministry was quoted?",
      "prediction": "The answer is \"U.S. military.\"",
      "ground_truths": [
        "Iraqi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "831bd2a4a96f4496947a5656ac12135d",
      "question": "What did the Interior Ministry say?",
      "prediction": "The Interior Ministry said that a U.S. helicopter crashed in northeastern Baghdad, resulting in the deaths of two Iraqi soldiers, one U.S. military spokesman, and two Iraqi soldiers.",
      "ground_truths": [
        "that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen."
      ],
      "em": 0,
      "f1": 0.4761904761904762
    },
    {
      "id": "99a7d09d04634cc08435d1e4bde155b5",
      "question": "Where were the helicopters trying to land in Iraq?",
      "prediction": "The answer is Iraq.",
      "ground_truths": [
        "Baghdad,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d809c82de0cc41a391a8476fbedc4638",
      "question": "How many Iraqi soldiers were killed?",
      "prediction": "Two U.S. troops and two Iraqi soldiers were injured.",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b7ccf71e49054da2a836cbd7dcca51ae",
      "question": "How many people were killed in this incident?",
      "prediction": "Two U.S. troops and two Iraqi soldiers were injured.",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91e7485f2caf4a2fb0099f4c75b5095e",
      "question": "1907 ball was covered with how many bulbs?",
      "prediction": "The answer is 1,415-pound ball.",
      "ground_truths": [
        "100"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1397fb85c744106923a646a76576d60",
      "question": "who many people were expected to attend?",
      "prediction": "According to the context, the answer is: New York (CNN) celebrates the centennial of its most famous New Year's tradition Monday, as organizers of the Times Square ball drop have given the crystal globe an environmental makeover.",
      "ground_truths": [
        "about a million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "125351aec7474e38b01e13323598d810",
      "question": "what kind of power usage is it?",
      "prediction": "The answer is \"heavy-duty, but with a significant increase in energy-efficient light-emitting diodes (LEDs).\"",
      "ground_truths": [
        "energy-efficient light-emitting diodes"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c16cdeb37272460f86fbd392245ce63f",
      "question": "What killed the woman?",
      "prediction": "The answer is: The woman who was killed by an insurgent small arms fire in a compound in Helmand province was killed by an insurgent small arms fire.",
      "ground_truths": [
        "crossfire by insurgent small arms fire,"
      ],
      "em": 0,
      "f1": 0.3448275862068966
    },
    {
      "id": "14d8b36c4c854cee90760516d799d7c3",
      "question": "Where did NATO say 12 militants died?",
      "prediction": "The context provided does not contain the answer to the question, and it is not provided in the context. Therefore, I can only answer by stating that the answer is not explicitly stated in the context.",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "430fcb6dfaaf44b48f79b64f77fa8a09",
      "question": "by whom was woman killed according to nato?",
      "prediction": "The answer is **I offer our heartfelt condolences to the loved ones of the brave soldier, and our support to the Afghan National Security Forces.**",
      "ground_truths": [
        "militants,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f231490345764c8ca9cf74cf86fdfbe5",
      "question": "Who was investigating the woman's death?",
      "prediction": "The answer is **Afghan National Security Forces**.",
      "ground_truths": [
        "Afghan National Security Forces"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "68f96b0c4d124a7e87e12b96e02bed15",
      "question": "Where was the battle?",
      "prediction": "The context provides:",
      "ground_truths": [
        "Now Zad in Helmand province, Afghanistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eac0e28fcd9146678e656bd4a88eda04",
      "question": "What is stretched to the limit?",
      "prediction": "The answer is: \"I will take care of them and won't let anything bad happen to them. ... They're basically family members.\"",
      "ground_truths": [
        "resources"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e3f4fb0782c04e9bb5f1cb09103e9b3c",
      "question": "Which shelter has been killing the horses?",
      "prediction": "The answer is: Lifesavers Wild Horse Rescue.",
      "ground_truths": [
        "Lifesavers Wild"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "09400b686045458e93e51f71357024d3",
      "question": "For what reason are the horses being abandoned?",
      "prediction": "The answer is: \"I will take care of them and won't let anything bad happen to them. They are family members.\"",
      "ground_truths": [
        "\"People have lost their homes, their jobs, their hope,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7f9035673c548bb8854b64481d758c6",
      "question": "What organization ordered warning to be put on certain antibiotics?",
      "prediction": "The answer is: The FDA ordered the makers of Cipro and Avelox to add a \"black box\" warning to alert patients of possible tendon ruptures and tendonitis.",
      "ground_truths": [
        "U.S. Food and Drug Administration"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "4d977e5ba1134167a59c03b6adc78b16",
      "question": "What box will alert patients of possible tendon problems?",
      "prediction": "The answer is \"black box warning,\" and it is not mentioned.  The FDA's (Cipro, Levaquin, Avelox, Noroxin, and Floxin) warning for Cipro and other fluoroquinolones is for doctors to inform patients of possible tendon problems.  This is confirmed by the public citizen's lawsuit.",
      "ground_truths": [
        "\"black box\" label warning"
      ],
      "em": 0,
      "f1": 0.13636363636363635
    },
    {
      "id": "9c961ca099aa4c18a45b492025dc071d",
      "question": "Which drugs are included in the black box labeling?",
      "prediction": "The drug products mentioned in the context are Cipro and Avelox, which are fluoroquinolones, and Levaquin, Avelox, Noroxin, and Floxin.",
      "ground_truths": [
        "include Cipro, Levaquin, Avelox, Noroxin and Floxin."
      ],
      "em": 0,
      "f1": 0.48
    },
    {
      "id": "8639290204ee4b349154594e72774990",
      "question": "What has the FDA ordered?",
      "prediction": "The FDA ordered the makers of Cipro and Avelox to add a \"black box\" warning to the label warning for tendon problems.",
      "ground_truths": [
        "add a \"black box\" label warning"
      ],
      "em": 0,
      "f1": 0.4347826086956522
    },
    {
      "id": "5138cd8c69c74702a6a6bd9f88ab354e",
      "question": "What other drug is relevent besides  Cipro, Levaquin, Avelox and Noroxin?",
      "prediction": "The answer is Cipro, Levaquin, Avelox, and Noroxin.",
      "ground_truths": [
        "Floxin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c7410c8df1f4ca7a61a17d4a9bcda59",
      "question": "What does the \"black box\" label do?",
      "prediction": "The \"black box\" label tells the FDA that the drug's warnings are not explicitly stated in a warning. This information is used to guide patients to alternative treatments before rupture. This is a crucial element in preventing people from causing unnecessary suffering.",
      "ground_truths": [
        "alert patients of possible tendon ruptures and tendonitis."
      ],
      "em": 0,
      "f1": 0.044444444444444446
    },
    {
      "id": "9fac8c8a718346b6b4f1fc4d10a6f2e1",
      "question": "What order is the consumer group happy about?",
      "prediction": "The consumer group's satisfaction with the FDA's order is indicated by the following:",
      "ground_truths": [
        "put the \"black box\" warning on Cipro and other fluoroquinolones,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "922195399a674a88909c8d7d4bb22654",
      "question": "What drugs were included in the FDA warning to put the strongest warning on their labels?",
      "prediction": "The FDA ordered the makers of Cipro and Avelox to add a \"black box\" warning to alert patients of possible tendon ruptures.",
      "ground_truths": [
        "Cipro, Levaquin, Avelox, Noroxin and Floxin."
      ],
      "em": 0,
      "f1": 0.23999999999999996
    },
    {
      "id": "eac66451df5e4241bb6b7f195d5a6cb5",
      "question": "What does the black box label mean?",
      "prediction": "The answer is: \"The black box label warns patients of potential tendon ruptures and tendonitis, and a medication guide for fluoroquinolone drugs, including Cipro and Avelox, Cipro, Levaquin, Avelox, Noroxin, and Floxin,.\"",
      "ground_truths": [
        "to alert patients of possible tendon ruptures and tendonitis."
      ],
      "em": 0,
      "f1": 0.31578947368421056
    },
    {
      "id": "8364ad576679429cae578898bcaa31be",
      "question": "Do groups believe that the FDA has done enough?",
      "prediction": "The answer is yes, the FDA has done enough.",
      "ground_truths": [
        "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\""
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "1cef8c20feb547078f9f6aa19ef33d42",
      "question": "What is the cause of the tendon problems?",
      "prediction": "The answer is \"the FDA will require a \"black box\" label warning for Cipro and fluoroquinolone drugs, including Cipro, Levaquin, Avelox, Noroxin, and Floxin, including the \"black box warning\" on all fluoroquinolones,\" and that the FDA sends a warning letter to physicians clearly describing possible adverse reactions.",
      "ground_truths": [
        "Cipro"
      ],
      "em": 0,
      "f1": 0.047619047619047616
    },
    {
      "id": "843521ec385d4ca797c3a37842424dae",
      "question": "What is the prosecutor accused of?",
      "prediction": "The prosecutor accused of is **Jacob Zuma**.",
      "ground_truths": [
        "buckling under pressure from the ruling party."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c00bac5c39534761b85d1adeea7bdd23",
      "question": "Name the president of the ANC?",
      "prediction": "The answer is South Africa's national party.",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2c278dfb89b444389ac7fa574c14697",
      "question": "What is the name of  the most powerful party in South Africa?",
      "prediction": "The answer is \"Opposition parties are accusing prosecutors of buckling under pressure from the ruling party.\"",
      "ground_truths": [
        "African National Congress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba52569b388b4b339129a0277c7eb2e8",
      "question": "Who is ANC president?",
      "prediction": "The answer is **JOHANNESBURG, South Africa.**",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e53753e6f784486091c8dce069d90994",
      "question": "Zuma is president of what party?",
      "prediction": "The answer is: Zuma is president of the African National Congress.",
      "ground_truths": [
        "African National Congress"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ff383ab32c9942908db5946a8fe654d3",
      "question": "Who denied claims of accepting bribes, money laundering, among others?",
      "prediction": "The answer is **JOHANNESBURG, South Africa. (CNN).**",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc27c4c56b014020bd9e8a799860b06e",
      "question": "What is the name of the president of the ANC?",
      "prediction": "The answer is **Jacob Zuma.",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ebe6da22787d42bab5690387505c2674",
      "question": "When is the vote expected?",
      "prediction": "The question is, when is the vote expected?",
      "ground_truths": [
        "April 22."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a36c55cdd449476f90afb817a588273a",
      "question": "Were there any fatalties reported?",
      "prediction": "The Answer: Yes, there were fatalties reported, and the recall was a voluntary expansion of 41,280 pounds of beef products produced by the U.S. Department of Agriculture (DCA) and the USDA.",
      "ground_truths": [
        "no fatalities had been"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b677213a397416480d12c20369f8da6",
      "question": "Who issued the original recall?",
      "prediction": "The original recall was issued by the Colorado Department of Agriculture (U.S. Department of Agriculture) on Sunday.",
      "ground_truths": [
        "JBS Swift Beef Company, of Greeley, Colorado,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "a5870a48cfa84c7e9cfa52794214ffd1",
      "question": "Where is the company located?",
      "prediction": "The answer is the Greeley, Colorado, plant in the U.S. Department of Agriculture.",
      "ground_truths": [
        "Greeley, Colorado,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "cd259ac2833f49f39de5b1904e515d11",
      "question": "What was the beef affected by?",
      "prediction": "The beef affected by the recall is JBS Swift Beef Company.",
      "ground_truths": [
        "E. coli"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04d4f617361a4901a70eda7bc93936b5",
      "question": "Who is the USDA spokesman?",
      "prediction": "The USDA spokesman is Chandler Keys.",
      "ground_truths": [
        "Brian Mabry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c045bb738b404d33bae09e79d127ac17",
      "question": "What company is involved in this recall?",
      "prediction": "The answer is the Colorado company, JBS Swift Beef Company.",
      "ground_truths": [
        "JBS Swift Beef"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "d66ab87ca38642a8ad7e469f0974d04c",
      "question": "How many pounds of meat were recalled?",
      "prediction": "The answer is: 380,000 pounds.",
      "ground_truths": [
        "41,280"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "586aa6e73def460c94ba612b73b94990",
      "question": "What did the spoke's person say about the recall?",
      "prediction": "The speech said: \"It is important for consumers to note that the recalled product was sold by JBS as whole muscle cuts, not ground beef, and the ground beef that was associated with illness was produced by other companies who do not use antimicrobial dipping steps to reduce the risk of the beef products. This is important for consumers to note that the recalled product was sold by JBS as whole muscle cuts, not ground beef. The recall was a result of an ongoing investigation into 24 illnesses in multiple states, of which at least 18 appear to be associated. The recall was voluntarily expanded to include additional 380,",
      "ground_truths": [
        "his company's products may have had nothing to do with the outbreak."
      ],
      "em": 0,
      "f1": 0.07272727272727274
    },
    {
      "id": "ecd82d14cb3e4e28b8307e44f4784ff5",
      "question": "is Child actor best known for Diff'rent Strokes?",
      "prediction": "Child actor best known for Diff'rent Strokes is Gary Coleman.",
      "ground_truths": [
        "Gary Coleman"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "bfb2c3311c5d45789500d104b3965147",
      "question": "What are the names of the cast members of the TV show \"Diff'rent Strokes\"?",
      "prediction": "The names of the cast members of \"Diff'rent Strokes\" are:",
      "ground_truths": [
        "Gary Coleman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1b8855de6ff4bf2b8afc5942486e88c",
      "question": "Will Hospital give details on his illness?",
      "prediction": "Answer: Hospital will not release any information regarding Coleman's condition.",
      "ground_truths": [
        "would not release any other information."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d7f9ec97994d42c3bf9dfc7e853a3d66",
      "question": "What did hospital say about Gary Coleman's condition?",
      "prediction": "Hospital said that Coleman was being treated in Provo, Utah, after being admitted on Wednesday.",
      "ground_truths": [
        "in critical"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "1854615c2d334e5aa3a608b90f0547b5",
      "question": "What is the name of the hospital where Gary Coleman was admitted?",
      "prediction": "The answer is \"Utah Valley Regional Medical Center.\"",
      "ground_truths": [
        "Utah Valley Regional Medical Center,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "f726d61075424d38b751748553a88ced",
      "question": "who is giving a speech?",
      "prediction": "The answer is:  The e-mail, containing details about Obama's schedule with no mention of the Vice Presidential candidate, contained details about his schedule with no mention of any of the potential vice presidential candidates.",
      "ground_truths": [
        "Democratic VP candidate"
      ],
      "em": 0,
      "f1": 0.0606060606060606
    },
    {
      "id": "aee05b8aa4e046ff8de171c4f0baed09",
      "question": "What did Barack Obama's campaign do?",
      "prediction": "The campaign's answer, based on the context, is:",
      "ground_truths": [
        "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bfd989110c54493bbb9a628ec703df9a",
      "question": "Who is considered top tier contenders?",
      "prediction": "The answer is \"Bayh,\" and he is one of the top contenders. The question asks who will be the Vice Presidential candidate, and he is nominated as the nominee for the 2024 election.",
      "ground_truths": [
        "Biden, Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine"
      ],
      "em": 0,
      "f1": 0.15789473684210525
    },
    {
      "id": "725a2fdd55e44ad28dda4216e620f070",
      "question": "What day does Obama have a big campaign event?",
      "prediction": "The answer is: \"Friday,\" and there is no such clear-cut answer. The e-mail-based text message \"Just kidding\" implies that Obama would choose his vice president.",
      "ground_truths": [
        "Saturday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11caee047c7c458da0dc2cc880574171",
      "question": "who has a big event?",
      "prediction": "The answer is \"With the clock ticking (the Democratic VP candidate delivers a big speech next Wednesday).\"",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "538a1322b4e84194b82f2614c9a2ae02",
      "question": "Who is giving a big speech at the convention?",
      "prediction": "The answer is: Barack Obama.",
      "ground_truths": [
        "Democratic VP candidate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "07f89034983c43de89406bab63985c58",
      "question": "what is the subject of the email?",
      "prediction": "The email contains the following:",
      "ground_truths": [
        "\"Vice presidential"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3f0a9fcbfeb422ca0996a6fc94fc5e4",
      "question": "What will the Democratic VP candidate do next Wednesday?",
      "prediction": "The answer is: \"Dick,\"",
      "ground_truths": [
        "delivers a big speech"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dda0f54af5084a31a2746f72210b0bd4",
      "question": "what was Al-Douri",
      "prediction": "The answer is \"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well, because the main assailant on the nation and on Palestine is the American imperialism.\"",
      "ground_truths": [
        "highest ranking former"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "775eadd02718492c8d6a6c09bf69f073",
      "question": "who was vice chairman?",
      "prediction": "The text provides information about Iraqi (CNN) recordings of a message attributed to Izzat Ibrahim al-Douri, the highest ranking former member of Saddam Hussein's regime.",
      "ground_truths": [
        "Al-Douri,"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "0ccce61a18fa45aebbd6583a6c92052d",
      "question": "how long was the broadcast?",
      "prediction": "The answer to the question is: \"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well, because the main assailant on the nation and on Palestine is the American imperialism.\"",
      "ground_truths": [
        "30-minute"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de7fae20ac624dc9af5949d97f30a79a",
      "question": "What was Al-Douri vice chairman of?",
      "prediction": "The answer is: Al-Douri, the highest ranking former member of Saddam Hussein's regime, is still at large.",
      "ground_truths": [
        "Hussein's Revolutionary Command Council."
      ],
      "em": 0,
      "f1": 0.10526315789473685
    },
    {
      "id": "a0eed9d8178c4928b016bdf106f59e9e",
      "question": "what did the US say",
      "prediction": "The answer is: \"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well, because the main assailant on the nation and on Palestine is the American imperialism.\"",
      "ground_truths": [
        "he has helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002."
      ],
      "em": 0,
      "f1": 0.11764705882352942
    },
    {
      "id": "3e8a38a0b3e64f809e791704ac768b95",
      "question": "what length was the message",
      "prediction": "The answer is directly from the context.",
      "ground_truths": [
        "30-minute recorded"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64ca162a7daf4ca0b747b8b3d75eca51",
      "question": "What is CNN trying to do?",
      "prediction": "CNN is trying to do what it can to provide information and answer questions, and the answer is simply \"just a message.\"",
      "ground_truths": [
        "verify the authenticity of the voice on the tape."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "031cc546d2904282a49ea01dcfa886c8",
      "question": "who does the voice belong to?",
      "prediction": "The answer is \"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well, because the main assailant on the nation and on Palestine is the American imperialism.\"",
      "ground_truths": [
        "Izzat Ibrahim al-Douri,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c69f8eda4633488393fff0fe8e8deb1f",
      "question": "What does the U.S. claim Al-Douri has done?",
      "prediction": "The answer is: \"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is also your victory, because the main assailant on the nation and on Palestine is the American imperialism.\"",
      "ground_truths": [
        "helped finance the insurgency against"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2f7ce0d6e2a4a048f97fb86e8836009",
      "question": "How long has Jacob been the top boy name?",
      "prediction": "Jacob has been the top boy name for 11 years.",
      "ground_truths": [
        "11th year in a row."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d7537366071415381b140fc2d6bd90b",
      "question": "What replaced Emma as the most popular name?",
      "prediction": "The answer is **Cullen**.",
      "ground_truths": [
        "Isabella"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a9c56b5a5ae43b78f7f7d9dccdca491",
      "question": "For how long has Jacob been the most popular boy name?",
      "prediction": "The answer is: Jacob has been the most popular boy name for 11 years.",
      "ground_truths": [
        "11th year in a row."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93447ee40a954604bc8e1c926079b785",
      "question": "Who replaced Emma?",
      "prediction": "The answer is Jacob.",
      "ground_truths": [
        "Isabella"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "671b2208301e460ba1fa588e350b82d6",
      "question": "Which series inspired the popularity of the name Cullen?",
      "prediction": "The series that inspired the popularity of the name Cullen was the \"Twilight\" books.",
      "ground_truths": [
        "\"Twilight\""
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "80a9457351e14111aaf3fb84d3528092",
      "question": "What is the most popular boy name?",
      "prediction": "The most popular boy's name for the 11th year in a row is Cullen.",
      "ground_truths": [
        "Jacob,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67a58f5e2ff7455f8807463f414f9b99",
      "question": "What names are on the way out?",
      "prediction": "The names on the way out are:",
      "ground_truths": [
        "Jonas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c076d8175484c409ffb8ff3dadd914d",
      "question": "Who is sending the marines?",
      "prediction": "The answer is: The president of the Joint Chiefs of Staff, Adm. Mike Mullen.",
      "ground_truths": [
        "President Obama's"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "4443abaa1aaf48d1b44d99d10e0968ec",
      "question": "Where are marines being sent?",
      "prediction": "The answer is \"Our Marine Corps is being sent to Afghanistan's Helmand province,\" with the following note:",
      "ground_truths": [
        "hostile war zones,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "331ff9acbc094ef7b4ecdd751637c564",
      "question": "Where do the marines head to this week?",
      "prediction": "The text is a letter from a North Carolina Marine Corps (CNN) to a new, potentially more inexperienced, base in Afghanistan. The letter expresses excitement about the deployment, highlighting the morale of the Marines, the potential for a surge into the worst fighting since the start of the war in Afghanistan, and the gratitude of the Marines for the deployment. The letter also mentions the challenges of combat deaths, and the Marines' ability to be motivated. Finally, it ends with a reflection on the potential for the Marines to become \"human,\" and the possibility of a strong, experienced team, and a commitment to the mission.",
      "ground_truths": [
        "Afghanistan's restive provinces"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89bec77a5e54425a8bfe2f014139705e",
      "question": "What branch of the military is being sent to Afghanistan?",
      "prediction": "The answer is: \"We're not in an ideal situation with respect to that.\"",
      "ground_truths": [
        "1,500 Marines will be part of the initial wave of President Obama's surge plan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef04d080cbcb459b8803c4c04a7ab199",
      "question": "Who is ecstatic about the sitaution?",
      "prediction": "The answer is: \"I am ecstatic about the situation. I have a good group of Marines behind me, so I am real excited about the deployment.",
      "ground_truths": [
        "Sgt. Jason Bendett"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9ca82e2c48540c0b4c004b86f316109",
      "question": "Who was the part of the initial surge plan?",
      "prediction": "The answer is \"But a dug-in Taliban force,\" from the context.",
      "ground_truths": [
        "1,500 Marines"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3648af18d4b400aaa4924a0eed0f5b4",
      "question": "Who says he is ecstatic about the situation?",
      "prediction": "The answer is: \"I'm absolutely ecstatic about the situation. I've a good group of Marines that are behind me, so I'm real excited about the deployment,\" said Sgt. Jason Bendett of the 3rd Platoon, A Company.",
      "ground_truths": [
        "Sgt. Jason Bendett"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "c1740d78b92a4377bcd6b4846b5b42c2",
      "question": "Who is part of initial wave?",
      "prediction": "The context states that the charge is a \"base accustomed to deploying Marines into some of the most hostile war zones, expecting some hesitancy when units from here were asked to surge into some of the worst fighting since the start of the war in Afghanistan.\"",
      "ground_truths": [
        "1,500 Marines"
      ],
      "em": 0,
      "f1": 0.048780487804878044
    },
    {
      "id": "a58047b20f864cdebdef070c674d4cae",
      "question": "What is the largest age group that gets injured?",
      "prediction": "The largest age group that gets injured is the age of 14 to 17.",
      "ground_truths": [
        "14 to 17."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "af6b6e66c61443b9a54c8008d9c2500f",
      "question": "What age group are often the most injured?",
      "prediction": "The answer is: The CPSC (Consumer Product Safety Commission) states that \"We know that 70 percent of all [fireworks-related] accidents occur between June 20 and July 20.\"",
      "ground_truths": [
        "14 to 17."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4819fd5cd1a64222a14e089b391f2f5e",
      "question": "In what months do the most accidents occur?",
      "prediction": "The most accidents occur between the ages of 14 and 17.",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "97aa5f73225c4f1782a0240cd5fbbb3e",
      "question": "When does the most number of accidents occur?",
      "prediction": "According to the context, the most number of accidents occurs between the ages of 14 to 17.",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "62b2d8702439498197acee357c30a8bd",
      "question": "What needed a warning?",
      "prediction": "The answer is: \"We need a warning to be sure fireworks are safe.\"",
      "ground_truths": [
        "urging families to put safety first when celebrating with fireworks."
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "402f5f93e4a04d998cb41298d57b8115",
      "question": "Who put the warning out on fireworks?",
      "prediction": "The warning was issued to families to put safety first when celebrating fireworks.",
      "ground_truths": [
        "the Consumer Product Safety Commission"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "2bae9bfec5c847b3baa6807ce263570d",
      "question": "When do 70% of fireworks accidents occur?",
      "prediction": "The answer is \"That was the time of year that people are using fireworks.\"",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09553563f3b94bfd88d2887043744b77",
      "question": "What percentage of fireworks accidents occur in June-July?",
      "prediction": "According to the CPSC, there were seven fireworks-related deaths and approximately 7,000 reported injuries in 2008.",
      "ground_truths": [
        "70 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d12c3c9ff7254b76a83a3c702e0f1411",
      "question": "What percentage of fireworks accidents happen between June 20 and July 20?",
      "prediction": "The answer is 7,000.",
      "ground_truths": [
        "70 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "15b1f7554b6b4c8b9c7d341211e79299",
      "question": "Which country is home to the $15 billion Bollywood industry?",
      "prediction": "The answer is \"India.\"",
      "ground_truths": [
        "India"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "bc829fbf47fa4579ab8bd284a198f698",
      "question": "What is the number of Bollywood movies that Kumar has appeared in?",
      "prediction": "The answer is 17.",
      "ground_truths": [
        "more than 80 features to his name,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28b0b2de8b1942a0b148be400c797bfc",
      "question": "Who finds himself at the forefront of where Bollywood meets Hollywood?",
      "prediction": "The answer is \"the global glitterati\" ‚Äì the prominent Hollywood men in Indian action films, often performing in suits.",
      "ground_truths": [
        "Akshay Kumar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf95ec8d543e4fbb858d992b7b40e850",
      "question": "what did kumar teach",
      "prediction": "According to Kumar, \"Kambakkht Ishq\" was filmed in a Hollywood stuntman's suit, and he is set to appear in a film with Snoop Dogg.",
      "ground_truths": [
        "martial arts,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9baaeedf23f47088fe44a56c08433ca",
      "question": "what is the second largest film industry",
      "prediction": "The second largest film industry in India is the...",
      "ground_truths": [
        "Bollywood"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff13ef6297264061a1e92721b068d241",
      "question": "What is the value of India's film industry",
      "prediction": "The answer is \"The value of India's film industry is not measured in money, but in the success and opportunities it provides to its citizens, as determined by the industry's economic growth.",
      "ground_truths": [
        "$15 billion"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8707b5b977a4a75b4052142553f008e",
      "question": "Were were zimbabweans fleeing from?",
      "prediction": "The answer is yes, and it is a clear and concise statement from the South African police chief.",
      "ground_truths": [
        "Johannesburg's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a46f17684194695b99b053c0a8ab490",
      "question": "At least how many were killed?",
      "prediction": "The answer is 22. The context mentions:",
      "ground_truths": [
        "22 people"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "a0d214f6804e4725b7de25e93c53c411",
      "question": "At leasst how many were arrestd?",
      "prediction": "The answer is 22.",
      "ground_truths": [
        "more than 200 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3db1657272374d31897a6e338e408644",
      "question": "Who is South Africa's Archbishop?",
      "prediction": "Answer: Archbishop of Cape Town, South Africa",
      "ground_truths": [
        "Desmond Tutu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95b699a446f346298c2fae56ed9e0fc9",
      "question": "how many people were arrested",
      "prediction": "The answer is 22.",
      "ground_truths": [
        "more than 200"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3fef6306e73548568dc9ea2747801b4b",
      "question": "How many did police arrest?",
      "prediction": "Answer: 22",
      "ground_truths": [
        "more than 200 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0fee8c420cf040f08425629d25214152",
      "question": "who condemned the attack",
      "prediction": "The answer is: South Africa's police chief, Achmat Dangor.",
      "ground_truths": [
        "The Nelson Mandela Foundation issued a statement condemning the \"senseless violence\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "baace490a2214705a98eb69dfe65893c",
      "question": "how many people died",
      "prediction": "The answer is 22.",
      "ground_truths": [
        "22"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2624dd60e3e94efca3172a09af392342",
      "question": "What is the senator doing about it?",
      "prediction": "The senator is doing the following:",
      "ground_truths": [
        "introduce legislation Thursday to improve the military's suicide-prevention programs."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb87b0e309514c2a9999aa0b3884248e",
      "question": "What is the Psychiatrist name?",
      "prediction": "The psychiatrist's name is Col. Elspeth Cameron-Ritchie.",
      "ground_truths": [
        "Col. Elspeth Cameron-Ritchie,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d092853597ce419a8c8b0374b3b0aa38",
      "question": "What type of care is currently offered?",
      "prediction": "The answer is \"a real person to talk to.\"",
      "ground_truths": [
        "an 800 number to call for help."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "330b66b7ec924ca2b07fe88a5b775ca5",
      "question": "What treatment does soldiers need?",
      "prediction": "The answer is: **psychiatry**",
      "ground_truths": [
        "mental health"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67a6c59fe0504df68a7509a1c1d0c4cc",
      "question": "What was Michael Phelps seen doing in NY?",
      "prediction": "The text tells of the mother's thoughts and feelings about her son's partying. She says he had a good time drinking whiskey, and she learned to be more mindful of alcohol, recalling a witness's comment, \"when the DJ started playing Mia's 'Paper Planes,' he got up started dancing like a loon and kept on yelling, 'shots.' Phelps also mentioned getting a bottle of Vodka. The author notes that the photo with the bong is \"tough for a mother,\" and she considers it a valuable, \"visual representation of life's complexities.\" She concludes by stating that she supports all three of her children",
      "ground_truths": [
        "partying"
      ],
      "em": 0,
      "f1": 0.02222222222222222
    },
    {
      "id": "7f63ffe42abf4714a5fcd629bc263216",
      "question": "What did Debbie Phelps say?",
      "prediction": "The answer is: \"I learn at a very early age, I don't get caught up in gossip columns. I know my son's values, and that's what I think about that. I don't get caught up in gossip columns. I know my son's values, and that's what I think about that. I think that's what I feel about the world, it's difficult to talk about it, but I know it is. I do so with my son. And I don't get caught by it, and I do so with my judgment.\"",
      "ground_truths": [
        "tells Larry King her son has strong values."
      ],
      "em": 0,
      "f1": 0.04210526315789474
    },
    {
      "id": "bfa3a133f4fc48eda18b7464dddc5e70",
      "question": "What did commuters express?",
      "prediction": "The answer is: \"For the life of me I can't believe the TWU walked away from that offer,\"",
      "ground_truths": [
        "\"They should be grateful for good jobs"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "18fa422e1baf476fbce6d230e0858000",
      "question": "What can't the transit authority and union agree on?",
      "prediction": "The answer is \"I can't agree on what the transit authority can do.\"",
      "ground_truths": [
        "disagreements"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7374f521a2854922b8f6f8890ecda23d",
      "question": "What does the strike affect?",
      "prediction": "The answer is:  \"For the life of me I can't believe the TWU walked away from that offer,\" and \"Local 234 is standing up for the union members.\"",
      "ground_truths": [
        "buses, subways and trolleys"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "055162a66045404b8d06e59773ef2e49",
      "question": "who walked out",
      "prediction": "The answer is \"sly Wagner\", who came out of the strike. He stated that he was shocked to find out the strike had started, and he told CNN affiliate WPVI-TV that he and his colleagues needed to look around for information, and he did so when he went to the station.",
      "ground_truths": [
        "Transport Workers Union leaders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8c5e2893db634635891ae6afd30261c4",
      "question": "How many people could they affect with this protest?",
      "prediction": "The answer is: Yes, many people could affect the question. The context states that \"All buses, subways, and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m. on Tuesday.\"",
      "ground_truths": [
        "almost a million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "82f2a270b951424e82df7540c14072d3",
      "question": "What forced the workers to walk out and protest?",
      "prediction": "The answer is:  \"For the life of me I can't believe the TWU walked away from that offer,\"",
      "ground_truths": [
        "a shortfall in their pension fund"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e91faab29ff8471fa9aa9b64c8165fad",
      "question": "The dad asked what of the judge?",
      "prediction": "The judge's question is:",
      "ground_truths": [
        "to stop Noriko Savoie from being able to travel to Japan for summer vacation."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1aa6667afb814849b2341323adacad27",
      "question": "who are now in jail in japan?",
      "prediction": "The answer is \"I am in Japan with the children. Christopher Savoie sits in a Japanese jail, accused of trying to kidnap them.",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "21fc57f2c1204f899cdc9311c7f65211",
      "question": "For what reason is Christopher Savoie now in a Japan jail?",
      "prediction": "The question asks for the reason for Christopher Savoie's current location in Japan. The answer is, \"I have never thought about taking children away from their father, never,\" according to court documents.",
      "ground_truths": [
        "Christopher Savoie is in jail in Japan after trying to get back his son, Isaac, and daughter, Rebecca."
      ],
      "em": 0,
      "f1": 0.21276595744680854
    },
    {
      "id": "19174950ad6b4759929f41cb2aa6b979",
      "question": "What has been under British rule since 1833?",
      "prediction": "Argentina has under British rule since 1833.",
      "ground_truths": [
        "The Falklands,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5d1601dece84ce895260d42931118c3",
      "question": "What are the tensions over?",
      "prediction": "The tensions over the Falkland Islands, Argentina's northern coast, are primarily caused by British oil companies' efforts to drill off the northern coast of the islands, which have been under British rule since 1833.",
      "ground_truths": [
        "territory"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72a4722fda5f436e81437aea9042aaa8",
      "question": "What did Argentina claim?",
      "prediction": "Argentina claims that all ships navigating from Argentina to the Falklands to carry a government permit.",
      "ground_truths": [
        "sovereignty over them."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca61917ae999408c84f855c26dc48c68",
      "question": "Which country requires ships to carry a permit?",
      "prediction": "The country requiring ships to carry a permit for the Falklands.",
      "ground_truths": [
        "Argentina"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ac7d29accd74afcbb72758df68371ab",
      "question": "Mourning of what?",
      "prediction": "The question asks for the answer to the context.",
      "ground_truths": [
        "Charles Jubert,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f387e74cf245420383e2a30d4fc287e4",
      "question": "Who have celebrated Carnival through dictatorships, military coups and bloodshed?",
      "prediction": "The answer is, \"Musicians fell silent, seamstresses stopped sewing costumes and ghostly skeletons of unfinished floats lay scattered on the outskirts of Port-au-Prince, with a month after the devastating January 12 earthquake.\"",
      "ground_truths": [
        "Haitians"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74893a21959544df8340dd43d3803ed5",
      "question": "Less than a week ahead of Haiti's Carnival celebration, what was replaced with mourning?",
      "prediction": "The answer is, \"we are living in a city that's like a cemetery.\"",
      "ground_truths": [
        "revelry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16d9f9f1862c42aeb427231e3a5ba863",
      "question": "How many days is the upcoming festivals?",
      "prediction": "The answer directly and concisely reflects the context. It is based on the carnival's decline and the memories of the victims' lives. The answer doesn't specify how many days of festivities are expected.",
      "ground_truths": [
        "three-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5ac07aac0de34bf88eec492a2a3fd199",
      "question": "Who scored twice in serie a?",
      "prediction": "The answer is: Kaka scored twice in his first and second goals for AC Milan.",
      "ground_truths": [
        "Kaka"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "bc65525f5d3e4e24a5e44c7043a0f580",
      "question": "Who scored twice?",
      "prediction": "The answer is: **Kaka, who played his first match since the collapse of a proposed world record transfer to Manchester City.**",
      "ground_truths": [
        "Kaka"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "3423cd2fdc574e95beaedc8b92873a08",
      "question": "Who hits first goal?",
      "prediction": "The answer is: **Kaka scored twice for AC Milan in a 4-1 win**.",
      "ground_truths": [
        "David Beckham"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22e9fd9280544f50a52d79f5662e016c",
      "question": "What did beckham do this week?",
      "prediction": "The answer is: Kaka's first goal for AC Milan.",
      "ground_truths": [
        "claimed his first goal in Italian football."
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "c33dc18ec5144e5987f9bbff8ccb49e2",
      "question": "When did the goals come?",
      "prediction": "The answer is: Kaka's first goal was scored in a penalty at 17 minutes after a 17-minute substitute's penalty at a time.",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5746b6864dc24560aff89223011a242b",
      "question": "Who is the Brazil star?",
      "prediction": "The answer is: Kaka, a Brazilian star who scored twice for AC Milan in a 4-1 series win.",
      "ground_truths": [
        "Kaka,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "acb6c90d9a64483e889a7752bd9f219c",
      "question": "Where are wooden clogs from?",
      "prediction": "The answer to the question \"Where are wooden clogs from\" is typically located in the \"Gamla Vaster neighborhood\" at the shop at the Form Design Center.",
      "ground_truths": [
        "Toffelmakaren."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "307baee708c242ba8ef6e62fbb84c35a",
      "question": "Who is a designer?",
      "prediction": "The answer is \"I am not sure.\"",
      "ground_truths": [
        "Marc Jacobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c15786561763476fbe723ff394d12d0b",
      "question": "What is a local Swedish designer?",
      "prediction": "The answer is:  I'd like to spruce up my home?",
      "ground_truths": [
        "Marc Jacobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b46daafe80c43b0a2a3c953a3a7cd5a",
      "question": "What are traditional Swedish shoes called?",
      "prediction": "The answer to the question is \"feminine footwear\" at Scardia.",
      "ground_truths": [
        "clogs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "916043a8be5f47ff886d8a93e751a204",
      "question": "Where should you go for fashion forward apparel?",
      "prediction": "The answer directly from the context is:",
      "ground_truths": [
        "Kit of Elsinore"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3facff8b0e6e4f36a362e784462b4595",
      "question": "What make great souvenirs?",
      "prediction": "The answer is **\"Scandinavism\"**.",
      "ground_truths": [
        "hand-painted Swedish wooden clogs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3d23205e1ff4203820bddb1cef6c47d",
      "question": "Where should you shop?",
      "prediction": "The answer is at the Form Design Center, in Hedmanska Garden, located in the picturesque Gamla Vaster neighborhood, in Milan.",
      "ground_truths": [
        "Form Design Center."
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "34103331d9c148c080d63622ec1662d9",
      "question": "What's a good place for eco friendly shopping?",
      "prediction": "The answer is *At the Form Design Center in Hedmanska Garden, South Bohemia, Sweden, and located in the lively Gamla Vaster neighborhood of Malmo.",
      "ground_truths": [
        "Drottningtorget"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "48ff8745a7404de7a2a37e7a75dbf6ae",
      "question": "who is without power",
      "prediction": "Answer: The Red River at Fargo, North Dakota, reached \"major flood stage\" early Wednesday as the National Guard and a small army of volunteers filled sandbags to keep the waters at bay.",
      "ground_truths": [
        "About 125,000 customers in New York, New Jersey and Connecticut"
      ],
      "em": 0,
      "f1": 0.05263157894736841
    },
    {
      "id": "291f806867f243fda01f7b182dbea6fc",
      "question": "what river is rising",
      "prediction": "The answer is \"Central Minnesota River.\"",
      "ground_truths": [
        "Red"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86cd984d3e61489e96d4c82e8469c5b5",
      "question": "What work are making about 300 workers?",
      "prediction": "Answer: The Red River at Fargo, North Dakota, reached \"major flood stage\" early Wednesday as the National Guard and a small army of volunteers filled sandbags to keep the waters at bay.",
      "ground_truths": [
        "helping on the sandbag lines"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e902ca034f7c443bb88cad1251b45b91",
      "question": "what River rises 12 feet above flood stage at Fargo?",
      "prediction": "The Red River at Fargo, North Dakota, reached \"major flood stage\" early Wednesday as the National Guard and a small army of volunteers filled sandbags to keep the waters at bay.",
      "ground_truths": [
        "Red"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "2d9aed3487ca4525a3ce8631ef89a750",
      "question": "Who won Bahrain's first-ever medal?",
      "prediction": "The answer is **Germany**.",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9824c871bc92468d964780dd27c6a665",
      "question": "who claimed gold in men's 1,500m?",
      "prediction": "The answer is: **Germany.**",
      "ground_truths": [
        "Yusuf Saad Kamel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d28acffe4fbd4653a8fcdf35eafb77ba",
      "question": "Who did the IAAF ask to undergo a gender test?",
      "prediction": "The IAAF asked the IAAF to undergo a gender test on the athlete.",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8090fcada6904680acfba927c67cd2e0",
      "question": "What event did Caster Semenya win gold in?",
      "prediction": "Caster Semenya won the women's 800 meters gold medal at the World Athletics Championships in Berlin, just hours after the IAAF's call for a gender test.",
      "ground_truths": [
        "women's 800 meters"
      ],
      "em": 0,
      "f1": 0.24000000000000002
    },
    {
      "id": "e4f358d3d4714b30b80d895b0251a95f",
      "question": "What country got the first ever medal?",
      "prediction": "The answer is Germany. The Jamaican sprinter, Robert Harting, won the men's discus gold with a throw of 69.43 meters.",
      "ground_truths": [
        "Bahrain's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f51e0704d694c92b06f362be43f38d3",
      "question": "who asked for gender test?",
      "prediction": "The answer to the question is \"Semenya\" (Caster Semenya).",
      "ground_truths": [
        "IAAF"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccf0a0ada422491d9e6b0daaa45b54d0",
      "question": "Who won a women's gold at World Athletics Championships?",
      "prediction": "The answer is Deresse Mekonnen of Kenya, who won the men's 1,500 meters race.",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce6b747cc3a140f8b3ca1bc9df4eafdc",
      "question": "Where did armed militants attack a mosque during midday prayers?",
      "prediction": "The answer is Rawalpindi.",
      "ground_truths": [
        "Rawalpindi"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a7be659d2b7f470c8ed212a4160145bf",
      "question": "Who is the mosque frequented by?",
      "prediction": "The mosque is frequented by **Four militants**.",
      "ground_truths": [
        "military personnel,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0864b8b2a9fc4087b6c3c2cd0ed5c938",
      "question": "What did a suicide bomber target on November 2?",
      "prediction": "The question asks what the target attacked on November 2.",
      "ground_truths": [
        "a bank"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a23ee41a56174a2eb02865e5ecaa13d3",
      "question": "How many were injured?",
      "prediction": "The answer is: 75.",
      "ground_truths": [
        "75."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7d8839e3e6a14b33ae41ec27e47ed765",
      "question": "Which birthday would Anne Frank be celebrating this week?",
      "prediction": "The answer is: **October 30th**.",
      "ground_truths": [
        "80th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a23f7acba534695acb96f8d150e0f67",
      "question": "What did eva schloss flee from?",
      "prediction": "The answer is, she fled from the Nazi concentration camps in the Netherlands.",
      "ground_truths": [
        "Nazis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f3b062728ba48a78f1f56afeea3fd54",
      "question": "what is one of the worlds most read books?",
      "prediction": "The answer is \"Schloss's diary,\" a book about her childhood friend Anne Frank's story.",
      "ground_truths": [
        "Frank's diary."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "9cbffa1d44044b6885bc0566f5ebfd0f",
      "question": "Where did anne frank die?",
      "prediction": "The answer is, in Amsterdam.",
      "ground_truths": [
        "a Nazi concentration camp,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2deb08987064f74bb987878d41a3346",
      "question": "Which birthday would Anne Frank have celebrated?",
      "prediction": "The answer is **August 11th**.",
      "ground_truths": [
        "80th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8557402c6bda4ed2859de806c39689e9",
      "question": "What was her age when she died at Auschwitz concentration camp in Poland?",
      "prediction": "According to Schloss, her age when she died at Auschwitz concentration camp in Poland was 11 to 12 years old.",
      "ground_truths": [
        "15."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "188ddb201c1e42eb91dab05b6afab94a",
      "question": "Where did Frank die?",
      "prediction": "The final answer is provided in the context, but it is not explicitly stated in the provided text. I have to assume the answer is intended to be a summary of the 80th birthday celebration at Schloss‚Äôs home and the subsequent revelations about her and her family's lives.",
      "ground_truths": [
        "concentration camps,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a0316e8dbe4c496a8dd5d3addc96fbae",
      "question": "what fears swirl around American Airlines?",
      "prediction": "The answer is yes, American Airlines has been losing money, and it's a strong indicator that its financial health is at risk. It's also worth noting that, as the first time in a few years, there was a fair bit of information about American Airlines' claims of bankruptcy. While it's true, the reports on the following week to the last of June, and the fact that it had more than $5 billion in the bank, and the possibility of it filing for bankruptcy, strongly suggest it's a last-dime, strategic situation. It's worth stressing about, as airlines are generally not liable",
      "ground_truths": [
        "bankruptcies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f5ec1c574c842cfa194293fecf7ab43",
      "question": "what is american airlines",
      "prediction": "Here's a concise answer, extracted from the context:",
      "ground_truths": [
        "carrier based in Texas."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44eeb9811ef7469aa6de979324bc43e8",
      "question": "What had Jackson said about his father?",
      "prediction": "The answer is \"quite surprising to learn of the request,\" which translates to \"surprisingly to learn of the request,\" and it's a line from a news report about Michael Jackson's death.",
      "ground_truths": [
        "criticized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c258d5feec51488082d163009bd23e15",
      "question": "What has Michael Jackson criticized?",
      "prediction": "The answer is a monthly allowance, under $86,000.",
      "ground_truths": [
        "his father's parenting skills."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "51162157d5a74f8a95994cbcb39d64a3",
      "question": "Who was left out of will?",
      "prediction": "The answer is: **Michael Jackson's mother, Katherine Jackson**.",
      "ground_truths": [
        "Joe Jackson,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "d8e8fac9eb654ea786830a96010c6b5c",
      "question": "Who is getting the bulk of Jackson's estate?",
      "prediction": "The answer is the estate's **mother, Katherine Jackson**, and her three children.",
      "ground_truths": [
        "mother and his three children receive a court-ordered allowance from the"
      ],
      "em": 0,
      "f1": 0.4210526315789474
    },
    {
      "id": "2d8b7ed4836443c0a4227146a31abda2",
      "question": "What does Joe Jackson want?",
      "prediction": "The answer is: **the estate to his mother, Katherine Jackson, his children, and the undisclosed charities**.",
      "ground_truths": [
        "a judge to order the pop star's estate"
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "958824a5f24e4f9c8845cb4615bd2b88",
      "question": "Who gets the bulk of Jackson's estate?",
      "prediction": "Answer: The bulk of Jackson's estate is divided between the Jackson family of Las Vegas, Nevada, and the estate's special administrators, who received a court-ordered allowance of over $86,000 a month, including maintenance and the mansion's owner's home.",
      "ground_truths": [
        "Katherine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc627af063d44b94a5bbcf9c0e0970f9",
      "question": "What does Joe Jackson want the judge to give him?",
      "prediction": "The answer is \"paying him a monthly allowance, according to court documents.",
      "ground_truths": [
        "a monthly allowance,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "6420f1540eeb4dc890dc682714ddb755",
      "question": "Michael Jackson had publicly criticized whos parenting?",
      "prediction": "The answer is:  The family of the 2002 will, which left a vast estate to his mother, Katherine Jackson, and her three children, including the estranged, undisclosed charities.",
      "ground_truths": [
        "his father's"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "fc702af4678a4967bd9d8200f35b5df3",
      "question": "who committed suicide at 27, 15 years ago",
      "prediction": "The answer is: *15 years ago, at a house adjacent to the park, Kurt Cobain's body was discovered by an electrician. The Nirvana bandmates' deep-seated addiction to drugs and cigarettes ultimately led to the demise of Kurt Cobain's former bandmates, Krist Novoselic and Dave Grohl, and the formation of other bands, including Hendrix and Janis Joplin, who were driven by addiction. This resulted in the deaths of the band's widows and their young children in Aberdeen, Washington. The Nirvana's music died, and the remaining money from royalties provided a significant financial windfall to the Aberdeen community,",
      "ground_truths": [
        "Kurt Cobain"
      ],
      "em": 0,
      "f1": 0.023529411764705882
    },
    {
      "id": "3dd8316956464ff9bbe2ba917cbd02ad",
      "question": "Who did Kurt Cobain play for?",
      "prediction": "According to the context, the answer is \"Nirvana band mates,\" with the following details:",
      "ground_truths": [
        "Nirvana"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "ff99affcb32442d08543e10db96a1157",
      "question": "At what age did Kurt Cobain die?",
      "prediction": "The answer is 15 years ago.",
      "ground_truths": [
        "27,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "342bc95b32974fda9e6737cb3f524f59",
      "question": "Which band was Cobain a part of?",
      "prediction": "The answer is \"Nirvana.\"",
      "ground_truths": [
        "Nirvana"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6118310e56f44b68bd3507c69aab4605",
      "question": "Who was Cobain's widow?",
      "prediction": "The answer is, Courtney Love.",
      "ground_truths": [
        "Courtney Love,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "adfc122496824596b75ef249b137e164",
      "question": "who is Cobain's window",
      "prediction": "The answer is: \"At one point I thought, 15 years on, no one would really know who Kurt Cobain was outside of a group of diehard fans,\" said Jeff Burlingame, the biographer of Kurt Cobain.",
      "ground_truths": [
        "Courtney Love,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "14399ecd55f84f16ad4928db308d6e93",
      "question": "When did Kurt Cobain die?",
      "prediction": "The answer is, Kurt Cobain died on Wednesday, January 30, 2016.",
      "ground_truths": [
        "Fifteen years ago Wednesday,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "7629a3dc263e48209e9b18f6c9e16a63",
      "question": "who was the frontman for Nirvana",
      "prediction": "The answer is \"At one point I thought, 15 years on, no one would really know who Kurt Cobain was outside of a group of diehard fans,\" and then the missing money from royalties to the Nirvana's songs.",
      "ground_truths": [
        "Kurt Cobain's"
      ],
      "em": 0,
      "f1": 0.05555555555555555
    },
    {
      "id": "93e125a935e64bd2b88901d36b50a516",
      "question": "what was the verdict",
      "prediction": "The answer is \"not guilty.\"",
      "ground_truths": [
        "of not guilty"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8ccdb4a0320940ea982529d8cb9c97a4",
      "question": "who says \"decent man and devoted husband\"?",
      "prediction": "The answer is \"Decent man and devoted husband.\"",
      "ground_truths": [
        "High Court Judge Justice Davis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "388fb8074ef94516862a0ffc9e46f530",
      "question": "where was the trial",
      "prediction": "The answer is \"We have offered no further evidence and asked the jury to return a simple verdict of not guilty.\"",
      "ground_truths": [
        "Swansea Crown Court,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1585635901ef4403a84ed3252832a1ae",
      "question": "what was he accused of",
      "prediction": "The answer is: \"The UK's Crown Prosecution Service requested that the case against Brian Thomas be dropped due to a \"unique set of circumstances.\"",
      "ground_truths": [
        "murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b60020073ada49a09f687e0788f24fa5",
      "question": "what do prosecutors say",
      "prediction": "The transcript of the London, England (CNN) interview, which includes the arrest of Brian Thomas, his wife, and the Crown Prosecution Service's request for a \"unique set of circumstances\" in the case against him, provides crucial details for the response from the Crown Prosecution Service.",
      "ground_truths": [
        "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\""
      ],
      "em": 0,
      "f1": 0.11267605633802817
    },
    {
      "id": "6e35ee6ee0a64726b3ba3d493c5add87",
      "question": "when were they on vacation",
      "prediction": "The answer is \"we are all deeply emotional.\"",
      "ground_truths": [
        "2008,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "61441c5df37b447a982cabc26fea8eaa",
      "question": "what  Obama offers?",
      "prediction": "The answer is:  \"I will not (or, as far as I can tell) support the American government's efforts to build a \"21st century infrastructure\" by ending the war in Iraq, reducing government waste, and charging polluters for greenhouse gas emissions. This is not a foreign policy. I will not support this.",
      "ground_truths": [
        "education, infrastructure, energy"
      ],
      "em": 0,
      "f1": 0.04081632653061224
    },
    {
      "id": "c6a9da345c8049d1b45204b5d5a958f7",
      "question": "What is being spent on renewable energy?",
      "prediction": "Based on the provided text, the answer is **$10 billion**.",
      "ground_truths": [
        "$150 billion"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "969ebd74ec8745dcaa074576e171b43a",
      "question": "Who offers plan to spend $10B on schools?",
      "prediction": "The answer is \"10 billion over 10 years to build '21st century infrastructure.\"",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db26836b6cf8475e97616c10654e52f0",
      "question": "what McCain's campaign says?",
      "prediction": "The answer to the question is: \"I reject the belief that we should either shrink from the challenge of globalization or fall back on the same tired and failed approaches of the last eight years.\"",
      "ground_truths": [
        "\"To help create jobs in America, we need to lower taxes and open up foreign markets to American goods,\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "24398c1a07234d538f257ff0be6ed411",
      "question": "Who rejects protectionist trade polices?",
      "prediction": "Based on the provided text, the answer is **Congress**.",
      "ground_truths": [
        "Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db9616cbdd2844cf9598a38c4c6fd8f5",
      "question": "What does Obama's plan to spend?",
      "prediction": "The answer is $60 billion.",
      "ground_truths": [
        "$60 billion"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "866d6db39ec6461790477b8dcef42c1e",
      "question": "Who said it could hurt economy?",
      "prediction": "The answer is \"To improve America's competitiveness, the senator said he would spend $10 billion on childhood education, $150 billion over 10 years on developing alternative energy and $60 billion over 10 years to build \"21st century infrastructure.\"",
      "ground_truths": [
        "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our"
      ],
      "em": 0,
      "f1": 0.07017543859649122
    },
    {
      "id": "ca5014ef14df4b26bac5ef092615dcb4",
      "question": "where presidential candidate would spend?",
      "prediction": "The answer is \"America's future,\" and it's directly from the text provided.",
      "ground_truths": [
        "America's infrastructure."
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "391928c43e8948d489166553b139bb4e",
      "question": "Who would spend $150B on renewable energy?",
      "prediction": "The answer is **$10 billion**.",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d3f87c46ec74f8da310cf30f72df338",
      "question": "Where is Mr. Sohail from?",
      "prediction": "The answer is Cooperstown, New York.",
      "ground_truths": [
        "Long Island"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe1a537935c349ff91672468348c55f1",
      "question": "what was wielded?",
      "prediction": "The answer is $40.",
      "ground_truths": [
        "baseball bat"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a0435c9a64724c84a091c7d2fac8aa23",
      "question": "what was offered?",
      "prediction": "The answer is $40.",
      "ground_truths": [
        "$40 and a loaf of bread."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "0727e0bae0224061be5c486adf21e088",
      "question": "who has the gun?",
      "prediction": "The answer is the man who volunteered for the role of a compassionate counselor during a robbery attempt.",
      "ground_truths": [
        "Mohammad Sohail"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0adf2b210210490ba1ee417a6fa58133",
      "question": "What did Sohail give to the theif?",
      "prediction": "Sohail gave $40 to the thief.",
      "ground_truths": [
        "$40 and a loaf of bread."
      ],
      "em": 0,
      "f1": 0.20000000000000004
    },
    {
      "id": "db2c0c695c8b42b8b252793a037be57d",
      "question": "What is the religion of the man?",
      "prediction": "The man's religion is not explicitly stated in the context.",
      "ground_truths": [
        "Muslim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "783ec86fa5ca450ebf038faae46cb32d",
      "question": "What did Sohail do when the man started crying?",
      "prediction": "Sohail did not say \"Hurry up and give me the money, give me the money!\" and \"Hold on,\" he described a surge of compassion and a commitment to a Muslim.",
      "ground_truths": [
        "gave him $40 and a loaf of bread."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "81f733a833b14d018b0b4d4492e92d20",
      "question": "what did vivanco say",
      "prediction": "Here's the answer based on the context:",
      "ground_truths": [
        "conviction of Peru's ex-president is a warning to those who deny human rights."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55f63d74a744491aae187434ac8c4809",
      "question": "did vivanco say anything",
      "prediction": "The answer is: Yes, in the context, the answer is yes.",
      "ground_truths": [
        "conviction of Peru's ex-president is a warning to those who deny human rights."
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "79b62004336a4ee8bc64e1b588407038",
      "question": "what was fujimori convicted for",
      "prediction": "The answer is: Peru's ex-president, Alberto Fujimori, was convicted of egregious human rights abuses, including the massacre of innocent civilians, and sentenced to 25 years in prison.",
      "ground_truths": [
        "death squad killings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b099694c8a4144c28e496f8b27303276",
      "question": "What was Fuijimori conviced of ?",
      "prediction": "Answer: Fuijimori was reportedly encouraged to believe that the conviction of his ex-president was a warning to those who deny human rights.",
      "ground_truths": [
        "authorizing killings and kidnappings by paramilitary death squads."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "31f8a4e5f2d542c9904b80d29a0e9dab",
      "question": "What was Fujimori convicted of?",
      "prediction": "The answer is 25 years in prison.",
      "ground_truths": [
        "for death squad killings carried out during his rule in the 1990s."
      ],
      "em": 0,
      "f1": 0.11764705882352942
    },
    {
      "id": "6f31f62bee784274b1f0f25497d50707",
      "question": "Who is Jos√© Miguel Vivanco?",
      "prediction": "The answer is: Jos√© Miguel Vivanco.",
      "ground_truths": [
        "executive director of the Americas Division of Human Rights Watch,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4931a582bfb048e29b40c732b2e994ce",
      "question": "Where was this case tried?",
      "prediction": "The answer is Peru's Supreme Court.",
      "ground_truths": [
        "The Peruvian Supreme Court"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "80b33fe3a59c4d25a5b6f62ae79f93cb",
      "question": "What is improving?",
      "prediction": "The answer is:  \"Making schools better should be only one part of our national strategy. Life at home has been a relatively neglected topic and needs to come out of the shadows. Especially in churches, neighborhood organizations, and informal social networks, helping parents do their best needs to be as big a priority as achieving excellent schools. This goes beyond public policies, and it is a key to both improving and improving learning outcomes.\"",
      "ground_truths": [
        "Test scores and graduation rates"
      ],
      "em": 0,
      "f1": 0.027027027027027025
    },
    {
      "id": "3c34d01e48c64f5081002fd67a272117",
      "question": "Whose achievement has been improving?",
      "prediction": "The answer to the question is: \"It's a complex issue with many factors, and it's important to consider.\"",
      "ground_truths": [
        "Blacks and Hispanics"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "9c7364bc94374e31a097ba17c443cd18",
      "question": "What will the vitality of the U.S. depend on?",
      "prediction": "The answer to the question is:  The vitality of the U.S. depends on the closing of racial gaps in education and learning, and more reading at home.",
      "ground_truths": [
        "closing these racial gaps."
      ],
      "em": 0,
      "f1": 0.23076923076923075
    },
    {
      "id": "2dae96a089cc424fb9886a641eb50949",
      "question": "Where is the man from?",
      "prediction": "The answer is Colorado.",
      "ground_truths": [
        "Bailey, Colorado,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "9b9f35bf604d432db2dd139fe20302ff",
      "question": "Where do they camp?",
      "prediction": "The answer is Colorado (7.6 unemployment), and North Dakota (4.2 percent unemployment), the nation's lowest unemployment at 4.2 percent.",
      "ground_truths": [
        "\"out in the woods\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3d4871c16764417a6f31fee26aaee78",
      "question": "How many were found dead in the immigration centre?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e4686a8f258b4bd4942f2304ee16c736",
      "question": "what are the suspects name?",
      "prediction": "The answer is:",
      "ground_truths": [
        "Jiverly Wong,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "650f37e79de742ccb8e372671e453720",
      "question": "Who survived the attack?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "Zhanar Tokhtabayeba,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3359a6b06b204873935ee40341d4e42f",
      "question": "How many people survived?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "37"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d0500984e084bf8b7f7aad64ba75019",
      "question": "Who was the suspect?",
      "prediction": "The answer is 911.",
      "ground_truths": [
        "Jiverly Wong,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf43702e012a4da2b2328e06961d7f06",
      "question": "Where did the shooting occur?",
      "prediction": "The answer is: C. Binghamton, New York, USA.",
      "ground_truths": [
        "Binghamton, New York,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "59c97e698fe0492890ba2dec20b8ff97",
      "question": "What date is the concert now?",
      "prediction": "The concert is now March 17, 2024.",
      "ground_truths": [
        "June 17"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "74953752801446a2abe6b330ce63c179",
      "question": "What show dates have been cancled?",
      "prediction": "Houston will miss Tuesday's show in Paris.",
      "ground_truths": [
        "first four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9723fcbfda3b41f1a2928a72b5614b05",
      "question": "What caused the postponement of the shows?",
      "prediction": "The postponement of the European leg of Whitney Houston's \"Nothing But Love\" comeback tour was caused by the doctors' recommendation to allow her to perform her first three UK shows.",
      "ground_truths": [
        "Illness"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c60d0e46b4a49068acd95570c520c6f",
      "question": "What's she suffering from?",
      "prediction": "Whitney Houston is suffering from an upper respiratory infection.",
      "ground_truths": [
        "upper respiratory infection,\""
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "ed981c4aeb414002ad995530646965da",
      "question": "Which cities are affected?",
      "prediction": "The cities affected by the illness are:",
      "ground_truths": [
        "Manchester,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4b8fa26b7e741fd8b4b8e4120d39d41",
      "question": "What is Houston's tour named?",
      "prediction": "Houston's tour is named Nothing But Love.",
      "ground_truths": [
        "\"Nothing But Love\""
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "4e6de19d338a44b6bd4bbaec2c3c0635",
      "question": "What is Houston suffering from?",
      "prediction": "Houston is suffering from an upper respiratory infection.",
      "ground_truths": [
        "upper respiratory infection,\""
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "8de2ee8d56464a9d971fdae65ae0f72b",
      "question": "What concert will now be held on May 1?",
      "prediction": "The answer is Wednesday. Houston will not be attending the first four shows of the European leg of \"Nothing But Love.",
      "ground_truths": [
        "Whitney Houston"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "3d674d0b82684e87bdc226d9d614e6e4",
      "question": "What star is ill?",
      "prediction": "Answer: Houston is ill.",
      "ground_truths": [
        "Whitney Houston"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "aa048de742634fc294e93e2471a6e0bf",
      "question": "What is the name of the exhibit?",
      "prediction": "The answer is: **Body Works.**",
      "ground_truths": [
        "\"Body Works\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ac59acc0c3b84767bf3118ef7a3c8799",
      "question": "how many human bodies were featured?",
      "prediction": "The answer is 200.",
      "ground_truths": [
        "200"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cbe01529cf464019bd71c4bdf75683ad",
      "question": "When was the first UK public autopsy in 170 years filmed?",
      "prediction": "The answer is **1701.**",
      "ground_truths": [
        "2002"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd845ef39f0944719eb67e6b8f95e18a",
      "question": "How many bodies are there?",
      "prediction": "The answer is 200.",
      "ground_truths": [
        "200"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f9e3c2ce88304e44bceda6f6556de1e2",
      "question": "Who performed the first UK public autopsy in 170 years?",
      "prediction": "The answer is **Germany's Cabinet of Ministers**, which used to be the German Ministry of Health.",
      "ground_truths": [
        "Gunther von Hagens'"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e8a99c1df04495f823dbdfb764ffe9a",
      "question": "What does Cycle of Life depict?",
      "prediction": "The answer is: \"To reveal and expose the anatomical structure of the human body and its function, in a unique lesson, and to teach us about the complexity of the human body, its anatomy, and the importance of leading a healthy way of life.\"",
      "ground_truths": [
        "200 human bodies at various"
      ],
      "em": 0,
      "f1": 0.048780487804878044
    },
    {
      "id": "4b545e66deaf45ff9296dd65db911414",
      "question": "Whats the exhibition called?",
      "prediction": "The answer is: \"To reveal and expose the anatomical structure of the human body and its function, in a unique lesson, and to teach us about the complexity of the human body, its anatomy, and the importance of leading a healthy way of life.\"",
      "ground_truths": [
        "\"Body Works\""
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "fa50aa7aa1e34f5e95a89f5182159659",
      "question": "When was the first UK public autopsy?",
      "prediction": "The answer is: German (2002)",
      "ground_truths": [
        "2002 for British broadcaster Channel 4"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "0f374544a6744955b8a0caeae293626f",
      "question": "Who was tortured for days?",
      "prediction": "Based on the context, the answer is Jaime Andrade.",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "fb6294881cc7497cba33ad6b2ef3683f",
      "question": "What do they do to get money",
      "prediction": "The answer is:  ‚ÄúWe‚Äôre taking your husband and SUV. We‚Äôll be watching your house. If you call the police, he's a dead man. And you can ask the police to tell you where Andrade hid his money, and we‚Äôll investigate that.‚Äù",
      "ground_truths": [
        "abducting each other for ransoms or retribution."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cee4a6b50334a829428d9b657afa7cc",
      "question": "who was tortured",
      "prediction": "Based on the context, the answer is Jaime Andrade.",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "dbf2fd52c52b4d22b9e952a72ba2c8fe",
      "question": "Who investigated the kidnappings",
      "prediction": "The answer is: Jaime Andrade was in the closet for three days without food and water.",
      "ground_truths": [
        "police"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "499adaa2b2934b7c83e3e56900f430a2",
      "question": "who was kidnapped because of her neighbour?",
      "prediction": "The answer is Jaime Andrade.",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ced6f2f26e2843dc9824aed928ddd1dc",
      "question": "Who did men kidnap?",
      "prediction": "Based on the provided context, the answer is: Jaime Andrade.",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f542a3f921174d47b54d203adea547da",
      "question": "How long has Barnett played Republicans in practice debates?",
      "prediction": "The answer is: \"I have played the role of Dick Cheney in 2000 and 2004.\"",
      "ground_truths": [
        "eight national presidential"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fec7fec9315345d08442effe788cb11f",
      "question": "Who will make the case aggressively?",
      "prediction": "The answer is: \"I'm going to make the case aggressively,\" or \"I'm going to make the case aggressively.\"",
      "ground_truths": [
        "Robert Barnett"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eab1831fc59f4491acc17e1f38edeff2",
      "question": "Who said he'll make sure to make the cases aggressively?",
      "prediction": "The answer is: \"I make sure to make the cases aggressively,\" in the context of DC's debate preparation.",
      "ground_truths": [
        "Barnett,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a381589e16c4e249160820a41252097",
      "question": "Who has played Republicans?",
      "prediction": "The answer to the question is \"Democratic.\"",
      "ground_truths": [
        "Robert"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f883fc267164b07a2b3ae3225a53376",
      "question": "Who said candidates must make sure they answer town hall questions?",
      "prediction": "The answer is: \"Treated with no kid gloves.\"",
      "ground_truths": [
        "Robert Barnett"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6588a9418fd841a68831126aa265cd81",
      "question": "What party is Barnett associated with?",
      "prediction": "The answer is: **Republican**.",
      "ground_truths": [
        "Democrats"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "643ed515d4554bc59480448e97351311",
      "question": "What has Barnett played?",
      "prediction": "The answer to the question is: **Dick Cheney**.",
      "ground_truths": [
        "the role of George H.W. Bush in practice debates with Geraldine Ferraro in 1984 and with Michael Dukakis in 1988, and practice debated Bill Clinton more than 20 times during the 1992 campaign. He also"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "173c195371c44efa8b41ab43de4cf834",
      "question": "What must candidates answer?",
      "prediction": "The answer to the question is:",
      "ground_truths": [
        "the question,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "64937a18ace249a0aed6fa80d0b779a8",
      "question": "What does Barnett try to prepare candidates for?",
      "prediction": "The answer to the question is, Barnett attempts to prepare candidates for the 2008 nomination for President.",
      "ground_truths": [
        "debate preparation."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47c2581d5c1e449e830ef3754d9e4473",
      "question": "how many finals are in American Idol?",
      "prediction": "The answer is 4.  The context states that \"American Idol\" has 4 finals.",
      "ground_truths": [
        "two contestants."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18ffd76b9b954bfea21323c0568cac34",
      "question": "Where is Adam Lambert from?",
      "prediction": "According to the context, the answer is *Kris Allen*.",
      "ground_truths": [
        "Southern California"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "189225ed625e43a6b8542f237e74aafd",
      "question": "Who developed a glam-rock persona?",
      "prediction": "The answer is \"I've never been to California except for 'American Idol,' and I want to feel an earthquake,\" where the contestants are not matched.",
      "ground_truths": [
        "Adam Lambert"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb30a08750a744b2ad37125b735959c1",
      "question": "Who has guy-next-door appeal?",
      "prediction": "The answer is \"Canny\".",
      "ground_truths": [
        "Kris Allen,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13961f0e464a435bb693eb52253911c7",
      "question": "When is the American Idol results show?",
      "prediction": "The American Idol's results show is on Tuesday at the Nokia Theater in Los Angeles.",
      "ground_truths": [
        "Wednesday night"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b889b1ad2a4144ff80a036267c53b416",
      "question": "Who are the final two contestants?",
      "prediction": "Answer: Allen and Lambert are the final two contestants of \"American Idol.\"",
      "ground_truths": [
        "Adam Lambert and Kris Allen,"
      ],
      "em": 0,
      "f1": 0.37499999999999994
    },
    {
      "id": "93af1a999ad0480592200969e79ad8bf",
      "question": "who resident in Arkansas?",
      "prediction": "Answer: Arkansas.",
      "ground_truths": [
        "Kris Allen,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "947a70e77e3947a5b677feb0c381d0cc",
      "question": "What human rights groups say Ethiopia failed to provde justice",
      "prediction": "The Swedish Foreign Ministry's spokesperson stated that their belief was that the court would think they were journalists and would be released, which they did.",
      "ground_truths": [
        "Reporters Without Borders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6be9520acc2e409594c65672107c545a",
      "question": "who is very disappointed by the verdict?",
      "prediction": "The final answer is: **The Swedish Foreign Ministry says that \"Our belief was that the court would think they were journalists and they would be released. This is what the prime minister has said before.**",
      "ground_truths": [
        "Tomas Olsson,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5436e24c1e06445da6163e646a165b25",
      "question": "Who says the two failed to prove their innocence?",
      "prediction": "The text provides a summary of the Swedish Foreign Minister's (CNN) decision to condemn the two Swedish journalists in Ethiopia for supporting terrorism.",
      "ground_truths": [
        "judge Shemsu Sirgaga"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8b81a14273744e48c1737ac742f6a65",
      "question": "What prison term could be a death sentence",
      "prediction": "The answer is 11 years in prison. The context simply states that the \"court would think they were journalists\" and that the prime minister has \"said it is not fair that they are sentenced since they are journalists on a journalistic mission.\" The answer is that the prime minister has \"decided that it is not fair that they are sentenced\" since they are journalists on a journalistic mission.",
      "ground_truths": [
        "11-year"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f48f89ef9f2f444b8438b4ecef84359a",
      "question": "what do human rights groups say?",
      "prediction": "The answer is: \"We are deeply disappointed.  We are very, very disappointed.  We are not going to ask for a pardon.  We are not going to ask for a pardon for those who committed crimes in Ethiopia, and we will not ask for a pardon for those who were imprisoned in Ethiopia. Our priority is our commitment to providing fair and humane treatment to our journalists and our people in all areas of the country. We are not going to ask for a pardon for the Ogaden because of the political verdict, and we will not ask for a pardon for the MP who has decided to appeal for another pardon.",
      "ground_truths": [
        "abuses against ethnic Somalis by rebels and Ethiopian troops are rampant."
      ],
      "em": 0,
      "f1": 0.03571428571428572
    },
    {
      "id": "cb271a55b4e14b5fb0606944599515cc",
      "question": "Who  says they are \"very disappointed\" by the verdict?",
      "prediction": "The answer is \"Our belief was that the court would think they were journalists and they would be released. This is what the prime minister has said before,\" and \"it is not fair that they are sentenced because they are journalists on a journalistic mission,\" and \"We are very disappointed,\" and \"Political verdict is not a real trial.\"",
      "ground_truths": [
        "Tomas Olsson, the journalists' Swedish attorney."
      ],
      "em": 0,
      "f1": 0.034482758620689655
    },
    {
      "id": "46414d4b1b7a4838987e3ee887a448bb",
      "question": "What are the names of the other top albums this week?",
      "prediction": "The answer is Chris Daughtry, whose \"Leave This Town\" bows atop the chart with a 269,000-copy sold, according to Nielsen SoundScan.",
      "ground_truths": [
        "(\"Number Ones,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5c6a6a15e594958ba61f8231ad1c9d0",
      "question": "How many copies sold Daughtry's?",
      "prediction": "The answer is 192,000.",
      "ground_truths": [
        "269,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3555cf721db54ce6a35be172f4f0213c",
      "question": "What is the name of Michael Jackson's biggest selling album in the past weeks?",
      "prediction": "The answer is Chris Daughtry's \"Leave This Town\" (269,000 copies sold).",
      "ground_truths": [
        "\"Number Ones\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d6a236fd6104736bfb0ae4404ee6ea9",
      "question": "Who has been biggest-selling album?",
      "prediction": "The answer is Chris Daughtry, whose \"Leave This Town\" sold 269,000 copies.",
      "ground_truths": [
        "Michael Jackson's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "283c5867606b46258127fa5fed7cf62d",
      "question": "How many copies of Doughtry's first album sell?",
      "prediction": "The answer is: 192,000.",
      "ground_truths": [
        "304,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2894e10d02a42c2b91b26caefadfbc9",
      "question": "What was the biggest selling album in the last two weeks?",
      "prediction": "The answer is Chris Daughtry's \"Leave This Town\" with a very nice 269,000 copies sold, according to Nielsen SoundScan.",
      "ground_truths": [
        "\"Number Ones\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7904eba267c04097b63fb228937b62bb",
      "question": "Which year was released first Daughtry's album?",
      "prediction": "The answer is Chris Daughtry's \"Leave This Town\" album, which is now sold handily at the top of the Billboard 200's chart.",
      "ground_truths": [
        "2006,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66d0b1463d394d90a300a5302c278a3d",
      "question": "How many copies did Daughtry's chart sell?",
      "prediction": "The answer is 192,000.",
      "ground_truths": [
        "269,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0134a374dcf1451a820c7fd997862851",
      "question": "How many decades was his career?",
      "prediction": "The answer is 40.",
      "ground_truths": [
        "20 years.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12859cbba89f494b98e6b7be8b4c88ff",
      "question": "Has he won any awards?",
      "prediction": "Yes, McGoohan has won two Emmys for his work with the British television show 'The Prisoner'. He was born in Astoria, New York.",
      "ground_truths": [
        "Emmy-winning"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2737e76f68ed4c889e75300d5d707fbb",
      "question": "Who is Patrick McGoohan?",
      "prediction": "Answer: Patrick McGoohan is an English actor.",
      "ground_truths": [
        "actor"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "dab26af5192549f1a07bed7fa63bf47e",
      "question": "What was he best known for?",
      "prediction": "Answer: McGoohan was best known for writing and starring in the British television show 'The Prisoner'.",
      "ground_truths": [
        "thrillers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08473edda228492e9cfe9b6583977e50",
      "question": "Who is the actor who passed away?",
      "prediction": "The actor who passed away is Emmy-winning Patrick McGoohan.",
      "ground_truths": [
        "Cleve Landsberg,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27c9d5c1e5d149a9824924cd86f3d48c",
      "question": "Did he win any Emmy awards?",
      "prediction": "Yes, he won two Emmys for his work on 'The Prisoner' in 40 years.",
      "ground_truths": [
        "Patrick McGoohan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "689c990ac1bc4eb38b2ebd26349a37f2",
      "question": "Who is the Emmy winning actor?",
      "prediction": "The Emmy-winning actor is Patrick McGoohan.",
      "ground_truths": [
        "Patrick McGoohan,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "bc20c452e45644e4beb92b0f35859105",
      "question": "What is the age of Patrick McGoohan?",
      "prediction": "The age of Patrick McGoohan is 80.",
      "ground_truths": [
        "80,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "35a6135bbde747f390617f9bb5cdd139",
      "question": "what happened to the stock market",
      "prediction": "The answer is, the stock market's performance is declining.",
      "ground_truths": [
        "opened considerably higher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "493a0407ac6747ee89cbd1944cd53d29",
      "question": "who elects the members of parliament of Egypt",
      "prediction": "The answer is: The Egyptian National Congress of the National Congress of the Communist Party of Egypt (NGC).",
      "ground_truths": [
        "Citizens"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ef9db99eba14a2db167a1a13f553a95",
      "question": "whats Voters pick members of the lower house of Egypt's parliament?",
      "prediction": "The answer is: \"Volunteers,\" with many more people being brought in.",
      "ground_truths": [
        "Citizens"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1239f593603846238d67b686d7fd6192",
      "question": "how many were wounded",
      "prediction": "Based on the context, the answer is 28.",
      "ground_truths": [
        "88"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "efe8d4d886a543a0b888a86a02c89940",
      "question": "what did the lower house do",
      "prediction": "The answer is: \"They treated at makeshift clinics in the square, but hospitals expected more people to be brought in. And, from the end of the day, 88 people were hit by rocks, glass bottles, birdshot and Molotov cocktails.\"",
      "ground_truths": [
        "will be tasked with drafting a new constitution"
      ],
      "em": 0,
      "f1": 0.04761904761904761
    },
    {
      "id": "7315c30824814fa4b9c02aa012a4ed93",
      "question": "who will have the task of drafting the new constitution of Egypt",
      "prediction": "The answer is: The new constitution of Egypt.",
      "ground_truths": [
        "lower house of parliament,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "d878c3d7ac484b759acbee38ce21b091",
      "question": "What role will EU forces take now?",
      "prediction": "The answer is: the EU's naval force will take over the role of escorting United Nations World Food Program vessels carrying food and relief supplies to war-torn Somalia, an EU news release stated.",
      "ground_truths": [
        "escorting United Nations World Food Program vessels"
      ],
      "em": 0,
      "f1": 0.3888888888888889
    },
    {
      "id": "b38edc1380ff473eb0e9126d03b25445",
      "question": "Who will take over the role of escorting U.N. World Food Program vessels?",
      "prediction": "The answer is: The European Union's naval force, and its support, will continue to operate in a place in the world that everybody's looking at because of the new problems related to piracy.",
      "ground_truths": [
        "EU naval force"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "49aa2a1e0fb443169edf8af22288973b",
      "question": "What will take over the role of escorting?",
      "prediction": "The answer is: the European Union's naval force.",
      "ground_truths": [
        "The EU naval force"
      ],
      "em": 0,
      "f1": 0.4444444444444444
    },
    {
      "id": "19d93283469843c5a343f26a38a4b76e",
      "question": "What country are the pirates from?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Somali-based"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac20eebd924947158dc6de1c9cb9039d",
      "question": "Warships from what areas also patrol the region?",
      "prediction": "The answer is the areas where the European Union will launch its first naval operation on Tuesday.",
      "ground_truths": [
        "United States, NATO member states, Russia and India"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f627367a8e64574b1a3fc00c386aac8",
      "question": "Where are the warships from?",
      "prediction": "The answer is: The European Union will launch its first naval operation Tuesday, protecting vessels from pirate attacks off the coast of Somalia, and the French army helicopter taking off from French frigate Nivose, on patrol in the Gulf of Aden.",
      "ground_truths": [
        "United States, NATO member states, Russia and India"
      ],
      "em": 0,
      "f1": 0.04545454545454545
    },
    {
      "id": "700122011f4e4d9494fa8b4a9e9bd808",
      "question": "Who has attacked almost 100 vessels off Somalia's coast this year?",
      "prediction": "The answer is: the Netherlands.",
      "ground_truths": [
        "pirates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89863b3cc756459497568b3bdc70588d",
      "question": "What countries have warships patrol the region?",
      "prediction": "The answer is: The European Union's naval force, and its European command.",
      "ground_truths": [
        "United States, NATO member states, Russia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "23406aeb357e4344bc1c22a24bf8b757",
      "question": "Who wil take over the role of escorting U.N. vessels?",
      "prediction": "The answer is: **European Council.**",
      "ground_truths": [
        "EU naval force"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e2b7b6be2784668bb3d2e749a2fe9e1",
      "question": "How many ships have pirates attacked?",
      "prediction": "The provided text does not contain the answer to the question \"How many ships have pirates attacked?\".",
      "ground_truths": [
        "almost 100 vessels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f96fb8c46a3f42caab3e23b301606942",
      "question": "What North American country sent warships to this region?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "United States,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b52c72e60b6d49a5a650724e57141268",
      "question": "Pirates have attacked how many vessels?",
      "prediction": "The European Union will launch its first naval operation Tuesday, protecting vessels from pirates off the coast of Somalia, EU policy chief Javier Solana announced Monday.",
      "ground_truths": [
        "almost 100"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf72c01037194351b8d906e8e8c4c0cc",
      "question": "What is the captain's name?",
      "prediction": "The captain's name is Marie Ali.",
      "ground_truths": [
        "Chesley \"Sully\" Sullenberger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8067ee0ec3df4208924631047ae0c537",
      "question": "What have been separated from the body of the aircraft?",
      "prediction": "The document does not provide any information about the body of the aircraft.",
      "ground_truths": [
        "The wings,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d89807b1fdda40c2bebb55edf64c3938",
      "question": "Where is the Airbus plane listed for sale?",
      "prediction": "The airbag plane is listed for sale in Kearny, New Jersey.",
      "ground_truths": [
        "at a salvage yard in Kearny, New Jersey."
      ],
      "em": 0,
      "f1": 0.47058823529411764
    },
    {
      "id": "fadd2528784f4a9fa3b391626e69ffc0",
      "question": "What is listed for sale?",
      "prediction": "The list for sale is \"AS IS/WHERE IS\" at a salvage yard in Kearny, New Jersey.",
      "ground_truths": [
        "an Airbus A320-214,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "644d4774db194bb0b491e58deff0fe6c",
      "question": "When is the auction set to end?",
      "prediction": "The auction is set to end on March 27 at 4:30 p.m. ET.",
      "ground_truths": [
        "March 27 at 4:30 p.m. ET"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b71eed014a8c4e51ba0bb48aeb1939ae",
      "question": "Where is the salvage yard?",
      "prediction": "The salvage yard is in Kearny, New Jersey.",
      "ground_truths": [
        "Kearny, New Jersey."
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "5d66710233f441d8b72e50abfae9b25e",
      "question": "What safely landed in the Hudson?",
      "prediction": "The safe landing in New York's Hudson River was in New York's Hudson River.",
      "ground_truths": [
        "US Airways Flight 1549"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3750487af26c40af96cb49550876a52a",
      "question": "What is the name of the plane's pilot?",
      "prediction": "The name of the plane's pilot is Capt. Chesley Sullenberger.",
      "ground_truths": [
        "Chesley \"Sully\" Sullenberger"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "01649970ebdc4958a48c133492620c31",
      "question": "What have the wings been separated from?",
      "prediction": "The wings have been separated from the body of the aircraft.",
      "ground_truths": [
        "the body of the aircraft"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "80f8912075b14d1a91b62f0c2d06ca84",
      "question": "Name the music label that owns studios",
      "prediction": "The answer is Terra Firma.",
      "ground_truths": [
        "EMI,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bdc7cf3241594e52a3164b489f50370d",
      "question": "Beatles made studios famous with what album?",
      "prediction": "The Beatles made studios famous with which album?",
      "ground_truths": [
        "\"Abbey Road.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "98bbd1deff4143b780e5d123925bdbee",
      "question": "What is Andrew Lloyd Webber's occupation?",
      "prediction": "Andrew Lloyd Webber's occupation is \"the composer of British musical theatre.\"",
      "ground_truths": [
        "composer"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "b5ada864c0934f8b98d919dae0bcec35",
      "question": "Andrew Lloyd Webber expressed interest in buying what?",
      "prediction": "The answer is **Andrew Lloyd Webber**.",
      "ground_truths": [
        "Abbey Road music studios"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f299e2d5b4264decb2fd2ab4d6a53c2d",
      "question": "does anyone own them",
      "prediction": "The answer is: YES. The answer directly answers the question.",
      "ground_truths": [
        "EMI,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94b8ba1c855e4789a90cd5692ee3e710",
      "question": "Who made studios famous with album \"Abbey Road\"?",
      "prediction": "The answer is: EMI.",
      "ground_truths": [
        "the Beatles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08183378ccb3472d903fd95b3b82e132",
      "question": "Where was the Swedish man poised to fly from?",
      "prediction": "The Swedish man was poised to fly from Ankara, Turkey.",
      "ground_truths": [
        "Amsterdam,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81a06cc7f0ed45fba03d40062246bf0f",
      "question": "What had he been charged with several years ago by Swedish investigators?",
      "prediction": "The charge with which the Swedish man was arrested was for forgery, and flying without a valid license.",
      "ground_truths": [
        "flying"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "b26cac2e25c243dd93e2ac30801d55c9",
      "question": "What did investigators charge him with several years ago?",
      "prediction": "The investigators charged the man with forgery and flying without a valid license for the last 13 years.",
      "ground_truths": [
        "flying"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "b8427699228e4350957f89c0fa5a3ac7",
      "question": "What did authorties charge the man with?",
      "prediction": "The answer is \"Investigators in Sweden charged the man with forgery and flying without a valid license.\"",
      "ground_truths": [
        "forgery and flying without a valid license,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "2c175bdf51494ae1a97be4ed1a365825",
      "question": "What is he charged with?",
      "prediction": "The answer is \"Forgery and flying without a valid license.\"",
      "ground_truths": [
        "forgery and flying without a valid license,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "f6ea20f22a394788814a7b4374246cf9",
      "question": "Where is the man flying?",
      "prediction": "The question asks for the man's flying place.",
      "ground_truths": [
        "from Amsterdam, in the Netherlands, to Ankara, Turkey,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6e9bca537a64d47a1f219d04aa484cf",
      "question": "What reasons do a man traveling with false documents?",
      "prediction": "The answer is \"only he knows.\"",
      "ground_truths": [
        "never renewed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "807640dab41142919b0c0452a24b2e79",
      "question": "Who hired this man?",
      "prediction": "The answer is the Dutch National Police Services Agency.",
      "ground_truths": [
        "a Turkish company"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4c1471da58b4cf087e179b9b71adef4",
      "question": "What did Reid win in 2004?",
      "prediction": "Reid won the Daytime Emmy Lifetime Achievement Award.",
      "ground_truths": [
        "Daytime Emmy Lifetime Achievement Award."
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "4d2ab04164304690b5bbc05618e82a59",
      "question": "What few years old actress Frances Reid died?",
      "prediction": "The few years old actress Frances Reid died on Wednesday at the age of 95.",
      "ground_truths": [
        "95."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "a73cab123714432c9731e0568fd9d43c",
      "question": "When did Reid die?",
      "prediction": "When did Reid died?",
      "ground_truths": [
        "Wednesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef15b705b7c54e77b276f22c804ce15d",
      "question": "Who died Wednesday at the age of 95?",
      "prediction": "The answer is Frances Reid.",
      "ground_truths": [
        "Frances"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b5736ba3b8574e3388b2a5ebccb15b09",
      "question": "What prize won in 2004?",
      "prediction": "The prize awarded in 2004 was a Daytime Emmy Lifetime Achievement Award.",
      "ground_truths": [
        "Daytime Emmy Lifetime Achievement Award."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "420222ccc3794c10a9750e897a4532e0",
      "question": "What roles were carried fame?",
      "prediction": "The roles carried fame were those played by Alison Sweeney, who was nominated twice for Daytime Emmys, and in 2004, in a Daytime Emmy Lifetime Achievement Award.",
      "ground_truths": [
        "Alice Horton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33ad75a283674ce5a8178438d8739810",
      "question": "What was she best known for?",
      "prediction": "The answer is \"true icon of the daytime genre and a true icon of the daytime genre.\"",
      "ground_truths": [
        "her decades-long portrayal of Alice Horton on"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "5ecdabf59c2343dbb91c9e4974e7512b",
      "question": "what has Demjanjuk been doing?",
      "prediction": "The answer is: U.S. immigration judges have granted a stay to John Demjanjuk, the Nazi war crimes suspect, to Germany, and for the past four years, he's been under a similar deportation.",
      "ground_truths": [
        "fighting charges of Nazi war crimes"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "277b88f5630044ec9dd5acfe543735e3",
      "question": "What is Demjanjuk accused of?",
      "prediction": "The answer is: Demjanjuk is accused of being an accessory to 29,000 counts of murder as a guard at the Sobibor death camp in Poland.",
      "ground_truths": [
        "Nazi war crimes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ea18cf48103402e90df928bd342898a",
      "question": "what does the judge reopen?",
      "prediction": "The answer is: The judge reopens deportation proceedings in the U.S. after the deportation was ordered.",
      "ground_truths": [
        "deportation proceedings."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "0ad390a8383b449d86576e0ce552596b",
      "question": "how long has Demjanjuk been fighting charges",
      "prediction": "The answer is \"After 2005.\"",
      "ground_truths": [
        "over two decades."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d226036b088f4ed68249977782b8abd1",
      "question": "Who is John Demjanjuk?",
      "prediction": "John Demjanjuk is a Ukrainian war crimes suspect, who was ordered deported to Germany in 1987. He is accused of war crimes, crimes against humanity, and the Holocaust, as well as numerous other crimes. He is extradited from the United States to Israel, where he is convicted of being an accessory to 29,000 counts of murder, and that he was also a prisoner of war at the Treblinka extermination camp. He has been fighting charges of Nazi war crimes for over two decades, and is deported to Israel. He is also a state-sponsored concentration camp guard. The",
      "ground_truths": [
        "Nazi war crimes suspect"
      ],
      "em": 0,
      "f1": 0.08602150537634408
    },
    {
      "id": "013ee76e68e646f3b80e544ff576f1db",
      "question": "Who accused him of accesory to murder at Sobibor?",
      "prediction": "The answer is: The accused accused Demjanjuk of being an accessory to 29,000 murders, and he was later stripped of his U.S. citizenship and deported to Israel.",
      "ground_truths": [
        "German authorities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cca9e488f0104c7fb0c2e2d384b34e5c",
      "question": "what does the attorney say about his client's health",
      "prediction": "The attorney's statement, \"In the four years since his deportation was ordered, his health has seriously deteriorated,\" reflects the seriousness of the case. The attorney, a former auto worker, reflects on his experiences in Germany, his struggles with his health, and his desire to celebrate his 89th birthday. He also states that his wife had a dinner at her home in Cleveland.",
      "ground_truths": [
        "has seriously deteriorated,\""
      ],
      "em": 0,
      "f1": 0.10344827586206896
    },
    {
      "id": "11807b5f5a844c9086e91a8de993695d",
      "question": "what does John Demjanjuk's attorney say?",
      "prediction": "John Demjanjuk's attorney says he believes the stay was ordered because he suffers from pre-leukemia, kidney problems, and a couple of types of gout.",
      "ground_truths": [
        "since his deportation was ordered, his health has seriously deteriorated,\""
      ],
      "em": 0,
      "f1": 0.12500000000000003
    },
    {
      "id": "eb3457e3628748db84d3fa4ea3228461",
      "question": "What does Paul McCartney think about his music?",
      "prediction": "Paul McCartney thinks the Beatles' music was \"quite amazing,\" and many of the children he knew and thought they were going to be very important. He also thinks the band's magic is the most important thing and that things are \"just \"good music\" and that the moptops (the youths who were the biggest fans) were really good. He also felt that when the record deal happened, he had to join the band. He thinks the music was \"nothing but good music,\" and that it made his \"important parts of the world\" uplifting and joyful. He says that the Beatles' music received a bit of a facelift",
      "ground_truths": [
        "we have left really good"
      ],
      "em": 0,
      "f1": 0.041237113402061855
    },
    {
      "id": "3dcf5ed5800a4b2ebd859d1d84051742",
      "question": "Did Larry King talk with former Beatles?",
      "prediction": "The answer is yes. Larry King did.",
      "ground_truths": [
        "Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32268b9dbb924482bba1be56dbaf21ef",
      "question": "When did Paul say his music is better?",
      "prediction": "The answer is yes, Paul McCartney said: \"We were just kids from Liverpool. And, yes, it is quite amazing, because as time goes on, it kind of becomes more and more of a phenomenon.\"",
      "ground_truths": [
        "in 5.1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f0394084598847c8838f40352bca705f",
      "question": "Who also appeared on Larry King?",
      "prediction": "The answer is: Larry King.",
      "ground_truths": [
        "Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "beda7e02aef94d1dbaf2f4cc9f0ae353",
      "question": "Whose widows also appeared",
      "prediction": "The answer is:",
      "ground_truths": [
        "of John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33f9b0041e204c29b91e907624f5e7d3",
      "question": "What CNN host interviewed the former Beattle",
      "prediction": "The CNN host interviewed Paul McCartney and Ringo Starr on Larry King's \"Larry King Live\" Tuesday.",
      "ground_truths": [
        "Larry King,"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "14d7e3999cbc40c4b39857a1c235f81d",
      "question": "Who's widows also appeared?",
      "prediction": "The answer is McCartney and Starr. They both appeared at the same party, and they talked about the celebration of the anniversary of John Lennon's death.",
      "ground_truths": [
        "John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "ce59f23189e04609b555d7ec7ce8b0c3",
      "question": "Who is paul mcCartney?",
      "prediction": "The answer is \"Paul McCartney.\"",
      "ground_truths": [
        "Former Beatles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c13d55e314a449d0995d6a4a5a7d2d36",
      "question": "Which former Beatles spoke with Larry King?",
      "prediction": "The answer is: Larry King.",
      "ground_truths": [
        "Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd49e8874cba45c0888db2a51756b3a0",
      "question": "Who did Larry King interview",
      "prediction": "The answer is: Larry King.",
      "ground_truths": [
        "Former Beatles Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "40edae0a62794df38c7beb0ea0b0a12e",
      "question": "Whose widows appeared?",
      "prediction": "The question asks which of the women who appeared at the Las Vegas \"Larry King Live\" interview on Tuesday was the widow of John Lennon and George Harrison, and who appeared at Cirque du Soleil's \"Love.\"",
      "ground_truths": [
        "of John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "1e56a4a2b1644ea4873578c3206ba949",
      "question": "Where is Paul from?",
      "prediction": "The answer is Las Vegas, Nevada.",
      "ground_truths": [
        "Liverpool,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ed1bc3066a44b759964f0abe310e725",
      "question": "what company does larry king work for?",
      "prediction": "The answer is the \"L\" for Larry King.",
      "ground_truths": [
        "CNN's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab608be8990f436382e64ca5cb3c10a8",
      "question": "What do the wives of the Beatles think about the band?",
      "prediction": "The wives of the Beatles think about the band:",
      "ground_truths": [
        "everything they left the world and left us is uplifting and joyful.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ec8e595494d4dd98aebe09545224171",
      "question": "Who do the former beatles talk to?",
      "prediction": "The answer is:",
      "ground_truths": [
        "CNN's \"Larry King Live.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dff8610ffbea4d1985f6746b99933f8f",
      "question": "Where did the  Beatles appear?",
      "prediction": "The answer is, the Beatles appeared in Las Vegas.",
      "ground_truths": [
        "\"Larry King Live.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1eb5c9be5fa41f589d2e9d56624312a",
      "question": "What did Harrison and Ono say?",
      "prediction": "Answer: Harrison and Ono said that the music's \"greatness\" is the most important thing, not the songs themselves. They said that \"life goes on, but the music is always with us.\"",
      "ground_truths": [
        "\"We feel so strongly about our husbands that sometimes it's hard for us, isn't it?\""
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "ce0ab333567e4a6ea61eff034e0bf5a2",
      "question": "What was the name of the talk show host?",
      "prediction": "The answer is \"Larry King.\"",
      "ground_truths": [
        "Larry King,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a4557a69506546079558b3c1cf41eb17",
      "question": "Who said their music sounds better",
      "prediction": "Answer: 1 (McCartney)",
      "ground_truths": [
        "Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b0d97074e304423bab66769456c4890d",
      "question": "Who is hot on Bayern's heels?",
      "prediction": "The answer is **Bayern Munich Munich**.",
      "ground_truths": [
        "Schalke"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "311ea5e6c28d46ecb378b855eb0ffe59",
      "question": "What does Marcell Jansen score?",
      "prediction": "The answer is: **Mourck Zidan scored twice in the second half to secure a 4-1 victory for Borussia Dortmund.**",
      "ground_truths": [
        "scored the only goal of the game"
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "aa9da743626a453780aaaf56e7816508",
      "question": "Where was a 4-1 win obtained?",
      "prediction": "The answer is **Hamburg** (Bayern Munich).",
      "ground_truths": [
        "Eintracht Frankfurt"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04aa223c25b44b43ba9c2cc94fb5ccc1",
      "question": "Who scored the only goal of the game?",
      "prediction": "The answer is: **Schalke** (as they are the top three teams in the German Bundesliga).",
      "ground_truths": [
        "Marcell Jansen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76ed5aaaa5bb47d7923d45360b3332ec",
      "question": "where did they cruise for a win?",
      "prediction": "Based on the provided text, the answer is **Dassenz, who scored two goals in the final after a late goal by Hamburg.  **",
      "ground_truths": [
        "Eintracht Frankfurt"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8447a26148c9454da18385e3d9fa4a3c",
      "question": "What does Bastian Schweinsteiger rescue?",
      "prediction": "The answer is: **Bayern Munich is at the top of the Bundesliga, and their first defeat since November.**",
      "ground_truths": [
        "a point"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "374bacece7a6435484e32852ba73f4a8",
      "question": "Who rescued a point?",
      "prediction": "The answer is: **Bayern Munich, Germany, rescued a point for Bayern Munich as they lost to Cologne, 1-1, in the 58th minute.**",
      "ground_truths": [
        "Schweinsteiger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc1fb48848df419fba9692dffef22844",
      "question": "where was the game?",
      "prediction": "The answer is Munich is now in the Europa League.",
      "ground_truths": [
        "RheinEnergieStadion."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0c1be19d610b4fa59930cc5cc930c9d2",
      "question": "In what year was the 1st ruling",
      "prediction": "The 1st ruling was in 2006.",
      "ground_truths": [
        "2006,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "22f46485365f4209a992b2f67feb0440",
      "question": "who is fisher suing",
      "prediction": "The answer is Matthew Fisher.",
      "ground_truths": [
        "former Procol Harum bandmate Gary Brooker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6082452e7aeb4e828079e15ab4e7a9a9",
      "question": "What is the name of the person who is suing?",
      "prediction": "The name of the person who is suing is Lord David Neuberger of Abbotsbury.",
      "ground_truths": [
        "Matthew Fisher,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "930d23fe662b49d4bb01a291e8a6ad98",
      "question": "what is fisher suing for",
      "prediction": "The answer is: Fisher is suing for compensation from the House of Lords for the music.",
      "ground_truths": [
        "a share in the royalties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec552a3b81734f36ae2a031fc34b7519",
      "question": "For what was Fisher seeking share of royalties",
      "prediction": "The answer is: The organist on the 1960s song \"A Whiter Shade of Pale\" wins a legal battle for royalties for the tune.",
      "ground_truths": [
        "1960s song \"A Whiter Shade of Pale\""
      ],
      "em": 0,
      "f1": 0.5217391304347826
    },
    {
      "id": "b1de8025e01448dcabb96c7e5fab0186",
      "question": "What is the name of the song?",
      "prediction": "The name of the song is \"Whiter Shade of Pale.\"",
      "ground_truths": [
        "\"A Whiter Shade of Pale\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8c513011d82543469328cb90f190d67c",
      "question": "What is the name of the person being sued?",
      "prediction": "The answer is \"Matthew Fisher.\"",
      "ground_truths": [
        "Gary Brooker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cac964bb57446e39b0f58d6043c12d7",
      "question": "Gary Brooker was sued by who",
      "prediction": "Answer: Gary Brooker was sued by who.",
      "ground_truths": [
        "Matthew Fisher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d001d1eb3aed47ecbc488bbd04cde7ec",
      "question": "what is his defense",
      "prediction": "The answer is:",
      "ground_truths": [
        "acute stress disorder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c236a6472cee467c9f07223ae7733fee",
      "question": "Are they trying to pursue the death penalty?",
      "prediction": "The provided context does not contain any information about the death penalty. Therefore, the answer is simply a statement based on the context.",
      "ground_truths": [
        "save their client from the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "722eae176eb34f668fc3e2dcc49d2112",
      "question": "when disorder did they fail to diagnose?",
      "prediction": "The answer is, during the charge of \"acute stress disorder,\" which is a military psychiatric nurse-practitioner's testimony that failed to diagnose the troubled infantryman, the psychiatric nurse-practitioner did not assign it to further treatment.",
      "ground_truths": [
        "acute stress"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "de09dd75884c4188a5d1190511a0a5a6",
      "question": "who is convicted of rape?",
      "prediction": "The answer is Kentucky.",
      "ground_truths": [
        "Steven Green"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2708a4ecb1f46bcb3ab479f79b044cd",
      "question": "what was steven green convicted of",
      "prediction": "The answer is: \"He was convicted of murder, rape, conspiracy and obstruction of justice, all at the state capital for his assault on a 14-year-old Iraqi girl. He was found guilty of killing the girl's parents and 6-year-old sister.\"",
      "ground_truths": [
        "raping and killing a 14-year-old Iraqi girl."
      ],
      "em": 0,
      "f1": 0.2439024390243902
    },
    {
      "id": "e7801d3c5fd846ef9062800d44834ac5",
      "question": "Who was convicted of rape?",
      "prediction": "The answer is Kentucky.",
      "ground_truths": [
        "Steven Green"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f53831743434c24b29ed9b858d87e4b",
      "question": "Who believes Schumacher will come out of retirement?",
      "prediction": "The answer is: Mercedes, the German manufacturer of Formula One, who had a controlling interest in 2009's Brawn double world champions' car.",
      "ground_truths": [
        "president Luca di Montezemolo"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ffc6152628c540ecae6e6bab7f420f28",
      "question": "Who says the seven-time champion will join Mercedes?",
      "prediction": "The answer is: Ferrari's official website.",
      "ground_truths": [
        "president Luca di Montezemolo"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "980522a80742433e96b144a86fd2ddce",
      "question": "Who is believed to possibly come out of retirement?",
      "prediction": "The answer is Ferrari. The text states that Luca Di Montezemolo believes seven-time Formula One world champion Michael Schumacher will come out of retirement and join the new Mercedes team.",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "143a0df7e66a473ba5804b1c75f56c86",
      "question": "What is his age?",
      "prediction": "The answer is 40.",
      "ground_truths": [
        "40-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45a6d0b6364d49129a59f0a062a2f9a8",
      "question": "Who was the 40-year-old unable to make a comeback with?",
      "prediction": "The answer is Nico Rosberg, who was a consultant with Ferrari.",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e4038716e3948b4b94243cad4cbfd1e",
      "question": "Where was he injured?",
      "prediction": "The answer is: **German manufacturer Mercedes-Benz**.",
      "ground_truths": [
        "neck"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8dc8f89ecde440e84e3efe2189030f6",
      "question": "Where did the criminals flee to?",
      "prediction": "The answer is Mexico City, Mexico.",
      "ground_truths": [
        "onto the college campus."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dfc221a3796f460ca42274a48ae054db",
      "question": "What were soldiers chasing?",
      "prediction": "The answer is: The Mexican government expresses its most deeply felt condolences to the families of two students who were killed early Friday in Mexico City.",
      "ground_truths": [
        "criminals who had fired on an army patrol"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "fa3a3ada93644452869b454ef6524da7",
      "question": "Where did this take place?",
      "prediction": "The answer is Mexico City, Mexico.",
      "ground_truths": [
        "Technological Institute of Higher Learning of Monterrey,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c8b13771a57b4743b822b8914ae68666",
      "question": "Where did they flee?",
      "prediction": "The answer is: Mexico City, Mexico (CNN).",
      "ground_truths": [
        "Some of the criminals had fled onto the college campus."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7cd7549b7d9041d9b9f4d730df07876c",
      "question": "What are the findings of the study?",
      "prediction": "The U.N. is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug's dominance in Afghanistan's economy has declined.",
      "ground_truths": [
        "export value of this year's poppy harvest stood at around $4 billion,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d21618a59053475dbfae6173d388ac04",
      "question": "What is the agency pushing NATO forces for?",
      "prediction": "The answer is U.N. (the United Nations) in its response to NATO's request to reduce the amount of opium in Afghanistan. The report's data shows that roughly 660 tons of opium were trafficked out of Afghanistan in 2007.",
      "ground_truths": [
        "stop the Afghan opium trade"
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "70f76d8aab60460f8933a443a2209d67",
      "question": "What narcotic makes up half of Afghanistan's GDP?",
      "prediction": "The answer is $4 billion.",
      "ground_truths": [
        "opium"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1776154b9a104d19991ffd5b8d6e9c6c",
      "question": "what is the skyrocketing problem?",
      "prediction": "The answer is, **both** is a complex issue.",
      "ground_truths": [
        "the narcotics trade."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09dc411bbb1f48eaa2263fd2feedfd7e",
      "question": "What drug makes up half of Afghanistan's gross domestic product?",
      "prediction": "The answer is **opium poppies**. The U.N. survey found that about a quarter of the earnings from opium go to farmers, and the rest goes to district officials, including drug traffickers and insurgent and warlord control.",
      "ground_truths": [
        "opium"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "7074419bbc394a039c03413007094b94",
      "question": "what NATO forces will attack?",
      "prediction": "The answer is: the U.N. (Congress of the United Nations) is calling on NATO to do more to stop the Afghan opium trade.",
      "ground_truths": [
        "Afghan opium trade"
      ],
      "em": 0,
      "f1": 0.2727272727272727
    },
    {
      "id": "d46d8974b9744a3bafb6d69a29735554",
      "question": "Which military force does the agency push to attack the problem?",
      "prediction": "The answer is: \"the Afghan government\"",
      "ground_truths": [
        "NATO"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "943da1a3b8b14a7f97e82a17737557a4",
      "question": "Who advocates providing income alternative for poppy farmers",
      "prediction": "The answer is: **the U.N. Office on Drugs and Crime.**",
      "ground_truths": [
        "Appathurai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25954c5bd38744d7872a71f0b604576c",
      "question": "What flower do the Taliban trade in?",
      "prediction": "The Taliban trade in opium poppies in Afghanistan.",
      "ground_truths": [
        "opium poppies"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "e77d740409e349ecbebe4fc81e84d520",
      "question": "What do drugs fund?",
      "prediction": "The answer is: opium.",
      "ground_truths": [
        "the insurgency,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1a99ce0121f4c7892a68a47221ff8ba",
      "question": "Which country did the study find opium made up half of it's GDP?",
      "prediction": "The study found that almost a quarter of Afghanistan's earnings go to farmers. It also found that Afghanistan's GDP is 29 per cent more than that of other countries, with more money going to district officials who collect taxes on opium.",
      "ground_truths": [
        "Afghanistan's"
      ],
      "em": 0,
      "f1": 0.05
    },
    {
      "id": "62c23c6b9c3b4d9896873ec4a545e70a",
      "question": "What is the U.N. drug chief advocating?",
      "prediction": "The U.N. drug chief is advocating for NATO to do more to stop the Afghan opium trade, in a statement released to CNN.",
      "ground_truths": [
        "urged NATO to take a more active role in countering the spread of the"
      ],
      "em": 0,
      "f1": 0.25806451612903225
    },
    {
      "id": "b6652e6bad6f4d90b962716fecbe5cf1",
      "question": "what is unchecked in Taliban strongholds?",
      "prediction": "The answer is:  \"The report from the U.N. Office on Drugs and Crime shows the export value of opium in 2007 was approximately $4 billion, a 29 per cent increase compared to 2006.",
      "ground_truths": [
        "poppy production"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b460e2ad790b419e84c17ee5ae051a26",
      "question": "What makes up half of Afghanistans GDP?",
      "prediction": "The answer is approximately 29 per cent of Afghanistan's GDP.",
      "ground_truths": [
        "opium"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f63891debcb465aaee5fcbd3ede0a86",
      "question": "What makes up half of Afghanistan's gross domestic product",
      "prediction": "The answer is **$4 billion**, which is 29 per cent more than 2006.",
      "ground_truths": [
        "opium"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6fc267ea05794476a82917156e3e519e",
      "question": "What does the U.N drug chief advocate?",
      "prediction": "The U.N. drug chief advocates for more active involvement in combating drug trade, not directly, but in supporting Afghan police through training and transport. He also mentions the increasing drug trade, and the collapse of the Taliban's funding source. He believes that the drug trade is a significant funding source for insurgency, and that by stopping it, NATO's self-interest in supporting Afghan forces in destroying drug labs, markets, and convoys is a concern. He also says that the situation is a \"live discussion for NATO.\"",
      "ground_truths": [
        "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\""
      ],
      "em": 0,
      "f1": 0.06818181818181818
    },
    {
      "id": "861b642c508e487798f314f2c3ade27a",
      "question": "What makes half of Afghanistan's gross domestic product?",
      "prediction": "The answer is **$4 billion**.",
      "ground_truths": [
        "opium"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "718236817764436eaf999352c380876e",
      "question": "Where is poppy trade unchecked?",
      "prediction": "The answer is Afghanistan.",
      "ground_truths": [
        "Taliban stronghold of Helmand,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "951d6908625f412daef5fc65020c52f0",
      "question": "What do the drugs fund",
      "prediction": "The U.N. was calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug is dominated in Afghanistan's economy.",
      "ground_truths": [
        "the insurgency,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9a18aead40e4b4191e1269e1f24bd8c",
      "question": "Where is the trade unchecked?",
      "prediction": "The answer is Afghanistan.",
      "ground_truths": [
        "in the lawless southern provinces and especially in the Taliban stronghold of Helmand,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb2604a388d048308734395577e3d885",
      "question": "What is funding the insurgency?",
      "prediction": "The answer is **opium poppies**.",
      "ground_truths": [
        "drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b95f7ffb86c34a2e82ac7ea8ac2a843f",
      "question": "What does UN Drug chief advocate?",
      "prediction": "The answer is: the U.N. Drug chief advocate for NATO.",
      "ground_truths": [
        "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\""
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "d360891ed1b84beea625c88b3d894e9b",
      "question": "What does the U.N. drug chief advocate?",
      "prediction": "The U.N. drug chief advocate argues that the Afghan opium trade is significantly more than half of Afghanistan's gross domestic product in 2007. He also criticizes NATO for its efforts, highlighting the high level of drug funding in Afghanistan, and blames the drug trade's contribution to decreased economic development and increased corruption in the districts of Amin and other drug-producing regions. He also suggests that Afghan government efforts are to destroy drug labs and markets to reduce funding for Taliban drug trafficking. He concludes by stating that the U.N. drug chief is encouraged to advocate for the Afghan government's support to eliminate opium poppies",
      "ground_truths": [
        "urged NATO to take a more active role in countering the spread of the"
      ],
      "em": 0,
      "f1": 0.09259259259259259
    },
    {
      "id": "bb9cd87d40ec484583f1d04e7cb6f0fc",
      "question": "Whose home was the actress found dead at?",
      "prediction": "The answer is \"Clarkson's home.\"",
      "ground_truths": [
        "Phil Spector"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c7150625e3f48dc92f4240336c4795f",
      "question": "Who was found dead at Phil Spector's home?",
      "prediction": "The answer is: The jury found Spector dead at his home in Los Angeles, California, in April 2003.",
      "ground_truths": [
        "Lana Clarkson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "874bbc325247476e9a6d8cbb373aa18b",
      "question": "What did the defense say Clarkson did?",
      "prediction": "The defense argued that Clarkson's death was a gun-toting menace, and he was a gun-toting criminal. The prosecution's testimony, including the witness stand, the evidence, and the prosecution's explanation for the crime, strongly suggests that Clarkson did not pull the trigger.",
      "ground_truths": [
        "grabbed the gun and"
      ],
      "em": 0,
      "f1": 0.05714285714285714
    },
    {
      "id": "af764b251ad54f0aada935fe01d05a7a",
      "question": "Where did the defense say Clarkson shoot herself?",
      "prediction": "The answer is: the defense states that Clarkson shot herself in the mouth.",
      "ground_truths": [
        "mouth."
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "ed9bbda118d04a9c8eab9ed97ade146e",
      "question": "Where did the defense say she shot herself?",
      "prediction": "The answer is: **in the mouth**.",
      "ground_truths": [
        "in the mouth."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d11eb1148eef497488293a76f1d2be07",
      "question": "Who was found dead?",
      "prediction": "The answer is, the judge declared a mistrial in the murder case.",
      "ground_truths": [
        "Lana Clarkson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5ce14ece239c49158eb1780e0fa5ed6c",
      "question": "when Rick Steves born?",
      "prediction": "The answer is that \"Rick Steves\" was born in Vienna, Austria, in 18th-century Vienna, Austria, in 18th-century Vienna.",
      "ground_truths": [
        "1979"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbfd988c52a043089ff4af3593d77131",
      "question": "What series is Steves working on?",
      "prediction": "The Iranian series is \"Steves.\"",
      "ground_truths": [
        "my recent 12-day trip to Iran to film a public-television show."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "046f31421adb4c58959d44292095a970",
      "question": "When did the rig sink?",
      "prediction": "The answer is \"during the blast,\" so the answer is TRUE.",
      "ground_truths": [
        "April 22,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "061a2eadb2db4bc6a8dfb22cc441e562",
      "question": "What did Landry say?",
      "prediction": "According to Landry, he said the first sign of trouble was when drilling \"mud\" (a mixture used to pressurize and lubricate the drills) fell onto the stern of his ship. He also noted that the captain told him there were problems with the well and instructed him to move his ship away.",
      "ground_truths": [
        "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top of the derrick,\""
      ],
      "em": 0,
      "f1": 0.16216216216216214
    },
    {
      "id": "ce27cf9031894392a2ada8c5c5933d8a",
      "question": "who radioed rig's bridge?",
      "prediction": "The answer is likely \"Landry\" (Louisiana).",
      "ground_truths": [
        "Landry"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "169e048fb1f84de0b9dff5bbe3ddfa9e",
      "question": "what was alongside Deepwater Horizon at time of blast?",
      "prediction": "The answer is: \"Mud\" (a mixture used to pressurize and lubricate drills).",
      "ground_truths": [
        "Alwin Landry's supply vessel Damon Bankston"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd80a000288946039bdf2cf26930cff1",
      "question": "Whose supply vessel was alongside Deepwater Horizon?",
      "prediction": "The answer is: Deepwater Horizon.",
      "ground_truths": [
        "Alwin Landry's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a4c865a666034aefa02347e9d9c5d9be",
      "question": "Where was the Deepwater Horizon at the time of the blast?",
      "prediction": "The answer is based on the context provided.",
      "ground_truths": [
        "Kenner, Louisiana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ba6e9a2c0ee4d4487d63a05d1570e50",
      "question": "Who is Alwin Landry?",
      "prediction": "The answer is Alwin Landry.",
      "ground_truths": [
        "the captain of a nearby ship"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fbb04100c444abda7f972b3c7ca55b9",
      "question": "What does Bloomberg blame the delay on?",
      "prediction": "Bloomberg blames the delay on \"disagreements with the Port Authority of New York and New Jersey\" for about $140 million, which has increased the cost of the National September 11 Memorial & Museum Foundation's infrastructure.",
      "ground_truths": [
        "\"disagreements\" with the Port Authority of New York and New Jersey,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5464eecea5074ee49a1f281e3855f2d1",
      "question": "Bloomberg blames delayed on where?",
      "prediction": "Bloomberg said the New York (CNN) work on the National September 11 Memorial & Museum has been halted over a budget dispute, meaning it's not open on time.",
      "ground_truths": [
        "\"disagreements\" with the Port Authority of New York and New Jersey,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "d7a9a5bd73444bfd8f0d54e0ee743304",
      "question": "Who said \"There is no chance of it being open on time\"?",
      "prediction": "According to Bloomberg, \"There is no chance of it being open on time,\"",
      "ground_truths": [
        "Bloomberg"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "386263cd9738415982f8a63c4d30c2c5",
      "question": "Will it be open on time?",
      "prediction": "The answer is: No, it will not be open on time.",
      "ground_truths": [
        "no chance"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "ef60d03b67d34a1190d04693fee8767f",
      "question": "where did it took place",
      "prediction": "The answer is: \"work has stopped,\" and the index of the answer is 1.",
      "ground_truths": [
        "Located underground in the former World Trade Center's \"archaeological heart,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b2726529c0374da2bcb670a3c89f58a4",
      "question": "what was scheduled to open?",
      "prediction": "The scheduled opening of the National September 11 Memorial & Museum was postponed.",
      "ground_truths": [
        "the National September 11 Memorial Museum"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "241f5227a1124be591341cf33b2ecb2f",
      "question": "When was the museum scheduled to open?",
      "prediction": "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.",
      "ground_truths": [
        "11th anniversary of the September 11, 2001,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "cff5c9d1f6fd4c6887578a64c02eb0b2",
      "question": "Who is the antichrist?",
      "prediction": "The answer to the question is \"Lars von Trier.\" He is a Danish provocateur who won the Cannes Film Festival with \"Breaking the Waves\" and \"Dancer in the Dark.\"",
      "ground_truths": [
        "Lars von Trier."
      ],
      "em": 0,
      "f1": 0.23076923076923078
    },
    {
      "id": "eb833d4f9eb4430689b0e718be45a714",
      "question": "Which film was met with a vitriolic reaction?",
      "prediction": "The film that was met with a vitriolic reaction was \"Chaos reigns!\"",
      "ground_truths": [
        "\"Antichrist.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b777ca5768e048c792d5c44d0c2b198c",
      "question": "Who does the film star?",
      "prediction": "The film stars Charlotte Gainsbourg and Willem Dafoe.",
      "ground_truths": [
        "Charlotte Gainsbourg and Willem Dafoe"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "46e965e7eede423cab40e895aa8c755b",
      "question": "Willem Dafoe and which other actor star in the film?",
      "prediction": "The answer is: Willem Dafoe.",
      "ground_truths": [
        "Charlotte Gainsbourg"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d0940d4e2f8b4c23817a178bf3f2ee85",
      "question": "how many nations meet?",
      "prediction": "Mexico City, Mexico",
      "ground_truths": [
        "30"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6eb7f187d8aa4f319e63ad269404dab2",
      "question": "from how many nations did the Leaders took place at the regional summit in Cancun?",
      "prediction": "The leaders of Latin American and Caribbean nations took place at the regional summit in Cancun.",
      "ground_truths": [
        "more than 30"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6778d6c630a84294a6dd81fbfabf9177",
      "question": "what will be discuss?",
      "prediction": "Answer: The United States and Canada will not be members of the OAS.",
      "ground_truths": [
        "whether to recognize Porfirio Lobo as the legitimate president of Honduras."
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "b612d7f272cd452dab8379571e41f3dd",
      "question": "What is the objective of summit?",
      "prediction": "The objective of the Summit of Latin America and Caribbean nations is to create a new organization to serve as a substitute for the Organization of American States.",
      "ground_truths": [
        "to take the Rio Group to a new level by creating the organization."
      ],
      "em": 0,
      "f1": 0.24242424242424243
    },
    {
      "id": "50b4c8f6f61a48c289fff1e90d3c1c68",
      "question": "Who meet at regional summit in cancun?",
      "prediction": "The question asks for the name of the meeting at Cancun.",
      "ground_truths": [
        "Brazil, Argentina, Mexico, Colombia and Venezuela."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "987b4118a9304f2f8193ab27a36b0489",
      "question": "what will the Summit discuss about?",
      "prediction": "The Summit will discuss the creation of a long-term plan for Haiti's recovery from the earthquake and the Argentine conflict over oil drilling offshore from the Falkland Islands.",
      "ground_truths": [
        "whether to recognize Porfirio Lobo as the legitimate president of Honduras."
      ],
      "em": 0,
      "f1": 0.06250000000000001
    },
    {
      "id": "0a8dca68f6c5497e9a6bbe453f46fb7c",
      "question": "What was important in his success?",
      "prediction": "The answer is, it was a clear strategy, a commitment to the party, and a powerful fundraising campaign that brought him to the national stage.",
      "ground_truths": [
        "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to"
      ],
      "em": 0,
      "f1": 0.20833333333333334
    },
    {
      "id": "e30767b1bf7e4bc89234ad7a4f69713b",
      "question": "who campaign helped fortunes soar?",
      "prediction": "The answer is the campaign that helped the party to grow and expand its national political power.",
      "ground_truths": [
        "Obama's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6858be8329164598910400cf9a7d4287",
      "question": "Who is Al-Shabaab linked to?",
      "prediction": "The answer is Al-Shabaab, which was linked to al-Qaeda.",
      "ground_truths": [
        "al Qaeda,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f52c8019fef94e7d88918d8444d0fdf3",
      "question": "Which governments pledged cooperation",
      "prediction": "The answer is: **Kenya pledged cooperation to resolve Somalia's territorial sovereignty.**",
      "ground_truths": [
        "Kenyan and Somali"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f542a53e2f7453fbeb43c7da3154697",
      "question": "Which countries troops were moved",
      "prediction": "The answer is: Kenya.",
      "ground_truths": [
        "Islamic militants"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20e05974b36d4960976e61bce8c630a2",
      "question": "What 2 governements are involved?",
      "prediction": "The answer is: The governments of Kenya and Somalia.",
      "ground_truths": [
        "Kenyan and Somali"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "3c814241efdb4f528a806c5090744d06",
      "question": "Against who is the fight",
      "prediction": "The answer is: Al-Shabaab.",
      "ground_truths": [
        "Al-Shabaab"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6cdb70e3b8ee43c08d280a50de57daf1",
      "question": "what are the kenyan and somali governments pledging",
      "prediction": "The question asks the Kenyan and Somali governments to pledge to cooperate in security and military operations to stabilize Somalia and enhance security.",
      "ground_truths": [
        "military"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "3ebbe2ff724e489f9707b61473c31ee5",
      "question": "what does al-shabaab deny",
      "prediction": "The answer is: Al-Shabaab denies the claim that they are responsible for the abductions.",
      "ground_truths": [
        "responsibility for the abductions."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "2f912f57b1ee4376a561c03370a092a3",
      "question": "What university does a student leader attend?",
      "prediction": "The answer is: The 12-year-old Stanford University student who helped teach the girls in Pakistan.",
      "ground_truths": [
        "Stanford"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "ab125bf5e094423eb2f4d733160ad567",
      "question": "Where were the soldiers and taliban fighting?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Swat Valley."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e99da60aff5a43b2892b05062baec6d5",
      "question": "Who is helping to lead them?",
      "prediction": "The answer is: \"A group of college students of Pakistani background are helping the girls in Pakistan.\"",
      "ground_truths": [
        "Shiza Shahid,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec9157de0a6b4e9b9624dd1903625528",
      "question": "Who are the victims?",
      "prediction": "The answer is: \"A group of college students of Pakistani background, including Shiza Shahid, 12-year-old Malila, and one 12-year-old, who also aspires to be a poet, who is 12-year-old Malila.",
      "ground_truths": [
        "School-age girls"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "942bf4f0c7314363b7e4c3ec2b28128f",
      "question": "Where was the fight?",
      "prediction": "The answer is Pakistan. (CNN)",
      "ground_truths": [
        "Swat Valley."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88ddb1ac0c1a4575aa6e022f3b851ede",
      "question": "What is the retreat giving girls?",
      "prediction": "The retreat is giving girls in Swat Valley the opportunity to learn in safer environments, enabling them to learn at their own pace and with the support of their peers.",
      "ground_truths": [
        "safer surroundings."
      ],
      "em": 0,
      "f1": 0.06896551724137931
    },
    {
      "id": "6e82a50369b147508672ea8bf7f81e33",
      "question": "In which city is the retreat?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Islamabad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "568e71d3966d49cd9d3ae1c99ac195d7",
      "question": "Where is the safe haven?",
      "prediction": "The answer is Pakistan.",
      "ground_truths": [
        "Islamabad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef2637a7f4cd4b1cb41e48fba82dfc56",
      "question": "When did the Delta Queen first serve?",
      "prediction": "The Delta Queen first served on the Ohio River System in 1927.",
      "ground_truths": [
        "1927"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "fdec7d459d40463c89b0fe6c78d175ea",
      "question": "what has been in service",
      "prediction": "The Delta Queen's final voyage was scheduled to depart Cincinnati on Tuesday on a 10-day cruise down the Ohio River, to Memphis, Tennessee, where it will unload what it could be its final passengers.",
      "ground_truths": [
        "The Delta Queen"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "626b8540033d498daef0671e87f1206d",
      "question": "What does the boat provide?",
      "prediction": "The boat provides economic opportunities in areas not typically tourist destinations, contributing to economic growth.",
      "ground_truths": [
        "economic opportunities."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "9fea05ecc2f44c2bbad533f62b3c9ddc",
      "question": "What will be an integrated part of the new OS?",
      "prediction": "The answer is: the iPhone 64.",
      "ground_truths": [
        "iCloud service"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d4215b15e07845fb921dca91acb0ff2f",
      "question": "What has been overshadowd",
      "prediction": "The answer is: Apple's iPod Nano and iPod Touch.",
      "ground_truths": [
        "iPods"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d1c7a748e934b4dbc7ee928eae541b9",
      "question": "What was overshadowed",
      "prediction": "The answer is: the iPod Nano and iPod Touch, and the iPod Classic and Shuffle (the Nano's click wheel) were all significantly reduced in price.",
      "ground_truths": [
        "iPods"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a9ab320d8a94bfdb157ed5e57c7eb0a",
      "question": "How many new clock faces were unvieled",
      "prediction": "The answer directly answers the question with the information provided in the context.",
      "ground_truths": [
        "16"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6f655d36c854ff98ea63df9d1e883cf",
      "question": "What will allow storage on remote servers",
      "prediction": "The answer is: \"It will get a 5GB of free storage, according to Cook.",
      "ground_truths": [
        "The iCloud service"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d50e709ce450451797883945fe60b25f",
      "question": "How many new clock faces did Apple unveil?",
      "prediction": "The answer is, Apple's iPod Nano and iPod Touch, which they both disappeared in the last year. Apple's online store showed both devices for sale. The answer is only 16 new clock faces for the 1.5-inch screen.",
      "ground_truths": [
        "16"
      ],
      "em": 0,
      "f1": 0.05882352941176471
    },
    {
      "id": "60ad8cecb37143498bfe42a3af10fed4",
      "question": "What ws unveiled",
      "prediction": "The answer provided, based on the context, is:",
      "ground_truths": [
        "new Touch,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c75b8a679d24dd192577a07dfed38a9",
      "question": "What devices got freshened  up by Apple?",
      "prediction": "The answer is, it's clear that the answer is: Apple's iPod Nano and iPod Touch.",
      "ground_truths": [
        "iPod Touch?"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "af5369c41cba4379978174fe353ddff6",
      "question": "What is the name of the shop?",
      "prediction": "The name of the shop is Haeftling.",
      "ground_truths": [
        "Haeftling,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "cff18af0aed84c39ab0d156de7f507f8",
      "question": "What prisoner rights organizations benefit?",
      "prediction": "The answer is: the prisoners at the company in Berlin, which has opened its first store in Germany.",
      "ground_truths": [
        "Amnesty International."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afa333ac95d848f0a60cde4f10d6e261",
      "question": "where do the profits go",
      "prediction": "The profits from the sales of the Haeftling store in Berlin are directly from the inmates' clothing, not from the company's own money.",
      "ground_truths": [
        "organizations that support prisoners' rights and better conditions for inmates, like Amnesty International."
      ],
      "em": 0,
      "f1": 0.06451612903225808
    },
    {
      "id": "56a527cc368e4b10ac88117b33ab6370",
      "question": "Where does a portion of the profit go?",
      "prediction": "The answer is: \"We want to have basic, durable, timeless, beautiful clothes.\"",
      "ground_truths": [
        "to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International."
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "57f26f2947864f429083b603d80000aa",
      "question": "who designs clothes",
      "prediction": "The answer to the question is: The company's English-language name is Haeftling.",
      "ground_truths": [
        "Inmates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "79e87ee729dd4f40b1492c9d24d26541",
      "question": "What are the prison inmates designing?",
      "prediction": "The answer is the Haeftling, which means inmate, has just opened its first store in Berlin.",
      "ground_truths": [
        "stylish clothes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d0a7880cce6f4bce95aaaf322c41355f",
      "question": "What portion of profits go to the prisoner rights organizations?",
      "prediction": "The answer is \"In the case of the female cartoon figure, this man was almost granted a stay from execution because of the design he did for us but in the end unfortunately the appeals court ruled against him,\" and the company's profits go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.",
      "ground_truths": [
        "part of the proceeds"
      ],
      "em": 0,
      "f1": 0.03773584905660377
    },
    {
      "id": "de51e8fbf7aa47c08b17fd80a80b4a38",
      "question": "When should you take special care?",
      "prediction": "The answer is \"Be vigilant.\"",
      "ground_truths": [
        "after a head injury,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "026a2a679a324a94ab9066e17933cc7d",
      "question": "What was the cause of death for Richardson?",
      "prediction": "The answer is \"Morgan was incredibly lucid, and she was okay with her illness.\"",
      "ground_truths": [
        "head injury."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8542e729373d4f66a5a663b56e6616f8",
      "question": "Whose death is raising questions?",
      "prediction": "The answer is \"Sometimes there's a gray zone, and there's no right answer.\"",
      "ground_truths": [
        "Natasha Richardson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "983656ab5e7a4ae9b635769d195b0d2b",
      "question": "Where do visitors take up Jewish  tradition of leaving stones as  grave markers?",
      "prediction": "The answer is Section 60, Washington, D.C.",
      "ground_truths": [
        "Arlington National Cemetery's Section 60,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "2aa6b5ff469342ad9ff08b7cc57d664c",
      "question": "Which culture leaves stones on grave markers?",
      "prediction": "The answer is a collection of people are leaving stones on graves, which is a deeply meaningful remembrance.",
      "ground_truths": [
        "Jewish"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d44d14a1f37e4cd7b716c2820f5ae814",
      "question": "The national cementary's Section 60 contains dead people from which wars?",
      "prediction": "The answer is: Section 60, Washington, D.C.",
      "ground_truths": [
        "in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d60ac112d634c7fbb6ea52d2c6f3776",
      "question": "What are some mourners doing?",
      "prediction": "The answer is, people come to the graves to connect to the past, and for many, the memory of their loved ones has brought them closer to their families and friends.",
      "ground_truths": [
        "walk slowly between the rows and rows of headstones, looking for a familiar name."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "506e226a5b1f417ba085b01d2cb18913",
      "question": "What is Section 60 holding?",
      "prediction": "Section 60 is the final resting place of many casualties of the Iraq and Afghanistan wars.",
      "ground_truths": [
        "many casualties of the wars in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0.6363636363636364
    },
    {
      "id": "b7377028d0274399b8189b9562f5bb1d",
      "question": "What kind of publication was placed behind store counters?",
      "prediction": "The answer is Lifeway Christian Stores.",
      "ground_truths": [
        "magazine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64e4eebd844f4487b8aef254b3a7bf0c",
      "question": "Who owns Lifeway stores?",
      "prediction": "The answer is **Lifeway Christian Stores**.",
      "ground_truths": [
        "Southern Baptist Convention,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47d30bda511f43808d50a044925ef722",
      "question": "Who was featured on the magazine's cover?",
      "prediction": "The magazine was featured on Lifeway Christian Stores' cover.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f3ed56417de4830bb62a1de86cee38f",
      "question": "What did Martin say about Lifeway?",
      "prediction": "Martin said he would vote for Barack Obama in November. He was the author of \"Listening to the Spirit Within: 50 Perspectives on Faith.\" Visit his Web site for more information.",
      "ground_truths": [
        "Christian Stores should be given hell for its actions against GospelToday."
      ],
      "em": 0,
      "f1": 0.05
    },
    {
      "id": "89b7bbc7406a47cfbb086010e72d1331",
      "question": "What was on the cover of the magazine?",
      "prediction": "The answer is: Lifeway Christian Stores should be given hell for its actions against GospelToday.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "736112d0d87748f0943c9088c28a57a2",
      "question": "Who was featured on the cover?",
      "prediction": "The answer is **Linton, the Southern Baptist Convention**, who wrote \"Listening to the Spirit Within.\"",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af104d8adfb148d68ee36715cbbd0d7c",
      "question": "Where did Lifeway stores put Christian magazine?",
      "prediction": "The final answer is **Brookhollow Baptist Church**, as the information was not provided. The question asks for the location of the Christian magazine, which is not provided.",
      "ground_truths": [
        "placed behind the counter."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2158e5af296446b2b3cef69beb058717",
      "question": "What was featured on the cover of the magazine?",
      "prediction": "The cover of the magazine featured Pastor Paula White's pastor, Rev. Dr. Ralph Douglas West Sr. (who is also a religious leader).",
      "ground_truths": [
        "celebrities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3dfb0ef71f1e4c4b88695ca0bcb4add2",
      "question": "Who put a Christian magazine behind the counter?",
      "prediction": "The answer is: **The Southern Baptist Convention.**",
      "ground_truths": [
        "Lifeway's 100-plus stores nationwide"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95e677b8475e40e3a6abedec6fe17104",
      "question": "What was the score?",
      "prediction": "The answer is **3-2**.",
      "ground_truths": [
        "3-2 victory."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "383db81acf1f476d8aa21be94cc9daa7",
      "question": "Who did Fulham beat in Switzerland",
      "prediction": "The answer is **Fulham defeated Basel 3-2 in Switzerland with a 3-2 victory.**",
      "ground_truths": [
        "Basel"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "60b894ff9c6c43c58bc90fcc2aa78e43",
      "question": "What countries were the games played?",
      "prediction": "The answers are:",
      "ground_truths": [
        "Switzerland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5b3c4c6f37c4de3907df3b82c13a2d7",
      "question": "Who beat Lisbon?",
      "prediction": "The answer is **(Bremen)**.",
      "ground_truths": [
        "Bundesliga Hertha Berlin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5c6d814143a4f1eb7cc7546d9654173",
      "question": "Who did Fulham beat 3-2?",
      "prediction": "The answer is **Fulham defeated Basel 3-2 in Switzerland.**",
      "ground_truths": [
        "Basel"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "d58a27153bc74f08bfa857c4a80b1c90",
      "question": "What was the score in the Werder Bremen Athletic Bilbao game",
      "prediction": "The answer is: FCKA Sofia won the group's other match with a 3-0 win.",
      "ground_truths": [
        "3-0"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "3dd9a7133b594635ac914a6abccf0e46",
      "question": "Who beat Bucharest?",
      "prediction": "The answer is:  **Bambina Romani won in Group F, defeating AC Milan in their final group match.**",
      "ground_truths": [
        "Panathinaikos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b583c0af110a4ab99bd001f7c487baae",
      "question": "Bishop was charged with what?",
      "prediction": "The answer is \"Bishop is under suicide watch in jail, and she is known to have killed some folks,\" and \"information about past runs-ins with the law\" came to light about Bishop's recent criminal charges, which are listed in the context.",
      "ground_truths": [
        "capital murder and three counts of attempted"
      ],
      "em": 0,
      "f1": 0.044444444444444446
    },
    {
      "id": "527ce6c8c0fb4e34a273e792ef21446d",
      "question": "When was the faculty meeting?",
      "prediction": "The answer is \"At the University of Alabama in Huntsville, Huntsville, Alabama,\" the faculty meeting in 1993.",
      "ground_truths": [
        "last week."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccc7685833724221ac2c3f779dbbfadd",
      "question": "What is Miller's first name?",
      "prediction": "According to the context, Miller's first name is \"Bishop\".",
      "ground_truths": [
        "Roy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9662731a19dd4d7eb774368c797ff071",
      "question": "what has roy miller said",
      "prediction": "The answer is: \"I think the case speaks for itself.\"",
      "ground_truths": [
        "\"something's wrong with this lady.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16a9d0ec55594c81a528e59ddb908fee",
      "question": "What is Bishop charged with?",
      "prediction": "The answer is \"She knows she's killed some folks, I'm sure.\"",
      "ground_truths": [
        "capital murder and three counts of attempted murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04a914d3e5484f15bce5c235722323ff",
      "question": "Lawyer told reporters he regrets calling client Amy Bishop what?",
      "prediction": "The answer is \"She is aware of what she's done and is very sorry for it.\"",
      "ground_truths": [
        "\"wacko.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "35110c01e79f454299244040123f6ef2",
      "question": "what did the lawyer say",
      "prediction": "The lawyer said he regrets describing Bishop as \"wacko\" and that he would not have made statements, and that he was trying to imply that the facts of the case \"speak for themselves.\" He added that he thought the case was \"wacky.\"",
      "ground_truths": [
        "he regrets describing her as \"wacko.\""
      ],
      "em": 0,
      "f1": 0.22727272727272727
    },
    {
      "id": "5a3b5274b19141bf9e88083177746248",
      "question": "Who called Bishop \"wacko?\"",
      "prediction": "The answer is \"She knows she's killed some folks, I'm sure.\"",
      "ground_truths": [
        "Roy Miller"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60df89b8d9d34129982e9438443b7d96",
      "question": "who were two victims",
      "prediction": "The two victims were Americans.",
      "ground_truths": [
        "Rob Lehr,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c5baaa251894e0688ca552be083606f",
      "question": "Was there any witnesses?",
      "prediction": "Yes, there were witnesses to the crash. A flight engineer, whose name is not released, was critically injured and in a coma. The crew reported that the pilot, Peterka, was killed in the crash. The crash is under investigation.",
      "ground_truths": [
        "flying unusually low,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "493624c8263b40ec97c127e45b45d5c1",
      "question": "Who was killed in the plane crash?",
      "prediction": "The killed in the plane crash is **Parris, 25**.",
      "ground_truths": [
        "The pilot,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e446b260a3db4e45b9948514d5d49865",
      "question": "Where did the plane crash?",
      "prediction": "The answer is Kenya.",
      "ground_truths": [
        "a three-story residential building in downtown Nairobi."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ae51cad51b340cb9ce156a667e11a5e",
      "question": "Were there any taken to hospital?",
      "prediction": "Yes, there were any taken to hospital.",
      "ground_truths": [
        "hospitalized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6536963fab4940258cbdea45da005467",
      "question": "who was killed",
      "prediction": "The killer was a pilot, whose name is not yet released, and died in the crash.",
      "ground_truths": [
        "The pilot,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "5ede5f452230413ca47938c5b9144011",
      "question": "What were the two victims shooting?",
      "prediction": "The two victims shooting were U.S. filmmakers, Dan Parris and Rob Lehr, who were in St. Louis, Missouri.",
      "ground_truths": [
        "an independent documentary on poverty in Africa."
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "051e0cb33cb64409bbf8e8e79165bacf",
      "question": "What made landfall near Cocodrie, Louisiana?",
      "prediction": "The answer is: New Orleans, Louisiana.",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bf907acc0204e0785974060d84db533",
      "question": "What will it do to Texas?",
      "prediction": "The answer is: It will likely continue to continue, and the state of Texas's hurricane weather pattern will likely intensify. The final thoughts of the forecasters reflect on the current nature of the storm's impact on the Gulf Coast and the likely decreased capacity of Louisiana's barrier islands and coast.",
      "ground_truths": [
        "\"exacerbate the threat of heavy rains and inland flooding.\""
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "bb0d3b5086f845df8acfbf503fa4c2ff",
      "question": "What category storm was it?",
      "prediction": "The answer is \"a bit more ragged.\"",
      "ground_truths": [
        "2"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa3b764edd7a4c44be5b8921112e8bf6",
      "question": "what time did this occur?",
      "prediction": "The answer is: New Orleans, Louisiana.",
      "ground_truths": [
        "8 a.m. CT."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "916d57f6183b4c52999933fe394cae6e",
      "question": "What is stalling over northeast Texas?",
      "prediction": "The answer is \"A bit more ragged,\" and the following is a note by Chris Lawrence:",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf18870732dc40f89d6dd59b7c67e691",
      "question": "What made landfall?",
      "prediction": "The answer is: New Orleans, Louisiana.",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e376a7767224f8381c8ee661c43ad16",
      "question": "What downgrades it to a Category 2 storm?",
      "prediction": "The answer is: \"A bit more ragged,\" and \"a bit more shortened.\"",
      "ground_truths": [
        "110 mph,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29cd58ae62e94dc4b7315ce4b50010ef",
      "question": "what were the hits",
      "prediction": "The answers to the question provided are:",
      "ground_truths": [
        "\"Hawaii Five-O\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a4a586dd10143f3a68247ae4c535713",
      "question": "What age was Bob Bogle when he died?",
      "prediction": "The answer is 1950s.",
      "ground_truths": [
        "75."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8657886cea34216b10630e96b7a2932",
      "question": "Who influenced countless bands with guitar sound?",
      "prediction": "The answer is **The Ventures**.",
      "ground_truths": [
        "Bob Bogle,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bc61a95b6c25463db3459cd626a11ad0",
      "question": "What hits did they have?",
      "prediction": "The answer is \"The Ventures\" and \"Play Guitar with the Ventures,\" and the article in CNN Radio (Ninette Sosa and Matt Cherry) describes the bands' instrumental work, including \"Walk -- Don't Run\" and \"Hawaii Five-O,\" which contributed to their lasting influence on instrumental music.",
      "ground_truths": [
        "Don't Run\" and \"Hawaii Five-O\""
      ],
      "em": 0,
      "f1": 0.23255813953488372
    },
    {
      "id": "73df4341944b4832a7110492fec95939",
      "question": "Who was Ventures' lead guitarist?",
      "prediction": "The answer is **Bob Bogle.**",
      "ground_truths": [
        "Bob Bogle,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "786e56a75654431f8335c0a57f0e3d3e",
      "question": "Which band was he lead guitarist of?",
      "prediction": "The band was led by Bob Bogle, who is 75.",
      "ground_truths": [
        "Ventures"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7538664bfbdf4f4686e914d02414532e",
      "question": "How long will the examination take?",
      "prediction": "The answer is \"after Peterson was killed.\"",
      "ground_truths": [
        "several weeks,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5c5aa44f8e2f4f8d9f649441002cf806",
      "question": "Where did the deputy shoot himself at?",
      "prediction": "The deputy at the scene fired a .40-caliber gun at a .40-caliber.40-caliber gun, which was in contact with his skin.",
      "ground_truths": [
        "in the head"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "bb4519ed1e374168848b6c5286447f32",
      "question": "Forensic examination of the deputy could take how many weeks?",
      "prediction": "The answer is likely 15.",
      "ground_truths": [
        "several"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e8a0ee8622a34c72b67364ba7b87d7cc",
      "question": "What did Peterson confess to friends?",
      "prediction": "The answer is: Peterson confessed to friends.",
      "ground_truths": [
        "about the shootings,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e45b34a0a6c46e98d2e25f54b59c906",
      "question": "What type of wounds were the shots consistent with?",
      "prediction": "The answer is, based on the context, the answer is \"nonfatal rounds with entry points below the chin.  The last shot was also fired from a rifle at distance.\"",
      "ground_truths": [
        "self-inflicted"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fb6d6705bd14c00912a97b755696ad6",
      "question": "How many people were killed at the party?",
      "prediction": "The answer is 20-year-old Tyler Peterson was killed by a police sniper's bullet.",
      "ground_truths": [
        "six young"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d962251db8ec4ea6ac1f7458725f09cd",
      "question": "What did the Attorney General say happened?",
      "prediction": "The Attorney General said that the final forensic determination of the three gunshot wounds to the head resulted in consistent self-inflicted wounds, and that the three head shots were consistent with self-inflicted wounds, not long-range rifle fire.",
      "ground_truths": [
        "himself three times in the head with a .40-caliber pistol,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "b4b3003e7d6b492e876c74ed3fcc7a20",
      "question": "How many timesdid the deputy shoot himself?",
      "prediction": "The answer is 3.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "423c074089a34a9087516b566aec95ac",
      "question": "How many people did Peterson kill at a party?",
      "prediction": "The answer is 6.",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28a94431b64446adbfcca8f7030b8374",
      "question": "Who shot six people at the party?",
      "prediction": "The answer is the state attorney general, J.B. Van Hollen.",
      "ground_truths": [
        "Tyler Peterson, a sheriff's deputy,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2586234cb1754cfdab82bd4d52a73115",
      "question": "Where did the deputy shoot himself?",
      "prediction": "The answer is: The deputy shot himself at home in the town of Argonne.",
      "ground_truths": [
        "head"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "129ac221f74d4da6a9d78059c18caf86",
      "question": "What does Jindal hope?",
      "prediction": "The transcript has been edited for brevity and clarity. The following is extracted directly from the transcript:",
      "ground_truths": [
        "people look at the content of the speech, not just the delivery."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b117989cc724148a5e4b3d94b8e0b9d",
      "question": "What did Jindal talk about?",
      "prediction": "Jindal discussed the war of words in the Republican Party's rhetoric, with the president addressing Congress, and the administration. He also spoke about the role of the federal government in the crisis.",
      "ground_truths": [
        "Barack Obama:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f9679f4837d4c1b809870b506382a30",
      "question": "Who is Jindal interviewing with?",
      "prediction": "The transcript has been edited for brevity and clarity. The transcript is from a private interview with Larry King.",
      "ground_truths": [
        "Larry King"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "11a8100cb9134bc88de9c849856ce25d",
      "question": "Who had the prime time exclusive?",
      "prediction": "The transcript has been edited for brevity and clarity.",
      "ground_truths": [
        "Larry King"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e021412ffee94b2b8a062f71b1298fd4",
      "question": "Who talked to Larry King?",
      "prediction": "The transcript is a transcript of a Q&A session between Larry King and Governor Bobby Jindal at the National Association for Democratic-Republican of Social Movement, which is a political party for the Democratic-Republican-Populist (D-SP) party. The Q&A session focused on the state of the party, the role of the federal government, the Democratic/Republican platform, and the challenges faced by the party.",
      "ground_truths": [
        "Republican Gov. Bobby Jindal"
      ],
      "em": 0,
      "f1": 0.0784313725490196
    },
    {
      "id": "66f90ce76e994be098c155d5873bba30",
      "question": "What does Jindal hope people judge?",
      "prediction": "Jindal hopes people will judge the speech, not just the delivery. He asks, \"What does it mean for the federal government to provide for the private sector?\" He emphasizes the importance of the fiscal responsibility of the private sector in the economy. He also asks, \"What is more important to the party?\" and \"Why?\"",
      "ground_truths": [
        "look at the content of the speech, not just the delivery."
      ],
      "em": 0,
      "f1": 0.18867924528301885
    },
    {
      "id": "fa043db827ff44d48d9ec1814c1c7223",
      "question": "What state does Bobby Jindal lead?",
      "prediction": "The transcript contains a conversation between Larry King and Bobby Jindal, where Bobby is told about Barack Obama's speech. The transcript is edited for brevity and clarity, and it is written for clarity. It also mentions the \"war of words\" in the Republican Party's platform, which the President explicitly stated.",
      "ground_truths": [
        "Lousiana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d35e612afe4d4fc694cf30e00b988d54",
      "question": "What did Jindal say about the content of his speech?",
      "prediction": "Jindal did not say anything about the content of his speech. He simply said, \"I'm certainly not nearly as good of a speaker as he is.\" He also said, \"I'm certainly not the only one that's got that opinion.\" He also mentioned the war of words in the Republican Party's party platform. He also stated that he hadn't said anything about the \"shovel-ready infrastructure spending.\" He also mentioned that the federal government supposedly has no voice in the crisis. He also mentioned the criticism of the Bush bailout of the automobile industry. He also said, \"Fundamentally, I",
      "ground_truths": [
        "\"I'm certainly not nearly as good of a speaker as he is.\""
      ],
      "em": 0,
      "f1": 0.2268041237113402
    },
    {
      "id": "afff7321cf184e6b97fd59a959a10bc0",
      "question": "Who is the Louisiana governer?",
      "prediction": "The transcript contains a transcript of a long-form interview with the Governor of Louisiana, Bobby Jindal, given to Speaker Larry King. The transcript also includes a brief summary of the speech, as requested by the original text.",
      "ground_truths": [
        "Bobby Jindal"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "f895a8e048ca4a8a860ede9678d9a572",
      "question": "How many people were killed this month by lightning strikes?",
      "prediction": "The answer is 11.",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fdf5459888074c2689115afbbdf1d86b",
      "question": "What season is the deadliest for lightning?",
      "prediction": "The answer is \"Florida does not receive the most lightning in the world.\"",
      "ground_truths": [
        "Summer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91aa94133dfd4f64904d1c79b36d8f10",
      "question": "How many lightning deaths have occurred in the US this year?",
      "prediction": "The answer is \"11.\"",
      "ground_truths": [
        "15"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3500fc318e674c8698b5f0859c22339b",
      "question": "This week is the first official what of summer?",
      "prediction": "The answer is \"Lightning is very random in terms of fatalities in the U.S. so far this year, according to the National Weather Service, and the 28 killed in 2008 in California, Texas and Florida, with the rest scattered through the South, Midwest and West. No deaths in the Northeast, so far, are mentioned.\"",
      "ground_truths": [
        "\"Lightning Safety Awareness Week\""
      ],
      "em": 0,
      "f1": 0.0392156862745098
    },
    {
      "id": "573580d045a1403d9082b4ee81aec116",
      "question": "Where did the 15 deaths occur?",
      "prediction": "The answer is: 15 deaths occurred in the United States in the U.S. last year, according to the National Weather Service.",
      "ground_truths": [
        "in the U.S."
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "583d02cebe5241639c457f4026bfb108",
      "question": "How many people have been killed already this month by lightning strikes in the U.S.?",
      "prediction": "The answer is: 11.",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cab4c82707424a6a81b9710b01c2c4fc",
      "question": "What is summer deadliest season for?",
      "prediction": "The answer is \"Florida does not receive the most lightning in the world.\"",
      "ground_truths": [
        "Lightning strikes"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "d5ba53a46f9b4a1bb0fbe9dcfd80ddf0",
      "question": "Where were the country's lawmakers stranded?",
      "prediction": "The answer is Djibouti. The text only provides the official information about the Somali Transitional Federal Parliament, the parliament building, and the outgoing presidential palace in Baidoa.",
      "ground_truths": [
        "Djibouti,"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "7554be436374465c81b24a7a2e0bd4c1",
      "question": "who seized control?",
      "prediction": "The answer is, \"We have nowhere to return to,\" and the Djibaran, the Somali National Party, is the leader of the transitional government.",
      "ground_truths": [
        "Islamist fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e4452ea2976418eb7099bc248dbed10",
      "question": "What did the home Depot ceo say?",
      "prediction": "The home Depot CEO's response to the comment, which suggests they were fired, was a statement about the workers' fears of unionization.",
      "ground_truths": [
        "If a retailer has not gotten involved with fight over this bill, he \"should be shot,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f107128551c149079e2da4669ceb0b8f",
      "question": "What was the rally in support of?",
      "prediction": "The answer is: The employee free choice act.",
      "ground_truths": [
        "Employee Free Choice act"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "d1fcfcaf63b94a048bcebf88ad55d818",
      "question": "In support of what is the rally held?",
      "prediction": "The answer is, \"Employees at the National Association of Color Workers (NAC) held a rally on Monday in Lafayette Square in Washington, D.C. to support the Union for Labor (E.F.C.) Act, a bill intended to increase the power of the federal minimum wage. The rally was not a union event.\"",
      "ground_truths": [
        "Employee Free Choice act"
      ],
      "em": 0,
      "f1": 0.04444444444444444
    },
    {
      "id": "fd25e49e70ae419e9a6b5e76355f0f86",
      "question": "Who says \"this is how a civilization disappears\"?",
      "prediction": "The answer is \"We are not\" and \"We don't say, we just say.\"",
      "ground_truths": [
        "Marcus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a229d468794241bbbc6ea1ed0993099d",
      "question": "How many days would the company have to negotiate?",
      "prediction": "The context provides information about the workers' position and the Congress's support for the Employee Free Choice Act. The question asks for the number of days the company would have to negotiate.",
      "ground_truths": [
        "90"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "159cd77edb104c1ebf8e3dbef2bab476",
      "question": "What did the \"reset\" button actually say?",
      "prediction": "The answer to the question is \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "\"peregruzka\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc5c610055d74dec9317c671ae0532ee",
      "question": "What country wants to reset relations with Russia?",
      "prediction": "The answer to the question is: \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "the United States,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2160f240eb1847239f1438b3d839f4b0",
      "question": "What countries want to reset relations?",
      "prediction": "The provided text, written in a formal, concise tone, discusses the Russian Foreign Minister's presentation on \"resetting our relationship with Russia\" after Russia's invasion of Georgia. The key takeaways include:",
      "ground_truths": [
        "Russia"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "441bc101ad874330a800cd51ac465226",
      "question": "Who is the US trying to \"Reset\" relations with?",
      "prediction": "The answer is \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "Russia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c332d3bebac4a458ad358a37ffcbb13",
      "question": "What does the text on the button actually translate to?",
      "prediction": "The text states that the button on the desk is \"peregruzka\" (Russian for reset), and that the answer to the question is: \"perezagruzka\" (the Russian word for reset).",
      "ground_truths": [
        "'overcharged.'\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4269197f565b46028a9227548797dbd1",
      "question": "What did the text on the button translate to?",
      "prediction": "The text on the button translates to \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "'overcharged.'\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1a605ea40554db7b369db840ed6d9a4",
      "question": "Which Secretary of State gave their Russian Counterpart a Reset Button?",
      "prediction": "The answer is: \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce4eaf2d23b14736a9a30d4e272b930f",
      "question": "What treaty did both countries want to renegotiate?",
      "prediction": "The answer is: \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "Strategic Arms Reduction"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6141c837f8445bc8f171995f15b8647",
      "question": "Who gave a reset button to her Russian counterpart?",
      "prediction": "The answer is \"We want to reset our relationship and so we will do it together.\"",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a82de30a1e546bbaf099a05d79a6230",
      "question": "What could the states refuse?",
      "prediction": "The states might refuse to renew registration until the manufacturer's fix is made.",
      "ground_truths": [
        "renew registration"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "07ebc87534e64dfca8d40500fe2ecd3b",
      "question": "Do they ensure auto owners comply?",
      "prediction": "No, they do not ensure auto owners comply.",
      "ground_truths": [
        "that"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2662231ec6934df58d84e851aaff80fc",
      "question": "What do most recalls involve?",
      "prediction": "The most involved recalls involve free fixes for the consumer, not a process to ensure compliance.",
      "ground_truths": [
        "free fixes for the consumer."
      ],
      "em": 0,
      "f1": 0.47058823529411764
    },
    {
      "id": "a10d412bc874427b9f8df9a729c686a8",
      "question": "What kind of process does not exist?",
      "prediction": "The answer is: Not a process to ensure that auto owners comply with recalls.",
      "ground_truths": [
        "to ensure that auto owners comply with recalls."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "61727044bdac4760964296c822ea1ffa",
      "question": "What percentage of consumers don't comply?",
      "prediction": "Answer: 25%",
      "ground_truths": [
        "25"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "614e09625ec04274b0a1981dc910ce79",
      "question": "What can insurers use VINs for?",
      "prediction": "Insurances can use VINs for a variety of purposes, including:",
      "ground_truths": [
        "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f74b173f48654bc9832934834956c10f",
      "question": "When will employees celebrate Steve Jobs' life?",
      "prediction": "When will employees celebrate Steve Jobs' life?",
      "ground_truths": [
        "October 19,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d54a2fd059b54e2d80b503109524db0a",
      "question": "What has Jobs battles for years?",
      "prediction": "Jobs battles for years to overcome cancer, a medical leave, and his declining health.",
      "ground_truths": [
        "cancer"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "f9bf09905dc249e69f69c4db9b323d0f",
      "question": "Who will celebrate Steve Jobs' life?",
      "prediction": "The answer is Apple.",
      "ground_truths": [
        "Apple employees"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "daa68770d88b4dbabfc2955ecc647750",
      "question": "What did Jobs battle?",
      "prediction": "Apple fought against what?",
      "ground_truths": [
        "cancer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "42c9c8fe7ee843dbbe578fdc4dbba39a",
      "question": "when did Apple employees will celebrate Steve Jobs' life ?",
      "prediction": "Apple employees will celebrate Steve Jobs' life on Monday at 3 p.m. at his home in Palo Alto, California.",
      "ground_truths": [
        "October 19,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bcdd509239274509afc8ae77cb5df604",
      "question": "Who co-founded Apple?",
      "prediction": "The answer is Apple.",
      "ground_truths": [
        "Steve Jobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "703c1f0b768d4732afa9e6a5a2d05091",
      "question": "when did Jobs die?",
      "prediction": "The answer is on the answer key.",
      "ground_truths": [
        "3 p.m. Wednesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20bdba32de3c4cd3829be8b6b99a004e",
      "question": "What are the slim risks?",
      "prediction": "The answer is that \"neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future.\"",
      "ground_truths": [
        "\"brain hacking\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9630268d37cf48ef8fe8d155ad2298b8",
      "question": "What says expert security?",
      "prediction": "The answer is \"technological innovation,\" which means developing and improving a new product or system.",
      "ground_truths": [
        "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\" said computer"
      ],
      "em": 0,
      "f1": 0.06896551724137931
    },
    {
      "id": "bd3b71b3e5014081a2bd8383ab23111c",
      "question": "Who can use thoughts to operate computers?",
      "prediction": "The answer is \"hard not to take them seriously. If we don't start paying attention to security, we might be in five or ten years, saying we've made a big mistake.\"",
      "ground_truths": [
        "Scientists"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d28c55c539de46f9922e84f3c87321fc",
      "question": "Who is game's creator?",
      "prediction": "The answer is **Alexey Pajitnov, the creator of Tetris**. He wrote the program in his spare time and used the shapes of four squares to create the game.",
      "ground_truths": [
        "Alexey Pajitnov"
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "dcfec78b61864784bec20d769fa6c923",
      "question": "How many years old is Tetris turning?",
      "prediction": "The answer is 25 years.",
      "ground_truths": [
        "25"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "62d1568d094842c390de7d3319f03c28",
      "question": "What percent of mobile games are Tetris?",
      "prediction": "The answer is 80 million.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9df988aed472449b894ff1f198dd4e3d",
      "question": "Who did CNN talk to?",
      "prediction": "The answer is: **Hk.  Alexey Pajitnov, CEO of Blue Planet Software.**",
      "ground_truths": [
        "Alexey Pajitnov,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "a8834ebda8454adcadf347f0eeb73d54",
      "question": "What kind of game is Tetris?",
      "prediction": "The answer is Tetris.",
      "ground_truths": [
        "the simple puzzle video"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a9c6dc3980740d18d876d0e75fe2f27",
      "question": "What percentage of all games was Tetris?",
      "prediction": "The answer is 25 years.",
      "ground_truths": [
        "10 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2616151407964974abead40e1f46a464",
      "question": "What is Tetris?",
      "prediction": "The answer is: Tetris is a simple, repetitive, and addictive puzzle game developed by Alexey Pajitnov in his spare time in 1984. It was a game played with four squares, requiring players to place and stack blocks to fill a grid. Players successfully completed sections would result in a higher score, and the game continued for hours, leading to immense popularity.",
      "ground_truths": [
        "simple puzzle video game,"
      ],
      "em": 0,
      "f1": 0.10169491525423728
    },
    {
      "id": "293d474acb924ae2af1cf26af60c1cd1",
      "question": "What game turned 35?",
      "prediction": "The answer is 35.",
      "ground_truths": [
        "Tetris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e78a6a43ee564f9f9c1718636a38c1b2",
      "question": "Who is supporting calls for an investigation into Gadhafi's death?",
      "prediction": "The answer is: The United Nations human rights organizations.",
      "ground_truths": [
        "United Nations and international human rights groups"
      ],
      "em": 0,
      "f1": 0.5714285714285714
    },
    {
      "id": "22f9b367696d43efb409a9f06a662261",
      "question": "Gadhafi's son Saadi is \"outraged\" over circumstances surrounding what?",
      "prediction": "The answer is: \"Saadi Gadhafi is outrageded by the brutal killing of his father and brother, along with the contradictions of the NTC's actions, as well as the failures of the NTC's leadership.\"",
      "ground_truths": [
        "the vicious brutality which accompanied the murders of his father and brother.\""
      ],
      "em": 0,
      "f1": 0.2702702702702703
    },
    {
      "id": "2b116d81534646e585d2c57d2f7915ec",
      "question": "What is the name of Gadhafi's son?",
      "prediction": "The answer is:  The name of Gadhafi's son.",
      "ground_truths": [
        "Mutassim,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d05c6f2b8364971bb5f58d40f42a5a7",
      "question": "On which day of the week did Gadhafi die?",
      "prediction": "The answer is: On Sunday.",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6011e253fa9a4402bf031d32ad510952",
      "question": "Doctors have not revealed additional details about their what?",
      "prediction": "The provided text does not contain any additional information to answer the question.",
      "ground_truths": [
        "findings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7ad336fd9164278b9f5af9c66bc17ff",
      "question": "Gadhafi was killed on which day?",
      "prediction": "The answer is: the autopsy of Moammar Gadhafi on Sunday, but the state of the bodies is unknown.",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a8b3bfa1d284b16a9bf7a36cf5c92f1",
      "question": "Who was Hulk Hogan married to?",
      "prediction": "The answer is: **Linda Hogan**.",
      "ground_truths": [
        "Linda"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "973faaebcd914fb4847b7e6a3d89a8d6",
      "question": "Which publication did the interview appear in?",
      "prediction": "The answer to the question is: The New York Times.",
      "ground_truths": [
        "Rolling Stone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49732a926aa84e26ada40a2ae4ed10ee",
      "question": "What was Hogans age?",
      "prediction": "Answer: Terry Bollea is 49 years old.",
      "ground_truths": [
        "49,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "327578d5238c4a138fa53efc5155622e",
      "question": "What magazine did Hogan talk to?",
      "prediction": "The magazine that Hogan talked to was Rolling Stone.",
      "ground_truths": [
        "Rolling Stone"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d544bfc71bbd45648168ab5b4c40a407",
      "question": "Who is in a bitter divorce?",
      "prediction": "The answer is Hulk Hogan.",
      "ground_truths": [
        "Hulk Hogan"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "cd6f21280a094412800c4f004572d539",
      "question": "What statement amounts to a death threat?",
      "prediction": "The answer is, \"Weighing all options necessary to protect his client.\"",
      "ground_truths": [
        "The pro wrestling legend said Wednesday that he \"took the high road\" and \"didn't do the O.J. Simpson thing\" despite the pain of his bitter divorce fight with wife Linda."
      ],
      "em": 0,
      "f1": 0.05555555555555555
    },
    {
      "id": "a518c99b699b46fd8ea244c26187de18",
      "question": "Was the Hogans' divorce amicable?",
      "prediction": "The Hogans' divorce is described as \"not condones the O.J. situation.\"",
      "ground_truths": [
        "bitter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90abd77cf12642bf8e438280c5706070",
      "question": "Who had a divorce?",
      "prediction": "The answer is: Linda Hogan.",
      "ground_truths": [
        "Linda and Hulk Hogan"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "df99b6074900447d958fc5c3648ba2b9",
      "question": "What may be telling people about you?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "cars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e15d3c388ed140f4ae8a2a90098fd9d4",
      "question": "Who would drive a Prius?",
      "prediction": "The answer is: \"The Prius.\"",
      "ground_truths": [
        "buyers are more often women, have fewer kids and more often have college educations.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cd727caa638247bb9753aeda19092113",
      "question": "What does a Prius tell about its driver?",
      "prediction": "The answer to the question is: \"It tells us that you love the environment and hate using fuel, and that the industry, in general, is more focused on the market, where women have less kids and more college education.\"",
      "ground_truths": [
        "shows the world that you love the environment and hate using fuel,\""
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "855125b07a3844ccadbc58809afbec07",
      "question": "What may be telling people all about you?",
      "prediction": "The answer to the question is: \"What may be telling people all about you, it is the success of the Toyota Prius and the people in the industry.\"",
      "ground_truths": [
        "their cars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f37d6e92ba9b4d0caa75d393f6b4ff09",
      "question": "Does the model of car you drive say something about your personality?",
      "prediction": "The answer is: Toyota Camry.",
      "ground_truths": [
        "people have chosen their rides based on what their"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "768c1a7348494f9d8a6501fd8bc2107f",
      "question": "What is a typical Corvette driver?",
      "prediction": "The answer is, \"A Toyota Camry.\"",
      "ground_truths": [
        "older than the industry average,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d3fb745f01b74425ac0fb8568b472006",
      "question": "What does a Prius say about it's driver?",
      "prediction": "AOL Autos.",
      "ground_truths": [
        "shows the world that you love the environment and hate using fuel,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c16ade387482433dac37b23b76d5eefc",
      "question": "Who did the Foreign Ministry describe as \"mentally deranged\"?",
      "prediction": "The answer is: \"a political charlatan who does not know where to stand, a mere puppet and a guy with a poor knowledge of history.\"",
      "ground_truths": [
        "U.S. Vice President Dick Cheney"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e486c5209904438a99a52fa1e7b0b1cc",
      "question": "What did the paper describe Bush as?",
      "prediction": "The paper described Bush as a \"chicken soaked in the rain,\" a \"funny lady,\" and a \"half-baked man in terms of morality and a philistine,\" and even a \"human butcher and fascist tyrant who puts an ogre to shame,\" referencing the \"blood spilled by so many people\" and the \"human butcher and fascist tyrant\"'s hands.",
      "ground_truths": [
        "\"A chicken soaked in the rain,\""
      ],
      "em": 0,
      "f1": 0.163265306122449
    },
    {
      "id": "65366b90dc6f45ceaafab48e4ee736d1",
      "question": "what paper called the president incompetant",
      "prediction": "The answer is: \"A political charlatan who does not know where to stand,\" and the word \"wicked\" is used to describe the president.",
      "ground_truths": [
        "Rodong Sinmun"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5165d2b3476341ab8e2205cafdce0110",
      "question": "Does the Foreign Ministry consider Dick Cheney to be mentally competent?",
      "prediction": "The Foreign Ministry's Foreign Office does not consider Dick Cheney mentally competent, as he is a political charlatan.",
      "ground_truths": [
        "\"mentally deranged person"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "f330e981d0f24684b5ebf162b114097c",
      "question": "who launches personal attack",
      "prediction": "The answer directly addresses the context provided.",
      "ground_truths": [
        "Korea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "320c4a080c3d4d3aabcdb8a9e5a161bc",
      "question": "What is the name of Hillary Clinton's job in the government?",
      "prediction": "The answer is \"Chall Clinton.\"",
      "ground_truths": [
        "U.S. Secretary of State"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b7695dac18f9424988da7bcfc565aaa1",
      "question": "Whom did N. Korea launch a personal attack on?",
      "prediction": "The answer is \"U.S. Secretary of State Hillary Clinton,\" because the KCNA news agency reported that the North Korean Foreign Ministry spokesman fired Clinton at a meeting of Southeast Asian nations in Phuket, Thailand.",
      "ground_truths": [
        "Clinton,"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "7d3a612ae4ba46b4be6b981fa3999a12",
      "question": "What types of troops will be deployed?",
      "prediction": "The answer is: \"at the fastest pace possible.\"",
      "ground_truths": [
        "30,000 additional"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91fa1c21d7e741eaba1f5dcb5ca323f9",
      "question": "What did the president say the goals were",
      "prediction": "The president stated that the deployment of 30,000 additional U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.",
      "ground_truths": [
        "part of a strategy to reverse the Taliban's momentum and stabilize the country's government."
      ],
      "em": 0,
      "f1": 0.6470588235294118
    },
    {
      "id": "6204fa51927744d486eaf1f3486ecec3",
      "question": "What did Obama say about troops?",
      "prediction": "At the U.S. Military Academy, Obama stated that the deployment of 30,000 additional U.S. troops to Afghanistan is a strategy to reverse the Taliban's momentum and stabilize the country's government.",
      "ground_truths": [
        "to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0bd15b16540c4322bfbde43fe61c0126",
      "question": "How many troops will be deployed?",
      "prediction": "The answer is 30,000.",
      "ground_truths": [
        "30,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0f9bb1d77bcd4bddb36982ac4a71ac63",
      "question": "What did the president say about goals?",
      "prediction": "The president said that the deployment of 30,000 U.S. troops to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.",
      "ground_truths": [
        "withdrawing most U.S. forces by the end of his current term,"
      ],
      "em": 0,
      "f1": 0.12500000000000003
    },
    {
      "id": "700d26f635674641bd9de55bad1ac975",
      "question": "Who will the troops fight while out on deployment?",
      "prediction": "The answer is the second increase of U.S. forces in the war-torn Islamic country.",
      "ground_truths": [
        "Taliban"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "abfb70e6902b46239b124dba7b1f52a7",
      "question": "Take a drive back where?",
      "prediction": "The answer is \"A Look Back,\" and it provides a concise summary of the previous, almost verbatim, text:",
      "ground_truths": [
        "in time"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe42a6cc7a4a4cc1a489093e09b26540",
      "question": "Who is concerned with security?",
      "prediction": "Based on the context, the answer is Carl Azuz, the Jackson University School News' anchor. The question is asked to Carl Azuz.",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65bfa2010969404a92b8a132ddee9c02",
      "question": "Who has cybersecurity concerns?",
      "prediction": "The answer is: *The Watergate scandal, which was a major piece of legislation, resulted in the company's bankruptcy. Christine Romans has been looking back at the history of GM and the American car industry, and recognizes the importance of the Watergate scandal in driving the nation's economic power. The Watergate scandal, a challenge to the company's success, played a crucial role in the ensuing investigation and ultimately, the bankruptcy of GM, leading to a significant loss in the stock price of the company, which was subsequently reduced to 19 percent of U.S. auto sales. The car industry was also influenced by the growth of",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6af5094254834df995e52a0779f96097",
      "question": "What do they call the campus of the school?",
      "prediction": "The answer is \"the campus of a school that's a model of sustainability.\"",
      "ground_truths": [
        "a model of sustainability."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3f2f51019a2c490b9fa0950fe702d3ae",
      "question": "Who's wheels came off?",
      "prediction": "The answer is Carl Azuz, the Jackson (CNN) student news anchor.",
      "ground_truths": [
        "General Motors'"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1c8f7e13689440aa1a4408dd5ea3641",
      "question": "Where was the photo donated?",
      "prediction": "The answer is: The photo was donated to the U.S. Holocaust Memorial Museum, which is a former donation of Jim Martin's belongings.",
      "ground_truths": [
        "U.S. Holocaust Memorial Museum"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "268aaf83d92e42b490cc7f29dacd4167",
      "question": "Where was the slave camp located?",
      "prediction": "The answer is: At Berga, Georgia.",
      "ground_truths": [
        "Berga an der Elster"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "45207b6ffb714233925df562354b1781",
      "question": "What is the new photo showing?",
      "prediction": "The new photo is a jarring image that shows Nazi Party members digging up graves of American soldiers holding U.S. soldiers, shovels in hand, digging up mass graves of U.S. soldiers.",
      "ground_truths": [
        "\"Nazi Party members digging up American bodies at Berga.\""
      ],
      "em": 0,
      "f1": 0.31578947368421056
    },
    {
      "id": "4b930ffbea3e483eb80725e988a583a7",
      "question": "Who was held in the camp?",
      "prediction": "The answer is \"Berga,\" a slave labor camp in Georgia, where 450 U.S. soldiers were beaten, starved, and forced to work in tunnels.",
      "ground_truths": [
        "American soldiers"
      ],
      "em": 0,
      "f1": 0.08695652173913042
    },
    {
      "id": "d010c18153064c8bb5bf99903ef5d63b",
      "question": "Who is holding Captain Phillips hostage?",
      "prediction": "The answer is: The U.S. Navy.",
      "ground_truths": [
        "pirates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd0437e47bee4d22972d1de07917139d",
      "question": "What did Gen. David Petraeus say?",
      "prediction": "The answer is: \"We are encouraged that most of the crew is safe. They have been resilient and courageous throughout this crisis.\"",
      "ground_truths": [
        "that the Bainbridge would be getting backup shortly."
      ],
      "em": 0,
      "f1": 0.07407407407407408
    },
    {
      "id": "7b08901fb3d74143840d29a608e4deac",
      "question": "Where did Maersk Alabama leave?",
      "prediction": "The answer is confirmed by the Maersk Alabama. The answer also contains a news statement from a U.S. Navy soldier, Kevin Speers, who stated that the crew had recovered the ship and were confident in their command's recovery. The statement also mentions the \"safe return of all parties and the opportunity for a full de-briefing\" as a critical point for the Navy's focus. The U.S. Navy requested the FBI's help, and the FBI's negotiators with the Bainbridge confirmed the captain's retreat and the ship's damage. Finally, the answer is confirmed by the U.",
      "ground_truths": [
        "Horn of Africa."
      ],
      "em": 0,
      "f1": 0.026666666666666665
    },
    {
      "id": "56cd3c16bce94b719b287ac4575ae645",
      "question": "Who left the scene with armed detail?",
      "prediction": "The answer is \"the U.S. Navy, which is in charge of the situation.\"",
      "ground_truths": [
        "the Maersk Alabama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ee9c1b4ff684e4c9a38415448a74952",
      "question": "Which agency is involved in negotiations with pirates holding Capt.Phillips?",
      "prediction": "The answer is the U.S. Navy, which is in charge of the situation.",
      "ground_truths": [
        "FBI"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d938320190741ec88254807ab655077",
      "question": "Who assisted in negotiations?",
      "prediction": "Based on the context, the answer is: \"the FBI. The FBI negotiators in the United States are in touch with the crew of the Bainbridge, which arrived on the scene earlier Thursday to assist, a senior U.S. defense official said.",
      "ground_truths": [
        "FBI."
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "2cf2f676009a42e18873dd7470397945",
      "question": "Were the protests violent?",
      "prediction": "The provided answer is simply \"Yes, the protests were violent.\"",
      "ground_truths": [
        "killed and 90 wounded"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2cb7794cd52940ddb5359060714f07bc",
      "question": "What was the cause of the deaths?",
      "prediction": "The causes of the deaths were attributed to the Coptic Church in the town of Helwan province, which was killed on New Year's Day.",
      "ground_truths": [
        "clashes between Coptic Christians"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "8c4728a5bb874d90b7d5dccf8500dc29",
      "question": "Who is launching an investigation?",
      "prediction": "The question asks who is launching an investigation.",
      "ground_truths": [
        "Egyptian military"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a82c8d581d8048049d215d9fb873f5b0",
      "question": "Copts protest what?",
      "prediction": "The question asks Copts to protest what.",
      "ground_truths": [
        "last week's burning of a church."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "751d7a50e0b94fd89ba99fc6210f2a4b",
      "question": "How many were wounded?",
      "prediction": "The number of wounded is at nine.",
      "ground_truths": [
        "90"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fff2f61569ca4862b0a408c0515a441b",
      "question": "Who burnt the church?",
      "prediction": "The question asks for the name of the church burned in Cairo.",
      "ground_truths": [
        "Palestinian Islamic Army,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4d67ed2995b40b58c85257f233d1f87",
      "question": "Who denies it spy agence helped plan bombing?",
      "prediction": "The United States believes some members of Pakistan's spy service provided support for the deadly bombing of India's Embassy in Kabul, a U.S. counterterrorism official told CNN.",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44fa4d16c1084a748db47393708efe30",
      "question": "Who do India say is involved?",
      "prediction": "The answer is India. The context states that India's intelligence does not say which member of Pakistan's spy service provided support for the deadly bombing.",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02dab2bb8ea24f2d8796223f83f28014",
      "question": "Which country's agents where involved in the attack?",
      "prediction": "The United States believes that some members of Pakistan's spy service provided support for the deadly bombing of the India's Embassy in Kabul, a U.S. counterterrorism official told CNN.",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "9a5df0f6413c4e8e91fed29d8279c438",
      "question": "What was the number killed in the bombing?",
      "prediction": "The United States believes that some members of Pakistan's spy service provided support for the deadly bombing of India's Embassy in Kabul.",
      "ground_truths": [
        "58"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d4ffb819b5545cf8e728c62d91ba641",
      "question": "Who said this was an 'effort to malign the ISI'?",
      "prediction": "The answer is \"A top CIA official, Deputy Director Stephen Kappes,\" who stated, \"The evidence was presented to senior Pakistani officials by a top CIA official, Deputy Director Stephen Kappes. The intelligence was then reported to senior Pakistani officials by a top CIA official, Deputy Director Stephen Kappes, who was then reported to the New York Times.\"",
      "ground_truths": [
        "Pakistani Maj. Gen. Athar Abbas"
      ],
      "em": 0,
      "f1": 0.03636363636363636
    },
    {
      "id": "d838b4a128d648ec8beda32fd8a4e19d",
      "question": "Who does U.S. intelligence point to in attack?",
      "prediction": "The answer is, the U.S. intelligence point to some unspecified aid, from Pakistan's Directorate of Inter-Services Intelligence, or ISI.",
      "ground_truths": [
        "some members of Pakistan's spy service"
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "0a3b187e277c41cca2bd6c9592f86150",
      "question": "Who accused Pakistan in a role in the attack?",
      "prediction": "The answer is, \"The Indian Embassy in Kabul was badly damaged in the July 7 bombing.\"",
      "ground_truths": [
        "Afghanistan and India"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29405d332ddb4385830e20d84df5f50b",
      "question": "What did Maj. Gen Athar Abbas say about the report?",
      "prediction": "The United States believes that some members of Pakistan's spy service provided support for the deadly bombing of India's Embassy in Kabul, a U.S. counterterrorism official told CNN.",
      "ground_truths": [
        "\"unfounded and malicious\" and an \"effort to malign the ISI,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "155ca8b5e3a94befbeea9a3dd8a1da2e",
      "question": "How much is being spent on the clean up?",
      "prediction": "The answer is $2 billion.",
      "ground_truths": [
        "nearly $2 billion"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "27f0b55a6463487c9e33e0473e7741a4",
      "question": "what is the cost of the clean up",
      "prediction": "The answer is approximately $2 billion.",
      "ground_truths": [
        "$2 billion"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "e05be86a52234d69a86ae37a4fd9a600",
      "question": "Where is the nuclear site ?",
      "prediction": "The answer is, Hanford Nuclear Site, Washington (CNN).",
      "ground_truths": [
        "Washington"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c3b4678773dc4e6c89054b908d400a1a",
      "question": "What does stimulus fund?",
      "prediction": "The answer is: The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site.",
      "ground_truths": [
        "clean up Washington State's decommissioned Hanford nuclear site,"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "46a118e8fbeb4526b7959d52324ffc5f",
      "question": "what is being cleaned up",
      "prediction": "The answer is dedicated to the Hanford nuclear site, Washington State, in a virtual ghost town.",
      "ground_truths": [
        "Washington State's decommissioned Hanford nuclear site,"
      ],
      "em": 0,
      "f1": 0.42105263157894735
    },
    {
      "id": "7876bfdb1fac411cb32f3b0837a064cf",
      "question": "What said about Hanford?",
      "prediction": "The answer is, Hanford Nuclear Site, Washington (CNN)",
      "ground_truths": [
        "\"Hanford is getting more money than many states in stimulus funds"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "4bffec9430e947afa5d74a1cff1e9509",
      "question": "Who says that the company has been restructuring over the last three years?",
      "prediction": "The answer is: **Chetry:**  Chetry says that \"we're going to continue working through that, and we don't expect to take taxpayer assistance from the emergency funds.\"",
      "ground_truths": [
        "Mark Fields,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d62d29c3780648ef890e64b2b625606d",
      "question": "Who says the retail market share has increased?",
      "prediction": "The answer from Ford is: \"We're in a different position than some of our competitors, and we're not seeking emergency taxpayer assistance. We're focusing on decisive action to streamline our business, but more importantly, we're going to continue working through that.\"",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d4efa88c9db4467a961182bb950f2d66",
      "question": "Who says they are in a different position to their competitors?",
      "prediction": "The answer is: **Chetry.**",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "98e977789cfb48d5a4e69d50f1e9e353",
      "question": "How many years has Ford been restructuring ?",
      "prediction": "Here's the answer, based on the provided context:",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1dbac370639f49c4abf926c2ba542713",
      "question": "What is the biggest issue in the marketplace?",
      "prediction": "The answer to the question is: Ford's success in the car industry is driven by consumer confidence.",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "e8be52fe629c4dfdab5b8dbb9796b7c1",
      "question": "What does Field say is the biggest issue in the marketplace?W",
      "prediction": "Here's a concise answer extracted from the context:",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cba966283084dc3834906561150aeaa",
      "question": "What has retail market shares done in last few months?",
      "prediction": "The answer is: \"We have been working through the consolidation process, and we have been making progress. We have experienced a significant increase in sales in the market, and we've taken on debt and reduced our debt. We have a more successful and profitable market, with increased consumer confidence. We have new partnerships with the UAW and a plan to reduce our debt. This has made the customer base better at finding the dealership, and this has also resulted in a more favorable perception of our vehicles and the U.S. economy.\"",
      "ground_truths": [
        "up"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6db33a2cb76b41afaf584d3af4868201",
      "question": "Who says they're in a different position to competitors?",
      "prediction": "Chetry claims Ford's company is the one that's gotten by without federal money.",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2b055c5420c4ac9ad825973907d8c1e",
      "question": "What is the biggest issue in marketplace?",
      "prediction": "The answer is: Consumer confidence is the most important thing for the auto industry to win back.",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "3cb0174eaaf54d35878f92077740136c",
      "question": "Who is now the official defender of America's cup?",
      "prediction": "The official defender of America's Cup is the **Oracle**.",
      "ground_truths": [
        "Golden Gate Yacht Club of San Francisco"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "054d98807bed447f896aaa343005bb89",
      "question": "Where is the America's Cup taking place?",
      "prediction": "The America's Cup is located in San Francisco.",
      "ground_truths": [
        "off Valencia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9827d6f7626d46c188fc893ff0e12fd9",
      "question": "Who is Larry Ellison?",
      "prediction": "The answer is Larry Ellison.",
      "ground_truths": [
        "software magnate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f73b3ea836548d496ef68c08f4369f4",
      "question": "who founded BMW Oracle over 10 years ago?",
      "prediction": "BMW Oracle founded the American Yacht Club of San Francisco.",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2242d2a664df4bf1967d3689e8b069d0",
      "question": "BMW Oracle take a winning lead of what?",
      "prediction": "BMW Oracle took a winning lead of 223ft (68 meters) on their catamaran.",
      "ground_truths": [
        "America's Cup"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7889488ae4d3460bafe900d6917b650e",
      "question": "Who owns the BMW Oracle?",
      "prediction": "The answer is the BMW Oracle.",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b455ff336ebe403e896b9be3465811f7",
      "question": "Who founded BMW?",
      "prediction": "The answer is: the American Yacht Club of San Francisco.",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d09f358431284dea95dec8e61590dbf4",
      "question": "Who beat the Swiss holders?",
      "prediction": "The answer is Oracle.",
      "ground_truths": [
        "BMW"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b43ea874d5ef47208ee0d3b5f0f30717",
      "question": "Who retains fourth place as Demba Ba scores hat-trick against Blackburn?",
      "prediction": "The answer is: **Newcastle United** (as they lost to Chelsea).",
      "ground_truths": [
        "Newcastle"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "28a0dae2562544dd8a4bcab4d422b79e",
      "question": "Manchester United drop first points of English season with 1-1 draw where?",
      "prediction": "The answer is Manchester United.",
      "ground_truths": [
        "Stoke City."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad3511f19a5845b7be81022faec9f4cf",
      "question": "What place retained fourth place",
      "prediction": "The answer is **Newcastle**.",
      "ground_truths": [
        "Newcastle"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fa92fc83863d46e9a66c60b76b2efd5d",
      "question": "What person beat swansea",
      "prediction": "The answer is: Chelsea.",
      "ground_truths": [
        "Fernando Torres"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b5a018ab1b044c29c0cc5999d50c8b4",
      "question": "Who heads table on goal difference from rivals Manchester City, who beat Everton?",
      "prediction": "The answer is Manchester United.",
      "ground_truths": [
        "United's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2297ef49d5a54ad8a415426c7760398a",
      "question": "what is the final score?",
      "prediction": "The final score is 2-1 between Manchester United and Stoke City.",
      "ground_truths": [
        "1-1 draw"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a78189a634e347cd97e9f6f3d554bd47",
      "question": "Did Manchester United lose?",
      "prediction": "The answer is yes, Manchester United lost to Stoke City.",
      "ground_truths": [
        "1-1 draw"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "df9dc201bee94070a6c46e8370e25824",
      "question": "What was the Barron's nickname?",
      "prediction": "The answer is \"Cesar Duarte, president of the federal chamber of deputies.\"",
      "ground_truths": [
        "\"the Strawberry,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25eed9d207a44fd8b3f5b05f35e6f8ae",
      "question": "What does he head?",
      "prediction": "The answer is: **Cesar Duarte, president of the federal chamber of deputies.**",
      "ground_truths": [
        "\"Michoacan Family,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ed5d7f29f1104c7097ee8adf54dd3e62",
      "question": "Who did Mexico arrest?",
      "prediction": "The answer is: **Cesar Duarte, president of the federal chamber of deputies.**",
      "ground_truths": [
        "Alberto Espinoza Barron,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5737be0b4281430dbcc27d3f2f98bf95",
      "question": "What is the number of dead killed by a grenade?",
      "prediction": "According to the context, the answer is **100.**",
      "ground_truths": [
        "eight"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cf15a557be045bf86125a1ff29cf4c1",
      "question": "for what crime was he arrested",
      "prediction": "The answer is based on the context provided. The text provides information about the actions of the Mexican government, the federal chamber of deputies, and the army, and the resulting arrest of the leader of a drug cartel.",
      "ground_truths": [
        "set off two grenades during a public celebration in September,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c8d70a0e1fb4f6ab0d90a651c331467",
      "question": "What is he the head of?",
      "prediction": "The answer is Cesar Duarte, president of the federal chamber of deputies.",
      "ground_truths": [
        "\"Michoacan Family,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47c39c5a4353463dae51fb43d3c1c7ff",
      "question": "When did the last bombing happen?",
      "prediction": "Based on the provided text, the answer is: **At least 33 people were killed and 20 wounded.**",
      "ground_truths": [
        "The blast follows another huge attack on Sunday,"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "66ef940edd3045d3a03ea5dd7e476bac",
      "question": "What did IRAQI Pm urge nation's sheikhs to do?",
      "prediction": "The Iraqis advised the nation's sheikhs to do the following:",
      "ground_truths": [
        "participate in Iraq's government."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fea7a36c699a4cd2a68f4049e19f9ee6",
      "question": "Who were attending a conference during the attack?",
      "prediction": "The answer is Abu Ghraib, the Iraqi government's attempt to foster national reconciliation between religious and ethnic groups.",
      "ground_truths": [
        "Sunni Arab and Shiite tribal leaders"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "1e30d3bec60e42f9a781558824495985",
      "question": "What is the number of dead?",
      "prediction": "The answer is 33.",
      "ground_truths": [
        "33"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "972169de02914a8b8afb656a20579a99",
      "question": "How many were killed in the suicide bombing?",
      "prediction": "The Iraq (CNN)",
      "ground_truths": [
        "At least 33 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3c8e1d09154b4b74873b9888b20d6e6f",
      "question": "When did the attack occur?",
      "prediction": "The answer is:  The attack occurred on Sunday, December 1, 2016.",
      "ground_truths": [
        "Sunday's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b43f21b5f3ff4cb39f781974666b0aaf",
      "question": "In what manner did they die?",
      "prediction": "The answer is **6**.",
      "ground_truths": [
        "suicide car bombing"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f1309e8046b49e1836ea7217aff694d",
      "question": "How many were killed in Baghdad suicide bombing?",
      "prediction": "The answer is 33.",
      "ground_truths": [
        "At least 33 people"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "7992ef8a5e8f4d9da5a3f75f157136a0",
      "question": "where will obama hold meeting?",
      "prediction": "The answer is in China.",
      "ground_truths": [
        "Shanghai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5544aab55fe744fab13706c13fcb4f96",
      "question": "What is the name of President Obama's half brother?",
      "prediction": "The answer is Ndesandjo.",
      "ground_truths": [
        "Mark"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe94d55d37024a71afeebcce19c9779c",
      "question": "how long is obamas trip?",
      "prediction": "The answer is, based on the text, the answer is: 8-days.",
      "ground_truths": [
        "eight-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "490da89fb4c743858167b5b75af14ddd",
      "question": "What countries is Obama visiting?",
      "prediction": "The answer is China.",
      "ground_truths": [
        "Japan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fe12d7bff7a46d79f76ccc655caf66c",
      "question": "Who lives in China",
      "prediction": "The answer is **China**.",
      "ground_truths": [
        "Hu Jintao."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0338d7711b1646d6be2b815c279cff54",
      "question": "Where will President Obama hold town hall-style meeting?",
      "prediction": "The answer is Beijing.",
      "ground_truths": [
        "Shanghai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb08cc75e99a4ec9807321f51dd40566",
      "question": "How many days long is President Obama's trip?",
      "prediction": "The answer is 11.",
      "ground_truths": [
        "eight-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "811785fb9d684ccd9577541cd4c94555",
      "question": "where does mark obama live?",
      "prediction": "The answer is China. The text states that Barack Obama is expected to meet with the Shanghai mayor and hold a town hall-style meeting. Also, it mentions that \"future Chinese leaders\" are discussed, and that the United States imports Chinese products and is a major economic player in the global market.",
      "ground_truths": [
        "Shenzhen in southern China."
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "dcfcb5a3789141ab83b5589c4af05105",
      "question": "Who are being paied tribute to ?",
      "prediction": "The answer is \"The world, and the future, as we learn from the wars of the past.\"",
      "ground_truths": [
        "military veterans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c410a3904061469597449e8e38019436",
      "question": "Which world leaders attended D-Day tribute?",
      "prediction": "The answer is France.",
      "ground_truths": [
        "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1b2349ba351429bb9601c27e5afb4eb",
      "question": "Who paid tribute to D-Day fallen?",
      "prediction": "The answer is \"You remind us that in the end, human destiny is not determined by forces beyond our control. We must be as if liberators for our day and our generation too,\" he said.",
      "ground_truths": [
        "World leaders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65f9fc6a0c8442eab8509181524c2b37",
      "question": "When did D-Day operations occur?",
      "prediction": "The context provides the following information:",
      "ground_truths": [
        "June 6, 1944,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "07e2bccb50e64fe5b550ffaa6a34733b",
      "question": "How many allied troops took part in D-Day ?",
      "prediction": "The context provided does not contain the answer to the question. The answer is directly from the context.",
      "ground_truths": [
        "More than 150,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "31df9cfaf26e42e988ae21e6ccad7ddc",
      "question": "How many allied troops took part in D-Day operations?",
      "prediction": "The answer is 150,000.",
      "ground_truths": [
        "More than 150,000"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "d1e55b947f6241c18e818b671aa800a4",
      "question": "When did the operation take part?",
      "prediction": "The answer is: \"It has always been up to us,\"",
      "ground_truths": [
        "June 6, 1944,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc6b46e7ec704ef283cb759cc4aef9c1",
      "question": "Who attended the tribute?",
      "prediction": "The answer is \"86-year-old former British soldier Jim Tuckwell.\"",
      "ground_truths": [
        "President Obama and Britain's Prince Charles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "822b186317c04e1db6315150718e194e",
      "question": "Who teamed up?",
      "prediction": "The answer is the \"Learjet 60 XR,\" a 110-strong, fuel-efficient mid-size aircraft in Europe, which is known for its long flight times and fuel efficiency.",
      "ground_truths": [
        "ClimateCare,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4189184928d9434294d6708f3ef5bae2",
      "question": "Which industry was unfairly criticized?",
      "prediction": "The incorrect answer is:",
      "ground_truths": [
        "The aviation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8821fe2537b8468c9e90ab7b6dde661f",
      "question": "Who teamed up with Climate Care?",
      "prediction": "The answer is the \"Learjet 60 XR,\" a fuel-efficient mid-size aircraft.",
      "ground_truths": [
        "Jet Republic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "417e56a4601b4d81aaaef763b97c99e5",
      "question": "What is aviation industry responsible for?",
      "prediction": "The aviation industry is responsible for what?",
      "ground_truths": [
        "around 3.5 percent of global greenhouse emissions."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c51a684309aa401cb036c5730dab58af",
      "question": "What percent of global carbon emissions is the airline industry responsible for?",
      "prediction": "The airline industry is responsible for **15% of global carbon emissions**.",
      "ground_truths": [
        "3.5"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4acfdfd9af1f459f9a0e34526b3bd68c",
      "question": "whom did they team up with",
      "prediction": "The team with whom the CEO of Jet Republic collaborated was Jonathan Breeze.",
      "ground_truths": [
        "ClimateCare, one of Europe's most experienced providers of carbon offsets,"
      ],
      "em": 0,
      "f1": 0.09523809523809525
    },
    {
      "id": "328dfae8a432499e91c71bad92e2bd79",
      "question": "How many times did Francesco Totti score",
      "prediction": "The answer is: **FC Milan scored nine goals in the last six matches.**",
      "ground_truths": [
        "twice."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5e7dc56ca2e4d68bc9b59696bca7f75",
      "question": "What did Inzaghi do?",
      "prediction": "Inzaghi did not contribute to scoring in the final minutes of the San Siro's goals.",
      "ground_truths": [
        "scored a hat-trick"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ead58bae18d04b9c89be6f0bbe3f155f",
      "question": "Who scored twice?",
      "prediction": "The answer is **Kwadwo Asamoah**.",
      "ground_truths": [
        "captain Francesco Totti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c452c1a025c44bdab1694894c2799e64",
      "question": "Did AS Roma beat Lecce?",
      "prediction": "The answer is yes, AS Roma defeated Lecce 3-2 in the final minute of the game.",
      "ground_truths": [
        "AS"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "8be65f22a2a0454eaea4506b06382027",
      "question": "Who continues a impressive scoring streak?",
      "prediction": "FC Milan's continuous goal scoring streak, which saw them score 9 in the last six matches, is described as a \"combative combination with England international David Beckham\" which electrified the Milan fans.",
      "ground_truths": [
        "Filippo Inzaghi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "996e6a4534b049b080d457235def719d",
      "question": "Where was the victim found?",
      "prediction": "The answer is Fort Bragg, North Carolina.",
      "ground_truths": [
        "in a motel,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a79bf4bbfee412d943b4c1c240dac5b",
      "question": "What are authorities in North Carolina investigating?",
      "prediction": "Authorities in North Carolina are investigating the death of a pregnant soldier, Megan Lynn Touma, who was assigned to the 19th Replacement Company at Fort Bragg in Fayetteville, North Carolina.",
      "ground_truths": [
        "the death of a pregnant soldier"
      ],
      "em": 0,
      "f1": 0.25806451612903225
    },
    {
      "id": "677c4e29d5d84e6c983c87988ba82274",
      "question": "Who was the pregnant service member?",
      "prediction": "The answer is Fayetteville Police.",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b849fa3ccdd941a2a81fab71b34644a2",
      "question": "Who was the victim?",
      "prediction": "The final answer is \"The victims of the death of a pregnant soldier in North Carolina.\"",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dbab094264f6482bb5dda2381fff9847",
      "question": "Who is investigating the death?",
      "prediction": "Fayetteville police are investigating the death of a pregnant soldier in Fayetteville, North Carolina.",
      "ground_truths": [
        "in Fayetteville, North Carolina,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5dd21e68a59d43b7b88e561d92bd7f3f",
      "question": "Was it natural causes or murder?",
      "prediction": "The answer is, there is no definitive answer. The context does not provide the direct answer to the question. The police said the cause of death was determined by the State Medical Examiner, and the death was due to a strong odor, not a natural cause.",
      "ground_truths": [
        "Laurean killed Lauterbach"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d58e98347651493d99ad62e97fbe7902",
      "question": "Who was found dead?",
      "prediction": "Fayetteville, North Carolina, found the body dead in a motel in Fayetteville, North Carolina.",
      "ground_truths": [
        "pregnant soldier"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "34856d67c1e14740a8629203fdae72e7",
      "question": "What are authorities investigating?",
      "prediction": "Authorities investigating the death of a pregnant soldier in Fayetteville, North Carolina, with the conclusion of a 19-year-old dental specialist from Cold Springs, Kentucky, being assigned to the 19th Replacement Company.",
      "ground_truths": [
        "the death of a pregnant soldier"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "4b718b75f3834c16bc4c74ff15d7e341",
      "question": "What happened to the pregnant soldier?",
      "prediction": "The answer is \"Touma was the second pregnant service member to die in North Carolina in recent months. The remains of Lance Cpl. Maria Lauterbach and her fetus were found beneath in a fire pit in Marine Cpl. Cesar Laurean's backyard on January 11, 2007, on a roadblock set up by a local anti-kidnapping task force investigating another case.",
      "ground_truths": [
        "death"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d301d0914b9847e892d259e8428b1fb4",
      "question": "Who was second N.C.-based pregnant solider found dead?",
      "prediction": "The second N.C.-based pregnant solider found dead was **Laurean Cpl. Maria Lauterbach**.",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53c64df9090c45cb80df1707e3276239",
      "question": "Who was found dead in motel room?",
      "prediction": "The final answer is **Fayetteville, North Carolina.**",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd551fb2f84a4f4eac4416b396eeae9a",
      "question": "where did she grow up",
      "prediction": "The answer is Sotomayor's birth in New York, New York, and her graduation in 1972.",
      "ground_truths": [
        "Bronx."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d575bcc01562419b8b038b257da7fad0",
      "question": "what High school classmate said she saw Sotomayor?",
      "prediction": "The answer is: Sonia Sotomayor, who was nominated for the Supreme Court by President Obama.",
      "ground_truths": [
        "Mary Procidano,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba86dc3a6b50472e8daa169f2eef0c51",
      "question": "what did the classmate say",
      "prediction": "The answer is \"Only then, Sonia would be allowed to go out,\" which translates to \"Only then, she would be allowed to go out.\"",
      "ground_truths": [
        "\"She was focused so much on learning that she didn't notice,\""
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "166f4f7cb93048bc8d6af0661c0aba69",
      "question": "who died when she was 9",
      "prediction": "The answer is Sonia Sotomayor, 55.",
      "ground_truths": [
        "Juan Sotomayor, Sonia's father,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "98bc1ca26fc045d3a1267a0c6051d16a",
      "question": "Who is holding her confirmation hearing?",
      "prediction": "The answer is Sonia Sotomayor, 55, who was nominated for the Supreme Court by President Obama.",
      "ground_truths": [
        "Sonia Sotomayor,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5d6dc2b279594850a8ffce1f1b7892aa",
      "question": "Where is the painting displayed?",
      "prediction": "The painting is displayed at Dancy-Power Automotive in Harlem, New York.",
      "ground_truths": [
        "Dancy-Power Automotive in Harlem, New York."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "8b0fb3b8cfee4daba5b93311dbe5c5b5",
      "question": "The painting shows jackson holding what?",
      "prediction": "The painting shows Jackson sitting in Renaissance-era clothes and holding a book, in Harlem, New York, and is sold for $2.1 million in 1990.",
      "ground_truths": [
        "a book."
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "f6b7de0428ac4bf18fe4707e7f59d047",
      "question": "which is close to Apollo Theater?",
      "prediction": "The answer is \"The painting on display in Harlem belongs to Marty Abrams, a friend and customer of the owners of the high-profile dealership.",
      "ground_truths": [
        "the showroom"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "733d2fd24b0b40f7ba5f0edaed20cca9",
      "question": "What does the painting show show?",
      "prediction": "The painting shows Jackson sitting in Renaissance-era clothes and holding a book, which is shown in the Dancy-Power Automotive showroom in Harlem, New York.",
      "ground_truths": [
        "Jackson sitting in Renaissance-era clothes and holding a book."
      ],
      "em": 0,
      "f1": 0.5517241379310345
    },
    {
      "id": "a697c765c3e74e0cb48088ea3ecdcdb1",
      "question": "What type of painting did michael jackson sit for?",
      "prediction": "The answer is \"The Book.\"",
      "ground_truths": [
        "oil"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f87d6a314604c62aa83466f74fdb86a",
      "question": "What is the only painting Jackson sat for?",
      "prediction": "The only painting Jackson sat for is \"The Book\" in the Harlem luxury car dealership in New York.",
      "ground_truths": [
        "\"The Book\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "82eb7d4504834ceabad83742c2d0ab5c",
      "question": "What kind of craft is Earthrace?",
      "prediction": "Earthrace is a green powerboat on the market for $1.5 million.",
      "ground_truths": [
        "green powerboat"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "8809bd64552f4f7aa8f53f493602ceae",
      "question": "How much is Bethune selling Earthrace for?",
      "prediction": "The answer is $1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b7e11924f8a34063ba5445261471e552",
      "question": "Where is trimaran currently touring?",
      "prediction": "The current tour of Earthrace is currently on the market for $1.5 million, with the car being sold for $1.5 million.",
      "ground_truths": [
        "Australia and New Zealand"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9724aa5ceb194973a23f99d088987015",
      "question": "What is the asking price for Earthrace?",
      "prediction": "The asking price for Earthrace is $1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "7876cd87c6e845f484f841d243a021a0",
      "question": "What does Earthrace hold the world record for?",
      "prediction": "Earthrace holds the world record for the fastest circumnavigation of the globe in a powerboat in 100 per cent renewable bio diesel.",
      "ground_truths": [
        "fastest"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "68d323c49f9740abbda05876aebfd5d7",
      "question": "How much is Earthrace selling for?",
      "prediction": "The answer is $1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8be91aa0873e4a4bb89703628089432a",
      "question": "Who owns Earthrace?",
      "prediction": "The owner of Earthrace, New Zealander Pete Bethune, owns the boat.",
      "ground_truths": [
        "New Zealander Pete Bethune,"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "361d3843c29e48cb89a94b00c8ff9017",
      "question": "which party the president belongs",
      "prediction": "The president belongs to the **left-wing party** of Brazil.",
      "ground_truths": [
        "Workers'"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2ff8a0d87c24abeae1d7612663751d0",
      "question": "who says \"the greatest gift that a president could have\"?",
      "prediction": "The answer is, \"to host the Olympic Games in Rio de Janeiro, as a former autoworker, former union organizer, and a man who fought for the underprivileged, to host the 31st Olympic Games in Rio de Janeiro, is the greatest gift a president can have.\"",
      "ground_truths": [
        "to host the Olympic Games in Rio de Janeiro."
      ],
      "em": 0,
      "f1": 0.35555555555555557
    },
    {
      "id": "157c103f7f3942c8be2ee84825ff7f32",
      "question": "What kind of heart issues does Elizabeth Taylor have?",
      "prediction": "Elizabeth Taylor has a \"procedure on her heart,\" she stated this Tuesday.",
      "ground_truths": [
        "leaky valve"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ebee9fdad5b64bab83267a364ea4d10b",
      "question": "what did taylor announce",
      "prediction": "Taylor announced she had a \"procedure on her heart,\" and she did not go into further details about the procedure. She also stated she was hospitalized for \"scheduled testing\" in summer. She also posted \"sore, but intact\" to her Twitter account.",
      "ground_truths": [
        "is having a \"procedure on her heart,\""
      ],
      "em": 0,
      "f1": 0.17777777777777778
    },
    {
      "id": "3e1bed5b98f248738744f4c55f0e6d06",
      "question": "did she go into details",
      "prediction": "Yes, the answer is yes.",
      "ground_truths": [
        "not"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec9dedf313ba4f90883a2b01909553b3",
      "question": "for what did she use twitter",
      "prediction": "\"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better,\" the actress wrote.",
      "ground_truths": [
        "to share personal information."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0984831a98b8496a96e880939f6f77b9",
      "question": "On what organ in the body was Ms Taylor having issues with",
      "prediction": "The answer is **\"sore, but intact\"**",
      "ground_truths": [
        "heart,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "edb3d5b64911443798e8e83b07ae5b99",
      "question": "For what reason did Elizabeth Taylor take to twitter",
      "prediction": "For what reason did Elizabeth Taylor take to Twitter?",
      "ground_truths": [
        "\"procedure on her heart,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d51096337128469da3308f102dbbb0b6",
      "question": "Who is Elizabeth Taylor?",
      "prediction": "Elizabeth Taylor is an 77-year-old English-born Hollywood actress who, as a wheelchair-assisted woman, has made significant advancements in her health and well-being by using Twitter to share personal information.",
      "ground_truths": [
        "actress"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "d16db94dc08840cb930ce9cf59151bef",
      "question": "Elizabth Taylor used twitter as to talk about how she cope after the death of whom",
      "prediction": "The answer is \"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better,\" with no further details about her heart condition.",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6bb51d8500504ea3bec2a62487e4ceb1",
      "question": "Where was the ship heading?",
      "prediction": "The answer is Kuwait.",
      "ground_truths": [
        "Jeddah, Saudi Arabia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f476af0b22074b3f9458d73e17415502",
      "question": "Who was headed to Saudi Arabia?",
      "prediction": "The answer is \"The European Union Naval Force.\"",
      "ground_truths": [
        "The Al Nisr Al"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c9487f1dd6a464584d6e3aeeb5476cd",
      "question": "What was hijacked?",
      "prediction": "The hijacked tanker was in Saudi Arabia, with a crew of 14.",
      "ground_truths": [
        "a tanker"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "0a255f0d0a964a13914f508a9a99f50c",
      "question": "Where is the ship being held?",
      "prediction": "The ship is being held in Garacad, Somalia.",
      "ground_truths": [
        "is near Garacad, Somalia,"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "1f851796112743eb9929ce58de7b2b38",
      "question": "Where is it being held?",
      "prediction": "The answer is **Saudi Arabia**.",
      "ground_truths": [
        "near Garacad, Somalia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "691ed5c57db64cdbb7b5ec4d1f71e14c",
      "question": "Where was the vessel hijacked?",
      "prediction": "The answer is provided in the context.",
      "ground_truths": [
        "Gulf of Aden,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "254f6546961e4eddb55cf96fa4810f9d",
      "question": "What is the name of the vessel?",
      "prediction": "The name of the vessel is the Al Nisr Al Saudi.",
      "ground_truths": [
        "The Al Nisr Al Saudi"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "34b9e47ee9754759ab352055e5ec6a0b",
      "question": "Who says Muslims worldwide have shared concern about key issues?",
      "prediction": "The answer is: \"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"",
      "ground_truths": [
        "Nafees Syed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11f7550099824a199a4b8396d0516ced",
      "question": "What did Obama's interview with Al-Arabiya talk of?",
      "prediction": "The answer to the question is: Obama's interview with Al-Arabiya focused on the challenge and the new partnership.",
      "ground_truths": [
        "Palestinian-Israeli issue"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ed9fee4ae4648c08bfd6771e299334c",
      "question": "Who said anything about Bush?",
      "prediction": "The answer is: \"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"",
      "ground_truths": [
        "Muslim countries,"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "b72b19ddb79a4c829f3bcfcb0348f52f",
      "question": "What is Obama doing?",
      "prediction": "Based on the context, the answer is: **Abraham Obama is a leader in the Muslim world, and he is engaged in a new path with the Muslim world, based on mutual interest and respect.**",
      "ground_truths": [
        "has given the Muslim community around the world the message we have been waiting for."
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "4d88557eb7e7407d890bcaa9f3f3a2d7",
      "question": "What Nafees Syed said about Obama`s reaching to Muslim world?",
      "prediction": "Nafees Syed's answer, based on the context, would be: \"To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect.\"",
      "ground_truths": [
        "positive signal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "222248c60f334686964c020608621580",
      "question": "Who elevated them to their current seat?",
      "prediction": "The answer is **President Obama** writes: \"the most humbling honor of my life.\"",
      "ground_truths": [
        "President Clinton."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "b191b0318ac5487a930d6523e1e44bd8",
      "question": "What did Sotomayor say?",
      "prediction": "From the context, the answer is: Sotomayor said she wanted to go into law, but she ultimately chose to become a lawyer, based on her own experiences with the field.",
      "ground_truths": [
        "the nomination is the \"most humbling honor"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "413e30cf485e4d24882f2e19dce26ebf",
      "question": "What is Sotomayor's age?",
      "prediction": "The answer is 54.",
      "ground_truths": [
        "54-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8d73509690a14ba2b2bb462c23e84c2f",
      "question": "Where was Sotomayor born?",
      "prediction": "The answer is Sotomayor born in the Bronx, New York.",
      "ground_truths": [
        "the Bronx"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "e729c9b6571b45c2bda8a3d5d1be79bf",
      "question": "Where was she born?",
      "prediction": "The answer is the New York City, New York State.",
      "ground_truths": [
        "in the Bronx"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5fb0382999c4519b92a6a68c4cb0a4a",
      "question": "Where was Sonia Sotomayor born?",
      "prediction": "The answer is SNOMY S. O. T. S. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y. O. T. Y.",
      "ground_truths": [
        "in the Bronx"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c019feb626ea4fd9b8ac736b2ffab022",
      "question": "Where did he grow up?",
      "prediction": "From the context, the answer is \"New York City,\" in photos.",
      "ground_truths": [
        "in a public housing project,"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "13bfebca830044c383f83fa8444e9aa5",
      "question": "Where did the break occur?",
      "prediction": "The answer is: The break occurred at the Markland Locks and Dam in Ohio, Kentucky, on Sunday.",
      "ground_truths": [
        "Ohio River"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "6d4b273cba0d4db4a320c92747c2378c",
      "question": "used to relieve blocked river traffic",
      "prediction": "The answer is: the Ohio River is a major artery for commodities shipping, with over 50 million tons of cargo passing through it annually.",
      "ground_truths": [
        "auxiliary lock"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f309b8814d048f28caf8c256a49f914",
      "question": "On what day did the lock break?",
      "prediction": "The answer is **Sunday**.",
      "ground_truths": [
        "Sunday,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "684a53de3b9143edbe480a9fa3e05e52",
      "question": "What can outdoorsy types catch up withq",
      "prediction": "The answer is: \"Pop Up Video,\" featuring a MC series from BBC America.",
      "ground_truths": [
        "\"Hillbilly Handfishin'\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa0c393490734dffa0a70fb6cbb33a74",
      "question": "Reed Between the Lines is on what network?",
      "prediction": "This text provides a list of all the activities and food offerings at the \"Turkey Day\" parade. The list includes:",
      "ground_truths": [
        "BET"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1aad0544e7814463b5f532d2e844c63e",
      "question": "What time does the Bones marathon start?",
      "prediction": "The answer is \"Whatever it is.\"",
      "ground_truths": [
        "10 a.m.,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e6ad1c38b6e4d0684675bedea267a87",
      "question": "Hillbilly Handfishin is on what network?",
      "prediction": "The answer is \"At a cafe.\"",
      "ground_truths": [
        "Animal Planet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b75cf2ceb1c41b78b4273119abf85f3",
      "question": "When is Bones airing",
      "prediction": "The answer is \"Bones\".",
      "ground_truths": [
        "all day starting at 10 a.m.,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e0a8f8db62043568324ed921d1202ce",
      "question": "What is the original offering of",
      "prediction": "The answer is \"Pop Up Video,\" which airs back-to-back from 12:30 p.m. to 2:30 a.m. on BBC America.",
      "ground_truths": [
        "\"Reed Between the Lines\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d416fa56e7c74be78686712a6be384e8",
      "question": "who is she speaking out for?",
      "prediction": "The answer is: *Bialek, the first woman to publicly accuse Cain of sexual harassment, after last week's disclosure*.",
      "ground_truths": [
        "the other women who couldn't or wouldn't.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1496858ec1dd447d974b8d466b0a68ec",
      "question": "Who appeared on CNN's \"Piers Morgan Tonight\"?",
      "prediction": "The CNN's \"Piers Morgan Tonight\" usually hosts, and often presents, a female, and a man, who accuses Cain of groping her after a 1997 dinner.",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "43ee55e2f5b6431ba69170c42b35de1f",
      "question": "Who appeared on CNN's Piers Morgan Tonight?",
      "prediction": "The answer is: **Piers Morgan.**",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47c10c1b7bd840139c5af3e82f478a7e",
      "question": "Who has denied the allegation?",
      "prediction": "The answer is **Bialek.**",
      "ground_truths": [
        "Cain"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66a767acd3a044378348a030fef5e233",
      "question": "What did Bialek say she was doing?",
      "prediction": "Bialek said she was doing what she was prepared to do.",
      "ground_truths": [
        "came forward Monday \"for the other women who couldn't or wouldn't.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "639ff711ef014a44b92f76df744b0eed",
      "question": "What network is this show on?",
      "prediction": "The answer is \"The National Restaurant Association,\" which is the name of the network that Cain led in the late 1990s.",
      "ground_truths": [
        "CNN's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "613e8614ab454406950db15bf973c1e9",
      "question": "What television show did Sharon Bialek appear on?",
      "prediction": "Sharon Bialek appeared on the television show \"Piers Morgan Tonight.\"",
      "ground_truths": [
        "\"Piers Morgan Tonight\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9d0f923d56ae49cd9b28741a94aed6bc",
      "question": "Who has denied any allegation of sexual harassment?",
      "prediction": "The answer is: Hillary Clinton.",
      "ground_truths": [
        "Herman Cain"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "676f69de7aec48e69edc1e4dc0299a6b",
      "question": "Who works for gore's current tv",
      "prediction": "The answer is **Euna Lee**.",
      "ground_truths": [
        "Euna Lee,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9fe87fea995f4463b50822a33ee3e8df",
      "question": "What does the US not have with North Korea?",
      "prediction": "The US does not have with North Korea any diplomatic relations.",
      "ground_truths": [
        "no diplomatic relations"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "619ae1269dda477eb2232aad82ba1d5e",
      "question": "Which country does the US not have diplomatic relations with?",
      "prediction": "The answer is **North Korea**.",
      "ground_truths": [
        "North Korea,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0cb2c91e1de243f988fb1da96906374f",
      "question": "What are the names of the reporters?",
      "prediction": "The answers to the questions are:",
      "ground_truths": [
        "Laura Ling and Euna Lee,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca8b2d0d0afc4128a3accb37ab6f442b",
      "question": "Who has no relation to north korea",
      "prediction": "The answer is: The U.S.",
      "ground_truths": [
        "United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0779694d75bc4c859ea6dd084ca62ae8",
      "question": "Where do Ling and Lee work?",
      "prediction": "The answer is \"California-based Current TV, which launched a CNN special report on Clinton's environmental summit in Seoul, South Korea.  The report, which is classified by the US agency, does not disclose the purpose of the visit.  The source provides information about the former president's movements, including a bouquet to Clinton.  The comment by Clinton's wife, U.S. Secretary of State Hillary Clinton, who said the Obama administration dropped a request for Ling and Lee to be released on humanitarian grounds, is a key point of note.",
      "ground_truths": [
        "Current TV"
      ],
      "em": 0,
      "f1": 0.05333333333333332
    },
    {
      "id": "2d0fdf9383ec439ab15a116a63b3421c",
      "question": "What does Chinoy say?",
      "prediction": "Chen Chinoy, an analyst at the Pacific Council on International Policy in Los Angeles, says that Clinton was going to seek the release of Laura Ling and Euna Lee, both reporters for California-based Current TV, and both were arrested for reporting on the border between North Korea and China.",
      "ground_truths": [
        "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\""
      ],
      "em": 0,
      "f1": 0.18181818181818185
    },
    {
      "id": "43ee49e31b614f168aa199dc923b09a3",
      "question": "How many protesters gathered at the Sudanese Embassy?",
      "prediction": "The answer is 18.",
      "ground_truths": [
        "3,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e5d337300d344dc5878baeb061f7a4d7",
      "question": "how many killed in conflict?",
      "prediction": "The answer is 200,000.",
      "ground_truths": [
        "more than 200,000 people,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "0152a392343845f093ed902e84665b09",
      "question": "What did protesters shout?",
      "prediction": "The protesters shouted, \"Hey Bush, you can't hide! Help us end this genocide!\" and \"President Bush! No more excuses!\"",
      "ground_truths": [
        "\"Hey Bush, you can't hide! Help us end this genocide!\" and \"President Bush! No more excuses!\""
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "065ce886fe8c41088ba6b71c82ab6229",
      "question": "Who is Ross Perot?",
      "prediction": "Ross Perot is an American businessman who sold a copy of the Magna Carta to the National Archives in Washington.",
      "ground_truths": [
        "businessman"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "480a34c34b094ac6b071bc658c0da810",
      "question": "how much is it expected to go to",
      "prediction": "The expected price to go to is $20 million to $30 million.",
      "ground_truths": [
        "$30 million,"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "625274d8050f451ebd131104306237c1",
      "question": "what is magna carta",
      "prediction": "The Magna Carta, an English document granting the English monarch the power to curtail the power of the king's seal to the common man, is the most important document on earth. It was signed in 1215 by King Edward I of England and, after being sealed in 1297 by King Edward I of England, it is one of only two copies of the charter that exist outside of England, and is then sold for at least $20 million to $30 million.",
      "ground_truths": [
        "The charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law."
      ],
      "em": 0,
      "f1": 0.1758241758241758
    },
    {
      "id": "531b64eef1bd44568360b3ab40da40da",
      "question": "what is expected price",
      "prediction": "The expected price of the Magna Carta is $20 million to $30 million.",
      "ground_truths": [
        "$20 million to $30 million,"
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "b33f0ec8e82f44baa01e6b09f9e69104",
      "question": "who bought the version",
      "prediction": "The answer is the Magna Carta.",
      "ground_truths": [
        "Ross Perot."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f724593e7f4842569187dae70a2560d9",
      "question": "Where will the auction be held?",
      "prediction": "The auction will be held at the National Archives in Washington, Washington, USA.",
      "ground_truths": [
        "Sotheby's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afc8672a21f441369ab4b0ffb8532c21",
      "question": "what will sotheby do",
      "prediction": "Sotheby's will auction off one of the earliest versions of the Magna Carta, the first version, later this year, the auction house announced Tuesday.",
      "ground_truths": [
        "auction off one of the earliest versions of the Magna Carta"
      ],
      "em": 0,
      "f1": 0.6206896551724138
    },
    {
      "id": "c89d314ad5224fe9896b825054544eed",
      "question": "Christie denied show tax credit because",
      "prediction": "The answer is: Christie denied the tax credit, but he also denied the grant to MTV's most popular show, \"The Jersey Shore,\" which had a $420,000 tax credit on TV shows filmed or produced in the state.",
      "ground_truths": [
        "it was unjustifiable"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81ac809606274fe3b7fa4191fda48454",
      "question": "what did Christie denied show tax credit ?",
      "prediction": "Christie denied the New Jersey Economic Development Authority's 20% tax credit for TV shows filmed or produced in the state.",
      "ground_truths": [
        "20% tax credit"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "82498b7e764c4a77ae54de9e49a211ce",
      "question": "Which museum is the world biggest repositories of ancient Egypt?",
      "prediction": "The answer is Egypt.",
      "ground_truths": [
        "France's famous Louvre"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cac7bd69b3e546a8aeccaeabc394f172",
      "question": "Which country accuses museum?",
      "prediction": "Egypt accuses the Louvre museum of failing to make good on promises to return antiquities.",
      "ground_truths": [
        "Egypt"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "7df3df70d1c74ae9a1255fbee295ef62",
      "question": "Who accuse the museum of failing to return stolen goods?",
      "prediction": "Egypt accuses the Louvre museum of failing to return stolen goods.",
      "ground_truths": [
        "Egypt"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "e89b8741115d4a248603dde70032bccd",
      "question": "Who suspends ties with France?",
      "prediction": "The answer is Egypt.",
      "ground_truths": [
        "Egypt"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "77581c9e1c504eb6a88f1cc8693a1f7b",
      "question": "Who did Egypt suspend ties with?",
      "prediction": "Egypt suspended ties with the Louvre museum.",
      "ground_truths": [
        "France's famous Louvre museum"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "af91062c61d0431fa90edb3cffc48da0",
      "question": "What is the one of worlds biggest repositories?",
      "prediction": "Egypt's one of worlds biggest repositories is the Louvre Museum in Paris.",
      "ground_truths": [
        "The Louvre"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "ca31a2be70584f73bd02f10dfb274f1b",
      "question": "Who pioneered inventions?",
      "prediction": "The answer is \"Steve Jobs.\"",
      "ground_truths": [
        "Jobs"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "112b26c0fb2a42f8ad17dedcfa162a14",
      "question": "Who made the first fully computer generated animated film?",
      "prediction": "The answer is \"George Lucas.\"",
      "ground_truths": [
        "Jobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6fd4830b5450487bbc05567bc69af124",
      "question": "What did Steve Jobs pioneer?",
      "prediction": "The answer to the question is: \"Apple's creation of a completely full-length, computer-generated animated film with Pixar's 'Toy Story' in 1995, capturing audience's hearts with its tale of the adventures of a group of children's toys.\"",
      "ground_truths": [
        "iTunes Music Store,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "374759f65388474086782e711a4bdb75",
      "question": "What did Pixar make?",
      "prediction": "The question asks what Pixar made.",
      "ground_truths": [
        "\"Toy Story\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "179a0bc5fd2d43e886af75064e440362",
      "question": "What made buying music easy?",
      "prediction": "The answer is, \"the ability to hold thousands of songs.\"",
      "ground_truths": [
        "iTunes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "067d7ffc28744dc280160df2299cb954",
      "question": "What did Jobs inventions do?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "changed the business of music,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8aa7295647af426cb083aedce76baaf9",
      "question": "Who made the  first fully computer-generated animated film?",
      "prediction": "The first fully computer-generated animated film was \"Toy Story,\" which was released in 1995.",
      "ground_truths": [
        "Pixar's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a92a2147d2454159bfb10fb49300c7c6",
      "question": "What was the result of Jobs invention?",
      "prediction": "The answer is, \"It was a team sport. They were team sports.\"",
      "ground_truths": [
        "left his indelible fingerprints on the entertainment industry."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "091ce4b419e4483888c050e9b7e4d26d",
      "question": "Who were arrested Tuesday?",
      "prediction": "The answer is the six members of the Missouri family of Lafayette County, Missouri, who were arrested for rape, a shift in alleged sexual abuse, and use of child in sexual performance.",
      "ground_truths": [
        "Mohler Sr. and his sons Burrell Edward Mohler Jr., 53; David A. Mohler, 52; Jared Leroy Mohler, 48; and Roland Neil Mohler, 47."
      ],
      "em": 0,
      "f1": 0.04
    },
    {
      "id": "bccbcc543c334277bf59683a13353d43",
      "question": "Who was charged with two counts of rape Friday?",
      "prediction": "The answer is: Darrel Mohler.",
      "ground_truths": [
        "Darrel Mohler"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ce76340da112438eb21e2fb526bb777a",
      "question": "What was Darrel Mohler charged with?",
      "prediction": "Darrel Mohler is charged with two counts of rape, Lafayette County Sheriff's Office said at a news conference.",
      "ground_truths": [
        "two counts of rape,"
      ],
      "em": 0,
      "f1": 0.38095238095238093
    },
    {
      "id": "9140e5ce275a4c9489694b8624c7d0f1",
      "question": "What years do the allegations come from?",
      "prediction": "The allegations come from the Missouri State Courts in charge of arrests.",
      "ground_truths": [
        "mid-1980s until 1995"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f121c2d17b442f08c3eed64fcc67102",
      "question": "How many people in total were arrested?",
      "prediction": "In total, 59 people were arrested.",
      "ground_truths": [
        "sixth"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c97e4992b2b44ab89e61a7f71effca3",
      "question": "Who were the accusers?",
      "prediction": "The answer is: The alleged abuse took place from the mid-1980s until 1995 and possibly beyond.",
      "ground_truths": [
        "relatives of the five suspects,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "463a29cc0b9a400c9fa0836baa81c58d",
      "question": "acres overlooking the Pacific",
      "prediction": "The answer is: \"The road to fun is back and affordable!\"",
      "ground_truths": [
        "127"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20d07b81a2454a0f8e835bf5075aec92",
      "question": "Where is the estate?",
      "prediction": "The answer is \"Hearst Castle. Los Angeles, California.\"",
      "ground_truths": [
        "San Simeon, California,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "16585bf289824fe78653e08246fd2ac6",
      "question": "What did Hearst fill the rooms with?",
      "prediction": "Answer: The rooms at Hearst Castle contain the \"jewels of the Hollywood district\" ‚Äì the ornate bronze and copper sinkwork, the elaborate ceilings, the stained-glass windows, the large deck, and the large, open swimming pool.",
      "ground_truths": [
        "Museum-worthy pieces"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad74e88eda9444b4a4255b4a8d4c7d33",
      "question": "Where is the estate located?",
      "prediction": "The answer is **Hearst Castle, California**.",
      "ground_truths": [
        "San Simeon, California,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "880a5931cab045d5ba624102c4b4a3af",
      "question": "Who entertained Hollywood stars at his estate?",
      "prediction": "The answer is \"The trolley of the Hollywood stars at Hearst Castle.\"",
      "ground_truths": [
        "William Randolph Hearst."
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "9032f2b4a10c49a58bfcaab938baba2b",
      "question": "How many rooms does it have?",
      "prediction": "The answer is: 127.",
      "ground_truths": [
        "165-room"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6dfdbc3e53da4319b04870079060d1bf",
      "question": "What did he fill the rooms with?",
      "prediction": "The answer is, \"the rooms were filled with religious icons, art and worldly treasures from around the world.\"",
      "ground_truths": [
        "Museum-worthy pieces"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49d9ebe7135b4a2890ced8b689c6dbd8",
      "question": "How many people did the Camorra clan kill?",
      "prediction": "The Camorra clan killed 26 people.",
      "ground_truths": [
        "an Italian and six Africans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cae54358803c4415988d062256f38b21",
      "question": "What larger crime organization is mentioned?",
      "prediction": "The larger crime organization mentioned is the Casalesi clan.",
      "ground_truths": [
        "Mafia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5223d20b95664375ad1b2133f36ba1ca",
      "question": "How many times has the army been sent in?",
      "prediction": "The answer is 2. The army has been sent 2 times in the second time since 1990.",
      "ground_truths": [
        "This will be the second"
      ],
      "em": 0,
      "f1": 0.11111111111111112
    },
    {
      "id": "88c911ecac0e4af284c5eb43fe8d9eb5",
      "question": "Where would a majority of the troops be sent to?",
      "prediction": "The answer is **North of Naples.**",
      "ground_truths": [
        "southern city of Naples"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "e4feceb16c4441d8b4ce29dcae36016f",
      "question": "Number of people that were murdered by the local Camorra clan?",
      "prediction": "The number of people who were murdered by the Casalesi clan in the Casalesi region of Sicily is **3**.",
      "ground_truths": [
        "an Italian and six Africans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aacdcc8d1b8345ce9f2a4199aac5f721",
      "question": "Six people were murdered by what clan last week?",
      "prediction": "The answer is **Casalesi**.",
      "ground_truths": [
        "The Casalesi Camorra"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "40a50aaeb28e4892956a7667321f0fd5",
      "question": "How many times since the 1990s has the army be sent to combat mafia crime?",
      "prediction": "The army will be sent to combat Mafia crime in the southern Italian city of Naples, which has been in the charge of Camorra since the 1990s.",
      "ground_truths": [
        "second"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "390ff5dbc5774ef6b94e51d2e2910123",
      "question": "Who is the Defense Minister that is quoted?",
      "prediction": "The Defense Minister is quoted as saying: \"The troops could be deployed for three months and the functions of manning check points.\"",
      "ground_truths": [
        "Ignazio La Russa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c289df329d5411989370e36f4d4fc97",
      "question": "Where would the majority of the tropps be sent?",
      "prediction": "The answer is **S Naples**.",
      "ground_truths": [
        "Naples"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c53f2dd5200e4beb9e81348f91a8e824",
      "question": "Who murdered six people?",
      "prediction": "The final answer is: The last victims in the 1990s were a string of 3,000 soldiers who were sent to Sicily, followed by 3,000 soldiers who were deployed in the southern Italian city of Naples.",
      "ground_truths": [
        "The Casalesi Camorra clan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f667caccc9e49a8b5a6dede296fbd1e",
      "question": "The majority of troops will be sent where?",
      "prediction": "The answer is the 500 soldiers, and they will be deployed to the southern city of Naples.",
      "ground_truths": [
        "southern city of Naples"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "cff10ff0cb9f4753af9add4c23a5a6a7",
      "question": "Who was the army sent to combat?",
      "prediction": "The army was sent to combat the Mafia crime in the southern Italian city of Naples.",
      "ground_truths": [
        "Mafia"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "9c0af32b6fce49cc84b8e5a98d5619e4",
      "question": "Who fled the wreckage?",
      "prediction": "The answer is: \"From the wreckage of President Robert Mugabe's Zimbabwe.\"",
      "ground_truths": [
        "Mawise Gumba"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65095a05ce414524b7799e1fa87e7f11",
      "question": "What is the percentage of inflation?",
      "prediction": "The answer is 5,000.",
      "ground_truths": [
        "close to 5,000 percent,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "a348607263324459aa73a678e65c2cac",
      "question": "Where do they flee to?",
      "prediction": "The answer is: **South Africa.**",
      "ground_truths": [
        "JOHANNESBURG, South Africa"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "2d419111063f489d8eefde205de5fa68",
      "question": "Where is Zimbabwe?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2911e86c54494f89abe1c854bb25830e",
      "question": "When did Mugabe become President?",
      "prediction": "The answer to the question \"When did Mugabe become President?\" is December 18, 1979.",
      "ground_truths": [
        "1980,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e39336ef5994bc9a78d2557d64331f6",
      "question": "what did he do",
      "prediction": "The answer is:",
      "ground_truths": [
        "fled Zimbabwe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09efa84b0e684c3f8c4d631b68f372ce",
      "question": "What happened to once prosperous nation?",
      "prediction": "The answer is: \"There was nothing for me there in our country any more. I had no job and I couldn't afford anything. Even when I was working life was tough,\" the former store clerk, whose name is not unusual in Zimbabwe, said.",
      "ground_truths": [
        "spiral into economic disaster."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db571ac79d34404b9a59b62c7fd38c56",
      "question": "What did the congressman say?",
      "prediction": "The congressman's statement, \"I will not support the Stop Online Piracy Act,\" is met with a strong reaction from the Internet, expressing concern about the bill's potential for undue regulation, censorship, and legal abuse.  He calls it a \"model for undue regulation,\" \"censorship,\" and \"legal abuse.\"",
      "ground_truths": [
        "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and"
      ],
      "em": 0,
      "f1": 0.14545454545454545
    },
    {
      "id": "a018ca4c1e8e4a44b48188655383c8a5",
      "question": "who is Congressman ?",
      "prediction": "The answer to the question \"Who is Congressman?\" is: Congressman Paul Ryan.",
      "ground_truths": [
        "Paul Ryan"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "44bf7ea0266142f6997e0596077986b7",
      "question": "What is Paul Ryan's position?",
      "prediction": "Based on the text provided, answer the question:",
      "ground_truths": [
        "will not support the Stop Online Piracy Act,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0cecc7cafa884126923093cb39466c96",
      "question": "Ryan says Internet is an expression of freedom and should",
      "prediction": "Here's a concise answer from Mashable:",
      "ground_truths": [
        "stay that way.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e88a0c1244b5484b830b44a6139bec80",
      "question": "who did  says Internet is an expression?",
      "prediction": "The answer is:  \"In the statement, Ryan calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history.\"",
      "ground_truths": [
        "Congressman Paul Ryan"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "928e2cc022da44a4893b47a7fa37e1c6",
      "question": "what did Congressman say?",
      "prediction": "The answer is: Congressman Paul Ryan said that he does not support the Stop Online Piracy Act.",
      "ground_truths": [
        "\"one of the most magnificent expressions of freedom and free enterprise in history\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ca689bb9bd94572af15fb45f2257266",
      "question": "What did Ryan say is an expression of freedom?",
      "prediction": "The answer is: \"It should stay that way.\"",
      "ground_truths": [
        "the Internet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a3111eeeaff40f8876c1d5ebc1f7855",
      "question": "What does the West suspect?",
      "prediction": "The West suspects Iran of attempting to build nuclear bombs.",
      "ground_truths": [
        "Iran of trying to build nuclear bombs,"
      ],
      "em": 0,
      "f1": 0.75
    },
    {
      "id": "3094f8c287a241f6a310c9518a88ca39",
      "question": "who is suspected to build bombs?",
      "prediction": "The answer is \"The West,\" because the information provided in the context directly states that Iran's missile production plants are built and delivered and thus, the West's suspicion has been developed. The answer explicitly states that this is a \"provocative act\" by Washington, which the Minister of Defense, Ahmad Vahidi, told Fars News Agency.",
      "ground_truths": [
        "Iran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "285b1eeffd2143f98100690ea1c00b92",
      "question": "what was considered a provocative act?",
      "prediction": "The answer is \"high penetration and destructive powers.\"",
      "ground_truths": [
        "Iran test-launched a rocket capable of carrying a satellite,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "77a760bc179749ddb620a86729122ce1",
      "question": "Who has been fired?",
      "prediction": "The answer is said to be \"A senior administration official has told CNN.\"",
      "ground_truths": [
        "Elizabeth Birnbaum"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd60a5cbf3f74a689e9996a315253cee",
      "question": "What has been introduced in part to help crack down on the MMS?",
      "prediction": "The answer is, \"Conflicting missions need to be separated.\"",
      "ground_truths": [
        "requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate."
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "34ba486115794ddea6fe7f05181250b5",
      "question": "who report inappropriate relationship?",
      "prediction": "The answer is: the Interior Department's Inspector General's report.",
      "ground_truths": [
        "Interior Department's inspector general,"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "b812244cbb374be1906ecd90aa03cac8",
      "question": "What does Interior Secretary Ken Salazar insist?",
      "prediction": "Interior Secretary Ken Salazar insists that she is a strong and effective public servant and that she is a good public servant.",
      "ground_truths": [
        "that Birnbaum had resigned \"on her own terms and own volition.\""
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "d897192ab88743b8bf6ea3c00bc845a8",
      "question": "What is the name of the PM?",
      "prediction": "The name of the PM is Abhisit Vejjajiva.",
      "ground_truths": [
        "Abhisit Vejjajiva"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1fe3dce165394bee88bb3829fc87ba0c",
      "question": "What does the opposition  red shirt demand?",
      "prediction": "The opposition red shirts demands that the prime minister dissolve the parliament within 15 days.",
      "ground_truths": [
        "that the prime minister dissolve the parliament within 15 days."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "d72847aa89bd4373b5f03086a4cbdd0b",
      "question": "How many protesters occupy commercial heart?",
      "prediction": "The answer is 50,000.",
      "ground_truths": [
        "50,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "293fe307f8554032bfd957d206dd780f",
      "question": "Who say they will continue until their demands have been met?",
      "prediction": "The answer is: The government will continue negotiating with the protesters to peacefully disperse the capital's commercial center.",
      "ground_truths": [
        "anti-government protesters"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "27b67e1a4f4d40b5b5d242ef0b789b9a",
      "question": "What is Shinawatra's first name?",
      "prediction": "The answer is **Abhisit Vejjajiva**.",
      "ground_truths": [
        "Thaksin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af4cc2d2ba214356adcef50279172202",
      "question": "Who demmand that PM Abhisit dissolver?",
      "prediction": "The answer is \"The Prime Minister of Thailand, Abhisit Vejjajiva.\"",
      "ground_truths": [
        "The opposition group, also known as the \"red shirts,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0372808323d540e3859a93d9ae4843f1",
      "question": "Who died in Afghanistan?",
      "prediction": "The answer is Delaware (CNN).",
      "ground_truths": [
        "Phillip A. Myers."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bae7071c3f5d47edac66d6ecf05633b7",
      "question": "Who died in Afghanistan on Saturday?",
      "prediction": "The answer is likely the U.S. Air Force.",
      "ground_truths": [
        "Phillip A. Myers."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8860bc24541401bb994c72c67e5c7af",
      "question": "Who lifted the media ban?",
      "prediction": "The answer is: The team lifted the media ban, with the donation of the family's name and the decision to exclude news coverage of the return of the fallen U.S. Marine in Afghanistan.",
      "ground_truths": [
        "President Bill Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8bc486ca6580487586a39a78cc2ce5d2",
      "question": "What rank was Phillip Myers?",
      "prediction": "The answer is **48th Civil Engineer Squadron**.",
      "ground_truths": [
        "staff sergeant"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "80726cf8c538458ea11fd99b21b45cf8",
      "question": "What can be covered by media for the first time in 20 years?",
      "prediction": "The answer is: The state of the country's national mourning process.",
      "ground_truths": [
        "return of a fallen U.S. service member"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "f07750ea1251487eaf95f98457f8e5bf",
      "question": "Who lifted the ban on media coverage?",
      "prediction": "The answer is: The Carter administration.",
      "ground_truths": [
        "Bill Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "63df6e921a2d41c3a6c8c2ed0b8de7c2",
      "question": "What was the pilot trying to do?",
      "prediction": "The Indiana court froze the assets of Marcus Schrenker, a suburban Indianapolis financial manager, by arresting him for marijuana possession and further offenses related to his business dealings.",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 0,
      "f1": 0.10810810810810811
    },
    {
      "id": "a89369b7e712438388b7ef5b61ae505d",
      "question": "What were the investigators investigating?",
      "prediction": "The Indiana court, where the case was filed, froze the assets of Marcus Schrenker, a suburban Indianapolis financial manager.",
      "ground_truths": [
        "his business dealings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c52b52625cd74d95a45e60169d9826dd",
      "question": "Who are officials searching for?",
      "prediction": "The answer is \"Investigators looking into his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court, said Jim Gavin, spokesman for the Indiana Secretary of State.",
      "ground_truths": [
        "Marcus Schrenker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65ac4247c8504132a16e920b53ac63d1",
      "question": "What did the manager in Alabama say about Shrenker?",
      "prediction": "The manager in Alabama stated that Schrenker was \"an accomplished pilot\" who owned two airplanes and flies regularly.",
      "ground_truths": [
        "Schrenker was the only guest overnight. He signed in as Jason Galouzs of Bolingbrook, Illinois,"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "2c36ce6243e44160a69bead31c2ac6e8",
      "question": "What did the manager say about Schrenker?",
      "prediction": "The manager said Schrenker was an accomplished pilot who owns two airplanes.",
      "ground_truths": [
        "is \"an accomplished pilot\" who owns \"a couple of airplanes\" and flies regularly."
      ],
      "em": 0,
      "f1": 0.47619047619047616
    },
    {
      "id": "a011e7f0db714fbcb0c05b9cd00b01e7",
      "question": "What did authorities say the pilot tried to do?",
      "prediction": "The answer is, the Indiana Department of Justice (D.O.C.I.) froze the assets of Marcus Schrenker, a suburban Indianapolis financial manager, by forcing him to crash his private plane into a Florida swamp.",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 0,
      "f1": 0.358974358974359
    },
    {
      "id": "aa3df64ad91a43e48011ccc75494e27d",
      "question": "Who did the manager of the hotel said stayed at the hotel overnight?",
      "prediction": "The answer is: \"The hotel manager, Yogi Patel, who identified Schrenker as a Jackson (Illinois) salesman,\"",
      "ground_truths": [
        "Schrenker"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "33de3e9db2a54cbea1114b4744212328",
      "question": "What type of violations are they looking into?",
      "prediction": "The answer is: The Indiana Department of Justice (D.C.S.O. 31025, 31034, 31034) investigates the investigation of Marcus Schrenker, a suburban Indianapolis financial manager, for alleged securities violations, including those related to his private plane crash.",
      "ground_truths": [
        "securities"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "646a975b50a442eabb7ec808f829bb09",
      "question": "What did the person try to do?",
      "prediction": "The Indiana Secretary of State's Temporary Restraining Order in Hamilton County, Indiana, froze the assets of Marcus Schrenker, a suburban Indianapolis financial manager, by cracking his private plane into assets.",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 0,
      "f1": 0.2564102564102564
    },
    {
      "id": "2825e144990a449cb613d652b4188c2e",
      "question": "Who was evicted?",
      "prediction": "The answer is: \"Brown was evicted because her landlord defaulted on the mortgage and the house fell into foreclosure.\"",
      "ground_truths": [
        "Brown and her family"
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "be34ac10abbf40838b850950f70bdcd5",
      "question": "Who is working on a law to help renters?",
      "prediction": "The answer is Jeff Klein, a New York State Senator. He is aware of the challenges renters face in the financial system, and is working on a law to warn renters of foreclosure proceedings. He says the law is intended to ensure renters are not evicted with nowhere to go.",
      "ground_truths": [
        "Sen. Jeff Klein"
      ],
      "em": 0,
      "f1": 0.08510638297872342
    },
    {
      "id": "ff9f6bcd4a914ee4b5d2a993bd5d9ea8",
      "question": "What happened to Lisa Brown?",
      "prediction": "Based on the provided text, the answer is:  Lisa Brown has to move out of her rental house because it is facing foreclosure.",
      "ground_truths": [
        "being evicted"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5baea57b39364a358c852ba09273d51d",
      "question": "Who is working on a law to warn renters of foreclosures?",
      "prediction": "The answer is \"New York State Sen. Jeff Klein.\" He says he is working on a law to warn renters of foreclosure.",
      "ground_truths": [
        "New York State Sen. Jeff Klein"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "128b2d769fe045ca8e1bff19e037b7da",
      "question": "Who is working on the law?",
      "prediction": "The answer is \"New York State Sen. Jeff Klein.\"",
      "ground_truths": [
        "Sen. Jeff Klein"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "e08060ab217a47f6b0fd8f4ca9d96c11",
      "question": "What did Brown want for her children?",
      "prediction": "The answer to the question is: \"I am not the only tenant caught off guard. According to the Center for Housing Policy, nearly 20 percent of all foreclosures are on rental properties, and tenants' rights are minimal. In most states, when a bank forecloses on a landlord, the tenant has no guarantee of being allowed to stay and is usually kept in the dark. New York State Sen. Jeff Klein says renters can run into problems with their rent payments, and the owner of the property where the rent was paid is pocketing the money. He notes that rental properties are involved in 50 percent of all fore",
      "ground_truths": [
        "\"to give my kids a better environment.\""
      ],
      "em": 0,
      "f1": 0.0196078431372549
    },
    {
      "id": "c9e7a34ffb9949a7a4cdc4fd89436a80",
      "question": "Whose landlord was foreclosed upon?",
      "prediction": "The answer is, \"New York State Sen. Jeff Klein.\"",
      "ground_truths": [
        "Lisa Brown"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f7e7b9139d849eda2591b83348e037e",
      "question": "What show features Wozniak this season?",
      "prediction": "The answer is \"Dancing With the Stars.\"",
      "ground_truths": [
        "\"Dancing With the Stars.\""
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "c989b740d7774626a6b659c806935559",
      "question": "What TV show is Steve Wozniak going to be on?",
      "prediction": "The answer is \"Camp Woz on ABC's 'Dancing With the Stars.\"",
      "ground_truths": [
        "\"Dancing With the Stars.\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5d27fdf94cc74998bd78ccda5998134a",
      "question": "Who co-founded Apple?",
      "prediction": "The answer is Steve Wozniak, and his partner, Katarina Smirnoff.",
      "ground_truths": [
        "Wozniak"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "e2e347c7201d45779244494690d6d481",
      "question": "What company did Wozniak help create?",
      "prediction": "The answer is \"Apple Inc. Co-founder Steve Wozniak.\"",
      "ground_truths": [
        "Apple Inc."
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "f07e6d91823b4cb8856d3672a83a75f3",
      "question": "Wozniak being supported via voting and what else?",
      "prediction": "The answer is: Wozniak was supported by a crowd of fans by voting on the dance team's scores.",
      "ground_truths": [
        "social networking"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "005040ca6e354afba3d171349bdde420",
      "question": "Who is supporting Wozniak?",
      "prediction": "The answer is Wozniak.",
      "ground_truths": [
        "fans of Woz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8f6c0a2a8ad24bed90083ee1fade53da",
      "question": "Who is the Apple co-founder?",
      "prediction": "The answer is Steve Wozniak.",
      "ground_truths": [
        "Wozniak"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "022385a9defe4d71b208f1b405b61b1b",
      "question": "Who spoke in favor of the judge's confirmation?",
      "prediction": "The answer is *Sotomayor*.",
      "ground_truths": [
        "New York City Mayor Michael Bloomberg"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf0c2ff686124703a81f6e2e492e57df",
      "question": "Who spoke in favor of judge's confirmation?",
      "prediction": "The answer is C. Frank Ricci.",
      "ground_truths": [
        "Arkansas Attorney General Dustin McDaniel testified on Sotomayor's behalf,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3121eac762014f83a9df1a81ede1386b",
      "question": "What did the firefighters claim?",
      "prediction": "The firefighters claimed that the city of New Haven violated its equal protection rule and attempted to cripple its firefighters by implementing a test of equal value, regardless of race.",
      "ground_truths": [
        "reverse discrimination"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "793f5116a5c44b67b76f7e63340cb867",
      "question": "Who was one of 20 firefighters who claimed reverse discrimination in promotions?",
      "prediction": "The answer is New Haven firefighters.",
      "ground_truths": [
        "Frank Ricci"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "124127b3d7994f27a8763e3bd21f369b",
      "question": "Who was the plaintiff in this case?",
      "prediction": "The answer is New Haven, Connecticut.",
      "ground_truths": [
        "Frank Ricci,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8546907869db4cf9bb91335962dad764",
      "question": "what was the ruling based on?",
      "prediction": "The answer is, the ruling was based on the **second circuit of the U.S. Court of Appeals**, which relied on the precedent of a quota case.",
      "ground_truths": [
        "\"a very thorough, 78-page decision by the district court\""
      ],
      "em": 0,
      "f1": 0.07407407407407408
    },
    {
      "id": "c7aace68925a4affbb0e4cbb611d9207",
      "question": "who is Frank Ricci?",
      "prediction": "Frank Ricci is the lead plaintiff in the controversial firefighter promotion exam case at New Haven, Connecticut, where the city of New Haven tried to dismiss the results of a 2003 firefighter promotion exam for its lack of racial diversity. The final verdict was overturned by the Supreme Court in June, after a 5-4 jury found that almost all minorities qualified for promotions, and the city of New Haven was at risk of being sued by employees who could show they were \"disparately impacted\" by the test. The case also highlighted the state's tendency to reward racial bias in admissions.",
      "ground_truths": [
        "New Haven, Connecticut, firefighter"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "4988ebcead954a1a9d712f4e3db68031",
      "question": "What undermined the concept of a merit-based civil service system?",
      "prediction": "The answer is, \"It only divides people who don't wish to be divided along racial lines. The very reason we have civil service rules is to root out politics, discrimination, nepotism, and the concept of \"merit and law\".\"",
      "ground_truths": [
        "that Sotomayor's rejection of his reverse discrimination claim"
      ],
      "em": 0,
      "f1": 0.09302325581395347
    },
    {
      "id": "1bfe21f2aa3b42e68d31695509ba53d9",
      "question": "What comments were a new low?",
      "prediction": "The answer is:  \"So, if you care about it, why don't you support stem cell research?\"",
      "ground_truths": [
        "a debate over who cares more for special needs children."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cdb6245f11c40f1b5e26abd657677d9",
      "question": "what has the attorney denounced",
      "prediction": "The attorney denounced the charge of attempted murder, a public defender, one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.",
      "ground_truths": [
        "the fact that the teens were charged as adults."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90238bd9190f43b9a104cc5d0a6f0583",
      "question": "what does the attorney denounce?",
      "prediction": "The attorney denounces the charges of attempted murder, a public defender for one of the boys, and the association of \"Two other boys ages 13 and 15,\" for setting a 15-year-old friend on fire.",
      "ground_truths": [
        "the fact that the teens were charged as adults."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37ce5358517f4fce998cca3ed31292aa",
      "question": "what percentage burns did Brewer suffer?",
      "prediction": "The answer to the question is that Brewer suffered 65% of his body.",
      "ground_truths": [
        "over 65 percent of his body."
      ],
      "em": 0,
      "f1": 0.4705882352941177
    },
    {
      "id": "36db4e1d502a4aae9f49373c4ccc53a7",
      "question": "How much of his body burned?",
      "prediction": "The answer is likely 65%.",
      "ground_truths": [
        "over 65 percent of"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "52cba38cc68d493bb73a3a751e0f849d",
      "question": "how many teens were charged?",
      "prediction": "The answer is: \"We cannot tell. The details of the charge are not provided. We cannot determine at this point.\"",
      "ground_truths": [
        "Three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e75b5fe1656c408a8091bae8561f4215",
      "question": "what have the boys been charged with",
      "prediction": "The boys are charged with attempted murder, a public defender for one of the boys said, and the use of a lighter to set fire to Michael Brewer.",
      "ground_truths": [
        "attempted murder,"
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "0c3b1ed51cc14bbea2ccc8f4ec29c94e",
      "question": "How many people where charged?",
      "prediction": "The answer is:  The answer is:  Two.",
      "ground_truths": [
        "Three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a90d7f4e49ed4041882d877e60323398",
      "question": "what does he have on his body",
      "prediction": "The answer is: \"Am I going to be OK?\"",
      "ground_truths": [
        "burns"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97409d2d3afe44e48f287814cdb36dfb",
      "question": "Who leads the Genocide Prevention Task Force?",
      "prediction": "The answer is: Madeleine K. Albright.",
      "ground_truths": [
        "Madeleine K. Albright"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "09c79cbef9b6454cac1fe2e0aeb0cbf8",
      "question": "When will the panel's final report be released?",
      "prediction": "The final report to the National Security and State Secret Security Task Force is to be released on the same date. The Executive Summary includes:",
      "ground_truths": [
        "next week.)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cd041a253c746be9389d119b5de2d14",
      "question": "What are the thoughts of Albright and Cohen?",
      "prediction": "From the context, the answers are as follows:",
      "ground_truths": [
        "we believe that leadership is an indispensable ingredient."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ecd8bbe295e4e25be8acdc72d307e86",
      "question": "Who is Madeleine Albright?",
      "prediction": "The answer is Madeleine K. Albright, former U.S. Secretary of State.",
      "ground_truths": [
        "former U.S. secretary of state."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7a21c1eb21aa49af8a0ba5450b7da9dc",
      "question": "What do Madeleine Albright and William Cohen co-chair?",
      "prediction": "Based on the context, the answer is Madeleine K. Albright.",
      "ground_truths": [
        "Genocide Prevention Task Force."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7173278ae5a5424d8b5b49177dae0264",
      "question": "When did the shooting take place?",
      "prediction": "The answer is, based on the provided context, the answer is definitively found to be: \"The shooting took place in the poor neighborhood of Rione Sanita, in the area where the video was taken. The video shows a man pushing a pistol to his head and then a man standing outside the store, looking around. As he exits, he pulls a pistol from his right front pants pocket and shoots a man standing outside the store. The killer, a bank robber, described this as a \"ugly side of the Camorra.\" The video shows a scene of a \"disgusting side of the Camorra, followed by the same",
      "ground_truths": [
        "May 11,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62cf86565d62481d8db7e902fa795eed",
      "question": "What did the police spokesperson say?",
      "prediction": "The answer is: The police spokesperson, Maria Rosaria Carfagna, stated that the video was \"tragic,\" implying that the killing was not a normal, natural event in Naples. She also mentioned that the video was considered \"shocking\" for the absence of censorship, even in a neighborhood like Rione Sanita. The video was filmed in the center of a great European city, and the speaker acknowledged the suffering of people, the dangers of organized crime, and the pain of Naples. She also concluded that the police was fighting organized crime, and that the number of arrests in Naples, and the Campania region, were shown without",
      "ground_truths": [
        "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county."
      ],
      "em": 0,
      "f1": 0.07920792079207921
    },
    {
      "id": "bc670fde5854495bb9cddeaaec024ce1",
      "question": "Who appeared unfazed by the neighborhood shooting?",
      "prediction": "The answer is: **a man standing outside the store, looking around, pulling a pistol and shooting a man standing outside the store.**",
      "ground_truths": [
        "Passers-by"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65eac78c98514b18b0d5462462cc29a8",
      "question": "What did the video show?",
      "prediction": "The video showed a man in a baseball cap, dark jersey, blue jeans and running shoes walking out of the store, then retreating to the back of the road. He pulled a pistol from his right front pants pocket and shot a man standing outside the store. The video captured the scene of the shooter's execution, with a police spokesman's comment stating it's \"tragic,\" and the victim's head falling to his knees. The video, while \"shocking,\" also revealed the presence of a man at the victim's knees, and the subsequent movement of the body. The video also mentions the city of R",
      "ground_truths": [
        "the execution."
      ],
      "em": 0,
      "f1": 0.02469135802469136
    },
    {
      "id": "2fd937838f0c4d2d84ca2688fc5e0ebc",
      "question": "What is the amount of the reward being offered by Italy's Green Party for informaation about the whereabouts of the shooter and accomplice?",
      "prediction": "The answer is 2,000 euros.",
      "ground_truths": [
        "2,000 euros ($2,963)"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "3842d04ed8de486f910aadd2233a5e2a",
      "question": "What mosque was raided?",
      "prediction": "The answer is the mosque in Rafah, Israel.",
      "ground_truths": [
        "in the southern Gaza city of Rafah,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "e65ae069b67b4bf69b25fc669ee363b0",
      "question": "What group blew up home?",
      "prediction": "The answer is: Al-Maqdessi, the leader of the Salafist jihadist group Jund Ansar Allah, and his leader, Abu al-Nour al-Maqdessi.",
      "ground_truths": [
        "Hamas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8fa77691446e44ad8f9cba8904cf69da",
      "question": "Who claims sheikh's followers are \"outlaws?\"",
      "prediction": "The answer is \"Al-Maqdessi,\" the leader of Jund Ansar Allah, the Salafist jihadist group.",
      "ground_truths": [
        "Taher Nunu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b3ab470f12a42239f0ab1a1f27acff5",
      "question": "Where did Hamas raid the mosque?",
      "prediction": "The answer is provided by CNN. The context is that Hamas raided a mosque in Rafah, Gaza, and seized control of it.",
      "ground_truths": [
        "in the southern Gaza city of Rafah,"
      ],
      "em": 0,
      "f1": 0.32
    },
    {
      "id": "288f88ddfaf34bdabc2ff3748757ac21",
      "question": "What sparked clashes with Hamas?",
      "prediction": "The answer is, the clashes with Hamas forces left 13 people dead, and Hamas sources say that Al-Maqdessi was killed.",
      "ground_truths": [
        "Sheikh Abu al-Nour al-Maqdessi,"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "9c8375d231b04c74b817c1ef9db3ec33",
      "question": "What did Hamas spokesman call sheikh's followers?",
      "prediction": "The Hamas spokesman called herikh's followers \"outlaws\" and stated they have been \"terrorizing the country and attacking civilians.\" He also called for a public meeting at the mosque. The source states that militants raided the mosque and seized it.",
      "ground_truths": [
        "\"outlaws\""
      ],
      "em": 0,
      "f1": 0.05882352941176471
    },
    {
      "id": "0a8eff0d1b134de888817e44d2e889b1",
      "question": "What did Jero do to the Japanese songs?",
      "prediction": "Jero did make old, new, and new again in Japan.",
      "ground_truths": [
        "is making old, new again in Japan."
      ],
      "em": 0,
      "f1": 0.588235294117647
    },
    {
      "id": "599f7e94db9a48019f887451a66c4e1d",
      "question": "When did he sing enka?",
      "prediction": "The answer is: The 27-year-old American, making old, new again in Japan.",
      "ground_truths": [
        "in early 2008,"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "b3d58c3cb86044e1ac53ac99a0246d3b",
      "question": "Where is Jero came from?",
      "prediction": "The answer is in the context.",
      "ground_truths": [
        "Pittsburgh"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62179d55c8ab4178bb0450eafda198cf",
      "question": "What ranking did his first single make?",
      "prediction": "Jero's first single made in Japan.",
      "ground_truths": [
        "No 4,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "17ca78d8ef664047ad404f91d1f62c4b",
      "question": "when was he in talent show?",
      "prediction": "The answer is: The 27-year-old American, Jero, made a name for himself singing enka, a traditional form of lounge music that flourished in Japan.",
      "ground_truths": [
        "2008,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "daccc4268b874f63a33873b7171ba8ff",
      "question": "what is an enka song?",
      "prediction": "The answer is Jero, a former American singer known for singing Japanese enka.",
      "ground_truths": [
        "traditional form of lounge music"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e62bc50eddf84d3a9d77bae7b77fc196",
      "question": "What did Bush say the U.S. will do?",
      "prediction": "Answer: The U.S. President George W. Bush told a global climate change conference Friday that the United States will do its part to improve the environment by taking on greenhouse gas emissions.",
      "ground_truths": [
        "its part to improve the environment by taking on greenhouse gas emissions."
      ],
      "em": 0,
      "f1": 0.5641025641025641
    },
    {
      "id": "53259c05362845bda84bd8a1783f10e7",
      "question": "What did the president propose?",
      "prediction": "The president stated that the United States will do its part to improve the environment by taking on greenhouse gas emissions.",
      "ground_truths": [
        "to come together and \"set a long-term goal for reducing\" greenhouse emissions."
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "c3ff75cbd8cf48ff834d6ef8803ff63f",
      "question": "did bush say something",
      "prediction": "The answer is yes, Bush said something at the Major Economies Meeting. The text simply states he said, \"We take this issue seriously.\"",
      "ground_truths": [
        "\"we take this issue seriously.\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "68021cc757c14f9199e2d4da38f53356",
      "question": "What did Bush ask nations to reduce?",
      "prediction": "Answer: The White House asked nations to reduce greenhouse emissions by setting a long-term goal for reducing emissions in a way that does not undermine economic growth or prevents nations from delivering greater prosperity for their people.",
      "ground_truths": [
        "greenhouse emissions."
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "362eea3f213c4d51a9994cf1d52a17bc",
      "question": "what did bush call for",
      "prediction": "The answer is:  \"By working together, we will set wise and effective policies.\"",
      "ground_truths": [
        "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal"
      ],
      "em": 0,
      "f1": 0.18749999999999997
    },
    {
      "id": "430156c52a22415980c710c67d9a19fe",
      "question": "What is the president's proposal?",
      "prediction": "The president's proposal is to \"set a long-term goal for reducing\" greenhouse emissions, and to \"acknowledge that there is a problem, and by setting this goal, we commit ourselves to doing something about it,\" and to \"call for the work of all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" and \"to set a long-term goal for a global consensus at the U.N. in 2009,\" and \"to develop its own separate strategies for making progress toward achieving this long-term goal.\"",
      "ground_truths": [
        "initiative to develop a common approach to combat global warming"
      ],
      "em": 0,
      "f1": 0.09302325581395349
    },
    {
      "id": "4c961dd89b054526b342c921486477a6",
      "question": "How many years did he win the title?",
      "prediction": "The answer is: Inter Milan won the Italian Serie A title for the fifth time in their history.",
      "ground_truths": [
        "18th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e5fb043f8b0245f2acc78136af1bb5dd",
      "question": "Who won the Italian Serie A title?",
      "prediction": "The answer is: **Inter Milan won the Italian Serie A title.",
      "ground_truths": [
        "Milan"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "62e9113dcca84b61af8304429b1570da",
      "question": "Who scored the only goal?",
      "prediction": "The answer is: The only goal was scored by Daniele De Rossi in the 39th minute.",
      "ground_truths": [
        "Diego Milito's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09a4f669c3ce4432954d3b5348ffde56",
      "question": "What did Inter Milan win?",
      "prediction": "Inter Milan won the Italian Serie A title for the fifth time after a 1-0 victory at Siena.",
      "ground_truths": [
        "Italian Serie A title"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "26c180c03a734a1aaee4138be308a078",
      "question": "who won the Italian Serie A title for a fifth successive year ?",
      "prediction": "The answer is: 1-0 (with Diego Milito's goal).",
      "ground_truths": [
        "Milan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba1325c4040040a0b7809a32536da5fe",
      "question": "in what positon do Rome finish?",
      "prediction": "Rome finishes in Serie B.",
      "ground_truths": [
        "second-placed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9966c7735a6649d88366224ff6552c68",
      "question": "who scores the only goal ?",
      "prediction": "The answer is: `Milito's shot with the outside of his right foot`.",
      "ground_truths": [
        "Diego Milito's"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "8bf5d3ca87e449fda5afd9410581c273",
      "question": "Where are US forces standing by?",
      "prediction": "The answer is: U.S. forces are standing by.",
      "ground_truths": [
        "Port-au-Prince"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d5a5752b1ea4b928cb778bc543ee042",
      "question": "What does the Haitian police force represent?",
      "prediction": "The Haitian police force represents the \"Lone Security Apparatus\" of the U.S. (the 9,000 non-state-affiliated security apparatus of the Caribbean nation).",
      "ground_truths": [
        "\"The first line of law and order"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "ac3359aa637e49f2b14c9cb0391999b4",
      "question": "What do the UN forces do?",
      "prediction": "The UN forces are responsible for security on the streets, with backing from U.N. peacekeepers.",
      "ground_truths": [
        "standing by to provide security"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "8f50caa41d034d3fa83ed2e4c4907830",
      "question": "What are US forces standing by to do if needed?",
      "prediction": "The answer is: \"We are standing by to provide security as needed.\"",
      "ground_truths": [
        "to provide security as needed.\""
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "76cefaf0a5ec4b3db7ddb400b9d6efce",
      "question": "What does the National police chief say?",
      "prediction": "The National Police chief says the Haitian police are responsible for security on the streets, and the U.S. is providing support to the Haitian police.",
      "ground_truths": [
        "the department has been severely affected by the earthquake,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9014fb3e6aec4cf0bfbdd77ddf364ec1",
      "question": "How many bodies are police looking to recover?",
      "prediction": "Police are looking to recover at the Garner facility, with 38 people in critical condition, and 300 people in the plant.",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cdd0335d31d4fdca006adeea4058692",
      "question": "How many people are in critical condition?",
      "prediction": "The answer is 38.",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "caeeaa8b6a4b422981f799e26dfb4eba",
      "question": "How many were taken to hospital?",
      "prediction": "The answer is 38.",
      "ground_truths": [
        "At least 38 people"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "cc31691c8a774a739d53ecc5a4f24279",
      "question": "What caused the roof collapse at ConAgra Foods in NC?",
      "prediction": "The answer is **Garnar, North Carolina**.",
      "ground_truths": [
        "cause of the blast was unknown,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cce903a7338748cdae3a5e8fdc9d97dd",
      "question": "What are the police looking for?",
      "prediction": "The police are looking for the missing people, including two males and two females, in the North Carolina food plant, Garner, NC.",
      "ground_truths": [
        "two bodies out of the plant,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "273287c070454cbc85b8affade5d1354",
      "question": "What is the cause of the explosion?",
      "prediction": "The answer is \"heavy ammonia leaks and a fire that was not extinguished until afternoon,\" based on the context provided.",
      "ground_truths": [
        "unknown,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9aceab5d60c648b89e292afe7c551041",
      "question": "What is the number of people in the hospital?",
      "prediction": "The answer is 38.",
      "ground_truths": [
        "38"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a1f9c266d93643f6aa8d1e452094db07",
      "question": "What collapsed?",
      "prediction": "The answer is: The truck (1,000,000 sq ft) lies atop cars in Garner, North Carolina.",
      "ground_truths": [
        "ConAgra Foods plant"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28f7a52945994183bf9e7aa9a78088ce",
      "question": "What are teams doing!",
      "prediction": "The answer is: **ConAgra Foods, the company that produced Slim Jim, has been working to provide support to its employees, and its store, Garner's Food Plant, is a key part of that support.**",
      "ground_truths": [
        "trying to get the two bodies out of the plant,"
      ],
      "em": 0,
      "f1": 0.15789473684210528
    },
    {
      "id": "b3ac5354aeb34281aad0d65e39aee055",
      "question": "Who issued military orders before his father's death?",
      "prediction": "Citing \"a South Korean source,\" Yonhap reported that Kim Jong Un issued his first military orders as leader of North Korea.",
      "ground_truths": [
        "Kim Jong Un"
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "4cc2ec62d53a4f6ab37f777a1c8f1884",
      "question": "When did Kim Jon Un issue military orders?",
      "prediction": "The answer is: **On Wednesday, as the North Korean state-run news agency, KCNA, issued its instructions to its officers, including Kim Jong Il's son, regarding military exercises and training.**",
      "ground_truths": [
        "before the death of his father"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc87a9e0038f41a7856ce71bc10a51ac",
      "question": "What does a defector tell CNN?",
      "prediction": "The answer is: \"Pig feed, that's all we can eat. There is no food, not even food from China. It's been blocked for three years.\"",
      "ground_truths": [
        "\"If anyone knows I'm talking, I would be sent to prison and there's no mercy there. I would be shot dead.\""
      ],
      "em": 0,
      "f1": 0.08888888888888889
    },
    {
      "id": "6ff81c4136e94b4791b5df78165c0613",
      "question": "What did a defector tell CNN about?",
      "prediction": "The answer is: North Korean intelligence likely tried to prevent defections.",
      "ground_truths": [
        "people are starving, aid is scarce, and the only operating factories serve the military."
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "f3a09e4e4c1e4595ba0ce01bf501c5e1",
      "question": "What did the state-run news agency say?",
      "prediction": "The answer is: North Korea has made its government announce its full support for Kim Jong Un's son's military orders, and it has denied any knowledge of the death. This was communicated through a TV news report, and the report also includes a defector's comment on North Korean spies.",
      "ground_truths": [
        "Kim Jong Un issued his first military orders as leader of North Korea just"
      ],
      "em": 0,
      "f1": 0.2413793103448276
    },
    {
      "id": "3d1f0f420458417fb7f17d81786eb726",
      "question": "Who controls the military, according to the state-run news agency?",
      "prediction": "The answer is: The State-run news agency KCNA.",
      "ground_truths": [
        "Kim Jong Un"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46921e1f3e304da5b75f8ca3afff20ff",
      "question": "What did the defector tell CNN?",
      "prediction": "The defector told CNN that he told CNN he would be sent to prison and would be shot dead.",
      "ground_truths": [
        "\"North Koreans don't speak openly,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c41854c556a044ccbf0031953cbf72cf",
      "question": "What were 2 top South Korean officials under fire for?",
      "prediction": "The answer is: North Korean state-run news agency KCNA.",
      "ground_truths": [
        "from TV news coverage,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "24826e64b8864860ad6daffddd3e2ade",
      "question": "What did Kim Jong Un do before his father's death?",
      "prediction": "The answer is: North Korea likely did not go through a failed military transfer, implying that North Korea has a potential capacity to defy its government.",
      "ground_truths": [
        "issued"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f3330082ab946a7b182367e4acf9923",
      "question": "What is Jonathan Pryce honored for?",
      "prediction": "The answer is \"Actor Alan Cumming.\"",
      "ground_truths": [
        "a Golden Globe-nominated actor who has had roles in the \"Pirates of the Caribbean\" movies, \"Ronin,\" and the James Bond film \"Tomorrow Never Dies.\" He won a best actor award in 1995 at Cannes"
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "d8a24a9b52424e14bc0d9973651d0254",
      "question": "When is the honor day?",
      "prediction": "The honor day is on April 10th.",
      "ground_truths": [
        "once on New Year's"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "11a57027e3ff46fb8465d96bd0fbb6fe",
      "question": "What is the weather like in the ITCZ?",
      "prediction": "The answer directly from the context is: \"There is a region of the world where the weather is always hot and humid and rains almost every day of the year.  This is a predictable, albeit potentially dangerous, weather state.  Air France Flight 447 went down in a volatile zone along the equator between South America and Africa.  The Intertropical Convergence Zone, which circles the Earth near the equator, is where officials say Air France Flight 447 plunged into the Atlantic Ocean. While weather has not been cited as a reason for the plane's crash, the tragedy is considered a critical event that calls attention to",
      "ground_truths": [
        "volatile"
      ],
      "em": 0,
      "f1": 0.02247191011235955
    },
    {
      "id": "1b9f87a19463462dac00774f0c77eb01",
      "question": "Where did the plane crash?",
      "prediction": "The answer to the question is: \"The ITCZ is a belt of low pressure that wraps around the planet.\"",
      "ground_truths": [
        "volatile zone along the equator between South America and Africa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c74487fc1c943879ac66dda1e03d85f",
      "question": "Which airlines plane was involved?",
      "prediction": "The answer is: \"Air France Flight 447 was involved in a volatile zone along the equator between South America and Africa. The Intertropical Convergence Zone, which circles the Earth near the equator, is where officials say Air France Flight 447 plunged into the Atlantic Ocean. While weather has not been cited as a reason for the crash, the tragedy does call attention to the region.",
      "ground_truths": [
        "Air France"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "6ac3aadc635c40e1a687524905e7cef0",
      "question": "Where is the ITCZ?",
      "prediction": "The answer to the question is the ITCZ, a belt of low-pressure zones around the Earth, located near the equator.",
      "ground_truths": [
        "near the equator,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5b4a540c8adf40a181b5fd70e898fad2",
      "question": "Where did the Air France plane go down?",
      "prediction": "The answer is the ITCZ, not the Air France plane.",
      "ground_truths": [
        "along the equator between South America and Africa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a73d6910ac6346e2a8251edac9754aca",
      "question": "What is the name of the airline?",
      "prediction": "The answer is: The airline.",
      "ground_truths": [
        "Air France"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a0112def1ba44d2b4b535e772a43041",
      "question": "Where is the zone located?",
      "prediction": "The answer is the ITCZ.",
      "ground_truths": [
        "The Intertropical Convergence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ff26483d64741f898660d55d3139a45",
      "question": "what is the pandemic",
      "prediction": "The answer is: \"I am in the infirmary with the rest of the family. I asked my friend Clif to write letters to my friends, without of course, telling them that I am a little under the weather.\"",
      "ground_truths": [
        "Spanish flu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65bf7e1dc06d4affa15e9f99b800d94a",
      "question": "when did it ravage military camps",
      "prediction": "The answer is yes, based on the context provided:",
      "ground_truths": [
        "1918-1919."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "991079f7ad3d4e86b206d44758bee040",
      "question": "When was the outbreak?",
      "prediction": "The answer is the last day of the train trip to Georgia.",
      "ground_truths": [
        "1918-1919."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ded3279534804212b9f775a74529df74",
      "question": "Who survived flu",
      "prediction": "The answer is: Vincent Culhane, a 21-year-old clothing salesman from Chicago in Illinois, who has been sick.",
      "ground_truths": [
        "Martin \"Al\" Culhane,"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "4897f36620f8405e8d1b6178fbd8fc1a",
      "question": "is the zetas cartel from the veracruz state",
      "prediction": "The answer is: **No, the video is not definitively proof of the formation of a vigilante group in Veracruz. The speakers do not claim responsibility for the deaths.**",
      "ground_truths": [
        "the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "465185c4d005477785c716fc985c1d64",
      "question": "will authoroties investigate these groups",
      "prediction": "The answer to the question is: **yes, the video is a potentially useful tool for the authoroties to investigate the formation and spread of a vigilante group in Veracruz.**",
      "ground_truths": [
        "it would"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac6b3390afcf49538fc8c6e4f4d90b92",
      "question": "Which state does the Zetas cartel operate in?",
      "prediction": "The Zetas cartel operates in Mexico.",
      "ground_truths": [
        "Veracruz, Mexico,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "9a38ee24afda42c78816baccc7c10da8",
      "question": "What is Zeta Killers' goal?",
      "prediction": "The answer is: The video's goal is to eliminate the Zetas cartel.",
      "ground_truths": [
        "to kill members of the"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "441a8fbb81d34b0cb294a47cb163900c",
      "question": "which is the goal of the group",
      "prediction": "The goal of the Mata Zetas is to kill members of the Zetas, a ruthless cartel.",
      "ground_truths": [
        "cartel from the state of Veracruz, Mexico,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "8ffd7b74fa0b4ffc9aa2efd87a33b330",
      "question": "Where was the video released?",
      "prediction": "The question asks where was the video released.",
      "ground_truths": [
        "YouTube"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eed13bd468bf433ca4c03cc75f8dce9d",
      "question": "What law might the deal break?",
      "prediction": "The answer is: **The London, England central government's labour policy, which aims to expand the number of jobs in the UK's energy sector, including in a minimum of nine weeks, if it's not a result of a compromise.**",
      "ground_truths": [
        "European labor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b57d7f7dff44384a3b3603662f82149",
      "question": "what was the reason  for the  striking",
      "prediction": "The answer is: \"The problem with the Lindsey oil refinery strike is the lack of a meaningful and comprehensive agreement between the British Union of Setting Workers (ACAS) and the French-based oil company Total. The union, in a written statement, stated that the agreement eliminates the possibility of hiring new jobs for at least nine weeks, with a minimum of nine weeks, and that the service contract for a full-time contract is rejected.\"",
      "ground_truths": [
        "opposed to plans to employ non-British workers"
      ],
      "em": 0,
      "f1": 0.030303030303030297
    },
    {
      "id": "27cb6b4286584508b80a414b95f48853",
      "question": "What caused the protests?",
      "prediction": "The answer is: \"The problem is not the workers from other European countries working in the UK, nor is it about foreign contractors winning contracts in the UK. The main problem is the government's failure to understand the fundamental issues, and the problem is not workers from other European countries working in the UK, nor is it about foreign contractors winning contracts in the UK.\"",
      "ground_truths": [
        "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England."
      ],
      "em": 0,
      "f1": 0.08571428571428572
    },
    {
      "id": "4dfec6615c4b490986e2c90a5587269b",
      "question": "where did this occur",
      "prediction": "The answer is: the London, England (CNN).",
      "ground_truths": [
        "Lindsay Oil Refinery"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bed2b391202744bd9619d3ecd0e410f8",
      "question": "What sparked the protests?",
      "prediction": "The context provides a list of key points about the London-based oil refinery, the Lindsay Oil Refinery, the eastern England's (central England) government, and the French-based oil company Total. These contribute to the answer by:",
      "ground_truths": [
        "hiring of hundreds of foreign workers"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "e4c238fe056f476cbdc11d6c081f5638",
      "question": "What is number of new jobs offered?",
      "prediction": "The answer is **102**.",
      "ground_truths": [
        "101"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3485ce6a752a4017bac723554c115dec",
      "question": "Who tries to snatch children from his ex-wife?",
      "prediction": "The answer is Tennessee.",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c614fa519d3c4da3a71fec5baec6fd60",
      "question": "What was the father charged with?",
      "prediction": "The father charged with the abduction of minors.",
      "ground_truths": [
        "the abduction of minors."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f2afec4767b340b1aa3a2504820aca8c",
      "question": "Who has legal custody in the U.S?",
      "prediction": "The answer is Japan, but the document does not provide specific details on who has legal custody in the U.S. The document does not specify which country has legal custody, implying it is a feature that is not explicitly stated.",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac0b5576a66945a299d5ff45ce412c7a",
      "question": "Ex-wife of who?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45b58998fc8d495d8f0bc8b2755c1b8a",
      "question": "Where is the wife living?",
      "prediction": "The answer is the Japanese city of Fukuoka, Japan.",
      "ground_truths": [
        "Fukuoka,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "5189071c6f2841a390270f836ae93532",
      "question": "Who did Woods play?",
      "prediction": "Based on the provided context, the answer is:",
      "ground_truths": [
        "Adam Scott,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5682e5cacae448f7aeb4d656fa1169aa",
      "question": "Who said he was sorry?",
      "prediction": "The answer is \"I apologize to Tiger Woods and anyone else I have offended.\"",
      "ground_truths": [
        "Steve Williams"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2ef4a73b4534189a675c9106a6fcc91",
      "question": "What started the incedent?",
      "prediction": "The answer is: \"Stevie is an outstanding caddy and a friend, and has been instrumental in many of my accomplishments. I wish him great success in the future.\"",
      "ground_truths": [
        "racially-tinged remark"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "78bc788e403442bb8ab8024f85ae1619",
      "question": "What did Woods say?",
      "prediction": "The answer is: \"I apologize for any unintentional racism. I apologize to Tiger and anyone else I have offended.\"",
      "ground_truths": [
        "\"It was a wrong thing to"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "defe2a4ffe5743fda12c7112beabd0c9",
      "question": "What is Tiger Woods famous for?",
      "prediction": "Here's the answer, based on the provided context:",
      "ground_truths": [
        "Golfer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aeacc63714a45baa4d6e99d076562c7",
      "question": "Where did they meet?",
      "prediction": "The answer is directly from the context provided.",
      "ground_truths": [
        "Lakes Golf Club in Sydney,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bc43174d52dc40a28f477aa1fdd10893",
      "question": "What was Williams's comment?",
      "prediction": "The answer is, \"I apologize for comments I made last night at the Annual Caddy Awards dinner in Shanghai, China, for racially-tinged remarks.",
      "ground_truths": [
        "\"I wanted to shove it up that black a--.\""
      ],
      "em": 0,
      "f1": 0.06896551724137931
    },
    {
      "id": "c30db74e7ad04470b02788614ef84a19",
      "question": "What does the Governor call for?",
      "prediction": "The Governor calls for 1,000 federal police officers to be held for 40 days.",
      "ground_truths": [
        "his brother to surrender."
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "39931ad8113f4243bad10af56e0a23f7",
      "question": "How many were killed?",
      "prediction": "The answer is 12.",
      "ground_truths": [
        "at least 18 federal agents and two soldiers have been"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6246b3d00d3a42af98aa3d31b06e6759",
      "question": "Who was arrested Saturday?",
      "prediction": "The arrested Saturday was **Alvo Rueda Medina**.",
      "ground_truths": [
        "10 municipal police officers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6376ffdc2d5a43059b06993284c0a527",
      "question": "Who were found Tuesday?",
      "prediction": "The answer is: \"So that you come for another. We will be waiting for you here.\"",
      "ground_truths": [
        "federal officers' bodies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0851a285aa9e4cd190820a5d7bc66eaf",
      "question": "Who was arrested/",
      "prediction": "The answer is **Mexico City, Mexico (CNN).**",
      "ground_truths": [
        "10 municipal police officers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9927fb78653545cf81d796c766606c28",
      "question": "Where  did it occur?",
      "prediction": "The answer is Mexico City, Mexico.",
      "ground_truths": [
        "in southwestern Mexico,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "dc506c99b8ad43b4b8b0ee48037a3dc6",
      "question": "Who shows CNN around her \"Golden City,\" Prague?",
      "prediction": "The answer to the question is Petra Nemcova, who was born in Czechoslovakia and brought to Prague to visit her aunt.",
      "ground_truths": [
        "Petra Nemcova"
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "6ab0e867881248999050368b8154ddb9",
      "question": "What did the supermodel show CNN around?",
      "prediction": "The CNN posed a question to Petra Nemcova about her childhood and her experience in Prague, asking about the first time she came to Prague. Nemcova's answer revealed that she was changed by the experience in Prague, where she was visited by her aunt and the city's cultural immersion, which she learned from a communist country and helped her appreciate the beauty of Prague's architecture, history, and the city's change. She also mentioned her love for the \"Golden City and her smiling face,\" despite the setbacks of her life. CNN also mentioned the vibrant lighting of Prague, the intense attention of the city's",
      "ground_truths": [
        "Prague and tells of her love for the \"Golden City,\""
      ],
      "em": 0,
      "f1": 0.16161616161616163
    },
    {
      "id": "7ee54cdbbd32446d838b0e66fbca3303",
      "question": "Who survived the 2004 tsunami?",
      "prediction": "Based on the context, the answer is Petra Nemcova, who grew up with few luxuries in Soviet-controlled Czechoslovakia. She says her most important work is her charity, the Happy Hearts Fund, which she has founded, as she shows CNN around Prague and shares her experiences with CNN. CNN's story reveals that Nemcova spent her childhood in a communist Czechoslovakia and, as a result, she gained a deeper understanding of the Soviet system and the importance of working in the \"Golden City, which was a foreign city\" rather than a British city. CNN's story also highlights the positive impact of the positive changes in Prague,",
      "ground_truths": [
        "Petra Nemcova"
      ],
      "em": 0,
      "f1": 0.043010752688172046
    },
    {
      "id": "03c1dba3b8b24280a68bf48f65005770",
      "question": "What did Nemcova say?",
      "prediction": "Petra Nemcova's answer to CNN's question about her charity, Happy Hearts Fund, is:",
      "ground_truths": [
        "her most important work is her charity, the Happy Hearts Fund."
      ],
      "em": 0,
      "f1": 0.5217391304347826
    },
    {
      "id": "2516b55124254e5188c40f9ca4803822",
      "question": "What does Nemcova say about Prague?",
      "prediction": "Petra Nemcova's answer to CNN's question about her experiences at Prague, a former Victoria's Secret supermodel, provides a brief glimpse into her feelings and her motivations. She describes the journey, highlighting her long-term commitment to her charitable work, her loving attachment to the \"Golden City, which she learned from a communist country, and the cultural changes and the \"wow\" experience in Prague. She also touches on her connection to her childhood, her appreciation for the \"golden city of Paris, as a bit of a rush and the intense mix of the Communist and the cultural changes, and the importance of cultural value",
      "ground_truths": [
        "is a city of romance, of incredible architecture and history."
      ],
      "em": 0,
      "f1": 0.08247422680412371
    },
    {
      "id": "79169de1c595423a8fe3e1554a5e9339",
      "question": "What is Prague like?",
      "prediction": "Petra Nemcova is a former Victoria's Secret supermodel, who, despite her past in the Soviet system, told CNN that her most important work is her charity, the Happy Hearts Fund.",
      "ground_truths": [
        "city of romance, of incredible architecture and history."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6dbdda734bb43baa65aa91d4323d5a9",
      "question": "Which president spoke out during the campaign?",
      "prediction": "The answer is: Clinton.",
      "ground_truths": [
        "Clinton"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "39b27b09528b44b6b53c10b42e0ba36b",
      "question": "who accuses the Obama camp of distorting her remarks on civil rights?",
      "prediction": "The answer is \"I've got before me a list of 80 attacks on Hillary Clinton in the latest back-and-forth bickering between the two rival Democratic presidential campaigns.\"",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "e403867966a540de9f7437bf6e78baa2",
      "question": "who speaks out in latest back-and-forth between campaigns?",
      "prediction": "The answer is, \"I believe in civil rights, and I believe in equality, and I believe in equal rights for all.\"",
      "ground_truths": [
        "President Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19d6ff42a894452a88032c752d5faf90",
      "question": "Whats the BET founder called?",
      "prediction": "The answer is \"African-American voters,\" and the context does not provide the definitive answer. The answer is based on the remarks made by Clinton and Obama in the aftermath of the Iraq War, and the discussion by the Clinton campaign, as expressed by Obama, at a rally in Reno, Nevada.",
      "ground_truths": [
        "Bob Johnson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fd8126d933f4bbeba9ce4310a7d0573",
      "question": "who founded BET?",
      "prediction": "The answer is: \"I've got before me a list of 80 attacks on Hillary Clinton that are quite personal by Sen. Obama and his campaign.",
      "ground_truths": [
        "Bob Johnson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ed50e9d4fab41f1b18c6c91425fc9bf",
      "question": "Who accused the Obaam camp of distorting her remark?",
      "prediction": "The answer is \"I disagree with Sen. Clinton or Sen. Edwards on how to get things done, but we share the goals of democrats.\"",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "d1381a9741714a0ea37173cfffae4471",
      "question": "What was the charge?",
      "prediction": "The charge was attempted burglary.",
      "ground_truths": [
        "attempted burglary"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "253b5699390142a6a05d5c8fa01ea03e",
      "question": "What was Brancato cleared of?",
      "prediction": "Answer: Brancato was cleared of life in prison for attempted burglary, but his accomplice, Steven Armento, was convicted in the shooting and sentenced to life in prison without parole.",
      "ground_truths": [
        "Enchautegui's death,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "892800797761464b8fb078e1582e470d",
      "question": "Who played a wannabe mobster?",
      "prediction": "The answer is Lillo Brancato Jr.",
      "ground_truths": [
        "Lillo Brancato Jr."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "d2f3f9d62ed945aea125ef7fcd1b2382",
      "question": "Who sought 15 years?",
      "prediction": "The question asks for the person who sought 15 years in prison for the botched burglary in New York.",
      "ground_truths": [
        "The Bronx County District Attorneys Office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a080a3af08b47bf9adda601a84ac0fc",
      "question": "Who was cleared in officer's death?",
      "prediction": "The answer is Lillo Brancato Jr.",
      "ground_truths": [
        "Brancato"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "2071b336b9de4a18a892086a553afd50",
      "question": "What was the allegation?",
      "prediction": "The answer is:  The allegation was that Brancato and Armento were drinking at a strip club when they decided to go hunting for valium.",
      "ground_truths": [
        "attempted burglary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5b5d4f85c274be49ad11c9d258a3a99",
      "question": "Who sought 15 years for Lillo Brancato Jr. ?",
      "prediction": "The question asks for the names of the 15-year-old New York City police officer who was convicted in a burglary that left an off-duty New York police officer dead.",
      "ground_truths": [
        "Bronx County District Attorneys Office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbf8dbab5324461592f31283f4beb63c",
      "question": "Did the court hear the cases?",
      "prediction": "The answer is yes, the court system, America's formal arbiter of truth, would evaluate the competing claims.",
      "ground_truths": [
        "would refuse to even consider them."
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "c432211d434a4182823641e78e5cbadf",
      "question": "who were the women accusing?",
      "prediction": "The answers to the questions in the context are:",
      "ground_truths": [
        "Herman Cain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d248197351dc49c69e5a91d0820211bb",
      "question": "what claims are often settled out of court?",
      "prediction": "The answers to the questions, based on the context provided:",
      "ground_truths": [
        "sexual harassment"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a60524d20f34373bc8dd3389afaab6d",
      "question": "who is accusing Cain?",
      "prediction": "The answer is the **National Restaurant Association**.",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1dc2fde787694953a64b5c5b60b4fd7c",
      "question": "What did Harrison die of?",
      "prediction": "Based on the context, the answer is:",
      "ground_truths": [
        "cancer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0eebb69418524a59813c284930197bc9",
      "question": "What did Harrison die from?",
      "prediction": "Answer: George Harrison died from cancer in 2001.",
      "ground_truths": [
        "cancer"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "4bbfb9a271d84fe688e97aeae21286c0",
      "question": "What was Harrison's star next to?",
      "prediction": "The answer is \"Let it Roll: Songs by George Harrison\".",
      "ground_truths": [
        "the iconic Hollywood headquarters of Capitol Records,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1ad33c195e24a21bb21cd62dd90083e",
      "question": "Where is Harrison's star?",
      "prediction": "The answer is \"Hollywood headquarters of Capitol Records, the distributor of much of Harrison's music for the past five decades.\"",
      "ground_truths": [
        "Walk of Fame."
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "e53e39ed53ca4ffb9cc6159bb9771a8b",
      "question": "Where is Capitol Records?",
      "prediction": "The answer is \"Capitol Records, California.\"",
      "ground_truths": [
        "Hollywood"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "abd3dc653b6c4e1184e70f270296d79f",
      "question": "What did Harrison pen?",
      "prediction": "The answer is \"Let it Roll: Songs by George Harrison,\" which translates to \"Let it Roll: Songs by George Harrison,\" and it's a very short, direct answer, based on the context.",
      "ground_truths": [
        "\"Let it Roll:"
      ],
      "em": 0,
      "f1": 0.19354838709677416
    },
    {
      "id": "da45498817ed4a53b4a18ee852c43ba5",
      "question": "what country was the quake",
      "prediction": "The earthquake caused a quarter-mile pier in Port-au-Prince, Haiti to crumble into the sea, and two of Thomas's trucks were also lost in the water. The port is closed, and the area is deserted. The crew of 35 trucks, the same crew employed by 50 employees, were out of work.",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0.046511627906976744
    },
    {
      "id": "e03180d675aa4e1eb05fb44121ffa00f",
      "question": "Where was the earthquake?",
      "prediction": "The earthquake caused a quarter-mile pier to crumble into the sea, and two trucks, the few workers who went into the water, were caught in the water. The port remains shut down, and desperately needed aid cannot be unloaded quickly.",
      "ground_truths": [
        "Port-au-Prince harbor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "434191151c744073952ffe063744b8c5",
      "question": "what were the damages",
      "prediction": "The answer is: \"We are not giving any concrete details on the damages. The fact that Haiti is a poorer country is a key indicator of a more violent and impoverished economic situation.\"",
      "ground_truths": [
        "a quarter-mile pier crumbling into the sea along with two of his trucks."
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "03292748a1724185affa6b5fb0e7624c",
      "question": "What happens without the port?",
      "prediction": "The answer is: \"We are not alone. I am not alone.",
      "ground_truths": [
        "serious consequences for Haiti,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d84a1972d944779becf26d14afdcce2",
      "question": "what city was damaged",
      "prediction": "The answer is Haiti.",
      "ground_truths": [
        "Port-au-Prince"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cfc8297fe16243e68fd7df070d7bd587",
      "question": "what does bill haas claimed?",
      "prediction": "Bill Haas claims a one-shot victory in the Bob Hope Classic on the final hole.",
      "ground_truths": [
        "a one-shot victory in the Bob Hope Classic"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1e326f32613a417fb55b23337862c62b",
      "question": "who completed victory?",
      "prediction": "The answer is:  \"it's not easy to win. I was so nervous coming down the stretch. I still don't know how I hit the shot on 18 - my hands were shaking.\"",
      "ground_truths": [
        "Bill Haas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8c780963994f44c7835580cb22bc2fdc",
      "question": "American Bill Haas has claimed what?",
      "prediction": "American Bill Haas claims a one-shot victory in the Bob Hope Classic.",
      "ground_truths": [
        "a one-shot victory in the Bob Hope Classic on the final hole"
      ],
      "em": 0,
      "f1": 0.631578947368421
    },
    {
      "id": "16d87b6d44b74c9d9c705b0471dcb4b2",
      "question": "who finished tie for second place?",
      "prediction": "The final two finishers were tied for second place, with Clark and Kuchar finishing second.",
      "ground_truths": [
        "Tim Clark, Matt Kuchar and Bubba Watson"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "c24d3c06799444db84d703cd715d2979",
      "question": "Haas completed his victory with what?",
      "prediction": "Answer: Haas completed his victory with a birdie four at the last hole after he struck a brave tee shot.",
      "ground_truths": [
        "one-shot"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38ec897108f245b19aeba0e93e304360",
      "question": "Who all finished tied for second?",
      "prediction": "The final hole in the Bob Hope Classic ended with Tim Clark and Bubba Watson tied for second place.",
      "ground_truths": [
        "Tim Clark, Matt Kuchar and Bubba Watson"
      ],
      "em": 0,
      "f1": 0.4166666666666667
    },
    {
      "id": "95ba4e0a74d24753bcb222ddfbfc9c32",
      "question": "What does the directive pertain to?",
      "prediction": "The directive pertains to the operations to reduce tension between the coalition's military forces and Afghan civilians in a concerted effort to maintain Afghan public support.",
      "ground_truths": [
        "coalition forces in Afghanistan"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "8ff1d1c601834b59be0b1500e3cf8887",
      "question": "Where were the raids?",
      "prediction": "The answer is: The official did not specify where the raids were conducted.",
      "ground_truths": [
        "Afghan homes and compounds,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2955002485c14b53a2044b2d8638b2fb",
      "question": "What did the directive pertain to?",
      "prediction": "The directive pertains to the NATO's (Washington) actions in Afghanistan to reduce tension between military forces and Afghan civilians.",
      "ground_truths": [
        "nighttime raids of Afghan homes and compounds,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "28a25e17eb8149e3bbd7f11b62f978b2",
      "question": "Who was trying to lower tension?",
      "prediction": "The answer is likely the U.S. (as the original prompt states).",
      "ground_truths": [
        "the coalition"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "918682dda07b40c2b5435339f4948ab9",
      "question": "What did the new rules call for?",
      "prediction": "The new rules called for:",
      "ground_truths": [
        "restrictions"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7bb51f59808149d2ba80ba54073183dd",
      "question": "Who apologized to his teammates and fans?",
      "prediction": "The answer is: **The Pittsburgh Steelers** apologized to their teammates and fans.",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b8f339350064021b229ab45ee46f007",
      "question": "Who won't appeal the six-game suspension?",
      "prediction": "The answer is \"I will not appeal the six-game suspension,\" and it doesn't specify which specific district attorney.",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8cb684a8be9a41dc9fcb35b383078039",
      "question": "Who is not pursuing charges, saying \"we do not prosecute morals.\"",
      "prediction": "The answer is \"We do not prosecute morals.\"",
      "ground_truths": [
        "Fred Bright,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76d462d4bd4748cc9eb14ebfeabe71e9",
      "question": "What is the Pittsburgh Steelers quarterback being accused of?",
      "prediction": "The Pittsburgh Steelers quarterback is being accused of failing to commit a crime. He is being accused of misconduct, including failing to commit a crime and making a mistake in his performance. He is being accused of failing to commit a crime.",
      "ground_truths": [
        "raping"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a17b9869bbd14bf2975af4343af6cdaa",
      "question": "Who is the Pittsburgh Steelers quarterback?",
      "prediction": "The Pittsburgh Steelers quarterback is said to be **Ben Roethlisberger**.",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "f5042953c54d4fd4b792e423cb3f8a96",
      "question": "What does the former shortstop have lined up?",
      "prediction": "The answer is \"Get in the Game: 8 Elements of Perseverance That Make the Difference.\"",
      "ground_truths": [
        "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\""
      ],
      "em": 0,
      "f1": 0.7333333333333334
    },
    {
      "id": "83641374d7ea41caaf1eee54213989b9",
      "question": "What does Ripken do now?",
      "prediction": "The answer is \"My business principles books,\" written by Donald T. Phillips, a former baseball manager who is now a business executive.",
      "ground_truths": [
        "is a businessman, team owner, radio-show host and author."
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "cc2e232f0d624c46a0c1af0788838811",
      "question": "What is Ripkin's brother's name?",
      "prediction": "The answer is Donald T. Phillips.",
      "ground_truths": [
        "Bill,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c580289b1a2d49aa86268e2b1aa1090f",
      "question": "When do President obama sign the Bill?",
      "prediction": "The answer is \"Washington (CNN).\"",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb9d02063e8a42ca8b34b7d82d4c4189",
      "question": "who signed the law?",
      "prediction": "The answer is \"President Obama.\"",
      "ground_truths": [
        "President Obama."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9d68b266ae434abb9d73cd2def6f8201",
      "question": "what is going to study the TSA?",
      "prediction": "The TSA is studying how to speed up screening of service members, and to the extent possible, their families.",
      "ground_truths": [
        "ways to speed up screening of service members and, to the extent possible, their families,"
      ],
      "em": 0,
      "f1": 0.8387096774193549
    },
    {
      "id": "f2512611b47d4f24bf38225a97ff0bc6",
      "question": "What the new law required?",
      "prediction": "The new law required the Transportation Security Administration to study ways to speed up screening of service members and, to the extent possible, their families, when the service members were in uniform and traveling on orders.",
      "ground_truths": [
        "the Transportation Security Administration to study ways to speed up screening of service members and,"
      ],
      "em": 0,
      "f1": 0.6086956521739131
    },
    {
      "id": "6d5eb886bc37490aa1854938f9b36903",
      "question": "From where did the miliitants cross?",
      "prediction": "From where did the militants cross?",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce3ef968a0704a3fa36f7f605ed0031a",
      "question": "Who many soldiers were killed?",
      "prediction": "The answer is \"Thousands of soldiers.\"",
      "ground_truths": [
        "40 militants and six Pakistan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2643b6d7fe94e54bb74da276f0a4646",
      "question": "how many militants are?",
      "prediction": "The answer is 40.",
      "ground_truths": [
        "Hundreds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "755056d3d07940929387e2e7e76f4acf",
      "question": "What happened?",
      "prediction": "Answer: Pakistan and Afghanistan share a porous 1,500-mile border.",
      "ground_truths": [
        "Hundreds of militants, believed to be foreign fighters, launched attacks"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96478949a8214ae4bd74130d64466203",
      "question": "who crossed into Pakistan From Afghanistan?",
      "prediction": "Answer: Pakistan",
      "ground_truths": [
        "the fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "006c10a191a846239c8314068e065ec3",
      "question": "How many killed into fighting?",
      "prediction": "The answer is 40.",
      "ground_truths": [
        "40 militants and six Pakistan soldiers"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "121d9ead9e87477baedf4605be92bbf7",
      "question": "Where did the attacks occur?",
      "prediction": "The attacks occurred at checkposts in the Mohmand agency district of Pakistan, in Afghanistan.",
      "ground_truths": [
        "Pakistan's border with Afghanistan"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "4cbd0481ba0f49cba4e5d1d6ef2d082f",
      "question": "What is to blame for the suicides?",
      "prediction": "The answer is: \"The complex factor is the time since the time of the suicide. The longer the deployment, the higher the chance of the suicide.\"",
      "ground_truths": [
        "Long troop deployments"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68efecfb6ac145c08a2be17c94889324",
      "question": "What does the Admiral say about suicides?",
      "prediction": "The Admiral's statement, \"If I survey the family, I'm going to get a different set of answers,\" indicates that the Admiral's comments about suicides are likely to be a strategic, perhaps a self-promotion tactic. He likely emphasizes the complexities of the situation, the impact of deployments, and the potential for changes in the military's response to psychological distress. He also expresses optimism about the efforts to prevent such a trend.",
      "ground_truths": [
        "\"stressed and tired force\""
      ],
      "em": 0,
      "f1": 0.03278688524590164
    },
    {
      "id": "50ddaaafdf0249f7afea9f2690b5a905",
      "question": "What are a third leading cause of death?",
      "prediction": "The answer is:  The third leading cause of death in the Navy is **unconfirmed**.",
      "ground_truths": [
        "suicides"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b6096cea90274f34908139e77db8caac",
      "question": "What is the third leading cause of death in the Navy?",
      "prediction": "The context provides information about the number of suicides among military personnel in the U.S. Central Command. It also mentions the number of counselors and the specific challenges faced by the Navy, including the issue of suicide rates and the potential for counselors to be passed over for promotions. The final sentence, \"If I survey the family, I'm going to get a different set of answers,\" indicates a potential discussion about the difficulties of providing support to family members.",
      "ground_truths": [
        "suicides"
      ],
      "em": 0,
      "f1": 0.02985074626865672
    },
    {
      "id": "52c9cb22b1f84a37b49234eb7862ce03",
      "question": "Who said we must find ways to relieve stress?",
      "prediction": "The answer is: \"We must find ways to relieve some of this stress.\"",
      "ground_truths": [
        "Gen. Peter W. Chiarelli,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dac3c581c100493a93baccc6a59a7959",
      "question": "What is cited as part of the problem?",
      "prediction": "The answer to the question is, \"The Army has addressed this problem in a very aggressive way, but some suicides appear to defy all prevention efforts.\"",
      "ground_truths": [
        "Long troop deployments in Iraq,"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "f5f68ca83de14fa8a5663c76fa27eeb1",
      "question": "What was said to contribute to the problem?",
      "prediction": "The answer is: \"We must find ways to relieve some of this stress,\"",
      "ground_truths": [
        "seeking help"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ddc318071f01453ab87ceebe18062971",
      "question": "Who is Harbhajan Singh?",
      "prediction": "Harbhajan Singh is the name of the Indian all-rounder who scored 109 runs for Sri Lanka in the third Test of their series against India in Mumbai.",
      "ground_truths": [
        "Off-spinner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "78112e7810214d80b8fab5fa51c61200",
      "question": "What team is in the lead?",
      "prediction": "The team in the lead is India.",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ebf5e6a8dd1742b68312c2c6ada5aad0",
      "question": "Against which team did he play?",
      "prediction": "The answer is India.",
      "ground_truths": [
        "India"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cae1c72a196e41c0919e17bc003a3a5f",
      "question": "Who scored the best?",
      "prediction": "The answer is **Sri Lanka**.",
      "ground_truths": [
        "Tillakaratne Dilshan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d62824da543e474da60ae07308009055",
      "question": "What countries where playing?",
      "prediction": "The countries where playing is: India.",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "455324ce37fa42c8af627f9f832cfb24",
      "question": "What is happened in Sri Lanka?",
      "prediction": "In Sri Lanka, Dilshan scored 109 runs for 109 in 20 overs, bringing the score to 116.  His 108 in the first Test was scored by Tharanga Paranavitana, who made 53 runs for 112.  The draw saw Mudhika Sreekal, who scored 10 twos and 7 wickets.  The fourth wicket was caught in the tea, and the final out was bowled by Harbhajan Singh.  The final score was 74.  The next test was in the innings with the final wickets",
      "ground_truths": [
        "Tillakaratne Dilshan scored his sixth Test century"
      ],
      "em": 0,
      "f1": 0.10526315789473682
    },
    {
      "id": "c6d27c29d4774501b4f046af3bb83dfd",
      "question": "Where did they play?",
      "prediction": "The answer is **Sri Lanka**.",
      "ground_truths": [
        "Mumbai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "69e712f057644faf86a9688a85a2deee",
      "question": "For which country did Dilshan scored ?",
      "prediction": "For which country did Dilshan score for Sri Lanka?",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "d156cb80d58d45778ef7cf9fa0522bee",
      "question": "Who is expected to make a full recovery?",
      "prediction": "Answer: Kerstin Fritzl is expected to make a full recovery in time.",
      "ground_truths": [
        "Kerstin"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "f711b14dc8b341deaa8c3bfd6302219a",
      "question": "Who spent her entire life in a cellar?",
      "prediction": "Answer: Kerstin Fritzl spent her entire life in a cellar.",
      "ground_truths": [
        "Kerstin Fritzl,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "6b5a9875a58c46deb2bf7ef981366e70",
      "question": "Where was she taken?",
      "prediction": "The answer is: Kerstin Fritzl is taken to a regional clinic in Amstetten, west of Vienna.",
      "ground_truths": [
        "a hospital"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7c65f086477430ea2d5692f28290f18",
      "question": "who is kerstin fritzl",
      "prediction": "The answer is: Kerstin Fritzl is the 19-year-old woman who was hospitalized in Austria, and she is recovering well.",
      "ground_truths": [
        "19-year-old woman whose hospitalization exposed a shocking Austrian incest case"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "cb946aafe087450ea5405c800ddd069d",
      "question": "who is josef fritzl",
      "prediction": "The answer is: **Joseph Fritzl**.",
      "ground_truths": [
        "Elisabeth's father,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dadebe93fbd14ac3bbb95f4dcd0a5427",
      "question": "Who has confessed?",
      "prediction": "The answer is: Kerstin Fritzl.",
      "ground_truths": [
        "Josef Fritzl,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "9e35535336794e239e8fd7be704a930f",
      "question": "where was the dungeon",
      "prediction": "The answer is: Cerstin is recovering in a hospital in Amstetten.",
      "ground_truths": [
        "in the cellar"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "c2d72fa301114b31b83223143318edf2",
      "question": "Who is reunited with her family?",
      "prediction": "The answer is \"Kerstin Fritzl,\" the 19-year-old woman whose hospitalization exposed a shocking Austrian incest case has recovered well and wants to see the ocean, a pop concert, and a concert in British singer Robbie Williams.",
      "ground_truths": [
        "Kerstin Fritzl,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "f892ceb5a8ff484b87750d97f6a28809",
      "question": "What is her age?",
      "prediction": "Answer: Kerstin is 19 years old.",
      "ground_truths": [
        "19-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "92149fe0d2d841c99595358e6fb2d914",
      "question": "What is Olivia Newton-John's profession?",
      "prediction": "Olivia Newton-John's profession is still \"finding plenty things to become hopelessly devoted to.\"",
      "ground_truths": [
        "actress,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f198fe791ff44a0f8ec3ba8e26f7404a",
      "question": "What musical did she star in?",
      "prediction": "The answer is \"Grease\" by the British-born actress, Sandy Olssen.",
      "ground_truths": [
        "\"Grease\"."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "d7df0dd3409f4db0bd2d1ec8bdae1165",
      "question": "Who is the hit star of \"Grease\"?",
      "prediction": "The answer is \"The British-born actress, who was raised in Australia from the age of five, was already an established singing star.\"",
      "ground_truths": [
        "Olivia Newton-John"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb8d260acb134ba1b5d63ec07b2d6029",
      "question": "What campaign does this actress promote?",
      "prediction": "This actress promotes a campaign to educate women about breast self-examination.",
      "ground_truths": [
        "education about rainforests."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "5f6b232704774af9bc55a6c9740fa718",
      "question": "What is she passionate about?",
      "prediction": "Newton-John is passionate about:",
      "ground_truths": [
        "early detection and helping other women cope with the disease."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e941c4d2c7a425dba59fbd1dfc48813",
      "question": "What does Olivia Newton-John advocate?",
      "prediction": "Olivia Newton-John advocates for educating women about breast self-examination and empowering them to become more observant of their health and to take proper breast health care. She also champions early detection and the importance of awareness in helping other women cope with cancer.",
      "ground_truths": [
        "early detection"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "fd89ef9e375742d8bd0ef102e999b8f8",
      "question": "Who did Newton-John speak to?",
      "prediction": "According to Newton-John, the answer is **Olivia Newton-John**.",
      "ground_truths": [
        "CNN's Max Foster,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b17e011932344d83a7ac20554d83877a",
      "question": "Who interviewed Olivia Newton-John?",
      "prediction": "The question is: Who interviewed Olivia Newton-John?",
      "ground_truths": [
        "CNN's Max Foster,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af446346d828479ca02c375ec2973773",
      "question": "Who were aboard?",
      "prediction": "The answer is the **Disney World employees**.",
      "ground_truths": [
        "eight people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef7ff14fda8347479995319185ade05f",
      "question": "Who says one monorail train rammed?",
      "prediction": "The answer is: *the two monorail trains at Walt Disney World.",
      "ground_truths": [
        "A witness"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d5bc4fc6f0c425b98565b8f80bb6e43",
      "question": "What is the name of the driver?",
      "prediction": "The name of the driver is Austin Wuennenberg.",
      "ground_truths": [
        "Austin Wuennenberg,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "986fec610e0e4bf9bb3de6be1fa0356e",
      "question": "How many people were on board?",
      "prediction": "The answer is 2.",
      "ground_truths": [
        "eight"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d18ef7feb7c345b589599ab84b02d3df",
      "question": "Where did the crash occur?",
      "prediction": "The answer is Disney World.",
      "ground_truths": [
        "Disney World"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1b9decbc39c949e8bf28c759dfe4baa2",
      "question": "Who was identified as Austin Wuennenberg?",
      "prediction": "The answer is: \"Austin Wuennenberg, a senior at Stetson University.\"",
      "ground_truths": [
        "the driver"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08b635995bc64aefba8d3fdb405e1b4b",
      "question": "What age was the  driver?",
      "prediction": "The age of the driver is not specified. The text does not give the age of the driver.",
      "ground_truths": [
        "21-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7dd1e96f50b44d3fbe98d02cb6856adc",
      "question": "Who is called a cavewoman?",
      "prediction": "The answer is \"The Tinkler.\"",
      "ground_truths": [
        "The Tinkler"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f889e3e683014dd686734d48bbbb1235",
      "question": "Where was eclectic assortment of alternative-energy vehicles seen this week?",
      "prediction": "The answer is, the cars run on electricity and biofuels as well as gasoline, and all of these were seen on Capitol Hill, Washington D.C. with a focus on car-related displays.",
      "ground_truths": [
        "Capitol Hill,"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "896fd94d638040e488decb12656d6a66",
      "question": "Did the display attract anyone?",
      "prediction": "The answer is \"it attracted some U.S. senators who couldn't resist taking the vehicles for a spin.\"",
      "ground_truths": [
        "some U.S. senators"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "1fbe4ecf8907445c896a9af395235274",
      "question": "who was the event organized by",
      "prediction": "The event was organized by Bright Automotive, a small carmaker from Anderson, Indiana, to showcase its IDEA, a new, 100-mpg plug-in hybrid electric vehicle.",
      "ground_truths": [
        "Bright Automotive,"
      ],
      "em": 0,
      "f1": 0.17391304347826084
    },
    {
      "id": "6dd0ca13b6304a9c9bc6db4983f8b06b",
      "question": "who took the cars for a spin",
      "prediction": "Based on the text, the answer is **\"the Washington (CNN) Democratic senator, Thomas Carper, D-Delaware, and the General Motors (for a test drive) sold the vehicles for a spin**.",
      "ground_truths": [
        "U.S. senators"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b96caff10a14ab8863236af774fa135",
      "question": "What is happening on Capitol Hill this week?",
      "prediction": "The answer is that the cars were parked on Capitol Hill, with a change in the DMV's register.",
      "ground_truths": [
        "\"The Energy and Environmental Showcase,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb8d2b66f60041de88554c497fc8f67f",
      "question": "Who organized the event?",
      "prediction": "The answer is: The energy and environmental showcase of the New Energy Vehicle (NEV) in Washington, D.C. was organized by Bright Automotive, a small carmaker from Anderson, Indiana, to showcase its New Energy Vehicle.",
      "ground_truths": [
        "Bright Automotive,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "06657adf0df043b8a5e47a0d276ff377",
      "question": "Who  couldn't resist taking the cars for a spin?",
      "prediction": "The answer is **the Democratic senator from Indiana, Tom Harkin**.",
      "ground_truths": [
        "U.S. senators"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "954351e660e347e29070d3f06766e253",
      "question": "What brand did the owners rave about?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "Saturn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c87043201d84e269c6f2019f9a5794f",
      "question": "What parade is mentioned?",
      "prediction": "The parade is mentioned at Soddy-Daisy, Tennessee, northeast of Chattanooga, for a Christmas parade in vehicles of the Saturn brand, with Judy Pearson and Connie Terrell's Sky roadsters, and the red 2007 RedLine's S-100 Sky.",
      "ground_truths": [
        "Sunday's Christmas"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "c9ccb7f8f9784e9f8205e5510c340b2e",
      "question": "How many owners of Sky roadsters drive in the Christmas parade?",
      "prediction": "The answer is: 2,500.",
      "ground_truths": [
        "2,500"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2666396df440428abe681afcaf00c173",
      "question": "Who makes Sky roadsters?",
      "prediction": "The answer is \"Kat Koonce,\" of Tennessee.",
      "ground_truths": [
        "Saturn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "874e8e95e0b6405bb51e97f09a2551bc",
      "question": "What brand is vulnerable?",
      "prediction": "The answer is \"Saturn.\"",
      "ground_truths": [
        "Saturn"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7115423510b54a3ab131a64a9b78d8e1",
      "question": "Where did Stephen Johns work?",
      "prediction": "The answer is, \"at the U.S. Holocaust Memorial Museum, Washington, D.C.\"",
      "ground_truths": [
        "U.S. Holocaust Memorial Museum,"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "ac5ab6db315d4483a9c584fe0b1d6c15",
      "question": "Who is accused of killing him?",
      "prediction": "The answer is \"Johns was killed by an 88-year-old white supremacist.\"",
      "ground_truths": [
        "88-year-old white supremacist"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0bc085b22b8f46a7aa0e0117495a60cd",
      "question": "What did the Gentle Giant do?",
      "prediction": "The answer is \"He stood 6 feet 6 inches, according to Carter. And he lived up to the very end.\"",
      "ground_truths": [
        "opening the door for the man who shot him,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "06d07db2a1554ed0923e70dbb93c9880",
      "question": "What enabled  his killer to get into the museum?",
      "prediction": "The answer is \"it seemed to be kind of laid-back -- it didn's seem to be that dangerous,\" and \"just backfired.\"",
      "ground_truths": [
        "Security officer Stephen Johns reportedly opened the door for the man police say was"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49d1aeccefc748c09d6350910ec681cc",
      "question": "What were his interests?",
      "prediction": "Based on the context, the answer is \"Washington Redskins.\"",
      "ground_truths": [
        "Washington Redskins fan and loved to travel,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "698a7fabf429487f95c64776a7da5bd1",
      "question": "What did the security guard love?",
      "prediction": "The security guard's love for his Washington Redskins fan and his son, Stephen Jr., is likely a personal, long-standing trait.",
      "ground_truths": [
        "loved to travel,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0353ec5692504be7803b7868ad8878a4",
      "question": "Who is the \"gentle giant\"?",
      "prediction": "The answer is \"James von Brunn,\" an outspoken Holocaust denater who served six years in prison.",
      "ground_truths": [
        "Stephen Tyrone Johns"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "380e41515ca8462f8f87a2721989f282",
      "question": "What was the security guard's age?",
      "prediction": "The answer is 11-year-old.",
      "ground_truths": [
        "39,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f80e02b065344e0b8b45656370f35395",
      "question": "how many countrys affect the virus?",
      "prediction": "The answer is 11.",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f4e61efd586744d9941da1aea777b7d7",
      "question": "How many counties encountered the virus?",
      "prediction": "The number of confirmed cases of the H1N1 virus has jumped nearly 30 percent with 331 people being infected.",
      "ground_truths": [
        "11 countries,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e3602ad5a2864891955a696e59594e76",
      "question": "Where was the largest outbreak?",
      "prediction": "The largest outbreak was in Mexico, with 156 confirmed cases, and 141 cases confirmed in the United States.",
      "ground_truths": [
        "Mexico"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "c9a9b912b07b4c91a98f4134fbc3c6f5",
      "question": "in what country was largest outbreak?",
      "prediction": "The answer is **Mexico**.",
      "ground_truths": [
        "Mexico"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e20dba2bc3be4f5e9829da374bbd0164",
      "question": "What is mcDonald's going to do?",
      "prediction": "The answer is: \"McDonald's will be offering its burgers and fries to people in the Louvre, unlike the Louvre's attempts to provide a similar experience with a massive, vibrant, and very large version of the Tu coq. The Louvre is a museum, and the museum will be a different institution.\"",
      "ground_truths": [
        "serving its fast burgers in the Carrousel du Louvre,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "68a598230ecc4f168727f19bde806e07",
      "question": "What has been attracting people in the museum?",
      "prediction": "Answer:  The response indicates the French website \"Louvre pour tous\" (Louvre for Everyone) is described as a \"bad taste\" and criticized for its failure to prevent \"fragrances of fries\" drifting under Mona Lisa's nose.",
      "ground_truths": [
        "restaurants and boutiques,"
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "02650d3813a44872ad30447adccc3f0a",
      "question": "Where is McDonald's going to open a new store at?",
      "prediction": "The answer is \"McDonald's has no significant plans to open a new store in France.\"",
      "ground_truths": [
        "the Louvre."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9952bc66f13406c9d502751699d4aef",
      "question": "Where is the restuarant not the only fast food joint?",
      "prediction": "The answer is: McDonald's.",
      "ground_truths": [
        "Carrousel du Louvre,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66b1143618d34663a5151d1dba850890",
      "question": "Who is opening up in the mall?",
      "prediction": "The answer is: The Parisien, a daily newspaper, describes McDonald's to be in the Louvre as a \"bad taste\" and \"fragrances of fries\" in the Carrousel du Louvre.",
      "ground_truths": [
        "McDonald's"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "41411cf3a7db452ea9f4059bf01969c0",
      "question": "Where is McDonald's to open a restaurant?",
      "prediction": "The answer is the Louvre.",
      "ground_truths": [
        "--the Louvre."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5f92118f88a84ea1805aa655a77e0757",
      "question": "Who did the crash kill?",
      "prediction": "The answer is \"Nours, and the Sole Survivor.\"",
      "ground_truths": [
        "Princess Diana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29790bf111e64372aaccd31af02cba74",
      "question": "Who employed Rees, the bodyguard?",
      "prediction": "The answer is \"Al Fayed.\"",
      "ground_truths": [
        "Princess Diana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "667d1d15e8634cfaa77c52ab1b9f5377",
      "question": "Who is Dodi Fayed?",
      "prediction": "The answer is Dodi Fayed.",
      "ground_truths": [
        "her boyfriend,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d46318c6e274504bbccc0a69725807e",
      "question": "Who died in the accident with princess Diana",
      "prediction": "The answer is the **crash** was a **accident**.",
      "ground_truths": [
        "her boyfriend, Dodi Fayed, and their driver, Henri Paul."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27cbadd4e283464483af698e10bb3d6c",
      "question": "After the crash who received threatening calls and letters?",
      "prediction": "Answer: The answer is \"We know who you are, we know where you are, and we know where you live,\" with a note of \"we know who you are, we know where you are, and we know where you live.\"",
      "ground_truths": [
        "Trevor Rees,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb25fe6199404b3ab0e45da51e453da3",
      "question": "Who died in the accident?",
      "prediction": "The answer is Al Fayed.",
      "ground_truths": [
        "Diana,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c629896db95e4b6a8fd3bc475603b485",
      "question": "Who was Rees employed by?",
      "prediction": "The answer is \"Rees, the front-seat passenger in the Mercedes that carried Diana, her boyfriend Dodi Fayed, and their driver, Henri Paul.\"",
      "ground_truths": [
        "working for al Fayed's security team"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "328f361c3c2445f7bf38a8c3c6adf490",
      "question": "Who released a list of best and worst used cars?",
      "prediction": "The answer is: The Lexus reputation is for high-end luxury, quality, and long-term dependability.",
      "ground_truths": [
        "Consumer Reports magazine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2de95c207ed54376a49d51d5d65f5489",
      "question": "Where do some buyers go to get a luxury car?",
      "prediction": "Based on the context, the answer is: **used luxury cars**.",
      "ground_truths": [
        "used-luxury market"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "82e6628da7fb4bf6acb8ff7b4faa635f",
      "question": "What is a reason some buyers go to used-car markets?",
      "prediction": "The answer is that some buyers may seek out used-luxury car models (specifically the Lexus, Lincoln, and Infiniti) for their higher-end, long-term dependability and \"luxury\" of traditional car offerings.",
      "ground_truths": [
        "a new model is simply out of their reach."
      ],
      "em": 0,
      "f1": 0.22857142857142856
    },
    {
      "id": "452c6099ee4a4cea83295935ac292139",
      "question": "Where were best and worst used cars listed?",
      "prediction": "The context provides a list of the best and worst used car models, ranked by price range. The \"worst\" and \"best\" were listed in the $24,000-30,000 price range. The \"best\" listed was the 3-liter, 253-hp 2-cylinder, 5-speed automatic with a 5-speed manual in 2005 and 2006 Acura MDX.",
      "ground_truths": [
        "Consumer Reports magazine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2dd91bd67c247dcbb01198427f8878a",
      "question": "What magazine is discussed?",
      "prediction": "The answer is the Lexus reputation, for high-end luxury, quality, and long-term dependability.",
      "ground_truths": [
        "Consumer Reports"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1bc4a407717f431aafc52dd9191baf1a",
      "question": "What car made the list of cars under 30k?",
      "prediction": "The answer is that Lexus' reputation is for high-end luxury, quality and long-term dependability.",
      "ground_truths": [
        "2005 & 2006 Acura MDX"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eaaea7421086462889eb8afd1484b01b",
      "question": "What are the names of Jackson's sisters?",
      "prediction": "The names of Jackson's sisters are not mentioned in the context provided.",
      "ground_truths": [
        "Janet and La Toya,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3de28833bb54a6d993201db9cee19aa",
      "question": "Which item did the doctors say was safe in the home?",
      "prediction": "The doctors said that Jackson's mother, Janet, and La Toya, and brother Randy Jackson were in court to watch Tuesday's testimony.",
      "ground_truths": [
        "the surgical anesthetic propofol"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9257f4722bf4248ae7f2bdcd0fd3079",
      "question": "what did jackson tell the nurse",
      "prediction": "Answer: Jackson told the nurse, Dr. Arnold Klein, that he was \"his principal physician, I thought he should know,\" as Jackson \"seemed a little distracted and not focused\" in a meeting after a visit to Klein's clinic.",
      "ground_truths": [
        "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored."
      ],
      "em": 0,
      "f1": 0.15094339622641512
    },
    {
      "id": "9501472c0039417f9f574fe8656627ea",
      "question": "Who attended court?",
      "prediction": "The answer is the head of the promotion company.",
      "ground_truths": [
        "Jackson's mother, Katherine, sisters Janet and La Toya, and brother Randy Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a242910379e44bcb85f26a05fa459d87",
      "question": "what would jackson have had to pay",
      "prediction": "Answer: Jackson would have to pay for all the production and rehearsal costs.",
      "ground_truths": [
        "all of the production and rehearsal costs,"
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "d70fb3550f2d4140b3b8531591ca46f0",
      "question": "What costs would Jackson have incurred if the tour was cancelled?",
      "prediction": "The cost of the London shows would be $40 million.",
      "ground_truths": [
        "production and rehearsal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a2b8b0d02074b15bc4b3cd8d9d28750",
      "question": "who attended court on tuesday",
      "prediction": "The answer is: Katherine, Andrew Klein, and the Nurse Cheryl Lee.",
      "ground_truths": [
        "Jackson's mother, Katherine, sisters Janet"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "962c66a71546468d8f6a0ece3c9a4b91",
      "question": "Who assured Jackson that propofol was safe?",
      "prediction": "The answer is:  All, with the information provided, the answer is:  Conrad Murray.",
      "ground_truths": [
        "doctors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c75b242082a488dbbf0c6feb4739e69",
      "question": "what did the police do",
      "prediction": "The police did not act to cover up the murder of Scarlett Keeling, who was killed in Goa. They tried to cover up the truth by arresting Samson D'Souza, a 29-year-old bartender at Liu's, who was also a victim of rape. The investigation at this time, the focus is on the case of Keeling's murder, and no police officers were involved.",
      "ground_truths": [
        "covered up the murder of a British teenage girl in Goa"
      ],
      "em": 0,
      "f1": 0.16129032258064518
    },
    {
      "id": "fbe718cccc7642fe807edbe160f1eb25",
      "question": "What was the girl's name?",
      "prediction": "The answer is Scarlett Keeling.",
      "ground_truths": [
        "Scarlett Keeling"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "310e44374a0c4af285ee82e916f46590",
      "question": "Who covered up the murder of a UK girl?",
      "prediction": "The answer is Delhi.",
      "ground_truths": [
        "Police"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "909be8a03d3640099f1f5094b47d9aa9",
      "question": "Where was the murder covered up?",
      "prediction": "The answer is Delhi.",
      "ground_truths": [
        "Goa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bde94c1ca900410fb83728d7a7a81621",
      "question": "Where did the killer find the UK girl?",
      "prediction": "The answer is, the killer found the UK girl in Goa.",
      "ground_truths": [
        "Goa"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "33729b7290694eb380e57b5083f50f26",
      "question": "What age was the murdered girl?",
      "prediction": "The answer is 15.",
      "ground_truths": [
        "15-year-old's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "757bb36f5eec45e3ae20ae5d5b9800c4",
      "question": "what did the mother say",
      "prediction": "The mother said she sent a letter to Goa's chief minister to ask for India's Central Bureau of Investigation to investigate the case.",
      "ground_truths": [
        "said she also believed police were trying to cover up the truth behind her daughter's murder,"
      ],
      "em": 0,
      "f1": 0.17142857142857143
    },
    {
      "id": "3db7c3d6835546f38741ee9abb96dc77",
      "question": "What age was the girl?",
      "prediction": "The answer is 15.",
      "ground_truths": [
        "15-year-old's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3067ebacbf8e45f693b0cc93838def01",
      "question": "What are the mother's reasons?",
      "prediction": "The mother's reasons for her decision to leave Goa are:",
      "ground_truths": [
        "police were trying to cover up the truth"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "f4583e6b781d42aca1bbc9360932b5b5",
      "question": "Who thinks police arrested wrong man?",
      "prediction": "The correct answer is: The Goa Police.",
      "ground_truths": [
        "Fiona MacKeown"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "072051235c1f43a1b0c1d37fb84beef3",
      "question": "what did the police cover up",
      "prediction": "The police covered up the murder of Scarlett Keeling, a British teenager, in Goa.",
      "ground_truths": [
        "the murder of a British teenage girl in Goa"
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "902802036d9346bba8f3742fa55bce01",
      "question": "What was the man's name?",
      "prediction": "The answer is **Scarlett Keeling**.",
      "ground_truths": [
        "Samson D'Souza,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1554e7b389d54ff7904f0bf2d82e269c",
      "question": "What team set a record?",
      "prediction": "The answer directly corresponds to the context: Bayern Munich inflicted a record defeat on Sporting Lisbon by 12-1, resulting in a high aggregate score. The players react to the crowd after their 12-1 defeat and the subsequent 7-1 victory at halftime. The team's second half saw a long-range counter-attack, but a missed tackle allowed Jo√£o Moutinho to score another goal.  The team's second-half goal was a late winner, and the final goal was scored by Klose, making it 9-1 for Bayern.  The time-hoddering in the final minutes",
      "ground_truths": [
        "Munich"
      ],
      "em": 0,
      "f1": 0.0273972602739726
    },
    {
      "id": "b20e7a05673a4b3c8ee29beaba23f73a",
      "question": "Who did Bayern Munich defeat?",
      "prediction": "The answer is Bayern Munich.",
      "ground_truths": [
        "Sporting Lisbon"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "805a6afa477c45d18a26ebba495ac1b7",
      "question": "What was the score in the match between Bayern and Portugal?",
      "prediction": "The answer is 10-1.",
      "ground_truths": [
        "12-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8da9a487491d4861815c860882b46f1b",
      "question": "Who did Bayern defeat on the first leg?",
      "prediction": "The answer is Bayern Munich.",
      "ground_truths": [
        "Sporting Lisbon"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08aee7c44c404089b766cbe0fa2d55f5",
      "question": "In what year did Lyon defeat Werder Bremen?",
      "prediction": "The answer is in the context of Bayern Munich's Champions League defeat to Sporting Lisbon, and the UEFA Champions League (Champions League) final, at which they defeated Sporting Lisbon.",
      "ground_truths": [
        "2005."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a33ecb066b9443e85b68c446f45b287",
      "question": "Who defeated Sporting Lisbon?",
      "prediction": "The answer is Bayern Munich.",
      "ground_truths": [
        "Munich"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "199178bd55f849279f0e163c691d0903",
      "question": "What was the final score of the Munich/Lisbon game?",
      "prediction": "The final score of the Munich/Lisbon Champions League game was 9-1.  The final score was a repeat of a 10-2 aggregate victory.  Bayern Munich scored a second goal in the 86th minute, followed by a first goal in the 89th minute, to bring the score to 9-1.",
      "ground_truths": [
        "12-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccb81b6865484234ab334d34412a6b35",
      "question": "What was the score of the Bayern match?",
      "prediction": "The Bayern Munich team scored a record 12-1 at the end of the aggregate, after a low-key first leg with a double from Lukas Podolski and Anderson Polga. The team then scored a later goal with a long kick from Lewandowski in the 34th minute. The final result was a 9-1 for Bayern.",
      "ground_truths": [
        "12-1"
      ],
      "em": 0,
      "f1": 0.047619047619047616
    },
    {
      "id": "3f70de0169eb42c3a4b40cd1e5f1be61",
      "question": "What country's unemployment rate is highest in Europe?",
      "prediction": "The unemployment rate in Spain is highest in the first quarter, with 20 percent.",
      "ground_truths": [
        "Latvia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7fafb4013664c78a931293de688bedb",
      "question": "By what percentage has Spanish unemployment grown over the previous quarter?",
      "prediction": "By what percentage has Spanish unemployment grown over the previous quarter?",
      "ground_truths": [
        "19 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10e96facf13e4091ba5c8cee71226ff0",
      "question": "What country has the highest unemployment rate in Europe?",
      "prediction": "The answer is **Spain**.",
      "ground_truths": [
        "Latvia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "738dc2a651c7477abe2efc0e5547054a",
      "question": "How many people are out of work in Spain?",
      "prediction": "The answer is 20 percent.",
      "ground_truths": [
        "4.6 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2cdea969eb1441afb20bee7330453eac",
      "question": "what are the investments",
      "prediction": "The Iraq's $2 billion investment in the Autonomous Region of Kurdistan (AKC) is expected to be in the thousands of square feet, with further foreign direct investment exceeding $40 billion. The project is designed to promote private sector investment in gas-related industries in the Kurdistan Region of Iraq. The initial investment is estimated at $3 billion, with further foreign investment exceeding $40 billion.",
      "ground_truths": [
        "$3 billion,"
      ],
      "em": 0,
      "f1": 0.06779661016949151
    },
    {
      "id": "3de977d9d8c142c79231c332dad7cfdf",
      "question": "where Work will start?",
      "prediction": "The answer is \"Gas Cities LLC.\"",
      "ground_truths": [
        "on the project,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a018a8b09d14b4cb164c0abfd1594bb",
      "question": "What wil Kurdistan Gas City will include?",
      "prediction": "Answer: Kurdistan Gas City will include industrial, residential and commercial buildings, a new city, and a new city with a central.",
      "ground_truths": [
        "industrial, residential and commercial buildings"
      ],
      "em": 0,
      "f1": 0.4347826086956522
    },
    {
      "id": "489edeed2f20435b9fc8ab429d160239",
      "question": "The project will start on what date?",
      "prediction": "The answer directly from the context is:",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6bf1ecc9589d4447b31afde347505919",
      "question": "when will work begin",
      "prediction": "The answer is: Nechirvan Barzani of the Kurdish Regional Government (KRG) announced that Iraq is investing in the Autonomous Region of Kurdistan (AKD) of the Iraqi Central Government.",
      "ground_truths": [
        "September 21."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1bf26f3035ec45e5874eb0de76a8d903",
      "question": "Significant investments are being made in Kurdistan by whom?",
      "prediction": "The answer is: \"We are investing in the Iraq's autonomous region of Kurdistan.",
      "ground_truths": [
        "Two United Arab Emirates based companies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62ccc3554230408fb85b346dedaeffa9",
      "question": "for how much is the investment?",
      "prediction": "The answer is 46 million square feet.",
      "ground_truths": [
        "$3 billion,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bccab12e798e428b8869c19eb4feac2c",
      "question": "what does this affect?",
      "prediction": "The answer is: \"Kurdistan Gas City will generate job opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services, and other business activities. This is not the first project for Dana Gas and Crescent Petroleum in Iraq's Kurdistan Region, and the same commitment to service agreements with the Kurdistan Regional Government, as well as the development of a 180 km natural gas pipeline, are noted.",
      "ground_truths": [
        "Iraqi economy.\""
      ],
      "em": 0,
      "f1": 0.031746031746031744
    },
    {
      "id": "bb2c93be295348eb933e57a16325af76",
      "question": "what Kurdistan Gas City will include?",
      "prediction": "The answer is \"Kurdistan Gas City.\"",
      "ground_truths": [
        "industrial, residential and commercial buildings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21c894485b7a4cb0b66313dd1802a3c2",
      "question": "what is the investment?",
      "prediction": "The answer is $3 billion.",
      "ground_truths": [
        "$40 billion during"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "93170fdc966948419d9801fc0b63e513",
      "question": "who is amitabh bachman?",
      "prediction": "The answer is Amitabh Bachman.",
      "ground_truths": [
        "Bollywood superstar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2811395b18944f3ca0b3ab89d737581a",
      "question": "How many times has an oscar winner starred in a bollywood film?",
      "prediction": "The answer is: yes, the answer is yes, and the film has been made.",
      "ground_truths": [
        "the first"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38d0c3d6862d423d86168e554aa1e80c",
      "question": "Is this high profile?",
      "prediction": "Yes, the answer is yes.",
      "ground_truths": [
        "In the most high-profile"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4773dd0dc5d4e45bf36d736162c184f",
      "question": "Who does Ben Kingsley star with?",
      "prediction": "The answer is Ben Kingsley.",
      "ground_truths": [
        "Amitabh Bachchan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3183fdced356412498a9ad782f2e79d3",
      "question": "What is Kingsley's first return to Indian filmaking after Gandhi?",
      "prediction": "The answer is: \"I think the two talents put together could be extraordinary,\" Kingsley told CNN.",
      "ground_truths": [
        "\"Teen Patti\" (\"Card Game\")"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e9c90ac121b476f95a1d297bbf5393d",
      "question": "which of the companies is targeted",
      "prediction": "The answer is **Stratfor**. The text states that Stratfor is a global intelligence company that has been hacked. The \"private client\" list is a list of members, not a list of individuals.",
      "ground_truths": [
        "Stratfor,"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "1cbf033bb39e4bd4aa343ba4419ffd6a",
      "question": "Which company is targeted?",
      "prediction": "The answer is: Stratfor.",
      "ground_truths": [
        "Stratfor,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7368b08b0dc04144b2d606ab0788ed32",
      "question": "What do the hackers have access to?",
      "prediction": "The answer is: \"Stratfor provides intelligence reports to subscribers. A recent e-mail discussed political prospects for Iraq. The information-sharing website Pastebin posted a news release from Anonymous that said the group had no to do with Stratfor's cyberattack.",
      "ground_truths": [
        "information on 4,000 credit cards and the company's \"private client\" list,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da396eb93e554ffb8ce05e3f21581409",
      "question": "Who was involved",
      "prediction": "The answer is: Stratfor, a global intelligence company.",
      "ground_truths": [
        "activist hacking group Anonymous."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ca77c1bdd464d58ae82f2ad3a689436",
      "question": "Which hacking group may have been involved?",
      "prediction": "The answer is: Stratfor.",
      "ground_truths": [
        "Anonymous."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a34379c3fa84c2c8fe02de997c17f78",
      "question": "Where was targeted",
      "prediction": "The answer is the website, Pastebin.",
      "ground_truths": [
        "Stratfor's website"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "4ce8a055d1d6472ca8edf2b84eb75e7d",
      "question": "what was the information on the client list about",
      "prediction": "The answer is: Stratfor's client list contains information on a daily publication, not specific individuals or entities.",
      "ground_truths": [
        "credit card"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "182724fabd2f4119ba9f5fcab23d4001",
      "question": "What was released",
      "prediction": "The answer is \"We have reason to believe that your personal and credit card data could have been included in the information that was illegally obtained and disclosed.\"",
      "ground_truths": [
        "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "febb62323b274b678dc47bb096688f4c",
      "question": "Who followed the bombers?",
      "prediction": "The answer is \"Col. Alexander Drobyshevsky,\" the Ministry of Defence.",
      "ground_truths": [
        "NATO fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb0a6c5287fc41d08464e736a987e53b",
      "question": "What did Interfax report?",
      "prediction": "Interfax reported that the Russian Defense Ministry stated that the Tupolev Tu-160, with its long-range bombers, landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights, then return to the base.\"",
      "ground_truths": [
        "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\""
      ],
      "em": 0,
      "f1": 0.6774193548387097
    },
    {
      "id": "ce3ecd44941f4597ae927b61841b860a",
      "question": "Who did the Venezuelan president say he'd welcome?",
      "prediction": "The Venezuelan president said he would welcome the Russian air force.",
      "ground_truths": [
        "Russian air force,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "595d19e84b3d4ab181e545880eeb4aef",
      "question": "Who is the president of Venezuela?",
      "prediction": "The answer is **Hugo Chavez**.",
      "ground_truths": [
        "Hugo Chavez"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "21976ca866684313b86aa9450c61406c",
      "question": "Who would welcome the Russian air force?",
      "prediction": "The answer is the U.S.",
      "ground_truths": [
        "Venezuela"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c992708e6d004bedb81d57b076e33286",
      "question": "Who will use airfield for training?",
      "prediction": "Answer: The U.S. will monitor the Russian training.",
      "ground_truths": [
        "Russian bombers"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "7baaa5545ac44114b79c5420d561d85e",
      "question": "Who did NATO fighters follow?",
      "prediction": "The NATO fighters followed the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic.",
      "ground_truths": [
        "the bombers"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "962454a456cc4bbb9a8d5bfba6abf79c",
      "question": "What highway did the bus overturn at?",
      "prediction": "The answer is U.S. 93 in White Hills, Arizona.",
      "ground_truths": [
        "U.S. 93"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "cdec7a0d975642d38b3b2d449bffc3e8",
      "question": "Where were many of the victims being flown?",
      "prediction": "Answer: The victims were flown from Kingman Regional Medical Center in Las Vegas, Nevada, to Arizona, where they are being treated for severe injuries.",
      "ground_truths": [
        "Kingman Regional Medical Center,"
      ],
      "em": 0,
      "f1": 0.29629629629629634
    },
    {
      "id": "d86c4a3e9c944a5595ffd076817380c9",
      "question": "The tour bus overturns where?",
      "prediction": "The answer is: The tour bus overturns.",
      "ground_truths": [
        "near Hoover Dam,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "59cca53e121949e39742ee34a89cffab",
      "question": "Who was flown to Las Vegas?",
      "prediction": "The answer is \"Arizona Department of Public Safety.\"",
      "ground_truths": [
        "five victims"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7951ebb574024ca7958ac8e4f56cc046",
      "question": "What country were the passengers on the bus from?",
      "prediction": "The answer is **Arizona**.",
      "ground_truths": [
        "Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a084e061c5614a088e39a8d8468caf71",
      "question": "The dead lay where?",
      "prediction": "The dead lay in body bags on the roadway near the bus, which was on its side across both lanes.",
      "ground_truths": [
        "in body bags on the roadway near the bus,"
      ],
      "em": 0,
      "f1": 0.5833333333333334
    },
    {
      "id": "116602c25c3e4b33b7a5a111f9174cbe",
      "question": "Where did the bus overturn?",
      "prediction": "The answer is \"A bus carrying Chinese tourists overturned near Hoover Dam, killing at least seven people and injuring at least nine, an Arizona Department of Public Safety spokesman stated.",
      "ground_truths": [
        "White Hills, Arizona, near Hoover Dam."
      ],
      "em": 0,
      "f1": 0.24242424242424243
    },
    {
      "id": "23d1c9c515f3417ab70aba3c3bc43ec8",
      "question": "Bus passengers were from what country?",
      "prediction": "The answer is that the bus was carrying Chinese tourists, and the fire at Hoover Dam resulted in the death of seven people and injuries to at least nine.",
      "ground_truths": [
        "Chinese nationals."
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "4182285804154f018d4bb9735a8dc3e1",
      "question": "As many as how many victims are being flown to Vegas?",
      "prediction": "The answer is:  As many as are being flown to Vegas.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03905a27a5b841e59153c2e3de3c99f4",
      "question": "What foreign nationality were several of the passengers?",
      "prediction": "The answer is \"Chinese.\"",
      "ground_truths": [
        "Chinese nationals."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "4453276e770a4ed7b7051d9d65d40975",
      "question": "Who were bus passengers?",
      "prediction": "The answer is:  The answer is:  A bus carrying Chinese tourists overturned Friday near Hoover Dam, killing at least seven people and injuring at least nine.",
      "ground_truths": [
        "Chinese tourists"
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "c1e18f0bb23a4243b70eec0f92d21f71",
      "question": "What is Maradona's relation to Caceres?",
      "prediction": "The question asks for Maradona's relation to Caceres.",
      "ground_truths": [
        "Boca Juniors teammate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff9c4b6aa34a4cc1ac06e9671dd284c2",
      "question": "What is wrong with Fernando Caceres?",
      "prediction": "Caceres is wrong with the answer. He is in a grave condition.",
      "ground_truths": [
        "shot in the head"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "2b614698b55045c78b57cc7ed925f504",
      "question": "Who is in grave condition?",
      "prediction": "In grave condition.",
      "ground_truths": [
        "Fernando Caceres"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9715d425d7044c5aa81dcf972628c47d",
      "question": "What did Caceres enjoy?",
      "prediction": "Caceres enjoyed playing for Argentina in the 1994 World Cup.",
      "ground_truths": [
        "Argentine league"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03bccc9e5f964e718c8aa85f321068ce",
      "question": "Who visited Caceres?",
      "prediction": "The answer is Diego Maradona.",
      "ground_truths": [
        "by his former Boca Juniors teammate and national coach Diego Maradona,"
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "ea7c8bc9cc85442091d22b7aa2b11cb3",
      "question": "What did Caceres do for a career?",
      "prediction": "Caceres did for a career at Argentinos Juniors, with a move to Independiente.",
      "ground_truths": [
        "Argentina in the 1994 World Cup,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "144ba08c90054d9f864538e8113ac853",
      "question": "What is used to lure students?",
      "prediction": "The answer is, as a college admissions are competitive, and not just from the student's side, some schools try to come up with unique perks that appeal to students, often in the form of free services.",
      "ground_truths": [
        "free services."
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "fb1c311b507c464b903c7b32210e9319",
      "question": "Do students at Davidson College pay for laundry?",
      "prediction": "The answer is: No, students at Davidson College do not pay for laundry.",
      "ground_truths": [
        "don't have to visit laundromats because they enjoy the luxury of a free"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8238f44246824f5f8bce4ace866087d8",
      "question": "What is given out for free at some colleges?",
      "prediction": "The answer is, Davidson College does the laundry for them.",
      "ground_truths": [
        "laundry service."
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "9b4be388317541d6b4d43c9efdc0a094",
      "question": "Which ones have free laundry?",
      "prediction": "The answer is: **None of the above.**",
      "ground_truths": [
        "Davidson college students"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e266e50c7eb94d938f889f8e66aa10c8",
      "question": "What lures students to colleges?",
      "prediction": "The answer is, it's likely to surprise students with free services, such as laundry, discounted printing, and the idea of having their t-shirts on hangers.",
      "ground_truths": [
        "free services."
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "91cdd493d501438c868c9f4a43ea3f06",
      "question": "Where can I get a free computer?",
      "prediction": "The answer is: Davidson College provides free computers, and the Lula Bell Houston Laundry offers free self-service washers and dryers. The dorms also have free laundry and a free-use washer and dryer.",
      "ground_truths": [
        "Wake Forest,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66ccd850630b4a069b80af81f0d8fa0c",
      "question": "Who works with Stella McCartney?",
      "prediction": "The answer is Adidas.",
      "ground_truths": [
        "Adidas"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ba9e926ece6b4b6e844f51d0b7b0f056",
      "question": "What will be the main priority for Adidas?",
      "prediction": "The main priority for Adidas is to continue its expansion into fashion, as it's a brand that values its fitness-loving fans and the European Champions League. It also notes the strength of Europe's market, which is a large and mature market, and the success of its sportswear brand, Adidas's name and a diversity of its product portfolio.",
      "ground_truths": [
        "sportswear,"
      ],
      "em": 0,
      "f1": 0.0392156862745098
    },
    {
      "id": "b0d95637bcb44afb96457d3053631d5f",
      "question": "who is main priority will always be sportswear?",
      "prediction": "The main priority for Adidas is sportswear.",
      "ground_truths": [
        "Adidas,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "85ff90e4fd9c4c8790b6414520150e51",
      "question": "What allowed adidas to navigate Europe's economic difficulties?",
      "prediction": "Adidas's entry into Europe's market requires it to adapt to the region's economic challenges, like challenges in the developing world, where new markets are emerging, and Russia's strong, diverse product portfolio.",
      "ground_truths": [
        "strength of its brand name and the diversity of its product portfolio,"
      ],
      "em": 0,
      "f1": 0.15
    },
    {
      "id": "a2374c9022504b94b1a832c91ab535fe",
      "question": "With who Adidas now has been working with?",
      "prediction": "The answer is Adidas.",
      "ground_truths": [
        "top designers, such as Stella McCartney,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afdf5764dddf4d858cff8be47f91fba7",
      "question": "What will the main priority be?",
      "prediction": "The main priority for Adidas is to expand into fashion, but it will not desert its fitness-loving fans.",
      "ground_truths": [
        "sportswear,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf40be104bfe45338fb4e03c699a2f7f",
      "question": "who was the guest speaker?",
      "prediction": "The answer is \"I believe that my next 100 days will be so successful, I will be able to complete them in 72 days on the 73rd day I rest.\"",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "629ca0ba60d84e678789916c527e5e74",
      "question": "What is essential to democracy?",
      "prediction": "The answer to the question is essential to democracy.",
      "ground_truths": [
        "ultimate success as an industry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5c48b89357aa48fd8a1aafaeabe73c4c",
      "question": "What did Obama deliver?",
      "prediction": "The answer is \"I would like to talk about what my administration plans to achieve in the next 100 days.\"",
      "ground_truths": [
        "some one-liners at the White House Correspondents' Association dinner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e62ce90267b149f7a65dc0901c614317",
      "question": "Which party does not qualify for a bailout?",
      "prediction": "The answer is \"I believe that my next 100 days will be so successful, I will be able to complete them in 72 days, and on the 73rd day I will rest.\"",
      "ground_truths": [
        "Republican"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "147cef44c6e64257a962d7ed7e7df893",
      "question": "when did this happen?",
      "prediction": "The answer is:  \"Across the country, there are extraordinary, hardworking journalists who have lost their jobs in recent weeks, recent months,\" said Obama.",
      "ground_truths": [
        "on Saturday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b999b0cedcd1447b822ddccf34cabec4",
      "question": "Who recounted details of Winehouse's last hours?",
      "prediction": "The answer is: \"Our inquest in London, on Wednesday, confirmed that alcohol toxicity caused the death of Amy Winehouse, with her blood-alcohol levels exceeding 5 times the legal limit for driving in the UK.\"",
      "ground_truths": [
        "Andrew Morris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbb53f49dd5746a19dc95b2ac047a687",
      "question": "What age was Winehouse when she died?",
      "prediction": "The answer is 27.",
      "ground_truths": [
        "27-year-old's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8fd0fb454968435f93da377bce88d65c",
      "question": "How many vodka bottles were found?",
      "prediction": "The answer is 3.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "558dea0334cf4e81ab5f22365e79ab5b",
      "question": "who talked about her last hours",
      "prediction": "The answer is Amy Winehouse.",
      "ground_truths": [
        "Andrew Morris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d83d818f3ba4db19f40eb155fd6e616",
      "question": "What age was the person who died?",
      "prediction": "The person who died was \"Amy Winehouse,\" a 27-year-old singer.",
      "ground_truths": [
        "27-year-old's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a103fee179d24e01afac355e24433e16",
      "question": "What was found in Winehouse's home?",
      "prediction": "The answer is \"No traces of illegal drugs were found in Winehouse's system. However, more details emerged about her loss with alcohol, including her losing battle with alcohol and a change of her behaviour, including a statement from her physician, Dr. Christina Romete, who had heard Winehouse's advice about drinking, stating she would call her to discuss it on the weekend.\"",
      "ground_truths": [
        "three empty vodka bottles,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3edccdb629a473aa46b74b5be302b9b",
      "question": "When did she die?",
      "prediction": "The answer is \"at her north London home, July 23, 2008\".",
      "ground_truths": [
        "July 23."
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "a9e037717c1240859e5593a0a6586ffa",
      "question": "what was found at winehouse's home",
      "prediction": "The answer is \"no illegal drugs were found in Winehouse's system,\" and \"no traces of tablets in Winehouse's stomach and organs were found at the coroner's court.\"",
      "ground_truths": [
        "three empty vodka bottles,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e381e6c7f5947febd026980b77ba9b1",
      "question": "What does Sahaab craft?",
      "prediction": "Tuba Sahaab crafts.",
      "ground_truths": [
        "poems"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8bc4173ee48b4782a06d0cb9b6560f42",
      "question": "Who is Tuba Sahaab?",
      "prediction": "Tuba Sahaab is a Pakistani girl of 11, from Islamabad, who is in a simple, cozy home in a suburb of Islamabad. She is described as a slight girl with pen-like bristles, her eyes reflecting an intensity of conflict, and she is known for her poems, which depict the pain and suffering of children, the restrictions of the Islamic militants, and the violence of the Taliban. She is also described as a strong and brave young girl who is reaching a larger audience, appearing in the media. She is said to be deeply religious and courageous.",
      "ground_truths": [
        "She is a slight girl of 11,"
      ],
      "em": 0,
      "f1": 0.1348314606741573
    },
    {
      "id": "80bd1a6acb4648c5978c1536ca9b87c1",
      "question": "What does Tuba craft?",
      "prediction": "Tuba Sahaab's poems are written in a creative and evocative style, using words and imagery to capture the pain and suffering of her children in the streets of Pakistan. She uses her poetry to express the experiences of a young girl and the struggle for peace in her country. She does not explicitly state what she crafts, but she is inspired by Obama, and she dreams of meeting her hero. She also says she desires to go to the White Palace and give her poems to Obama to show what is happening and ask him to come to Pakistan. She concludes with a strong sentiment for her country and a desire to give peace to her nation",
      "ground_truths": [
        "poems"
      ],
      "em": 0,
      "f1": 0.018348623853211007
    },
    {
      "id": "ebf019efe7f1441f9b913bc872202133",
      "question": "What does she do?",
      "prediction": "Tuba Sahaab is a Pakistani-American girl of 11. She is in a simple home in a suburb of Islamabad, with her pen as her weapon. She is brave and tells the Taliban of the pain and suffering of her children, and the hard-core Islamic militants, which spreads their influence across Pakistan.",
      "ground_truths": [
        "crafts poems telling of the pain and suffering of children just like her;"
      ],
      "em": 0,
      "f1": 0.2456140350877193
    },
    {
      "id": "7eed7785f1af4c2a80fab37a4cb64027",
      "question": "What is Tuba Sahaab's age?",
      "prediction": "Tuba Sahaab is 11 years old.",
      "ground_truths": [
        "11,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "475e7a194700436280b109877f5938eb",
      "question": "Where have girls been banned from?",
      "prediction": "Here's the answer, based on the context:",
      "ground_truths": [
        "school,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "40b316579aa445ebbf8dedce7b5722fb",
      "question": "What does Sahaab refuse to do?",
      "prediction": "Tuba Sahaab refuses to do anything, as she says she will give peace to her country. She also says that she will fight for it. She tells me the prisoners of the Taliban's rule, and she says that this is very shocking to hear that, but she does not be silent. She says that the victims of the Taliban are \"not removed from the Stone Age.\" She also says that the C-Sats Valley is no-go zone, and that all the prisoners die with notes saying \"do not remove for 24 hours.\" The people of Swat Valley also speak of a place called \"slaughter",
      "ground_truths": [
        "be silent."
      ],
      "em": 0,
      "f1": 0.04166666666666667
    },
    {
      "id": "9f0cc93d1bb54d99aac6cc95a24a1c39",
      "question": "How old is Tuba Sahaab?",
      "prediction": "Tuba Sahaab is 11 years old.",
      "ground_truths": [
        "11,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "736c1b0a529245568a535a39619721cf",
      "question": "How old is Sahaab?",
      "prediction": "Tuba Sahaab is 11 years old.",
      "ground_truths": [
        "11,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "a9572e6dd5424d60b2fe711663259df2",
      "question": "What have girls been banned from?",
      "prediction": "The answer is: \"small, book-burning, and the hard-core Islamic militants spread their reign of terror across Pakistan.\"",
      "ground_truths": [
        "school,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b99d58f4d2f4bd9bdd4ab030850c3c1",
      "question": "Who has been banned fro school?",
      "prediction": "The answer is Tuba Sahaab.",
      "ground_truths": [
        "girls"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "464ae299794f477b86ee4ed46db930ec",
      "question": "What have poets long written about?",
      "prediction": "Here's a concise summary of the answer, based on the context:",
      "ground_truths": [
        "Mother's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9a879f717b9402d983846f5b7d875aa",
      "question": "What does Robert Louis Stevenson suggest his mother listen out for",
      "prediction": "The answer is \"To My Mother,\" by Robert Louis Stevenson.",
      "ground_truths": [
        "The little feet along the floor."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4427862bd5fc445a92b61bb409830823",
      "question": "What did Louis Stevensons poem say?",
      "prediction": "The poem said, \"To My Mother, accept, I pray My offering. And may you happy live, And long us bless; Receiving as you give Great happiness.\"",
      "ground_truths": [
        "You too, my mother, read my rhymes For love of unforgotten times, And you may chance to hear once more The little feet along the floor."
      ],
      "em": 0,
      "f1": 0.32653061224489793
    },
    {
      "id": "25c4f5cc2f944db88c3f41e4b8e9ce52",
      "question": "What are the poems about?",
      "prediction": "The poem addresses the poet's memories of his mother.",
      "ground_truths": [
        "Mother's Day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d1f3d2d289044d683e89b0166d6c11a",
      "question": "To whom have poets long written about",
      "prediction": "The poem \"Mother Nature Network\" contains a list of poems by Robert Louis Stevenson, many of which address the poet's memories of her mother. The poem also includes works by Allen Ginsberg and Christina Rosetti, and the Nobel laureate's poem \"Goblin's Market\" at its final entry, which expresses the poet's sentiment about her mother's death.",
      "ground_truths": [
        "mother."
      ],
      "em": 0,
      "f1": 0.039999999999999994
    },
    {
      "id": "0433b48954e94a5d96a3fe71cbd2e145",
      "question": "What is it that Julia's Kasdorf's mother taught her to do",
      "prediction": "The answer is: \"Life for me ain't been no crystal stair. It's been a tough, but well-lived life with my mother. I learned to comfort, to offer healing, and to say 'Blessing of your voice, your chaste touch.' And, I think, I've found that the best lessons in life are not in the material stuff, but in the people we connect with, and the lessons we learn from them.\"",
      "ground_truths": [
        "comfort those in mourning,"
      ],
      "em": 0,
      "f1": 0.05970149253731343
    },
    {
      "id": "36b302d3fb454f10b36bffcc08451bbe",
      "question": "Who asked for filming to be stop?",
      "prediction": "Based on the provided context, the answer is: **Rihanna, who was born in Barbados but has Irish ancestry.**",
      "ground_truths": [
        "Alan Graham"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2fc4e20d2244d3ea34c2b03ebac828e",
      "question": "Who says the Barbados-born singer went topless",
      "prediction": "The answer is based on the context provided. The context states that Rihanna is in Northern Ireland and has been offered to film a video for her song \"We Found Love\" in a muddy barley field, and this happened on Tuesday.",
      "ground_truths": [
        "Graham's wife"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc63ecaff9e64b1889456cdbf14d252c",
      "question": "What did his wife say?",
      "prediction": "The answer to the question is: \"The controversy has erupted in Northern Ireland and was even discussed by lawmakers in the province's elected Assembly in Belfast.",
      "ground_truths": [
        "Alan was particularly objecting to"
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "e3ca0e8b58b64a9b8c4f4026946b648e",
      "question": "What song was the music video for?",
      "prediction": "The music video for \"We Found Love\" was for Rihanna.",
      "ground_truths": [
        "\"We Found Love\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9d7993ab762b4739b2da434533f6bab9",
      "question": "What did the farmer ask them to do?",
      "prediction": "Based on the context, the answer is **'I had a conversation with Rihanna; we shook hands; we parted company on good terms. I found her very gracious, and I wish her well.**",
      "ground_truths": [
        "to cover up"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d18a11868b6e470f91c9abb1e338f3cb",
      "question": "Who stripped to bikin top",
      "prediction": "The answer directly from the context is: \"We Found Love\" (a song by Rihanna, a Christian farmer in Northern Ireland, and a protest against a muddy barley field in Bangor, Maine, 10 miles from Belfast.\"",
      "ground_truths": [
        "Rihanna"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "0aca308fce9b4cb4b883b413dfad75b4",
      "question": "What was the pop singer filming?",
      "prediction": "The answer is: Rihanna.",
      "ground_truths": [
        "a music video"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3d4805f37b84f3092a97f8978bc43cf",
      "question": "Where was the Pop singer filming",
      "prediction": "The answer is directly from the context.",
      "ground_truths": [
        "Belfast."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2b3ef7f242c47b0858142af1dc91c2e",
      "question": "What type is Chuck Bass?",
      "prediction": "The answer is, \"Chuck Bass is one of the most memorable and controversial actors of the 1990s, especially his role in \"Gossip Girl\".\" He's a known for his dark, often sarcastic, and sometimes manipulative wit.",
      "ground_truths": [
        "Moody and sinister,"
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "e1a72b05da3c4bb9b0bd99452f34df94",
      "question": "Who is the aging party boy?",
      "prediction": "The answer is *Chuck Bass*.",
      "ground_truths": [
        "Hank Moody"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0db987c2ec664e7a820ac63141d1a84e",
      "question": "Hank Moody is from what tv show?",
      "prediction": "Based on the context, the answer is **\"Entourage\":**",
      "ground_truths": [
        "(\"Californication\"):"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad250ef9a417474c954df4e6e1fc9501",
      "question": "How should one learn who not to date?",
      "prediction": "The answer is, \"You should avoid dating a Hank Moody type.",
      "ground_truths": [
        "Television"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d3542dface0247cdb0e57be88b21c2e0",
      "question": "Which character from Mad Men is mentioned?",
      "prediction": "The answer is Vince Chase.",
      "ground_truths": [
        "Don Draper"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "750fe50e52844d08b695bedd58eeb642",
      "question": "Who is a smoldering liar and cheat?",
      "prediction": "The answer is Vince Chase.",
      "ground_truths": [
        "Don Draper"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb400d0b145641c5b055f45c29050377",
      "question": "What show is Chuck Bass from?",
      "prediction": "The answer to the question is \"Gossip Girl\".",
      "ground_truths": [
        "(\"Gossip Girl\"):"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "88c6ef9d9a3c436abbad77bf67d0902e",
      "question": "What show is the character Don Draper from?",
      "prediction": "The answer is: \"Mad Men.\"",
      "ground_truths": [
        "(\"Mad Men\"):"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "26782a326a394d688dda2441308bf00a",
      "question": "When was the first of the impeachment chargers brought against Arroyo?",
      "prediction": "The first of the impeachment chargers was brought against Arroyo in 2005 for vote-tampering.",
      "ground_truths": [
        "vote-tampering."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "e62ec634d4b44a50842d1913910877a5",
      "question": "What did the opposition parties call into question?",
      "prediction": "The opposition parties called into question the legitimacy of the impeachment complaint, alleging that Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm.",
      "ground_truths": [
        "the legitimacy of that race."
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "642ade1eebc04179b6ee599028cbb9b0",
      "question": "What does the complaint allege?",
      "prediction": "The complaint alleges that Arroyo and her husband, Gloria Macapagal Arroyo, directly engaged in an internet broadband deal with a Chinese firm.",
      "ground_truths": [
        "Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm."
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "8b9a2f29b35b41e38cb94347237d59d7",
      "question": "what was being called into question",
      "prediction": "The question asks for the answer directly.",
      "ground_truths": [
        "legitimacy of that race."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "775656e96e4747f5a17cc2b57edd12b7",
      "question": "when were the impeachment charges bought",
      "prediction": "The impeachment charges were bought in 2010.",
      "ground_truths": [
        "2005"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "194bd503229c46c3b308896e770e1eb1",
      "question": "What did the complaint allege that Arroyo and husband do?",
      "prediction": "The complaint alleges that Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm.",
      "ground_truths": [
        "directly involved in an Internet broadband deal with a Chinese firm."
      ],
      "em": 0,
      "f1": 0.6923076923076924
    },
    {
      "id": "1cf1d0fdb27f4ba6a2399a4a86b09b8e",
      "question": "What type of charge was brought against Arroyo?",
      "prediction": "The answer is \"insufficient in substance\" (in the context of the Congress's impeachment accusations).",
      "ground_truths": [
        "corruption"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5b94852dff984b19bf8264cd8bc1caa3",
      "question": "Who called into question the legitimacy of Arroyo's win?",
      "prediction": "The question asks who called into question the legitimacy of Arroyo's win in the 2006 and 2007 presidential elections.",
      "ground_truths": [
        "opposition parties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a154a6b5c364e79b30a1bd2c9e47e0c",
      "question": "who were involved in the deal",
      "prediction": "The answer is \"the state-run Philippines News Agency.\"",
      "ground_truths": [
        "Arroyo and her husband"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41d5a6d229964b50b3c73d5bc9a5d1fa",
      "question": "How many cases have been reported in 26 states",
      "prediction": "The number of cases reported in 26 states is 133.",
      "ground_truths": [
        "133"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "c5559fa0b9a0438a9b843d4207061a3d",
      "question": "The outbreak has been deadliest since when",
      "prediction": "The outbreak of cantaloupe, a highly deadly food-borne illness, has been the deadliest in the United States since 1998. It's caused by the bacterium *Listeria monocytogenes. The outbreak is considered the deadliest food-borne illness in the United States. The last area of culcata cantaloupe was disposed of as refrigeration does not kill bacteria.  Therefore, consumers should not wash off cantaloupe.",
      "ground_truths": [
        "1998."
      ],
      "em": 0,
      "f1": 0.038461538461538464
    },
    {
      "id": "fcf679e5b27f454aa3da8e1ecf8226bb",
      "question": "What was recalled?",
      "prediction": "The answer is \"The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28. The Centers for Disease Control and Prevention (CDC) said Tuesday, Tuesday, Tuesday, Tuesday, and Tuesday.",
      "ground_truths": [
        "Rocky Ford brand cantaloupes"
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "1416c66b2eb543aea04972d5780b31d6",
      "question": "In how many states were the cases reported?",
      "prediction": "The number of states reported is 28.",
      "ground_truths": [
        "26"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd66178403e8487382fdc1b84bb2881c",
      "question": "What is not over",
      "prediction": "The answer is \"The number of new cases is falling.\"",
      "ground_truths": [
        "the outbreak"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac55787c46dc4d7b84b2856b8bf2dc93",
      "question": "How many cases have been reported?",
      "prediction": "The number of cases linked to cantaloupes contaminated with Listeria monocytogenes has risen to 28, with 133 people in 26 states.",
      "ground_truths": [
        "133 people"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "e8ac0eb458ea4dcabf252184b9da9bae",
      "question": "What the activist animal rights says?",
      "prediction": "The activist animal rights activist in Bucharest, Romania, says that a \"good vegan cupcake has the power to transform everything for the better.\"",
      "ground_truths": [
        "\"A good vegan cupcake has the power to transform everything for the better,\""
      ],
      "em": 0,
      "f1": 0.6896551724137931
    },
    {
      "id": "6961695f789f4da9a9c3fa718b135d0e",
      "question": "When does the sale run?",
      "prediction": "The answer is, \"A good vegan cupcake has the power to transform everything for the better,\" which translates to \"A good vegan cupcake has the power to transform everything for the better.\"",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95fd83262d2f47cfbb801bd5ca266c1b",
      "question": "How many groups there are about?",
      "prediction": "The answer is **6**.",
      "ground_truths": [
        "More than 120"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "031e600d36e74011aa477308544ebc84",
      "question": "What do the tasty treats not contain?",
      "prediction": "The answer is \"A good vegan cupcake has the power to transform everything for the better,\"",
      "ground_truths": [
        "animal products."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa666bde7da647179528a5814dac9fb7",
      "question": "What are 120 groups selling?",
      "prediction": "The answer is \"A group of animals, but they may consume dairy and eggs.\"",
      "ground_truths": [
        "vegan bake sales"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "df292cfff75d46a8963765ee12c341f0",
      "question": "On what date runs Vegan Bake Sale",
      "prediction": "The answer is: \"In New York, on April 24 through May 2.\"",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "58224fe8e4ed454f9c42ea3331a9a46f",
      "question": "What did animal rights activist say?",
      "prediction": "Animal rights activist said, \"A good vegan cupcake has the power to transform everything for the better,\"",
      "ground_truths": [
        "\"A good vegan cupcake has the power to transform everything for the better,\""
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "ef463e57439c4b01af0a1f501f2a074f",
      "question": "When does the bake sale run?",
      "prediction": "The answer is in the context, as stated in the prompt.",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70131d6d1a9a4f74a8005efd5d92fb47",
      "question": "What has helped his church according to preacher?",
      "prediction": "The preacher's answer to the question likely comes from the context, where he says that the Bible says that \"Every place the sole of your foot should tread upon, I'll give it to you,\" which indicates that the preacher's faith in God is based on the idea of the Bible's instruction to \"every place the sole of your foot should tread upon,\" suggesting that the congregation, in its own way, knows the Bible's guidance and that the church's work is ultimately done.",
      "ground_truths": [
        "natural gas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5aa43a0714d54bc99836dfbd5049fe40",
      "question": "What was the score at Bremen?",
      "prediction": "The answer to the question is: **Bremen had a golden chance to put one hand on the league title if they beat Stuttgart on Sunday.**",
      "ground_truths": [
        "3-3"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21d9ee49ecc144ab9ca0a426c203d559",
      "question": "Which team won against Hamburg?",
      "prediction": "The answer is Bayern Munich, as they lost to Stuttgart on Sunday.",
      "ground_truths": [
        "Schalke"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2761f819ecb5477f91696a9510a7c2d8",
      "question": "Who scored against Hamburg?",
      "prediction": "The answer is: **Damske striker Kevin Kuranyi scored a goal in a 1-0 victory for Schalke in a Bundesliga match against Bayern Munich.**",
      "ground_truths": [
        "Kevin Kuranyi"
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "622bd69fdf1a42e28e638b0fbbae1e21",
      "question": "Which position is Schalke now level in?",
      "prediction": "The answer is: **Hambrought**",
      "ground_truths": [
        "joint second"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac1d6afd28024e4bba04d2fb8ae65b61",
      "question": "Who scored an early goal for Schalke?",
      "prediction": "The answer is **Schalke**.",
      "ground_truths": [
        "Kevin Kuranyi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9adce0e50b4f440f847b00061534308c",
      "question": "Who does Bayern have to beat in Sunday?",
      "prediction": "Bayern has to beat Stuttgart in Sunday's league match.",
      "ground_truths": [
        "Stuttgart"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "8c87c6cbfac04e7b9b1cc82b5ba7c679",
      "question": "Who are Schalke now in second position with?",
      "prediction": "The answer directly from the context is:",
      "ground_truths": [
        "Bayern Munich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4325e8d9dc2437c86c45614ec0c210b",
      "question": "What position are Schalke in?",
      "prediction": "Schalke is in the second tier of Germany's Bundesliga.",
      "ground_truths": [
        "joint second"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "b8e2019642b940749d6bcd95b2acfa06",
      "question": "When is the next game?",
      "prediction": "The answer is: Bayern Munich's next game is on Sunday.",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "136d8cfca1384d459b264083a4f98710",
      "question": "Who won the game?",
      "prediction": "Answer: Bayern Munich won the game, with a 1-2 scoreline, 2-2 difference.",
      "ground_truths": [
        "Schalke"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0beffe10acae4b82a1e79284189e07c8",
      "question": "When will Bayern Munich play Stuttgart to all but clinch the title?",
      "prediction": "The answer is: **Bayern Munich will play Stuttgart on Sunday.**",
      "ground_truths": [
        "Sunday."
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "c5a96f6032194077b291d81e38925761",
      "question": "Where is the judge who needs to sign bail paperwork?",
      "prediction": "The answer is: **The judiciary.**",
      "ground_truths": [
        "on vacation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09f2baa54b7c4f12b1775f1f1814cb3d",
      "question": "Josh and Shane were held for being what?",
      "prediction": "The answer is: \"not able to get a signature on bail paperwork because a judge is on vacation until Tuesday, and he must return then.\"",
      "ground_truths": [
        "as spies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ae54b7d35b3414c94f430c3ef5dd589",
      "question": "What country is  Ahmadinejad president of?",
      "prediction": "The answer is Iran.",
      "ground_truths": [
        "Iran's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76927bab1bc841c1b200dd9810afcb06",
      "question": "Who has requested their release?",
      "prediction": "The answer is: **The Iranian judiciary, including Mohammad Javad Larijani.**",
      "ground_truths": [
        "a delegation of American Muslim and Christian leaders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f872a701af41474e84deef08fea13528",
      "question": "How long have Fattal and Bauer been held?",
      "prediction": "The answer is: The answer is 8 years.",
      "ground_truths": [
        "more than two years,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "1bd2e0c1318f4375ac049bd71a8b6876",
      "question": "Who asks President Ahmadinejad for their release?",
      "prediction": "The answer to the question is: **President Ahmadinejad.**",
      "ground_truths": [
        "a delegation of American Muslim and Christian leaders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d8a017ec36c420984653e239a12606b",
      "question": "What did billboards use to encourage GOP votes?",
      "prediction": "The answer is: \"Please Don't Vote for a Democrat.\"",
      "ground_truths": [
        "an image of the burning World Trade Center"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eae081ec9fb347678ec391e2394f7418",
      "question": "What was said about president Clinton?",
      "prediction": "The answer is: Democrats and Republicans are saying that 9/11 was a \"clear sign that the president's actions in the following days prevented the attack on the Navy warship USS Cole in Yemen.  The text doesn't explicitly say what was said about the president, so the answer is largely implicit.",
      "ground_truths": [
        "should have put a stop to Osama bin Laden and al Qaeda before 9/11."
      ],
      "em": 0,
      "f1": 0.07407407407407408
    },
    {
      "id": "be9df164a96b4ae8a5cad6d0bdffb257",
      "question": "What did billboards use to encourage votes?",
      "prediction": "The answer is: \"Please Don't Vote for a Democrat\" (with a disclaimer).",
      "ground_truths": [
        "an image of the burning World Trade Center"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0718876d8bd9412b8a546bb3df09e72f",
      "question": "WHat are used to encourage GOP voters?",
      "prediction": "The answer is, \"Please Don't Vote for a Democrat.\"",
      "ground_truths": [
        "using billboards with an image of the burning World Trade Center"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "71ed6dfba6704b05b0d0aac8c79665d4",
      "question": "Who says the image is wrong?",
      "prediction": "The answer is \"Please Don't Vote for a Democrat\".",
      "ground_truths": [
        "The local Republican Party"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3aa16adb30c463f9d467da047b09044",
      "question": "What was the Best Picture winner?",
      "prediction": "The Best Picture winner was \"Jonas Brothers: The 3D Concert Experience\" (No. 9), which lost a staggering 78 percent to its previous sixth-week count. This year's biggest second-weekend dip was the fifth-largest in the history of the box office, with $2.8 million.",
      "ground_truths": [
        "\"Slumdog Millionaire\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "177ef67e2d3241cb88c1f65a90e1d9b8",
      "question": "What was the amount it grossed?",
      "prediction": "The $55.7 million, with $5.5 million from 124 IMAX screens, is significantly smaller than the $70.9 million made at the last R-rated movie, \"Watchmen,\" earned on its opening weekend two years ago. The second-best debut of 2009, though, resulted in $15.4 million, a smaller figure than the $70.9 million made at the opening weekend.",
      "ground_truths": [
        "$55.7 million"
      ],
      "em": 0,
      "f1": 0.08163265306122448
    },
    {
      "id": "589652b567004418a17c2aedea6f6ddd",
      "question": "What was the gross for Tyler Perry's \"Madea Goes to Jail\" (No. 2)?",
      "prediction": "The gross for Tyler Perry's \"Madea Goes to Jail\" (No. 2) is $11.5 million, which is 11.5 million dollars.",
      "ground_truths": [
        "$8.8 million"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "e22b09d953fc4594906f1d83f2194a99",
      "question": "What premiered in more theaters than any other R-rated movie in history?",
      "prediction": "The answer is \"Watchmen\" (No. 1) grossed $55.7 million during its first weekend.",
      "ground_truths": [
        "\"Watchmen\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "4c74a0f5264d4bb8bb2b63898af2333a",
      "question": "What was the movie rated?",
      "prediction": "The movie was \"Watchmen\" (No. 1) grossed $55.7 million during its first weekend, which included $5.5 million from 124 IMAX screens. It also scored the biggest debut of 2009, with a total of $5.5 million, and despite a potentially problematic long running time, it still earned $15.4 million. It also scored the largest debut of 2009 in 2009, with a total of $5.5 million, from 124 IMAX screens, despite a significantly smaller budget.",
      "ground_truths": [
        "R-rated"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b873841cfbc42a793a7a1ba9987a415",
      "question": "Which slot the \"Slumdog Millionaire\" take?",
      "prediction": "The \"Slumdog Millionaire\" took $6.9 million.",
      "ground_truths": [
        "(No. 4)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94b10c393a7e4c55959a283da2c4d78f",
      "question": "What did Madea Goes to Jail gross?",
      "prediction": "\"Madea Goes to Jail\" grossed $11.5 million on its third weekend, placing it in the bottom of most projections.",
      "ground_truths": [
        "$8.8 million"
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "d9d36c6631284d1091e578b9813afe6f",
      "question": "What did Watchmen do?",
      "prediction": "Answer: \"Watchmen\" won the first weekend's box office.",
      "ground_truths": [
        "grossed $55.7 million during its first frame,"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "34fa5a8aa22d49fb90a1eabfaaeaf1c0",
      "question": "What was slumdog millionaire?",
      "prediction": "The answer is \"Jonas Brothers: The 3D Concert Experience.\"",
      "ground_truths": [
        "Best Picture winner \"Slumdog Millionaire\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b3d2d83d34a42848409272d4c9248b9",
      "question": "How many weeks did the pus by Al-Shaabab and Hisb-ul-Islam militias last?",
      "prediction": "The answer is 204,000.",
      "ground_truths": [
        "eight-week"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c170e4dcb7d4be0a567d55d531ba222",
      "question": "Where is the U.N. conflict happening?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Mogadishu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e9ac1c4bf0845eab105c8b1b9265db2",
      "question": "Where is the conflict?",
      "prediction": "The answer is **Somalia**.",
      "ground_truths": [
        "Somali capital of Mogadishu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94ae84b9e54947f9973d73153d50c57f",
      "question": "Has the conflict in Mogadishu had a positive impact on the city's polulation",
      "prediction": "The answer is \"The ongoing conflict in Mogadishu has had a devastating impact on the city's population, causing enormous suffering and massive displacement, and a surge in internally displaced people, a large number of which includes 1.2 million people, in the capital's neighborhoods.\"",
      "ground_truths": [
        "is having a devastating"
      ],
      "em": 0,
      "f1": 0.10256410256410256
    },
    {
      "id": "58c9a110235b4879950e679babb53318",
      "question": "how many people were displaced in Somalia?",
      "prediction": "The answer is approximately 1.2 million.",
      "ground_truths": [
        "more than 1.2 million"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "f57595f77502405fbe6baf093d8baea0",
      "question": "The push by Al-Shabab and Hisb-ul-Islam lasted how long?",
      "prediction": "The answer is: The push by Al-Shaabab and Hisb-ul-Islam militias lasted for approximately 204,000 people.",
      "ground_truths": [
        "eight-week"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "857c29add0314727bee40735ec864631",
      "question": "Approximately how many people were internally displaced in Somalia according to the U.N.?",
      "prediction": "The answer is approximately 1.2 million.",
      "ground_truths": [
        "1.2 million"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "5d6123571c4f4e5cb365ed1f2f6453f6",
      "question": "When did the Somali civil war start?",
      "prediction": "The answer is \"Around 204,000 people have fled their homes in the Somali capital of Mogadishu.\"",
      "ground_truths": [
        "in 1991,\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "c00bbb93fc2947108a477eccccd8d900",
      "question": "Who is the Republican representative",
      "prediction": "The answer is: Jared Polis.",
      "ground_truths": [
        "Jason Chaffetz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e6c47b5fac44a56814f058af7cd3121",
      "question": "Who documented their experince",
      "prediction": "The answer is: Chen-Pols.",
      "ground_truths": [
        "Jason Chaffetz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e250c9e8c70843569a6bc75aa759e58a",
      "question": "what is the name or the Democrat representig Colorado's Second district?",
      "prediction": "The answer is: Colorado's Second district.",
      "ground_truths": [
        "Jared Polis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cae7898982c428aa4712f0d6964c67b",
      "question": "Who is the Democratic representative",
      "prediction": "The answer is: Jared Polis.",
      "ground_truths": [
        "Jared Polis"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "559100c2d1c24e8fae0ba6dcea6e7a69",
      "question": "two fresman representatives documented what?",
      "prediction": "The answer is two Fresman representatives documented their stories.",
      "ground_truths": [
        "\"Freshman Year\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "839679ea2bc546d3aa71072f7edfd81b",
      "question": "what's the name of the Republican representig Utah's Third disctrict?",
      "prediction": "The name of the Republican representig Utah's Third district is:",
      "ground_truths": [
        "Chaffetz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9ad601900986433a9bd99b3a7af2dbb5",
      "question": "Who does Jason Chaffetz represent?",
      "prediction": "Jason Chaffetz is a freshman Republican congressman representing the 3rd District of Utah. He was elected in November, winning two-thirds of the vote. He is a parent of three children.",
      "ground_truths": [
        "conservative Republican"
      ],
      "em": 0,
      "f1": 0.07142857142857144
    },
    {
      "id": "a790d391a6c74fdfadcdeee082a871eb",
      "question": "What are the two freshman documenting?",
      "prediction": "The two new U.S. representatives are documenting their \"Freshman Year\" experiences at CNN.com.",
      "ground_truths": [
        "it really like to be a new member of the world's most powerful legislature?"
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "567c70fec2634f439e055ef5fb30135d",
      "question": "Who is the president?",
      "prediction": "The president is Sheikh Sharif Sheikh Ahmed.",
      "ground_truths": [
        "Sheikh Sharif Sheikh Ahmed"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "ecccb9b15eb3413d8f3a5ad8d8983716",
      "question": "Who is fighting in Somalia?",
      "prediction": "The answer is \"Ethiopia's troops at its request in December 2006.\"",
      "ground_truths": [
        "rebels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5098bc1ebaeb4c25831309d74b7461b6",
      "question": "who won't agree to a strict interpretation of Islamic law?",
      "prediction": "The answer is: The Ethiopian military, under the command of the United States.",
      "ground_truths": [
        "President Sheikh Sharif Sheikh Ahmed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9238d14529524f97a206ee0d51f157b8",
      "question": "What is the aim of the concession?",
      "prediction": "The aim of the concession is to **prevent the implementation of strict Islamic law in Somalia.**",
      "ground_truths": [
        "to halt fighting between Somali forces and Islamic insurgents."
      ],
      "em": 0,
      "f1": 0.18181818181818185
    },
    {
      "id": "dca240ce71894a9798451f919b5d9df0",
      "question": "who has asked African peacekeepers to stand down?",
      "prediction": "The answer is: The African Peacekeepers.",
      "ground_truths": [
        "The president,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89079a330c38458eb5b697271e1e2f0f",
      "question": "What did he ask African peacekeepers to do?",
      "prediction": "The question asks what did Ahmed ask the African peacekeepers to do.",
      "ground_truths": [
        "stand down."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68a10085a6d942d090063841c1d024d3",
      "question": "What will President Ahmed not agree to?",
      "prediction": "President Ahmed will not agree to a strict interpretation of the law.",
      "ground_truths": [
        "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television."
      ],
      "em": 0,
      "f1": 0.24242424242424243
    },
    {
      "id": "3dab0763a0764e6bb2bd2215fda60e7b",
      "question": "What did the Defense Secretary report?",
      "prediction": "The Defense Secretary reported that the U.S. military is \"doing everything possible to free Bergdahl,\" said he.",
      "ground_truths": [
        "U.S. forces in Afghanistan are doing everything possible to free Bergdahl,"
      ],
      "em": 0,
      "f1": 0.5384615384615385
    },
    {
      "id": "d418c84f766f459b90af95983b15f522",
      "question": "What captured soldier said about his future?",
      "prediction": "The answer is:  \"To my fellow Americans who have loved ones over here, who know what it's like to miss them, you have the power to make our government bring them home. Please, please bring us home so we can be back where we belong and not over here.\"",
      "ground_truths": [
        "might never see them again"
      ],
      "em": 0,
      "f1": 0.03846153846153846
    },
    {
      "id": "dcd7d07dad674809bb1ab707f47914d5",
      "question": "What U.S. Defence Secretary said about finding Bergdahl?",
      "prediction": "The U.S. Defence Secretary said that he was disgusted at the video that Bergdahl's captors released.",
      "ground_truths": [
        "forces in Afghanistan are doing everything possible to free"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b03da7faf474f56bb9d4447f62f0c9d",
      "question": "What friend to soldier said?",
      "prediction": "The answer is \"To my fellow Americans who have loved ones over here, who know what it's like to miss them, you have the power to make our government bring them home. Please, please bring us home so we can be back where we belong and not over here.\"",
      "ground_truths": [
        "\"stand tall, stand firm.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76a9bca3223248d393c03b2583aa76fb",
      "question": "What is one soldier from Idaho afraid of?",
      "prediction": "The answer is prayer.",
      "ground_truths": [
        "I might never see them again and that I'll never be able to tell them I love them again. I'll never be able to hug them."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff0b3796a6dc40fc8d87d3532065b3f1",
      "question": "Who was captured?",
      "prediction": "The answer is \"Bowe Bergdahl\" (from the CNN affiliate KTVB-TV).",
      "ground_truths": [
        "U.S. soldier"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a940d9cc02a74c0ea64944ea9d6c4842",
      "question": "Where is the soldier from?",
      "prediction": "The answer is \"Idaho.\"",
      "ground_truths": [
        "Ketchum, Idaho."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b7643157f5e54f3f8195949fe0804a3b",
      "question": "what are the results based on",
      "prediction": "The results of the \"Marcus Autism Center in Atlanta\" study, which found that 1 percent of children aged 3 to 17 have autism, increased over previous estimates.",
      "ground_truths": [
        "a national telephone survey"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "747b8f3339c94fa293f30e648670e0f1",
      "question": "What kind of survey was used?",
      "prediction": "The answer is \"the federal government's 2007 national survey of children's health.\"",
      "ground_truths": [
        "national telephone"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "e4864c159e354a4cbc7b2e4fdfd2c4a2",
      "question": "At what age is autism a huge risk?",
      "prediction": "The text also states that \"a significant issue that needs immediate attention\" is identified as autism, and that \"a concerted effort and substantial national response is warranted.\" It also mentions the 2007 national survey of children's health, and the 2003 CDC-funded study, indicating a slight increase in the prevalence of ASD in the United States among children ages 3 to 17 at 110 per 10,000. It also notes that \"boys were four times as likely as girls, and non-Hispanic black and multiracial children were less likely than non-Hispanic white",
      "ground_truths": [
        "3 to 17"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "c83e142be48c493d995b27b01eaeda65",
      "question": "who took part in the survey",
      "prediction": "The answer is: \"The researchers asked parents whether a health care provider had ever told them their child had an autism spectrum disorder. Nearly 40 percent of the parents and guardians said no. \"",
      "ground_truths": [
        "78,000 parents of children ages 3 to 17.iReport.com:"
      ],
      "em": 0,
      "f1": 0.11111111111111112
    },
    {
      "id": "a737b734b5b949009898da99b81e82ac",
      "question": "What percentage of children have autism?",
      "prediction": "The answer is 110%.",
      "ground_truths": [
        "about 1 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22536e61437f4cc59db250397061e8db",
      "question": "who has autism or related disorder",
      "prediction": "The answer is \"This is a behavioral diagnosis, and it's difficult to make, and it‚Äôs difficult to make at young ages,\" and the researchers say that \"110 per 10,000 of the United States children have ASD, with 673,000 children in the United States having ASD.\"",
      "ground_truths": [
        "3 to 17"
      ],
      "em": 0,
      "f1": 0.046511627906976744
    },
    {
      "id": "34532b23c4624e6e92c4faf30085297d",
      "question": "What did the DA's office say about the suspects?",
      "prediction": "The DA's office said the DA's office said the gang's \"largest and most sophisticated ring in U.S. history\" is a \"major and potentially highly sophisticated ring of its kind in U.S. history.\"",
      "ground_truths": [
        "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0.09836065573770493
    },
    {
      "id": "0986a62e4b4b48be90e2c9172a5b9617",
      "question": "What did the suspects do according to the DA's office?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "identity theft"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d69a7141ec1c4110ac124dd58d19b8b4",
      "question": "What did the DA say?",
      "prediction": "The DA said the DEA, under the special powers of the Department of Justice, had conducted a \"largest and most sophisticated ring of its kind in U.S. history.\"",
      "ground_truths": [
        "Several suspects are believed to have engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0.15625
    },
    {
      "id": "2f56933ee8024412b6ef05faf87e7a46",
      "question": "What did the suspects do?",
      "prediction": "The suspects did not tell the authorities what they did.",
      "ground_truths": [
        "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d4f644a529f41c4b257f9ffb9ce9c87",
      "question": "Where did the credit information come from?",
      "prediction": "The answer is New York (CNN).",
      "ground_truths": [
        "American and European consumers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "407b7cab4614445f991abdc1b724f793",
      "question": "Who was a popular casting choice?",
      "prediction": "Based on the context, the answer is Derek Mears, an actor known for his horror genre roles. He's cast as Jason Voorhees in the new \"Friday the 13th\" movie.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "be452005b70e4c44b514569be76510c2",
      "question": "What does Mears say of the iconic role?",
      "prediction": "Mears' comment on the \"Friday the 13th\" movie's success, his subsequent talk to CNN, and his experience as a stuntman in \"Pirates of the Caribbean\" films, both indicate his willingness to be sincere and acknowledge his shortcomings in his previous work. He also mentions his recent speaking to CNN about his shifts in his \"ultimate evil\" persona and his commitment to his \"father issues.\" Finally, he says, \"It's just a bit weird. It's funny with acting, we all wear masks in our normal lives. Not to sound like a weirdo in the hockey mask, but",
      "ground_truths": [
        "It's so weird. There's two different versions. There's my version of how it went about, and there's the producer's"
      ],
      "em": 0,
      "f1": 0.09433962264150944
    },
    {
      "id": "6c6bfb3e518f4c11a1215c7a6c21ddc6",
      "question": "Who takes on role of Jason in new \"Friday the 13th\"?",
      "prediction": "Based on the context, the answer is Derek Mears. He is cast as the iconic boogeyman Jason Voorhees in the new \"Friday the 13th\" movie.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "d94bbc9c885346efbf9fed0c63ad7ff4",
      "question": "Who takes on role of Jason?",
      "prediction": "Here's the answer directly from the context:",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c429c7cffb24fa0afbdef4ca48e33d1",
      "question": "Who is taking on the role of Jason?",
      "prediction": "The answer is Derek Mears, a long-time horror actor who has retired from the \"Over the Top\" franchise. He's not that bloody nice, and he's definitely not that scary. He is really excited about the film and will start to speak faster. CNN's question asks him to tell CNN about his role as Jason Voorhees.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "666120b705a140c191a4f26d3827bd7d",
      "question": "Who says he tried to \"definitely make it my own\"?",
      "prediction": "The answer is Derek Mears, who is cast as Jason Voorhees in \"Friday the 13th.\" He says he's really excited about the film and playing the character, and he's already started to speak more \"fast.\" CNN notes his intense emotional response to the film.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.09756097560975609
    },
    {
      "id": "03b048d6c9c7420997c632cb9b29c106",
      "question": "Who heard from producers that he was a popular casting choice?",
      "prediction": "The answer is Derek Mears, who played Jason Voorhees in the \"Friday the 13th\" movie.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "a41630aeea734939ad1f2f6d7b44a9b5",
      "question": "Who trid to \"definitely make it my own\"?",
      "prediction": "The answer to the question is Derek Mears. He is described as a \"big hockey mask to fill\" and has a career with extensive work as a stuntman in the \"Pirates of the Caribbean\" films, with his new \"Friday the 13th\" movie as his final, and arguably more terrifying, work. He also has a history of the \"Monster\" franchise and mentions his previous experiences as a stuntman in \"Pirates of the Caribbean\" films.",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 0,
      "f1": 0.06349206349206349
    },
    {
      "id": "b466d0d5acdb4c96bb1d4bd6d19548c4",
      "question": "Who are not doing great?",
      "prediction": "The provided text does not contain a question. It only states that Amanda Knox's aunt Janet Huff was watching television news coverage about her niece's conviction for murder. Therefore, it does not answer the question.",
      "ground_truths": [
        "Knox's parents"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "023e088f89274e2abd8a49edd96a408d",
      "question": "Who is Janet Huff?",
      "prediction": "Based on the context, the answer is Janet Huff.",
      "ground_truths": [
        "Amanda Knox's aunt"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8972465dad04d5f8b2fcf659f8aeb25",
      "question": "who was convicted of murder",
      "prediction": "The answer is: Amanda Knox.",
      "ground_truths": [
        "Amanda Knox's"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "21eca67432494831b67960018d045fd7",
      "question": "who was murdered",
      "prediction": "The answer is: \"It was terrible, it was gut-wrenching just to hear them say it,\" and \"Knox and his Italian former boyfriend, Raffaele Sollecito, were convicted of murdering Knox's roommate Meredith Kercher. Knox was sentenced to 26 years and 25 years in prison.\"",
      "ground_truths": [
        "Meredith Kercher."
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "fca02df414b8412b83e2f9d34be46341",
      "question": "what did huff say",
      "prediction": "Answer: Hillary Clinton said she is \"saddened by the verdict, and I will be vindicated.\"",
      "ground_truths": [
        "\"It was terrible, it was gut-wrenching just to hear them"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0cee4e0245b54d429412963ad22f68ab",
      "question": "What the police investigating?",
      "prediction": "The answers to the question depend on the context provided. Here's the answer:",
      "ground_truths": [
        "investigator into the deadliest terrorist attack in Argentine history that he was kidnapped and tortured by men who said they were national intelligence agents."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ebf6a7cef1a4f75aaa2ffb7510a3c8f",
      "question": "Who does he say tortured him?",
      "prediction": "The answer is: **the assailants tortured him**.",
      "ground_truths": [
        "said they were national intelligence agents."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a32771820f14982b983662bca90f254",
      "question": "What is the name of the lawyer?",
      "prediction": "The answer is: **The lawyer** who was arrested and removed from the AMIA investigation.",
      "ground_truths": [
        "Lifschitz,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3dd74943ebab4b1d828661d36a868dab",
      "question": "The bombing of a Jewish center in Buenos Aires took place in which year?",
      "prediction": "The answer is Argentina. (Buenos Aires, Argentina)",
      "ground_truths": [
        "1994,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "35dc385df2a9486a8a23a951e5f249d7",
      "question": "What day was he abducted?",
      "prediction": "The answer is Monday.",
      "ground_truths": [
        "Friday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b8eb81e9e5f84de29e31354f7288d548",
      "question": "What did she win?",
      "prediction": "The answer is: **Mariek** (who was not in the Canadian team).",
      "ground_truths": [
        "Olympic medal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb3ba559580b4d6d8123dd7de545c1de",
      "question": "What has Joannie Rochelle earn?",
      "prediction": "Joannie Rochelle has earned 228.56 points in the medal category.",
      "ground_truths": [
        "bronze medal in the women's figure skating final,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "2a015deb51034910a9b0c15e43c72c5f",
      "question": "what does rochette says abouth her mother?",
      "prediction": "Rochette says that her mother was the biggest supporter and biggest critic, and she was always encouraging, too, even when she was finding faults.",
      "ground_truths": [
        "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,"
      ],
      "em": 0,
      "f1": 0.20512820512820512
    },
    {
      "id": "55e0a00ae647444abe645a821f0cd737",
      "question": "What does Rochette say of her mother?",
      "prediction": "Rochette says, \"It feels good for me to talk about her,\" and \"I have no doubt that she had that strength.\"",
      "ground_truths": [
        "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,"
      ],
      "em": 0,
      "f1": 0.2162162162162162
    },
    {
      "id": "a2211516832146c59692b15720041f8f",
      "question": "When did she skate?",
      "prediction": "The answer is, Rochette had skatebeded for four days after Therese Rochette's death.",
      "ground_truths": [
        "Tuesday night's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec9acff63b5e401e8ba12f7828de0cfe",
      "question": "What is her sport?",
      "prediction": "Based on the context, the answer is: **Canadian's collective heart, relying on the memory of her mom's dedication and her own intense focus, and her own determination to overcome the pain of her mother's death.**",
      "ground_truths": [
        "women's figure skating"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e2e3e8fa88145c9b7c4b4b98e54bcbb",
      "question": "Which group did North Korea participate in?",
      "prediction": "The answer is North Korea participated in the 2010 World Cup.",
      "ground_truths": [
        "2"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c396c2292f664fddbd09c3dfcb7d85f0",
      "question": "How many goals were there in the Australia-Japan match of Group 1 of Asian ?",
      "prediction": "The answer is: Australia won 2-1.",
      "ground_truths": [
        "2-1"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "f64f4338826d4bd194bb0209f0d73afd",
      "question": "What was the final score between Australia and Japan?",
      "prediction": "The answer is **1-1**, with Argentina being the winner.",
      "ground_truths": [
        "2-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c0aa93109c14bec9d9250084876e556",
      "question": "What was the score that allows North Korea to qualify ?",
      "prediction": "The score at the final whistle at the Stadium of Dreams in Riyadh confirmed that North Korea qualified for the World Cup finals for the first time since 1966. The team won the match 0-0, with Manchester United midfielder Park Ji-Sung scoring a brilliant goal in the 40th minute.",
      "ground_truths": [
        "0-0 draw"
      ],
      "em": 0,
      "f1": 0.047619047619047616
    },
    {
      "id": "a081bc6140dd47baba7768168a3bb4f6",
      "question": "What country qualified from group 2 with 0 to 0 draw against Saudi Arabia?",
      "prediction": "The answer is South Korea. The team that qualified from group 2 with a 0-0 difference against Saudi Arabia won the World Cup final in South Africa, and they will compete against the best teams in the world, as they won the 2010 World Cup final in Riyadh.",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.046511627906976744
    },
    {
      "id": "4918b1401f964c9dadb693941ca7e05c",
      "question": "Who clinched the playoff spot from Group 1 with 1-0 win?",
      "prediction": "The answer is:  The North Korean team won the final match in South Africa.",
      "ground_truths": [
        "Bahrain"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6d3dc82139f04c41a99e454f2f6eeed2",
      "question": "What group qualified with 2 to 1 win over Japan?",
      "prediction": "The group that qualified with 2 to 1 win over Japan was the North Korean team.",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "02da09b14bdb44abb93daf65aa4e4c09",
      "question": "Against what nation does Bahrain played and win ?",
      "prediction": "The answer is: **Saudi Arabia** played against the World Cup final.",
      "ground_truths": [
        "Uzbekistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21532189e740426194a08808390eb2b4",
      "question": "The American was taken into custody after entering from where?",
      "prediction": "The answer is the US State Department.",
      "ground_truths": [
        "China"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "23f830d9e6c24104852473a27360624c",
      "question": "Who is believed to be held in North Korea?",
      "prediction": "The answer is:  North Korea.",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f97a45e3576644eda7c95849e50babef",
      "question": "Who was trying to sneak in to bring message of \"Christ's love and forgiveness\"?",
      "prediction": "The answer is: *North Korea.*",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "912cb6aecbec4dc2b44ee22a076c3e2a",
      "question": "Who was taken into custody after entering from China?",
      "prediction": "The answer is:  *The North Korean state-run Korea Central News Agency did not identify the man who was taken into custody after he delivered the message. It was a message from Jesus Christ, the Son of God. And the North Korean authorities did not indicate the man's arrest in the same way as the Washington State News Service did.",
      "ground_truths": [
        "a Korean-American missionary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f811a0a4db94a648917355fabbd40ae",
      "question": "Where is he believed to be held?",
      "prediction": "The answer is North Korea.",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3f28f7be13ad43cda7681b7fa874843c",
      "question": "What is the name of the Korean-American missionary?",
      "prediction": "The answer is **Robert Park**.",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "902f161fe57f4998b07fd0de6d95270d",
      "question": "What Park said?",
      "prediction": "Based on the provided context, the answer is: **\"He is a very special member of our family. We miss having his love and compassion in our home.\"**",
      "ground_truths": [
        "he was trying to sneak into the isolated communist state to bring a message of \"Christ's love and forgiveness\" to North Korean leader Kim Jong Il."
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "8ce493334e064c2f97343909d7ddb91e",
      "question": "What did rescue workers do?",
      "prediction": "Rescue workers did not pull a body from underneath the rubble. They found the remains at about 2 a.m. Sunday morning (8 p.m. ET Saturday).",
      "ground_truths": [
        "have pulled a body from underneath"
      ],
      "em": 0,
      "f1": 0.22222222222222218
    },
    {
      "id": "287086d87ca540629ea9a5d1f3e42827",
      "question": "What hasn't been determined?",
      "prediction": "The answer is \"It is still not clear what the cause of the collapse of the building, which contained the city's historical archives, brought down parts of the two nearby structures.\"",
      "ground_truths": [
        "caused the collapse of the building"
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "d1b4f5f6d643456aad1526324d39c259",
      "question": "Who was pulled from the rubble?",
      "prediction": "The final answer is \"It is still not clear what caused the collapse of the building which contained the city's archives, bringing down parts of the two nearby structures.\"",
      "ground_truths": [
        "body"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa55b1829c9c41e28f3998d32da669ec",
      "question": "Who pulled the body from the collapsed building?",
      "prediction": "The body was pulled from the collapsed building in Cologne, Germany.",
      "ground_truths": [
        "Rescue"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "06752c5554924c339e8cc99f3e4776aa",
      "question": "What collapsed?",
      "prediction": "Answer: The collapse of the Cologne's archive building, which contained the city's archives, caused the two missing men to be found at about 2 a.m. Sunday morning.",
      "ground_truths": [
        "apartment building"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "f540c016913344238538408d0b3ac731",
      "question": "What happened in Cologne?",
      "prediction": "The answer is \"Not clear what caused the collapse of the city's archives building, bringing down parts of the two nearby structures.\"",
      "ground_truths": [
        "workers have pulled a body from underneath the rubble of a collapsed apartment building"
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "d031593cc6f142f6bba7e64ea9224dea",
      "question": "Where did the building collapse?",
      "prediction": "The building collapsed in Cologne, Germany.",
      "ground_truths": [
        "Cologne, Germany,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "963f7654e83b425bbeadf56527740ba0",
      "question": "Have authorities determined what brought down the structure?",
      "prediction": "Answer: The collapse of the city's archives building in Cologne, Germany, and the subsequent search and rescue efforts, which involved the removal of the two missing men, did not determine what led to the collapse of the city's archives.",
      "ground_truths": [
        "still not clear"
      ],
      "em": 0,
      "f1": 0.05714285714285714
    },
    {
      "id": "19fdbf5cb17843f1b6afc14de8604661",
      "question": "Who is facing misdemeanor assault charges?",
      "prediction": "The answer is: *Bryant Purvis*",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2c390a19b735414bbf4928f8c37aa879",
      "question": "Teen is awaiting trial for what?",
      "prediction": "The answer is: Teen is awaiting trial for questioning.",
      "ground_truths": [
        "second-degree aggravated battery."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "28dbd333877249afbb95296f3404dfd7",
      "question": "Who is facing the charges?",
      "prediction": "The answer is **Bryant Purvis**.",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "56aa271b35074e4fba0d82728d99c5c0",
      "question": "Who did Purvis allegedly choke?",
      "prediction": "The answer is Arrested for Second-Degree Attempt Murder.",
      "ground_truths": [
        "An 18-year-old student"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "538b9b0a443147089da51e3cbd31de77",
      "question": "What is Bryant Purvis facing?",
      "prediction": "Bryant Purvis is facing misdemeanor assault charges, and he is being transferred to the Denton County Detention Facility. He is one of six former students of Jena, Louisiana who were accused of being involved in a white student's beating. He's awaiting trial in the second-degree aggravated battery case. He is being transferred to the Denton County Detention Facility. He was charged with second-degree aggravated battery.",
      "ground_truths": [
        "misdemeanor assault charges"
      ],
      "em": 0,
      "f1": 0.09375
    },
    {
      "id": "63002f70e2f74001b710cc5b6c97c79c",
      "question": "What does the fight not appear to be?",
      "prediction": "The answer is: Arrested.",
      "ground_truths": [
        "racially motivated."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec7ec97bf1624e75b422a32cfa68c104",
      "question": "who allegedly choked a teen?",
      "prediction": "The answer is: \"A child. The student who was struck by a man's hand.\"",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d679f2ddc15e438591bc73863dfc23cf",
      "question": "Where were the bodies discovered?",
      "prediction": "The bodies were found inside their homes in the capital of Villahermosa, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico City, Mexico",
      "ground_truths": [
        "inside their home in the capital of Villahermosa"
      ],
      "em": 0,
      "f1": 0.13043478260869565
    },
    {
      "id": "a09179766c3345a1a9d54f959ba12b7c",
      "question": "How many sons did Jose Francisco Fuentes Esperson and his wife have?",
      "prediction": "The answer is:",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "39b334c095e8467981db2a6689f92914",
      "question": "Who did Schlesselman threaten to kill on October 23, 2008?",
      "prediction": "Washington (CNN)",
      "ground_truths": [
        "then-Sen. Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2683ec5ca85f422da7ebd56b903a78ab",
      "question": "What did Schlesselman threaten to do?",
      "prediction": "Schlesselman threatened to kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.",
      "ground_truths": [
        "kill then-Sen. Obama"
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "c6d223e334824da48b2c3885756f06d2",
      "question": "Schlesselman made a deal with federal prosecutors from what state?",
      "prediction": "Answer: Tennessee",
      "ground_truths": [
        "Tennessee."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4ad67beb0c1c4f9eb667ab93e3f4c322",
      "question": "How many years in prison does Schlesselman face?",
      "prediction": "According to the context, Schlesselman faces 10 years in prison in Tennessee.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "2b9f8d5e6f234a8ab99b25b44a529086",
      "question": "What racial group did Schlesselman focus on targeting?",
      "prediction": "Arrested",
      "ground_truths": [
        "African-Americans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "553920fb5b924a5896e590136039b57c",
      "question": "Who did Schlesselman threaten to kill?",
      "prediction": "Answer: Jefferson Adams.",
      "ground_truths": [
        "then-Sen. Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "747f1582934e443b9ce8b6a8a35718d7",
      "question": "How many years does Schlesselman face in prison?",
      "prediction": "The answer is 10 years.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "fbbce6b6fec84eaf8c77bf9182971bef",
      "question": "Who made a plea deal?",
      "prediction": "Answer: The Justice Department made a plea deal with the FBI.",
      "ground_truths": [
        "Paul Schlesselman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb390e324ea348b19dd922a431b57c93",
      "question": "Who do officials blame for attacks on police?",
      "prediction": "The officials who blame for attacks on police:",
      "ground_truths": [
        "Taliban and their al Qaeda associates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37a00147dbc44e0c8b9c7693bea1d3f3",
      "question": "When were dozens of Taliban militants killed?",
      "prediction": "The answer is:  In Kabul on Sunday, with 8900 additional police officers.",
      "ground_truths": [
        "Sunday,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "4445a2b47bb1498fa032751e2d3f0224",
      "question": "For what reason were extra police deployed around Kabul?",
      "prediction": "The answer is: **for increased foot patrols were observed.**",
      "ground_truths": [
        "Monday's 89th observance of Afghanistan's independence from Great Britain."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cb0b177021d4e22aba08856e9a81ee3",
      "question": "Independence from what?",
      "prediction": "The answer is: **Afghanistan's independence from Great Britain was the most significant security operation in Kabul on Sunday.**",
      "ground_truths": [
        "Great Britain."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "66485f565d3c4c248cafd2ef5708d9d7",
      "question": "How many officers were killed?",
      "prediction": "The answer is 32.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad75ca9e4c034d1f99fcf36517260cc5",
      "question": "Who is to blame for the bomb?",
      "prediction": "The answer is: A Taliban.",
      "ground_truths": [
        "Taliban and their al Qaeda associates"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "3f513129872a41228cb7a0aada0f81d5",
      "question": "What city has extra police officers?",
      "prediction": "The answer is **Kabul**.",
      "ground_truths": [
        "Kabul"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1958d14b7db54bffbbdd57e3677c08e7",
      "question": "What was his novel called?",
      "prediction": "J.G. Ballard's novel was made into a movie by Steven Spielberg.",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eeef45fe8f664128b6f49991e5695c3e",
      "question": "What did he die from?",
      "prediction": "J.G. Ballard died Sunday at age 78.",
      "ground_truths": [
        "cancer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96b2b16f84e8465d9de71f9fa9718941",
      "question": "When did he start writing fiction?",
      "prediction": "The answer is \"in the 1950s, with his work taking on a strong, psychologically-abstract approach in later decades.\"",
      "ground_truths": [
        "1950s,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "3f1a03dd30304609a4e646069b0deffd",
      "question": "What was the name of the film",
      "prediction": "The film was made into a movie by Steven Spielberg.",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6071cc99d5744a17814ad38e72b53f3b",
      "question": "What did he write about?",
      "prediction": "The answer is \"his semi-autobiographical account of the war years, written in a significant, deeply autobiographical manner, which became his best-known work, and it was made into a film by Steven Spielberg in 1988.\"",
      "ground_truths": [
        "boyhood experience in a World War II internment camp"
      ],
      "em": 0,
      "f1": 0.10526315789473685
    },
    {
      "id": "d5012bef02214f0c9698b7a828959e39",
      "question": "What book was made into a film?",
      "prediction": "The answer is \"Empire of the Sun.\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "8ff3ddfe7f3a41e294a831e452189dd5",
      "question": "What books did he wrote",
      "prediction": "The answer is \"J.G. Ballard wrote science fiction fiction in the 1950s, with a strong psychological, apocalyptic bent, and an early in the 1950s, strongly psychological, and cannibalistic account of the war years. He made it into a film by Steven Spielberg in 1988.",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 0,
      "f1": 0.05
    },
    {
      "id": "c46c0d1751554ccea747cb3825bdbe66",
      "question": "Who died from cancer?",
      "prediction": "The answer is Steven Spielberg.",
      "ground_truths": [
        "British"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18cc95b228434b23a3969ac0d65687fa",
      "question": "What did they write",
      "prediction": "The answer is \"his semi-autobiographical account of the war years, his best-known work,\" and it is Steven Spielberg's film adaptation of \"Crash.\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "504074fdd18a4bfdb3c4d8b3d6a4b589",
      "question": "which player was the favourite",
      "prediction": "The answer is: **Venus Williams**.",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fc9ff2ab888d45b79fa4959911c65e59",
      "question": "which title number was this",
      "prediction": "The title number of the Madrid Open was 6.",
      "ground_truths": [
        "44th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5b80b9f943d4bd3b11c84700bff495f",
      "question": "When did the Madrid Open final take place?",
      "prediction": "The Madrid Open final took place on Sunday.",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "368647e758554203bcd6310b0cb41a9f",
      "question": "what was the victors seed",
      "prediction": "The victors seed of the Madrid Open is Aravane Rezai.",
      "ground_truths": [
        "Unseeded"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c551f0e759174fbc80c4d851c51c31fd",
      "question": "What age is Rezai?",
      "prediction": "The answer is \"23-year-old Frenchwoman Aravane Rezai.\"",
      "ground_truths": [
        "23-year-old"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "52f45355af90482cb6a950e995783d4b",
      "question": "Who was favourite to win?",
      "prediction": "The answer is: \"Venus Williams.\"",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ba4602ee540d4fcfb52af541b3e7dfd8",
      "question": "What Aravane Rezai claims the player?",
      "prediction": "Aravane Rezai claims the player, who had only claimed WTA Tour titles at Strasbourg and Bali before Madrid Open, claims the player.",
      "ground_truths": [
        "WTA Tour titles"
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "b997384c17614f7ba1954363aa86dade",
      "question": "where was the shooting",
      "prediction": "The answer is Arkansas. The prompt does not specify where the shooting occurred. The answer then tells the story of a man who was angry at the US military's actions in the past.",
      "ground_truths": [
        "Arkansas"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "50cc5fd5df684303b18d21732c69c85e",
      "question": "what was the motive?",
      "prediction": "The answer to the question is: Cordial.",
      "ground_truths": [
        "\"political and religious\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "acc714436403401f87f020ab900438cd",
      "question": "What happened in Monday's shooting?",
      "prediction": "The answer is: Pvt. William Long, 23, of Conway, Arkansas, was killed in a shooting at a Little Rock military recruiting center in Arkansas.",
      "ground_truths": [
        "killed one soldier and wounded another at a Little"
      ],
      "em": 0,
      "f1": 0.20689655172413796
    },
    {
      "id": "df4b2f48ebd849209e17392d1f220418",
      "question": "What do the police say?",
      "prediction": "The police state that the Arkansas man, William Long, 23, of Conway, Arkansas, was killed in a shooting at Little Rock military recruiting center, and that he was a practicing Muslim who committed a crime.",
      "ground_truths": [
        "He faces one count of capital murder and 16 counts of engaging in a terrorist act,"
      ],
      "em": 0,
      "f1": 0.17777777777777776
    },
    {
      "id": "49db254b761e4e4199cd037cf89f3575",
      "question": "what was the motives",
      "prediction": "The answer is: the motives behind the shooting were unknown.",
      "ground_truths": [
        "angry over the treatment of Muslims,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e4e50ab633a4bd79a08dcc39d7a6b00",
      "question": "where did this happen?",
      "prediction": "The answer is: **at Little Rock Military Recruiting Center in Arkansas.**",
      "ground_truths": [
        "Rock military recruiting center"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "1e425208f97249e5a81e6454cc03f5e1",
      "question": "What television showed footage?",
      "prediction": "The television show \"We Are at Home\" showed a video of the villa and its pool, where the nine women, dressed in mini-skirts and bikinis, posed by a hostess, Zeynep Karacan, in a red bikini. The website's rating system displayed the second-most popular contestant as a woman from Kocaeli, who was 18-year-old, born on September 14th, 1990, but in her on-camera appearance, she was 16. The website's rating system also listed Tugce as a 18-year-old,",
      "ground_truths": [
        "Turkish"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e806b10c581643d7aec4fe0c185c60e8",
      "question": "Did the women sign contracts?",
      "prediction": "The Turkish National Television (NTV) website's web page, \"We Are At Home,\" contains videos of the nine female contestants' photos, including a list of ratings for viewers, and the \"virtual gifts\" they were offered with.",
      "ground_truths": [
        "were signed with the contestant girls,\""
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "c149bbaa57fb45f1b8dcf83ce591934f",
      "question": "How many young girls were returned?",
      "prediction": "The answer is **9**.",
      "ground_truths": [
        "eight or nine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f7d5f07f09814ae6bde8fd535228203f",
      "question": "What is name of the place with the best burger?",
      "prediction": "The name of the place with the best burger in Columbia, Missouri is Booches Billiard Hall.",
      "ground_truths": [
        "Booches Billiard Hall,"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "daf9ef11c5214622aff43957272a6899",
      "question": "What sport is Edwards in?",
      "prediction": "According to the context, the answer is:",
      "ground_truths": [
        "NASCAR"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2321a5071574ea5bc97d8ee97b3358f",
      "question": "Which state does Edwards talk about his favorite spots?",
      "prediction": "The answer is: Columbia, Missouri.",
      "ground_truths": [
        "Columbia"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "6a3cfa2f24494994bb285aa0a444492e",
      "question": "Where is the best burger?",
      "prediction": "The answer is Boches Billiard Hall, in Columbia, Missouri.",
      "ground_truths": [
        "Booches Billiard Hall,"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "f28b7801ac5e4385a6e8999492e8974f",
      "question": "What did the Tech crunch founder call silicone valley?",
      "prediction": "The answer is, \"With all due respect, the Silicon Valley tech market is a meritocracy. There are very few businesses where an individual in his or her bedroom can create a piece of software that can touch millions of people, and do this without seeking additional capital. This is a state of meritocracy, and people, even the most talent-based entrepreneurs, seek out and reward those who excel in the field.\"",
      "ground_truths": [
        "a pure meritocracy,"
      ],
      "em": 0,
      "f1": 0.03125
    },
    {
      "id": "cc82210865b84c0fba91c2ec5b5f534d",
      "question": "What Hank Williams said about founder of TechCrunch?",
      "prediction": "The answer is: Yes, Hank Williams said that he saw significant bias in Silicon Valley, and that even a strong entrepreneur can be successful because he or she has a \"big brain.\"",
      "ground_truths": [
        "I think Arrington truly believes everything he has"
      ],
      "em": 0,
      "f1": 0.1081081081081081
    },
    {
      "id": "6f86cbec527c4c75862e16b938872404",
      "question": "Who has been involved in a Twitter spat?",
      "prediction": "The answer is: New Promised Land: Silicon Valley.",
      "ground_truths": [
        "Michael Arrington,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c9ff035684f49099a2147c4f17a2992",
      "question": "Who called Silicon Valley a meritorcracy?",
      "prediction": "The answer is, \"I believe that everything he has said about the tech world is a meritocracy. There are very few businesses where an individual in his or her bedroom can create a piece of software that can potentially touch millions of people, and do this without seeking any additional capital. This is not a person's right. And this is a meritocracy. The market makers work in a world that is not particularly evenhanded. In conclusion, I do not say that any VC (venture capital firms) says anything at a meeting, because I am not. I am not saying at a meeting. I am not saying that they say",
      "ground_truths": [
        "Michael Arrington,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "349744fdc69c490ba50ba01f9d31f9a3",
      "question": "Who said it was not so?",
      "prediction": "The answer is: **It was not so.**",
      "ground_truths": [
        "Vivek Wadhwa,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "24809353158b4ac89851907981f74f68",
      "question": "Who turned fire hoses on vessel?",
      "prediction": "The answer is: The U.S. claims are gravely in contravention of the facts and are totally unacceptable to China.",
      "ground_truths": [
        "The Impeccable"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f7de5ba2f53f4a35af0e9cfad20792e5",
      "question": "What was the Navy surveillance vessel  violating?",
      "prediction": "The Navy surveillance vessel, the U.S. Navy's underwater acoustic array, violated China's maritime laws by allowing it to tow its array of monitoring equipment, resulting in a loss of the sonar equipment, which was left behind.",
      "ground_truths": [
        "Chinese and international laws"
      ],
      "em": 0,
      "f1": 0.05555555555555555
    },
    {
      "id": "0f06f03e9d4d42aca200b9604dc7efc6",
      "question": "On what date did the fire hose incident take place?",
      "prediction": "The answer is: The fire hose incident took place on March 8, 2023.",
      "ground_truths": [
        "March 8"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "a7d83f75668d49f4ba1ee28570d5b09d",
      "question": "What did China say was violating Chinese and International law?",
      "prediction": "The answer is: \"Lou and Wilson man the hoses while the Chinese irritate us to tears.\"",
      "ground_truths": [
        "the ship"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef734b3df1aa44bd9e1f32f8f4da6f19",
      "question": "who has more marine power",
      "prediction": "The answer is: the Pentagon.",
      "ground_truths": [
        "Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb2d1f54601e4f89be70d07393db45f4",
      "question": "Who launched the Nook Tablet?",
      "prediction": "The answer is: Nook Tablet.",
      "ground_truths": [
        "Barnes & Noble CEO William Lynch"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "17a72d69d7fd4e2fb9c562451e5434ef",
      "question": "Who has launched the Nook tablet?",
      "prediction": "The answer is \"Nook Tablet,\" and it's 16GB of 8GB RAM.",
      "ground_truths": [
        "Barnes & Noble"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5b7387ec945d47b8b624c2b55b6a80c1",
      "question": "Which tablet is the most popular?",
      "prediction": "Here's the answer, based on the context:",
      "ground_truths": [
        "Apple iPad,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30913ce681084f3f8357bef6147fe049",
      "question": "How much does the Nook cost?",
      "prediction": "The answer is \"Nook Tablet: $249.\"",
      "ground_truths": [
        "$249"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "6d34f597fdc54dd2aa1a8a9e32e596cb",
      "question": "Is the Kindle Fire cheaper than the Nook Tablet?",
      "prediction": "The answer is yes. The context explicitly states that the Kindle Fire is \"deficient for a media tablet,\" meaning it is considered less suitable for a media tablet. The Nook Tablet is specifically known for its \"content-rendering\" which is described as better on Kindle Fire. Therefore, the answer is yes.",
      "ground_truths": [
        "costs $50 less,"
      ],
      "em": 0,
      "f1": 0.04347826086956522
    },
    {
      "id": "d5f0273d41ad45ab9a9819cb49e29b02",
      "question": "How much cheaper is Amazon's Kindle fire?",
      "prediction": "The answer is \"Nook Tablet: $249, Kindle Fire: $199.\"",
      "ground_truths": [
        "$50"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "92100c8dcef141f69ab22ef3ad13d9d5",
      "question": "Name Ali's wife?",
      "prediction": "Answer: Lonnie Grady",
      "ground_truths": [
        "Lonnie"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "112542d377134435a5e6e79b2e9a5c1a",
      "question": "What did Ali have to say?",
      "prediction": "Answer: The answer is, \"It was incredible. We've had so much rain, and yet it was beautiful. The rain held off wherever Muhammad Ali went,\" Frankie Neylon said, the mayor of Ennis, County Clare, said.",
      "ground_truths": [
        "not speak"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6029989106ef4c32bc7eb342670dc8c1",
      "question": "What did Ali unveil in town?",
      "prediction": "Answer:  \"It was incredible. We've had so much rain, and yet it was beautiful. The rain held off wherever Muhammad Ali went.\"",
      "ground_truths": [
        "plaque"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8eacd2be836f472a83d7008f33ed8506",
      "question": "Where is Muhammad Ali's ancestral town?",
      "prediction": "The answer is **Ireland**.",
      "ground_truths": [
        "Ennis, County Clare"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01b98eb2ff194b198d052f759f6f12bb",
      "question": "What do nutritionists warn?",
      "prediction": "The nutritionists warn against using injectable vitamin supplements because the quantity of vitamins and minerals are not regulated.",
      "ground_truths": [
        "against using injectable vitamin supplements"
      ],
      "em": 0,
      "f1": 0.47619047619047616
    },
    {
      "id": "3257fb95269946feab9eeec2ebcd3522",
      "question": "What do the injections target?",
      "prediction": "The injections target the blood in the bloodstream, which helps to maintain the body's health and vitality.",
      "ground_truths": [
        "a particular health ailment or beauty concern."
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "0e8a39c6c38742ef85f518e5746185c5",
      "question": "What are nutritionists warning of?",
      "prediction": "The nutritionists are warning against the use of injectable Vitamin Supplements, arguing that the quantity of vitamins and minerals may be not clinically relevant, and that the potential for toxic toxins, especially fat-soluble vitamins A, D, E, and K, and C, are considered to be potentially harmful. They recommend that the use of injectable Vitamin Supplements be avoided in high doses. They emphasize that the vast majority of nutrients we can obtain from fruits and vegetables are absorbed by the body, and that food is essential for optimal health.",
      "ground_truths": [
        "using injectable vitamin supplements"
      ],
      "em": 0,
      "f1": 0.07058823529411765
    },
    {
      "id": "9c4719555e46425496a22559eff66115",
      "question": "What is happening in Japan?",
      "prediction": "The answer is, \"At the end of the day it will go into the blood stream.\"",
      "ground_truths": [
        "customers are lining up for vitamin injections that promise"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "547765c1158b4a5a8100d6a300aa88b1",
      "question": "Where are intravenous vitamin boosts the latest fad?",
      "prediction": "The answer is: C. The latest fad in Japan is the IV drip.",
      "ground_truths": [
        "Japan:"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "97cb3c77ddba4ab9ae6bfdc00eb09c42",
      "question": "What is the health fad in Japan?",
      "prediction": "The health fad in Japan is the use of intravenous vitamin injections as a way to improve health and beauty.",
      "ground_truths": [
        "intravenous vitamin \"drips\""
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "e504aa46a10248e8b18cb4639dce0467",
      "question": "Who are vitamin injections popular among?",
      "prediction": "The answer is \"More is not necessarily better...some vitamins and minerals can be toxic in high doses, particularly the fat-soluble vitamins A, D, E, and K, and the Vitamins A, D, E, and K, which are stored in the body's cells. The body stores these vitamins in the body's blood, so some of them are toxic.",
      "ground_truths": [
        "Tokyo customers are lining up for"
      ],
      "em": 0,
      "f1": 0.037037037037037035
    },
    {
      "id": "89abc7236e6941ee88a263d88d6d299a",
      "question": "What do vitamin injections do?",
      "prediction": "The answer is: \"At the end of the day it will go into the blood stream. Therefore, the vitamins we can get from foods are better absorbed by the body if we consume them in foods.\"",
      "ground_truths": [
        "improve health and beauty."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94175e0177bc46c1929d3ecf1b017fa9",
      "question": "Who uses vitamin injections?",
      "prediction": "The answer is \"Many of these men who work in the same office building in Tokyo, with 10 different varieties of \"orange\" and \"placenta pack\" varieties.  They are generally expected to use at least 30-40 people each day.  The NHS (National Health Service) is not generally considered to be regulated by medical authorities and thus, they's not necessarily recommended to use injectable vitamins.",
      "ground_truths": [
        "Tokyo customers"
      ],
      "em": 0,
      "f1": 0.03225806451612903
    },
    {
      "id": "3e064f85e8d242b485d48b6b7d0c45b9",
      "question": "Who called the father/",
      "prediction": "The answer is Drew Kesse, the father of Jennifer Kesse.",
      "ground_truths": [
        "David Russ,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "440e77465a4c487a86f1eda1727f70a2",
      "question": "Who says inmate's information is nothing new?",
      "prediction": "The answer is \"The investigators have messed this case up from the beginning and cannot be trusted.\"",
      "ground_truths": [
        "Sgt. Barbara Jones"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93a06e7bf23e44ce95a1bbf50f8604c9",
      "question": "when did she disappear?",
      "prediction": "The missing person is Drew Kesse, a 24-year-old financial adviser, who was brought to the Seminole County Jail in Florida on January 24, 2006, and subsequently in Florida. He has no witnesses to his whereabouts. His disappearance is considered a potential break in the case, and the Florida Police Department is cautiously optimistic. The missing person's personal items, such as her purse, wallet, and cell phones, were not found. The video surveillance tape shows a person walking away from the car, through a parking lot. A forensic test of the vehicle's damage and the missing",
      "ground_truths": [
        "January 24, 2006."
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "c134778e95754456b71869ea92b80b5d",
      "question": "Who was told to change course prior to crash?",
      "prediction": "The answer is: The Lebanese Transportation Minister, Ghazi al-Aridi, told CNN Tuesday.",
      "ground_truths": [
        "flight"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "248d9e97bb0543c9a394d488ac6b48ce",
      "question": "How many bodies have been found so far?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5d5743c3626d45d5934e0dddc83e958d",
      "question": "What do you tell the pilot before crashing?",
      "prediction": "The pilot of an Ethiopian Airlines flight in Lebanon was told to change course before crashing into the sea, after the plane's flight data and cockpit voice recorders were recovered.",
      "ground_truths": [
        "to change course"
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "058520090325403b9c133b6f1b9c2e49",
      "question": "How many bodies were found so far?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "60dbe06ef944496d8be8be4bed4b5b12",
      "question": "The pilot was told to do what, prior to the crash?",
      "prediction": "The pilot was told to do what, prior to the crash.",
      "ground_truths": [
        "change course"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e444494f4df6449e89599fdaea3f84f7",
      "question": "What combs Lebanese coast following Ethiopian Airlines plane crash?",
      "prediction": "The answer is:",
      "ground_truths": [
        "international search team"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd84ca081b0c45ea8d37bee8cec35ce2",
      "question": "Which airline owns the plain which crashed?",
      "prediction": "The answer is the United States.",
      "ground_truths": [
        "Ethiopian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f67d24b577114f6aac716d82d82371ae",
      "question": "How many people have been rescued?",
      "prediction": "The answer is 14.",
      "ground_truths": [
        "No survivors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5228e62244542428fd65303ec23dd11",
      "question": "How many life-size models are ther?",
      "prediction": "The answer is 45.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e385644e99a94097915a45a834d84dbb",
      "question": "What can visitors explore?",
      "prediction": "The answer is, visitors explore the home, including the distillery and gristmill, a reconstructed slave structure, and the open blacksmith shop, showcasing the new blacksmith shop.",
      "ground_truths": [
        "the estate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e7c7187fe8a4636ac095b31bfc91433",
      "question": "how many people",
      "prediction": "The answer is 10.",
      "ground_truths": [
        "700"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "416153d5b74943cf889a0e53fc6c7857",
      "question": "At what ages do the models depict Washington?",
      "prediction": "The models depict Washington at his 19 year old, 6'2\", strong, and handsome.",
      "ground_truths": [
        "19,"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "5c7129352d6443beb845ecd46a492c0a",
      "question": "who is depicted in the new models",
      "prediction": "The answer is, \"The new faces in the museum, and the education center at Mount Vernon, depict George Washington.\"",
      "ground_truths": [
        "George Washington"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "6a420f8f42204d968a137faeb1d73784",
      "question": "Where could the weapons reach?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "U.S. military bases in the Pacific Ocean territory of Guam"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aece58a299d84cd88579bca72c73da46",
      "question": "How many miles?",
      "prediction": "The answer is 3,000 kilometers.",
      "ground_truths": [
        "(1,900 miles),"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f00e9c077324f689c8b0746c6381e0e",
      "question": "How far can the missiles travel?",
      "prediction": "The answer is approximately 3,000 kilometers (1,900 miles).",
      "ground_truths": [
        "3,000 kilometers (1,900 miles),"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "692352f2fd7d4a389c1f1ca96570be32",
      "question": "Where are tensions running high",
      "prediction": "The answer is South Korea.",
      "ground_truths": [
        "Pyongyang and Seoul"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0754e951855e46df80b0269253e4acf7",
      "question": "Weapons could reach what places?",
      "prediction": "The answer is: weapons could reach places.",
      "ground_truths": [
        "U.S. military bases in the Pacific Ocean territory of Guam"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8f3827c3491247c7ab4d7706cbb8ef6d",
      "question": "Tensions are running high where?",
      "prediction": "Tensions are running high where.",
      "ground_truths": [
        "between Pyongyang and Seoul"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da54c67b92b74a2c91d2227d09a41438",
      "question": "Who has the weapons?",
      "prediction": "The answer is the South Korean military.",
      "ground_truths": [
        "United States, Japan, Russia, South Korea"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "dc5438680eeb4547b962a6d05a58125d",
      "question": "Which US state could the missile reach",
      "prediction": "The answer is USA.",
      "ground_truths": [
        "Alaska or Hawaii."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4dd31c6180e64c4eb59983819779cf2c",
      "question": "How far can the new missiles travel",
      "prediction": "The answer is directly from the text provided.",
      "ground_truths": [
        "3,000 kilometers (1,900 miles),"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eed4baf04e844c1a88bac329ce16c664",
      "question": "How many were wounded?",
      "prediction": "The number of wounded is 30.",
      "ground_truths": [
        "30"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "2f351229f6cd4761b7b0d45e3594031e",
      "question": "How many were killed?",
      "prediction": "The number of killed was 10.",
      "ground_truths": [
        "a dozen people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0580a5ffe5e41cd8e035466823d8d9b",
      "question": "Where the bomb was detonated?",
      "prediction": "The bomb was detonated near the village of Dara Bazar in the Bajaur Agency of Pakistan's tribal region.",
      "ground_truths": [
        "Quetta,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "786765cfe8b44449af887152402aaa29",
      "question": "In what cities were the protests?",
      "prediction": "The protests in Egypt were held in various cities outside the Egyptian Embassy in London.",
      "ground_truths": [
        "various"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "8cd035648dd64edb912f01d1b01e6b89",
      "question": "What languages were used",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "Arabic, French and English,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8444096a84e9455287180d0b1a928142",
      "question": "In what languages were the protest slogans?",
      "prediction": "The protest slogans were written in Arabic, French and English, and in some Canadian translations.",
      "ground_truths": [
        "Arabic, French and English,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "a2f796f7536947518cb11c541c564c83",
      "question": "Protests are held in London, Geneva and where else?",
      "prediction": "True: The Egypt-Western protest in London, Piccadilly, and the Toronto protest in Canada all featured similar actions.",
      "ground_truths": [
        "Canada."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "60f332bc68e64d8680e179e359b0d6fa",
      "question": "In Geneva, people chant slogans in Arabic, French and which other language?",
      "prediction": "In Geneva, people chanted slogans in Arabic, French and English, declaring \"Freedom. Social Justice. Democracy\" and \"Free Egypt.\"",
      "ground_truths": [
        "English,"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "a1747749a3904c53b124b17ba86b1df6",
      "question": "Texas is awarded cash relief for what?",
      "prediction": "Texas is awarded $31,319,686 for the following:",
      "ground_truths": [
        "a series of wildfires"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29b82a59344f4d38854c476a31827ba7",
      "question": "What are the funds based on?",
      "prediction": "The funds are awarded to the states in North Dakota, Alabama, and Missouri, for recovery efforts.",
      "ground_truths": [
        "housing, business and infrastructure repairs,"
      ],
      "em": 0,
      "f1": 0.10526315789473682
    },
    {
      "id": "894cfdb45c9149f6bb42906c842ec632",
      "question": "What states receive aid for tornadoes?",
      "prediction": "The states receiving aid for tornadoes are:",
      "ground_truths": [
        "New York"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "527a875dd5504c12b1e26cec3ff31fb6",
      "question": "Which states received aid for tornadoes?",
      "prediction": "The states that received aid for tornadoes were:",
      "ground_truths": [
        "Alabama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cf94611bbdb74f89b38fd1ac06c27777",
      "question": "What year's disasters are the basis for the funds?",
      "prediction": "The answer is **North Dakota was awarded $79,358,648 to recover from severe flooding.**",
      "ground_truths": [
        "2011."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab7f7e70dde447fcaa29fdc20ba659db",
      "question": "what were The funds are based on?",
      "prediction": "The funds are based on the $400 million emergency aid to the states that experienced the biggest natural disasters in 2011.",
      "ground_truths": [
        "natural disasters"
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "500e35b80a13424c8eb9c6e6a0b9af15",
      "question": "Alabama and Missouri receive aid for what?",
      "prediction": "Alabama and Missouri received $55,566,078 for recovery from last spring's tornado, severe storms, and flooding in Tuscaloosa, Marion, Jefferson, and DeKalb counties and in Birmingham.",
      "ground_truths": [
        "natural disasters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6bdd81ddfa4d41f9a8dac2c1ce206945",
      "question": "What is Texas receiving aid for?",
      "prediction": "Texas is receiving $31,319,686 for a series of wildfires from late summer through autumn in Bastrop County.",
      "ground_truths": [
        "a series of wildfires"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "7f271dc40b1540828bc913ffc1d7dbf4",
      "question": "Who ran his political campaigns?",
      "prediction": "The answer is \"Mark Sanford.\"",
      "ground_truths": [
        "Jenny Sanford"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "68aacaf01d6848be9738b0e986cb1236",
      "question": "who is Jenny Sanford?",
      "prediction": "The answer is, Jenny Sanford is a former Wall Street executive who was fired from the South Carolina GOP and later resigned from the state's first vice chairwoman of the GOP after she quit the office. She was not there facing the cameras at the state's first lady's house. She is not going to humiliate herself by standing next to a story, and she will stand next to Mark Sanford, who is emotionally, but cannot stand in the glare of others. She is not going to a single, public expose, so she made a clear decision to stay. She was not going to a private, public, or any",
      "ground_truths": [
        "the wife of Gov. Mark"
      ],
      "em": 0,
      "f1": 0.04040404040404041
    },
    {
      "id": "861c2f0524104e6291b91efcea22adc2",
      "question": "What is he avoiding?",
      "prediction": "The answer is, \"Margaret\" (Jenny Sanford).",
      "ground_truths": [
        "public exposure.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ded71c940b4408794baf52361a3e87d",
      "question": "Who is avoiding media?",
      "prediction": "The answer is Jenny Sanford.",
      "ground_truths": [
        "Jenny Sanford,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c12ef3e9f94740edb9565c036be79d3e",
      "question": "What was called \"novel that you would be embarrassed to buy\"?",
      "prediction": "The answer is \"like a novel that you would be embarrassed to buy\".",
      "ground_truths": [
        "\"[The e-mails]"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "890f0fb25e9545798b2f743b121d3c69",
      "question": "What were the e-mails called?",
      "prediction": "The e-mails were published by the State newspaper in Columbia, South Carolina's capital, which said the governor's office confirmed they are authentic.",
      "ground_truths": [
        "romantic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "897068e062c44e01add9c98a80eb63d0",
      "question": "Who died of cancer at age 86?",
      "prediction": "The answer is Bea Arthur, who died of cancer at age 86.",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "8bd2d3430c07471e886b1dfe30b061e8",
      "question": "Who has died at 86?",
      "prediction": "The answer is Bea Arthur.",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "6c779c5d72af4eb69b69977fde49e8ff",
      "question": "Who is survived by two sons and grandchildren?",
      "prediction": "The answer is: Bea Arthur is survived by her sons Matthew and Daniel and grandchildren Kyra and Violet.",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "d16493e3db7c4bd5b494934febcb82b4",
      "question": "What did she die from?",
      "prediction": "The answer is, Bea Arthur died of cancer, a family spokesman said.",
      "ground_truths": [
        "cancer,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "49ab77dcd0394a898aec0b35440b572a",
      "question": "What tv show did she star in?",
      "prediction": "The answer is \"The Golden Girls\" in 2008.",
      "ground_truths": [
        "\"The Golden Girls,\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9fdc5aba0df54f148cc213911a81cd37",
      "question": "who was at home with family in Los Angeles?",
      "prediction": "The answer is Bea Arthur, the actress best known for her roles as television's \"Maude\" and Dorothy on \"The Golden Girls.\"",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "1041e2ddf5454bc9abd397681fbaf1e4",
      "question": "What did Burnette say he threw at the shotter?",
      "prediction": "Answer: \"I threw at the shotter.\"",
      "ground_truths": [
        "folding table"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bdb30a392d79453992b27a1cae25efec",
      "question": "Where is Burnette being housed?",
      "prediction": "Answer: Burnette is being housed at Fort Hood, Texas.",
      "ground_truths": [
        "Fort Hood, Texas,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "02bb9745c7f8413fa1c809402b8ea81f",
      "question": "Where did the nightmare day take place?",
      "prediction": "The answer is Fort Hood, Texas.",
      "ground_truths": [
        "Fort Hood,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "732c42970ce142f8a28b7f3d5e3695d5",
      "question": "How many people were shot on Spc. Logan Burnette's original day at Fort Hood?",
      "prediction": "The answer is, \"He was very swift, very tactical.\"",
      "ground_truths": [
        "Twelve soldiers and one civilian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ec338260290400aa6b3ae6685666a80",
      "question": "Where did the shooting take place?",
      "prediction": "The shooting took place at Fort Hood, Texas, a week ago, on November 5 at Fort Hood.",
      "ground_truths": [
        "Fort Hood"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "6368dd06f186478dae5ba3271569ef66",
      "question": "What says official about opposition?",
      "prediction": "The answer is: **The official does not say if the MDC party has signed a deal with Mugabe.** They stated that the power-sharing deal with the MDC offshoot was part of a larger deal that hasn's been signed by any party. This is a key point of the text.",
      "ground_truths": [
        "deciding the duties of the new prime minister has been a sticking point in the negotiations."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "d313b671236c48f3b029c72fbc46971b",
      "question": "Who denies discussions?",
      "prediction": "According to CNN's Nkepile Mabuse, after a meeting with the MDC splinter group, Tsvangirai stated that he still is involved with the talks, and that the power-sharing deal is part of a larger deal that has not been signed by anyone.",
      "ground_truths": [
        "Mugabe's opponents"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "36ab12ff07cf46ed9af03891964f20de",
      "question": "Who is not part of the deal?",
      "prediction": "The answer is: **Tsvangirai, the head of the MDC splinter group.**",
      "ground_truths": [
        "Thabo Mbeki,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d468e9a904314a5a92582bcfcc7b7635",
      "question": "Who makes power-share deal with splinter group?",
      "prediction": "The answer is **South African President Thabo Mbeki**, the mediator in the talks.",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "374f14d949354e9aa9bdcfdc090433d7",
      "question": "Who denies he's out of power-sharing discussions?",
      "prediction": "The answer is: Tsvangirai.",
      "ground_truths": [
        "Mugabe's opponents"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e7a5a32f59b49ccacc7c1408799a32a",
      "question": "Who made a power-share deal with a splinter group?",
      "prediction": "The answer is: **Mbare, the party's breakaway faction.**",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6eeccb736c5f4d72944f4c46e4859c9d",
      "question": "Who is  not part of deal, official says?",
      "prediction": "The answer is **Mabele Tsvangirai**.",
      "ground_truths": [
        "Morgan Tsvangirai."
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "fa15e1b710b94eac8122358024750aa8",
      "question": "What was Hopper dying of?",
      "prediction": "The answer is: Hopper died of prostate cancer.",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "88be8aa0f5fc4d13b318a5925f63eddf",
      "question": "What illness is Hopper stricken with?",
      "prediction": "The answer is \"prostate cancer,\" as described by Hopper.",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "ed21c1e730a24590bd0d16f6125a1af1",
      "question": "What did Hopper's doctor recommend?",
      "prediction": "The doctor recommended that Hopper's *3500* was \"extremely weak,\" but he walked on his own during the ceremony.",
      "ground_truths": [
        "has to do with his estranged wife at this time, the more likely he is to have his life extended.\""
      ],
      "em": 0,
      "f1": 0.11428571428571428
    },
    {
      "id": "1cc24ab9ce0b40c5a4adec0fc985ea81",
      "question": "Which cable network paid for Hopper's star?",
      "prediction": "The answer is \"The Starz cable network.\"",
      "ground_truths": [
        "Starz"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "65786f4f7af04a3bb1bfff6044b8d526",
      "question": "What is Hopper dying of?",
      "prediction": "The answer is, \"distinguished, in his final bow on Hollywood's Walk of Fame. His star was dedicated on the Walk of Fame.\"",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "75962804944c4efcb636e1c78acfd974",
      "question": "What country does Bill Gates thinks has made great progress in recent times?",
      "prediction": "The answer is: \"The power of all international aid pledges is in the action. Industrial nations, through a combination of a reduction in budget, increased public expenditure, and increased investment in non-oil sectors, have been able to help developing countries, with a significant increase in aid spending, by yielding additional resources in terms of health aid. China's recovery also achieved a high level of financial aid spending, which, while not a direct monetary amount, was a significant step in the way of which developed nation's progress can be attributed. This is a positive note, and a valuable example for the rest of the world, as it",
      "ground_truths": [
        "Africa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4774d59215b4aa5935ff8e40927f723",
      "question": "Which couple sends billions of dollars to developing nations?",
      "prediction": "The answer is: **France** sends billions of dollars to developing countries.",
      "ground_truths": [
        "Bill & Melinda Gates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b05e48090e341199366a661b7723199",
      "question": "what does the bill & melinda gates foundation do",
      "prediction": "The answer to the question is:  \"It is a charitable foundation, working in partnership with the Gates Foundation, that focuses on assisting countries in poverty and hunger, donating billions of dollars in health aid,\"",
      "ground_truths": [
        "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid"
      ],
      "em": 0,
      "f1": 0.4444444444444444
    },
    {
      "id": "3e039dbfc29f4a16ba51c79e9be4d0d6",
      "question": "Was she abducted alone?",
      "prediction": "The answer is yes, she was abducted in Basilan, Philippines.",
      "ground_truths": [
        "The gunmen also took hostage Lunsmann's 14-year-old son, Kevin, and her 19-year-old Filipino nephew, Romnick Jakaria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6916c3d355a940bb9458167bb9826ff8",
      "question": "When was she abducted?",
      "prediction": "The answer is July 10, 2019.",
      "ground_truths": [
        "July"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "03c09d9f7c2f45138f9f2d5063ed55f0",
      "question": "where were they abducted from?",
      "prediction": "The answer is **Basilan, Philippines**.",
      "ground_truths": [
        "Philippines"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "1b27e0d27887439e8efe5b6a46bbc236",
      "question": "when was she taken?",
      "prediction": "The answer is *Basilan*.",
      "ground_truths": [
        "July"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb7f292ef67f47e1bb188b922f6b4cab",
      "question": "what island was she released on?",
      "prediction": "The answer is Basilan.",
      "ground_truths": [
        "Suba Kampong township"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85924e5341834bbaa0625029a2d0e1a8",
      "question": "They paid ransom for the liberation of women?",
      "prediction": "The answer is: yes, they paid ransom for the liberation of women.",
      "ground_truths": [
        "declined to confirm whether officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "78c8514c2c314cfabb1b15b88add2d64",
      "question": "What country is this show being aired?",
      "prediction": "The answer is \"The United Kingdom.\"",
      "ground_truths": [
        "UK"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a00ed0e35e5443f2b7916e3da4dc143e",
      "question": "What has been compared to reality show Big Brother?",
      "prediction": "The answer is \"adult reality show Big Brother.\"",
      "ground_truths": [
        "Boys And Girls Alone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a0c5863f36ce4b2f9ffd2a1207d65f11",
      "question": "what kind of show is it",
      "prediction": "The answer is \"the kind of tussling you'd get in any playground. But no physical violence.\"",
      "ground_truths": [
        "reality"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b52a67e9b1f3432aa139bd8fad4c99d5",
      "question": "What is the name of the show?",
      "prediction": "The name of the show is \"Kid's Self-Societies\" (which is, as specified, a British equivalent of \"Big Brother\" for the American program).",
      "ground_truths": [
        "Boys And Girls Alone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b90c21ab99a94720afca525bb58500b7",
      "question": "What does program show?",
      "prediction": "Answer: The program shows how children cope without their parents for two weeks.",
      "ground_truths": [
        "allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "34541edc94984ce2b60e211b15de0acc",
      "question": "is this show legal",
      "prediction": "The answer is: Yes, this is legal.",
      "ground_truths": [
        "the program was made with the parents' full consent."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a08a4ab897de471ba93b91ce672e62c3",
      "question": "What type of show is this being compared to?",
      "prediction": "The answer is \"the kind of tussling you would get in any playground. But no physical violence.\"",
      "ground_truths": [
        "adult reality"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ffe71377d09d4ca3acfdf061885f6496",
      "question": "who is making the show",
      "prediction": "The answer is: The show is made with the parents' full consent.",
      "ground_truths": [
        "Channel 4"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a838e3905c2641b6a15817e1647641be",
      "question": "Who traveled to India?",
      "prediction": "Here is the answer, based on the context:",
      "ground_truths": [
        "Christiane Amanpour"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62644011bef24c2d963961879c34902c",
      "question": "When did the Dalai Lama flee Tibet?",
      "prediction": "The answer is, Dalai Lama.",
      "ground_truths": [
        "1959."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a10f8733a55144f2b2c4fe81f2478cf1",
      "question": "what does the dalai lama do",
      "prediction": "The Dalai Lama, a former Tibetan leader, is described as a deeply insightful and compassionate individual who was not afraid of the political process. He was a keen observer who was fascinated by the emotional and spiritual landscape of the Tibetan people. He had a strong attachment to the \"Buddha's Warriors\" and the Tibetan Buddhist philosophy of tolerance and non-violence, and he was particularly touched by the Dalai Lama's courage and generosity in addressing the crowd, even with the political pressures of China. He had a complex, yet equally significant, personality, embodying a strong sense of unity and compassion.",
      "ground_truths": [
        "cracked jokes and chatted"
      ],
      "em": 0,
      "f1": 0.022727272727272724
    },
    {
      "id": "b0a22b1eed43466983c5b9edaf1fa825",
      "question": "In what year did Dalai Lama flee?",
      "prediction": "Here's the answer, extracted directly from the context:",
      "ground_truths": [
        "1959."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0514fdee611444382522aa5246f03e2",
      "question": "Where did Amanpour travel?",
      "prediction": "The Dalai Lama and the Dalai Lama, and the Tibetan exiles, visited in a \"Long Life Ceremony,\" where they all came to pray for their loved ones' health and longevity. They also gifted cubes of cheese, textiles, and tapestries, all of which were held up to the light of incense and the sun.",
      "ground_truths": [
        "Dharamsala, India."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12710fddf22f49c388bb51066604fe09",
      "question": "Who made recent attacks?",
      "prediction": "The answer is **Chevron**.",
      "ground_truths": [
        "Militants from the Movement for the Emancipation of the Niger Delta,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e8ea585b9e94a22bde67b4af2d1af8e",
      "question": "Which country is Africa largest oil producer?",
      "prediction": "The answer is **Africa**.",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af0ad17709c94941a8928f11ff29d3c6",
      "question": "where is the game played?",
      "prediction": "The answer is Nigeria.",
      "ground_truths": [
        "Nigeria"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "da6f0e91b9044dc89759497adae15cf1",
      "question": "when is the tournament to take place?",
      "prediction": "The answer is: *in the west African nation later this year.*",
      "ground_truths": [
        "between October 24 and November 15,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2821cf5d9a04d9187f12a4087a1cd72",
      "question": "Which country is Africa's largest oil producer?",
      "prediction": "The answer is Africa's largest oil producer.",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "79ec6637b0cf49ae8a75b26bbfb11ff8",
      "question": "Attacks by who have limited supplies?",
      "prediction": "The answer is: \"Only two out of the nine stadiums in Nigeria are close to being ready for the tournament. The Movement for the Emancipation of the Niger Delta, with its \"Hurricane Piper Alpha,\" targeted Chevron's Abiteye flow station in the Niger Delta region, on Monday, June 15, 2009, at about 0200 Hrs, where it hit the Abiteye flow station operated by Chevron.",
      "ground_truths": [
        "MEND"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ffa942875124cafb8cf0eb5a87392ef",
      "question": "where is oil produced?",
      "prediction": "The answer is in the email.",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0de2319530fa45f99a79b4c4333c9ff1",
      "question": "What is five hours from Berlin by train?",
      "prediction": "Here's the answer:",
      "ground_truths": [
        "Sylt"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c7a1cbd3aa6e458db4b66664e2107e0c",
      "question": "What is Sylt known for?",
      "prediction": "The answer is Sylt is known for its picturesque beaches, charming cottages, and a network of cosy hotels, like the Dorint S√∂l'ring Hof (1 Am Sandwall, Rantum; 49-4651/836-200; doubles from $550), a 15-room inn nestled in the dunes on the southern coast with its own hidden stretch of sugary sand.",
      "ground_truths": [
        "tranquil beaches,"
      ],
      "em": 0,
      "f1": 0.04347826086956522
    },
    {
      "id": "b68565e5c8a54a72889714bc0ded15d8",
      "question": "What attracts German celebrities to the island?",
      "prediction": "The answer to the question is \"For those who want to be closer to the action, there's the Hotel Stadt Hamburg (2 Strandstrasse; 49-4651/8580; doubles from $550). This charming inn in the village of Westerland is a good base for exploration. The restaurant's North Sea-meets-Asian dishes, like fresh mussels in a sweet curry sauce, are a potent draw. In Kampen, you'll find Greta's Rauchfang (5 Str√∂nwai; 49-4651/8580; dinner for",
      "ground_truths": [
        "dining scene"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d524dd73ccf34b32a7c8c5bbb6935f55",
      "question": "What  is five hours from Berlin by train?",
      "prediction": "The answer is *Berlin by train from 5 hours. Here is a detailed breakdown of what you need to do for planning a beach getaway in Sylt, Germany:",
      "ground_truths": [
        "Sylt"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "6f9255058a62405f911a081a2ee6e251",
      "question": "What attracts a number of German celebrities?",
      "prediction": "The answer is \"For those who want to be closer to the action, there's the Hotel Stadt Hamburg (2 Strandstrasse; 49-4651/8580; doubles from $550). In the charming village of Westerland, there is the restaurant that attracts a number of German celebrities (Claudia Schiffer, Boris Becker), and the Sansibar (80 H√∂rnumer Str., Rantum; 49-4651/964-656; dinner for two $130).",
      "ground_truths": [
        "The island's dining scene"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91ee88ab11164eb9ae6b7520537baffb",
      "question": "Have the teens charged pled guilty?",
      "prediction": "The answer is \"Yes, the teens have been charged with a count of aggravated assault with a modified weapon (metal) and a second-degree marijuana charge. They are also charged with battery with a modified weapon. They are also charged with driving a modified weapon in a non-public location.",
      "ground_truths": [
        "All three pleaded not"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b14a0ef7a9484617b9a6551c0f7f802d",
      "question": "What happens to 15-year old Michael Brewer?",
      "prediction": "The answer is: Michael Brewer is a 15-year-old who was burned over 65 percent of his body, and his mother, Valerie Brewer, tells CNN that he is strong enough to make it through.",
      "ground_truths": [
        "burned over 65 percent of his body after being set on fire,"
      ],
      "em": 0,
      "f1": 0.3255813953488372
    },
    {
      "id": "dd2beb51267c4da6823dcdd446ab8e49",
      "question": "What is he healing from?",
      "prediction": "From the context, the answer is \"chronic injuries, major surgery, rehabilitation, and therapy,\" and the description of the teenage boy's statement is a cry for his family's support.",
      "ground_truths": [
        "burned over 65 percent of his body"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "e8ea4ee3633f4b46bbe40f20c3c015be",
      "question": "What teen uses to get through painful therapy?",
      "prediction": "Based on the provided context, the answer is \" Ozzy Osbourne.\"",
      "ground_truths": [
        "\"He focuses on Ozzy,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "0302a6e98184410dbc50f961abf3486c",
      "question": "Whose CD does the teen use during therapy?",
      "prediction": "The answer is: Trevor Brewer, who was burned over 65 percent of his body after being set on fire, and whose son was charged in the October 12 attack in Deerfield Beach, Florida.",
      "ground_truths": [
        "Ozzy Osbourne"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88a599b815dd4e5898e8b2e840fef6ef",
      "question": "How many attacked Michael Brewer?",
      "prediction": "The answer is: 12.",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6520a4ec91834f1db374c4bded23c4cf",
      "question": "What is observation of mother of burned teen?",
      "prediction": "Based on the context, the answer is \"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\" and \"I want to express my deepest sympathy to Mikey and his family.\"",
      "ground_truths": [
        "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\""
      ],
      "em": 0,
      "f1": 0.763157894736842
    },
    {
      "id": "e9e6e952876b4c2b864f9c5fd00a3d8a",
      "question": "Which commodity will have its price frozen?",
      "prediction": "The answer is **Mexican Gasoline**.",
      "ground_truths": [
        "gasoline"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "fcccb4baf6314a589f6725cf1dc37979",
      "question": "What size companies will be helped?",
      "prediction": "The answer is 2.2 billion pesos ($163 million).",
      "ground_truths": [
        "small- and medium-size"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2bffb804aab041ee9605adbff83fe141",
      "question": "Who is the Mexican leader that's being quoted?",
      "prediction": "The Mexican leader that's being quoted is **Felipe Calderon.**",
      "ground_truths": [
        "President Felipe Calderon"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "6803dc0693214791b3058a87480b15d6",
      "question": "What prices will be frozen for the rest of the year?",
      "prediction": "The answer is **17 billion pesos ($1.2 billion).**",
      "ground_truths": [
        "gasoline"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "299726f939f0420aaeb6e5f33a325d39",
      "question": "Who will be aided by the plan?",
      "prediction": "The answer is Mexico City (CNN).",
      "ground_truths": [
        "unemployed Mexicans,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e412730b37274c11bad105b38ed9e196",
      "question": "Number of pesos that will be spent?",
      "prediction": "The answer is **$12 billion**.",
      "ground_truths": [
        "570 billion"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "a645bb94c6194b119832d8e395b47a0c",
      "question": "What else did the plan also call for?",
      "prediction": "The answer is: The plan also called for:",
      "ground_truths": [
        "rebuild the nation's highways, bridges and other public-use facilities."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ea97cfbe22340cdabec8b67b75b0f0e",
      "question": "How many pesos will be spent?",
      "prediction": "The answer is 17 billion pesos ($1.2 billion).",
      "ground_truths": [
        "570 billion"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "1f2ee8a4ce5d448e8ed30e7ca27b2b32",
      "question": "How long will the prices be frozen?",
      "prediction": "The answer is **10%**.",
      "ground_truths": [
        "for the rest of the year"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d08f62337054caa8e0104c19a653158",
      "question": "What is Mexican currency called?",
      "prediction": "The answer is **Mexican Won (MXN).**",
      "ground_truths": [
        "pesos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1aa0b7493d9b47bbb4625d39922576b6",
      "question": "How much money will be spent?",
      "prediction": "The answer is 17 billion pesos ($1.2 billion).",
      "ground_truths": [
        "570 billion pesos ($42 billion)."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f6d66062548343c2a3fa727cb5f78e2e",
      "question": "What happened to the ice jam?",
      "prediction": "The answer is:  The ice jam was deployed in south of Bismarck.",
      "ground_truths": [
        "Explosives are set off in the Missouri River"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "f7d944ddbf964356a88835f02f839e78",
      "question": "When are they hoping the sandbag goal in Fargo will be met?",
      "prediction": "The answer is: Fargo is prepared for the flooding.",
      "ground_truths": [
        "Thursday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d966325ec53b4bc6b6351a5f7b0ee6e6",
      "question": "Where were residents rescued after residential dike gives way?",
      "prediction": "The answer is Fargo, North Dakota. The context provides information about the impact of a flash flood, and the information about the Mississippi River and the red river.",
      "ground_truths": [
        "Oxbow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f09bb7c330874e1493fe95436c2566dc",
      "question": "What goal is Fargo halfway to?",
      "prediction": "Fargo is halfway to its goal to reach its target of 95 percent of the 2 million sandbags needed for the flood.",
      "ground_truths": [
        "volunteers had filled more than 1 million sandbags out of the needed 2 million."
      ],
      "em": 0,
      "f1": 0.30303030303030304
    },
    {
      "id": "776c4ccd1f4e44a1a89ddbf4aacdd8c2",
      "question": "What complicates preparations?",
      "prediction": "The answer is: Ford's North Dakota (CNN) predicts record-breaking flooding in Bismarck and surrounding areas, possibly exceeding 1,000 feet in Fargo.",
      "ground_truths": [
        "Snow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6530fe0dc1774470b59562813af55aab",
      "question": "Where was the ice jam?",
      "prediction": "The answer is Camp  Bismarck, North Dakota.",
      "ground_truths": [
        "south of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5fdc81c791924ab6875608a54b1b6581",
      "question": "What gave way south of Fargo?",
      "prediction": "The answer is:  Fargo, North Dakota (CNN).",
      "ground_truths": [
        "a residential dike"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6e3b10812dd41d488e85954b27fb762",
      "question": "What is the sandbag goal?",
      "prediction": "Answer: The sandbag goal is determined by the Federal Emergency Management Agency (FEMA) by calculating the amount of water that will be released to the Missouri River over a specified period. The goal is to mitigate the flood, which is expected to be a record-breaking number of flood levels.",
      "ground_truths": [
        "2 million."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "856b14030ac342bb979ca5091377cff0",
      "question": "Who is the prime minister?",
      "prediction": "The prime minister of India is",
      "ground_truths": [
        "Manmohan Singh's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e7792c3711b4a139cf99e7dec1b7e26",
      "question": "who is seeking re-election?",
      "prediction": "The answer is: The fifth and final phase of India's marathon general election to choose a federal government.",
      "ground_truths": [
        "Prime Minister Manmohan Singh's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8dbad2970e8945f0a4595ccae78e57e9",
      "question": "what is the prime minister name?",
      "prediction": "The prime minister's name is **Manmohan Singh**.",
      "ground_truths": [
        "Manmohan Singh's"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "f8b5c43adc974d82aaab3f81649ca754",
      "question": "What is the number of states that voting was spread across?",
      "prediction": "The number of states that voting was spread across is 28.",
      "ground_truths": [
        "28"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "b2a92b434a344e49b3a6911bfd86e98f",
      "question": "Whose party is seeking re-election?",
      "prediction": "The answer is \"The party, which has been in power since 2004.\"",
      "ground_truths": [
        "Prime Minister Manmohan Singh's Congress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0db9e6fa3f21435996d0bf1e3b3f8adb",
      "question": "What country is voting?",
      "prediction": "The answer is **New Delhi.**",
      "ground_truths": [
        "India"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "044418fd6fd646ec89a54a7013712c9d",
      "question": "till how far Voting in the month-long exercise was spread ?",
      "prediction": "The answer is 62 percent.",
      "ground_truths": [
        "across 28 states and seven federal territories."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2324b8d221fa41d58267b515b1a1109a",
      "question": "How many states voted during the month long exercise?",
      "prediction": "The answer is 714 million.",
      "ground_truths": [
        "28"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "092f821b8fd243efb2bfd2f200cb99ee",
      "question": "What brand were most of the sport cars?",
      "prediction": "The answer is, \"The New Holland,\" because it's the most expensive and most likely the sport car.",
      "ground_truths": [
        "Ferraris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a1d3756a44c4a5ba9b30237dd1c807f",
      "question": "what was the tag of Lamborghini?",
      "prediction": "The answer is:  \"BADBUL\"",
      "ground_truths": [
        "\"BADBUL,\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "02e40f5399fb4b2386f784865610ff5a",
      "question": "how many exotic sport cars?",
      "prediction": "Answer: There are 20 exotic sport cars.",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "109740b4edfa49c5961632436d1df48c",
      "question": "What is the fine for speed racing?",
      "prediction": "The fine for speed racing is $627.",
      "ground_truths": [
        "$627,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "9c03696110b94d56bbfbc83746f79a94",
      "question": "what kind of cars?",
      "prediction": "Answer: The answers to the question are:",
      "ground_truths": [
        "exotic sports"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11452fbd4fdb40d8bd8f10d24a8087e2",
      "question": "where were they going",
      "prediction": "The answer is: **Oregon State Senior Trooper David Petersen.**",
      "ground_truths": [
        "Grand Ronde, Oregon."
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "0099081da3eb4d36bc020201d4f97a27",
      "question": "what speeds were they travelling?",
      "prediction": "The answer is, \"They were all good little soldiers and pulled right over.\"",
      "ground_truths": [
        "about 100 mph"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83966e58471549e2adc73c25f58c846b",
      "question": "What were they making?",
      "prediction": "The answer is \"both of the cars were part of a group of 20 similar cars making an annual road trip, the news release said.",
      "ground_truths": [
        "an annual road trip,"
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "8383bee1823d4a178d9746f9c5425d24",
      "question": "what time did this event happen?",
      "prediction": "The answer is:  The answer is **Monday**.",
      "ground_truths": [
        "5:20 p.m."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b7e037cf27d46fbbc6fc50bb7f30e35",
      "question": "What terminal was closed?",
      "prediction": "The answer is \"Terminal C.\"",
      "ground_truths": [
        "C"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "0423f569713641c5827ce4a31cda24fc",
      "question": "where has the officer been reassigned to?",
      "prediction": "The answer is: the TSA.",
      "ground_truths": [
        "performing non-screening duties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90aaf736a3284105817eef47e461fec6",
      "question": "When did security breach happen?",
      "prediction": "The answer is \"The TSA was reviewing the circumstances surrounding the incident, and we are making an assessment as to what disciplinary action will be taken against [the] officer posted at the exit in question.\"",
      "ground_truths": [
        "Sunday's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "95588acac903452eb46741382017346b",
      "question": "What did man do improperly at security screening?",
      "prediction": "The answer is: \"we look at what happened. We have no specific details, but we are looking at the situation to determine what disciplinary action will be taken.\"",
      "ground_truths": [
        "walked through an exit on the public side to the secure \"sterile\" side"
      ],
      "em": 0,
      "f1": 0.05555555555555555
    },
    {
      "id": "03987c7b8e9f44f7a471610e6de64f6c",
      "question": "When was the breach?",
      "prediction": "The answer is: The TSA's assessment is \"evaluative\" and \"made an assessment as to what disciplinary action will be taken against the officer. For now, the TSA is reviewing the circumstances, and there is no definitive answer for the event.",
      "ground_truths": [
        "Sunday evening"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "373bec1f959948aba63fe708ef5aaf4d",
      "question": "How long was it closed for?",
      "prediction": "The answer is \"For the process was completed on Monday.\"",
      "ground_truths": [
        "hours"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c067406d1ca644f68fcc91b86cbd302a",
      "question": "What forced closure of Terminal C?",
      "prediction": "The answer is: The TSA is reviewing the circumstances surrounding the incident, and we are making an assessment as to what disciplinary action will be taken against the officer posted at the exit in question.",
      "ground_truths": [
        "security breach"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "672a0eb014e2448d8dd9fd945f587d23",
      "question": "what did human rights watch official say",
      "prediction": "The answer is: \"While the administration's actions in Myanmar, and their focus on a different problem of human rights around the globe, led to a tension with China, and several signals suggesting that the administration is putting human rights issues to one side, this is where we've seen the tension play out, with some signals that have been sent suggesting that the administration is putting human rights issues to one side.\"",
      "ground_truths": [
        "Obama should have met with the Dalai Lama."
      ],
      "em": 0,
      "f1": 0.057142857142857134
    },
    {
      "id": "14e32051f99643d788c0471074d4597c",
      "question": "An Ex UN official said what about Obama's strategy",
      "prediction": "The answer is: \"It's very slow.\"",
      "ground_truths": [
        "\"gotten the balance right\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf0ef6764dd64e91a7f16852e78867b2",
      "question": "who followed the bombers",
      "prediction": "The answer is the U.S.",
      "ground_truths": [
        "NATO fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f86dfe94c7784de99ef22445f3d8d371",
      "question": "what did spokeman say",
      "prediction": "The spokeman said that the Russians would not object to that.",
      "ground_truths": [
        "\"We exercise all around the globe and have joint exercises with countries all over"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d745bee67227422b824140f2734b0621",
      "question": "what does news agency say",
      "prediction": "The news agency says the Russian bombers, Tupolev Tu-160, landed at a Venezuelan airfield, with training flights being conducted over neutral waters. The Moscow Ministry of Defence stated that these flights were carried out in strict compliance with international rules, and they did not violate any other states' borders.",
      "ground_truths": [
        "Venezuelan President Hugo Chavez, whose comments have frequently antagonized Washington, said it would welcome the Russian air force,"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "027b6f1bd664457db6e6063da4fcfe6b",
      "question": "What would Venezuelan president welcome?",
      "prediction": "The Venezuelan president would welcome the Russian air force, and would not object to them.",
      "ground_truths": [
        "Russian air force,"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "86a8ab5a4a8046d49add6140bbcac0a6",
      "question": "who bombed the country",
      "prediction": "The answer is the U.S.",
      "ground_truths": [
        "Russian bombers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "58487a8f39d745abb6c2823ba3bcd2ab",
      "question": "What will help less fortunate tribes?",
      "prediction": "The answer is, \"financial gaming is not an option.\"",
      "ground_truths": [
        "Keeping the dollar within Indian Country, Bowers hopes,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cf7fd231c584939838ed6ec3f192f26",
      "question": "Which tribe owns the property?",
      "prediction": "The answer is the Seminole Tribe of Florida Inc.",
      "ground_truths": [
        "Seminole"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "36243b6f2e134d4f94c63ff7f739150e",
      "question": "How many Native-owned businesses are a part of The Native American Group?",
      "prediction": "The Native American Group is a coalition of tribes in the United States and Canada. They are founded in 2007. The group's goal is to promote Native American-owned businesses and services in the United States and Canada, aiming to help tribes with basic needs like housing, food, education, and economic opportunities. They's based on a desire for \"historic significance,\" and they strive for success in gaming, which they aim to replicate elsewhere. The group's success in gaming doesn't reflect the lack of native populations in other countries, so the Crow Nation is seeking coal to unlock the potential for clean energy in",
      "ground_truths": [
        "more than 100"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cacc201f681d4e33ac294b7210b3f030",
      "question": "How many businesses are owned by the Native American Group?",
      "prediction": "The Native American Group is owned by 100 tribes (of 100 Native-owned businesses).",
      "ground_truths": [
        "more than 100"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "894e88efeef14e0e87486904b79bc5aa",
      "question": "Who owns Hard Rock properties?",
      "prediction": "The answer is the Seminole Tribe of Florida Inc.",
      "ground_truths": [
        "The Seminole Tribe"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "cab4cf53e8694388b5596d14d729756a",
      "question": "What Native American tribe owns Hard Rock properties?",
      "prediction": "The answer is the Seminole Tribe of Florida Inc.",
      "ground_truths": [
        "Seminole Indian"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "e76d0d1755a446e398a114d49c9cd6cb",
      "question": "What is the 19th century bikini made of?",
      "prediction": "The 19th century bikini was made of either heavy flannel or wool fabrics, and covered the entire body, with ropes strung from the beach to offshore buoys.",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "74fe5cbec55e4c79b40ae84889093d27",
      "question": "What ode has Brian Hyland`s 1960 hit single?",
      "prediction": "The answer is: \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"",
      "ground_truths": [
        "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\""
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "e846a76cf896416fa46846010b343285",
      "question": "In what way in 19th-century bikinis has been made?",
      "prediction": "The answer is: In 19th-century bikinis has been made.",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "631d516c9682428ba9adab73196e9b74",
      "question": "When Bikinis first appeared?",
      "prediction": "The answer is: 1964.",
      "ground_truths": [
        "1946."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67e2faa59e874c85954b0c7f05378a61",
      "question": "What appeared for the first time in Roman mosaics?",
      "prediction": "The answer is: 1,700 years old Roman mosaics.",
      "ground_truths": [
        "oldest documented bikinis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e3d546814314fab91adebab3557fb1e",
      "question": "When did bikinis first appear?",
      "prediction": "The answer is: 1964.",
      "ground_truths": [
        "1,700 year old Roman mosaic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff89789e7aab4c31b15fed6597f67770",
      "question": "What was the bikini of the version of the 19th century made?",
      "prediction": "The bikini of the 19th century made was made out of either heavy flannel or wool.",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "17ba9abcc2d34d24b383137f643a09c0",
      "question": "What did Brian Hyland release in 1960?",
      "prediction": "Based on the text, the answer is:",
      "ground_truths": [
        "hit single, \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c64f2789544c45c8bf9dceebb35cfa53",
      "question": "What has he won?",
      "prediction": "Pacquiao won his sixth world world title at the 147-pound welterweight.",
      "ground_truths": [
        "WBO welterweight title"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "6616d9ae236f46acb990d2a879bf346f",
      "question": "Who returned home after his win?",
      "prediction": "The answer is Manny Pacquiao.",
      "ground_truths": [
        "Manny Pacquiao"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ea6918ad41e84412b642d9ac01b6d839",
      "question": "Where has Manny Pacquiano gone?",
      "prediction": "Manny Pacquiano has returned to the Philippines with a bag of his usual fare, his fans' welcome.",
      "ground_truths": [
        "Philippines"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "1b99dffb92c14a838fdb91967645cbf0",
      "question": "To where does Manny Pacquiao return?",
      "prediction": "Manny Pacquiao returns to his home in the Philippines, he is a champion at the 147-pound welterweight class.",
      "ground_truths": [
        "Philippines"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "37c0ef1f9eb3412594ff30f21611b882",
      "question": "Who is his next opponent?",
      "prediction": "The answer is: \"Pacquiao.\"",
      "ground_truths": [
        "American Floyd Mayweather"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7fa327c5d6d443e8b5528c521db7bf65",
      "question": "Who did Pacman beat?",
      "prediction": "The answer is \"The world's greatest boxing champion. Pacman beat Cotto on Friday at Las Vegas.\"",
      "ground_truths": [
        "Miguel Cotto"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "e98ac3de59a646c69b6eb17d3e3a4d0d",
      "question": "What did he win?",
      "prediction": "In the context, Pacquiao won his sixth world title at the 147-pound welterweight.",
      "ground_truths": [
        "WBO welterweight title"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "a23bfc2ed0ea4b19b736c30f73f7e406",
      "question": "Who did he beat?",
      "prediction": "The answer is \"Pacquiao.",
      "ground_truths": [
        "Miguel Cotto"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3038ae539727412fa4c113ae79580b91",
      "question": "How many others shared the room in Mumbai?",
      "prediction": "The answer is 10.",
      "ground_truths": [
        "eight people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d476ed59bc845d484ec9a8098d2a71d",
      "question": "Who played the quiz show host?",
      "prediction": "The answer is \"The quiz show host,\" to the tune of a song by a famous Bollywood singer.",
      "ground_truths": [
        "Anil Kapoor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf211b6bda2644e5801ce89678246fae",
      "question": "What did Kapoor say?",
      "prediction": "The answer is \"It's the children who have done it, not us,\" as he said in his acceptance speech at the Screen Actors Guild Awards in London.",
      "ground_truths": [
        "\"I also started from scratch, went from rags to riches,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6b1a287f63345cd9df7f78f9b9e8d9a",
      "question": "Who did the bollywood star play?",
      "prediction": "The answer is **Anil Kapoor**.",
      "ground_truths": [
        "the creepy quiz show host"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18ed9e010d6d41c8b1d69f1cf1834dc8",
      "question": "where did he grow up",
      "prediction": "The answer is in the context, but it's not explicitly stated. It's only provided as a summary of the context.",
      "ground_truths": [
        "a tenement in the Mumbai suburb of Chembur,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "282d649f73264f2a97d76f8a7f3a0b9c",
      "question": "who is a bollywood legend",
      "prediction": "The answer is Anil Kapoor.",
      "ground_truths": [
        "Anil Kapoor."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d3fdb0181051401cab348f6162104860",
      "question": "who is the bollywood star",
      "prediction": "The answer is \"Kapoor.",
      "ground_truths": [
        "Anil Kapoor"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "e10fb0470ea641d8924989aeca3b9c67",
      "question": "who quit the board",
      "prediction": "The answer is: **House Page Board.",
      "ground_truths": [
        "Brown-Waite"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f92118c87d7c45b6beb825d91b0cbeba",
      "question": "who is recommending \"corrective actions\"?",
      "prediction": "The recommendation is to recommend the appropriate and necessary corrective actions to be taken by the House Page Program Board.",
      "ground_truths": [
        "and"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "ae0abe6f6b7641e0977cb649ed87be36",
      "question": "who got fired for alleged oral sex in public?",
      "prediction": "The answer is **Nancy Pelosi**.",
      "ground_truths": [
        "Two pages"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2956c65db9643d6868d9a15adf1850a",
      "question": "who were \"enablers\"?",
      "prediction": "Enablers = \"the chapter participants\"",
      "ground_truths": [
        "observers and other page participants"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "e2ff35af7519458ab90fd7816840c6b2",
      "question": "how many farmers have been affected since 2000?",
      "prediction": "The provided text only contains the following information:",
      "ground_truths": [
        "4,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c47056d75f4421589fe4dc9bf60f5ac",
      "question": "What are farmers fighting over?",
      "prediction": "From the context, the answer is: \"Our power-sharing agreement, including an undertaking by both parties to ensure property rights are upheld but farm attacks and invasions continue unabated in Zimbabwe.\"",
      "ground_truths": [
        "to hold onto his land"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "c0890f632248463e98bfe78ec9ff750e",
      "question": "Since 2000, how many commercial farmers have been driven off their land?",
      "prediction": "The answer is 400.",
      "ground_truths": [
        "4,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "832ba598aeab4cd082be7dba8f62ce4b",
      "question": "What policies enable controversy?",
      "prediction": "The answer is, \"No, but we can investigate the government's policies. We can try to understand what they are doing to the land and the people.\"",
      "ground_truths": [
        "President Robert Mugabe's policy of redistributing white-owned farms to landless blacks."
      ],
      "em": 0,
      "f1": 0.060606060606060615
    },
    {
      "id": "09909b2908834b4f871ae1e0e7bd63f1",
      "question": "Who is battling continuing policy of land redistribution?",
      "prediction": "The answer is: the ZANU-PF (Zim Unite) party.",
      "ground_truths": [
        "400 farmers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29d9d1f8844c429db35562d9d85d1a27",
      "question": "What does one farmer show CNN?",
      "prediction": "The answer is, \"We are just left vulnerable.\"",
      "ground_truths": [
        "workers were standing idle."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7cf0ed6ffcc456a9c49c01cff79384f",
      "question": "Where and when was Harry Patch wounded in the World War One?",
      "prediction": "The answer is: Harry Patch was wounded at Ypres, Belgium, in 1917.",
      "ground_truths": [
        "Ypres, Belgium, in 1917"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "f0026bd57477438b9abfca1e085c890f",
      "question": "last British survivor of",
      "prediction": "The answer is **Harry Patch**.",
      "ground_truths": [
        "World War I"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd8b0bbb4f084371a16c57f4bb73bc47",
      "question": "Who is Harry Patch?",
      "prediction": "The answer is Harry Patch.",
      "ground_truths": [
        "last surviving British soldier from World War I"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5853b72f18f44468b7a88b25e486ac22",
      "question": "What did Harry Patch try to do all his life?",
      "prediction": "Harry Patch tried to do all his life.",
      "ground_truths": [
        "\"He tried"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "116e002d700e4d349581f1a4937dfba8",
      "question": "Where was the bus travelling to?",
      "prediction": "The answer is **San Antonio's Brooke Army Medical Center**.",
      "ground_truths": [
        "Matamoros, Mexico,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94b1187d3e5c4282983df4ab44d3d46b",
      "question": "What happened to the bus?",
      "prediction": "The bus crashed on its right side, and it landed on its right side.",
      "ground_truths": [
        "rolled over"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "549215ba9f05415f88e0c7b58148d436",
      "question": "Which company was operating the bus?",
      "prediction": "The answer is: **Texas Department of Public Safety**.",
      "ground_truths": [
        "Americanos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "75dc5bcd1b77473789e0fff4f89aafef",
      "question": "Where were the injured taken?",
      "prediction": "The answer is Buffalo, Texas.",
      "ground_truths": [
        "San Antonio's Brooke Army Medical Center and University Hospital,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7de2c568942c4160a93c7c80f264030f",
      "question": "How many people died in the accident?",
      "prediction": "The answer is: 2.",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6b25cbff83ef403ba7c2ee3b03a1d8be",
      "question": "How many people were rescued?",
      "prediction": "The answer is 19.",
      "ground_truths": [
        "At least 15"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fcf533dede444f2a0296b3bf112e0bd",
      "question": "Where did the military transport plae crash?",
      "prediction": "The answer is Jakarta.",
      "ground_truths": [
        "East Java"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a9db144769c4ce29c4cdb431d756043",
      "question": "What did the crash destroy ?",
      "prediction": "The crash destroyed four homes and killed two people who lived in at least one of the homes.",
      "ground_truths": [
        "destroyed four homes"
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "4f66952eeb534a109a11044fcfecf033",
      "question": "What is the new death toll?",
      "prediction": "The new death toll is 112.",
      "ground_truths": [
        "112 people"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "06ad4f09434a47e7a9563d96bde0e71f",
      "question": "What did the Karas village leader says?",
      "prediction": "The Karas village leader said that 19 people were brought to the hospital, with some with serious injuries, and 19 people from the flight had been rescued.",
      "ground_truths": [
        "The crash destroyed four homes and killed two people who lived in at least one of the homes,"
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "dcb53cda7e6144fca04963ac7581bcdd",
      "question": "Whose transport plane crashed into residential area in East Java ?",
      "prediction": "The answer is: The Indonesian military transport plane crashed into a residential area in East Java.",
      "ground_truths": [
        "An Indonesian military"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "f5627bd7cf9545e78204161253ca2e8e",
      "question": "What plane crashed into residential area?",
      "prediction": "The answer is: **Indonesia's military transport plane crashed into a residential area in East Java, killing 98 people.**",
      "ground_truths": [
        "An Indonesian military transport"
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "ecbae504766b467fb0dd39213c22ff21",
      "question": "Which country owned the military transport plane?",
      "prediction": "The answer is **Indonesia**.",
      "ground_truths": [
        "Indonesian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "822d82601bda4b509b180096346c1885",
      "question": "How many people from the flight  wererescued ?",
      "prediction": "The answer is 19.",
      "ground_truths": [
        "At least 15"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "80489a118e244f1ab4b2843692ab1b34",
      "question": "who was arrested?",
      "prediction": "The answer is Vicente Carrillo Leyva, a leader of the Carrillo Fuentes drug cartel.",
      "ground_truths": [
        "Vicente Carrillo Leyva,"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "376e8a711bc649d285bde4d7b21f21b3",
      "question": "What is the name fo the person arested?",
      "prediction": "The answer is Vicente Carrillo Leyva.",
      "ground_truths": [
        "Vicente Carrillo Leyva,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "11c406199ecc4e92a99602a002316f9e",
      "question": "Who are authorities blaming for the surge in violence near the border?",
      "prediction": "The answer is: Mexican authorities, with the U.S. State Department.",
      "ground_truths": [
        "drug cartels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1bc7d549f7514a3fb84a2e0f5e68f90a",
      "question": "What age is Vicente Carrillo Leyva?",
      "prediction": "The answer is Vicente Carrillo Leyva. Mexico City is a Mexican city, and he was arrested as a leader of the Carrillo Fuentes drug cartel. The answer is 32 years old.",
      "ground_truths": [
        "32,"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "d7f96c16afcc447fbf22d059a50abf08",
      "question": "Where was Leyva arrested according to officials?",
      "prediction": "The answer is Mexico City.",
      "ground_truths": [
        "in a park in a residential area of Mexico City,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "13957e04af1f45c9b2390e590a3024aa",
      "question": "Wher ewas the suspect arrested?",
      "prediction": "The answer is Mexico City, Mexico.",
      "ground_truths": [
        "in a park in a residential area of Mexico City,"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "93c7f5400de8418caa74475873be170a",
      "question": "who is blamed for surge in violence?",
      "prediction": "The answer is \"The federal attorney general's office,\" and not the Mexican authorities.",
      "ground_truths": [
        "drug cartels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9f8104092c54b1caab016d6280a0433",
      "question": "What does the professor think about the show's inspiration?",
      "prediction": "The professor thinks the show's inspiration stems from the audience's desire to shed a few pounds.",
      "ground_truths": [
        "she wonders if part of the appeal of plus-sized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ef4a3522b7c418eaaaa6970d6914b45",
      "question": "What drew over a million viewers?",
      "prediction": "The answer is \"people who look more like the viewing public,\" from the context provided.",
      "ground_truths": [
        "\"Dance\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55aa567e83e84daf8f24f0b4464bd525",
      "question": "What is the show about?",
      "prediction": "The answer is \"Dance Your Ass Off,\" and it's about the process of weight loss, with contestants learning to identify their own weight and what they've lost.",
      "ground_truths": [
        "weight-loss"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f61cf4c0ecff40e294bf410889610098",
      "question": "What show drew more than a million viewers for its premier?",
      "prediction": "The answer is \"Dance Your Ass Off,\" which is rated highly for Oxygen's \"Dance Your Ass Off\" television show.",
      "ground_truths": [
        "\"Dance Your Ass Off.\""
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "b34e08c6a5384be89492250bfb3e11bf",
      "question": "What company were they fired from?",
      "prediction": "The company they were fired from is identified as \"Grasiano Transmissioni\" in New Delhi.",
      "ground_truths": [
        "Graziano Transmissioni"
      ],
      "em": 0,
      "f1": 0.13333333333333336
    },
    {
      "id": "aa5ab129da0a48d0a29725bf34d1694e",
      "question": "Did India's labor minister apologize for his statement?",
      "prediction": "The answer is yes, India's labor minister, Oscar Fernandes, apologized for his statement about Chaudhary's death, and the ICC (International Criminal Court) stated the company's actions were \"violent forms of protest by self-proclaimed workers' representatives.\"",
      "ground_truths": [
        "later apologized,"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "676a7a901dc44a4d9d6768721b362662",
      "question": "what did the mob do",
      "prediction": "The mob acted in retaliation for the death of the Chief Executive of a Italian car parts manufacturing company, Oscar Fernandes, in Noida, India.",
      "ground_truths": [
        "attacked L.K. Chaudhary,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b7366a8b06543e88c6498f81ebdaefb",
      "question": "where did the police charge 63 people",
      "prediction": "The answer is provided in the context of the CNN-IBN news report.",
      "ground_truths": [
        "India"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cde0a15396548a8acebfe775446d3c0",
      "question": "What crime were the 63 people charged with?",
      "prediction": "The 63 people charged with murder in Delhi were among 137 people police had rounded up by Wednesday.",
      "ground_truths": [
        "murder in the beating death of"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "5a9c1f83248e41e595cc49ca6321e207",
      "question": "for what did the minister apologize",
      "prediction": "The minister apologized for his comments, and the official said the incident was \"tragic, unwanted and gory.\"",
      "ground_truths": [
        "his comments"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "aab04244f10344df84ff56eaf98084d4",
      "question": "What was the apology issued by India's labor minister?",
      "prediction": "The answer is \"The Italian Embassy stated the company had, for several months, been facing 'violent forms of protest by self-proclaimed workers' representatives.\"",
      "ground_truths": [
        "comments had been taken out of context."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "0f5470b87d79473c8fa5c8d85822f01f",
      "question": "What type of company was run by the chief executive who was attacked?",
      "prediction": "The answer is \"Italian car parts manufacturing company.\"",
      "ground_truths": [
        "Italian car parts manufacturing"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "16d963b0845842e78d18606036243158",
      "question": "What were the sixty-three people charged with?",
      "prediction": "The answer is 63.",
      "ground_truths": [
        "murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e68dcc48e03c4743b168512f576639af",
      "question": "In India, how many people did police charge with murdering their company boss?",
      "prediction": "The answer is 63.",
      "ground_truths": [
        "63"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ad096092524147ab9ea6e4cc7b3dc4fc",
      "question": "What number of people were charged?",
      "prediction": "The answer is 63.",
      "ground_truths": [
        "63"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "95b7241ff4724cebbdfdbb7908e03cee",
      "question": "How much did the US  provide?",
      "prediction": "The US provided $500,000 to help Haiti's children in trafficking.",
      "ground_truths": [
        "million dollars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6da32ee0b18a487192dc8897872719f4",
      "question": "How much money did the US provide?",
      "prediction": "The United States provided $500,000 for programs to protect children in Haiti from trafficking.",
      "ground_truths": [
        "additional million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccf4158795544791820f591e3ea877a0",
      "question": "How much did the U.S. provide to protect Haitian children from trafficking?",
      "prediction": "The U.S. provided $500,000 to protect Haitian children from trafficking.",
      "ground_truths": [
        "$500,000"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "40ca2fea196041abaa5ecb9644adeea7",
      "question": "How many people are laboring in bondage?",
      "prediction": "The question asks how many people are laboring in bondage.",
      "ground_truths": [
        "12.3 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b78695b228247bc88e1c3680aaa1083",
      "question": "How many people does The International Labour Organization report are 'laboring in bondage'?",
      "prediction": "According to the context, \"The International Labour Organization (ILO) reports that, with a 12.3 million, \"laboring in bondage\" is not a single number. The report is based on the estimates of 27 million, with 60% of the 12.3 million people in the world working in bondage, and 9 million to 10 million of them are females.",
      "ground_truths": [
        "12.3 million"
      ],
      "em": 0,
      "f1": 0.07999999999999999
    },
    {
      "id": "b57dc6e739304b1da3ec67de17fdf72e",
      "question": "Who has anecdotal evidence about the trafficking of Haitian children?",
      "prediction": "The answer is \"we don't have hard evidence on this. We hear about men coming into the camps offering food and water to girls in trucks. We don't have hard evidence on this. We hear about men coming into the camps offering food and water to girls in trucks.",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38407a483d224ec1b4a43154b2a8a69b",
      "question": "who has anecdotal evidence of the trafficking of Haitian children?",
      "prediction": "The answer is \"We have anecdotal evidence from UNICEF about trafficking of children in Haiti, but no firm figures on this.\"",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "4bcd1a9273904f42b28a195a0790d89d",
      "question": "Who has anecdotal evidence about the trafficking?",
      "prediction": "The answer is \"We have anecdotal evidence, but we do not have firm figures on it.\"",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9d48615144a4cdbb8f41bf8e0bddbb5",
      "question": "How many states do lawmakers say wish to follow Arizona's lead regarding the bill?",
      "prediction": "The answer is: Arizona lawmakers say they wish to follow the state's lead in its approach to immigration.",
      "ground_truths": [
        "four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5fc7ecf64ee84d9f8bebbc8762e8de37",
      "question": "Which state official has not signed the Arizona bill?",
      "prediction": "The answer is the state's Republican Gov. Jan Brewer.",
      "ground_truths": [
        "Gov. Jan Brewer."
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "d4b92af0485149e2a48214b9df6fd9f0",
      "question": "What does the Arizona bill require?",
      "prediction": "The Arizona bill requires Mexican-Americans to carry their alien registration documents at all times and requires police to question people if there is a reason to suspect they are in the United States illegally. It also targets those who hire illegal immigrant day laborers or knowingly transport them. The bill also requires that police only ask about the legality of hiring illegal immigrant day laborers and that they would not ask about the legality of transporting them.",
      "ground_truths": [
        "orders immigrants to carry their alien registration documents at all times"
      ],
      "em": 0,
      "f1": 0.21951219512195125
    },
    {
      "id": "e0b3dc2268b34678acf05945eee8298b",
      "question": "Who has not yet signed bill?",
      "prediction": "The answer is: Not yet.",
      "ground_truths": [
        "Republican Gov. Jan Brewer."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "69ffbc465b9d4e9fa198209656825e2e",
      "question": "What does the Arizona bill require police to do?",
      "prediction": "The Arizona bill requires police to question individuals if they are in the United States illegally.",
      "ground_truths": [
        "question people if there's reason to suspect they're in the United States illegally."
      ],
      "em": 0,
      "f1": 0.5384615384615384
    },
    {
      "id": "bdb371e606c74ea8957172d6ba4f398c",
      "question": "What did the  ACLU attorney say?",
      "prediction": "The ACLU attorney's statement, \"Arizona is the place to do it,\" in the Arizona bill, expressed the conviction that the bill will lead to a \"state-based response to this enormous problem.\"",
      "ground_truths": [
        "following in Arizona's footsteps would take states in the wrong direction."
      ],
      "em": 0,
      "f1": 0.05714285714285714
    },
    {
      "id": "a0cdfbc45b3f4641b64a2dfd3bde4c2b",
      "question": "Who did the Coast Guard search for?",
      "prediction": "The Coast Guard searched for the two remaining crew members from the helicopter, which went down around 11:30 p.m. Tuesday, in the waters off San Diego, California.",
      "ground_truths": [
        "the two remaining crew members"
      ],
      "em": 0,
      "f1": 0.29629629629629634
    },
    {
      "id": "e4533ea2a1404bc285933f7355d58203",
      "question": "Where did the crash occur?",
      "prediction": "The answer is **NCO (Navy Corps of Special Operations)**.",
      "ground_truths": [
        "the waters off San Diego, California,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3c5e3c93b4f04892a42057005a46caaa",
      "question": "How many people are missing after the crash?",
      "prediction": "The missing people are: 2",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60dd6a100f3b42598ab0d32f254b1662",
      "question": "What are the Coast Guard using to search?",
      "prediction": "The Coast Guard is using the National Coast Guard Search and Rescue (NCSAR) to search for three members of a U.S. Navy helicopter crew.",
      "ground_truths": [
        "helicopters and boats, as well as vessels from other agencies,"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "f532114560ac430e8d8e5e9929829182",
      "question": "Where did the copter crash?",
      "prediction": "The answer is: The crash occurred off the Mexican coast.",
      "ground_truths": [
        "It crashed about 20 miles off the Mexican coast,"
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "a5c81a5e5d3c42d089cd98f0b556de2f",
      "question": "How many people are still missing?",
      "prediction": "The missing people are the two remaining crew members from the U.S. Navy helicopter, which went down in the waters off San Diego, California.",
      "ground_truths": [
        "two remaining crew members"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "1fcf01709c0b404da2b3024ac0a02eb0",
      "question": "Who is using helicopters?",
      "prediction": "The answer is \"Coast Guard helicopters and boats.\"",
      "ground_truths": [
        "U.S. Navy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b01a7aac7fb40979eccbed5da004372",
      "question": "What day did the crash occur?",
      "prediction": "The crash occurred on Wednesday.",
      "ground_truths": [
        "11:30 p.m. Tuesday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89a41de7e5ca416cae37603aab8638c3",
      "question": "who was mentoring",
      "prediction": "The answer is \"The mobile county circuit judge.\"",
      "ground_truths": [
        "Thomas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe662df179fe47f1a9fa2c634c58c26a",
      "question": "who was denied allegations",
      "prediction": "The answer is: **The alleged victims were given preferential treatment at some point, but the state would say it was extremely lenient sentences.**",
      "ground_truths": [
        "Judge Herman Thomas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2286074ceb04295a819efc7e503fe39",
      "question": "who has  denied allegations",
      "prediction": "The answer is **Thomas Clark**. He has not denied the charges. He has pleaded not guilty and has denied any wrongdoing.",
      "ground_truths": [
        "Former Mobile County Circuit Judge Herman Thomas"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "b76738b25e7a44f89547dda91c1ce73b",
      "question": "who is accused",
      "prediction": "The answer is \"Thomas\", not \"The Mobile County Circuit Judge.\"",
      "ground_truths": [
        "Judge Herman Thomas"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "25cdce361ace48b99c156c6e70b5f03e",
      "question": "who is herman thomas",
      "prediction": "Answer: Hermann Thomas",
      "ground_truths": [
        "Former Mobile County Circuit Judge"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2784aee1dd324e3f8945b73963b7785f",
      "question": "What did the township ban?",
      "prediction": "The township banned inflatable or portable signs and banners on public property.",
      "ground_truths": [
        "inflatable or portable signs and banners on public property."
      ],
      "em": 0,
      "f1": 0.9
    },
    {
      "id": "470161e89bbf46fcaeb483641c98ebb5",
      "question": "What is banned?",
      "prediction": "The answer is: the local union has a right to display its 10-foot-tall, black, rat-shaped balloon at a rally held outside a fitness center.",
      "ground_truths": [
        "inflatable or portable signs and banners on public property."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "073515a682484ceaac87a83d1ab38a06",
      "question": "What do towns have the right to enforce?",
      "prediction": "The town has the right to enforce its ordinances regarding public property.",
      "ground_truths": [
        "an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "61d661961fb143578d6e07aeab16ae66",
      "question": "What did the court say the towns have the right to enforce?",
      "prediction": "The court stated that the town of Lawrence Township has the right to enforce its \"aesthetic environment\" and public safety by prohibiting inflatable or portable signs and banners on public property.",
      "ground_truths": [
        "maintain an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "1d5e987319ba4158a12cab23543a04aa",
      "question": "What do towns have the right to do?",
      "prediction": "The answer is, towns have the right to do what they want.",
      "ground_truths": [
        "maintain an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "969af4d0da444448906032c0784076e6",
      "question": "What's the name of the balloon displayed at union rallies?",
      "prediction": "The answer is \"Scabby.\"",
      "ground_truths": [
        "Scabby"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4ab8ad68491d42d590acd464b7e19e82",
      "question": "Who can disrupt the Tennessee Valley Authority?",
      "prediction": "The answer is \"The Tennessee Valley Authority.\"",
      "ground_truths": [
        "skilled hacker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a0dab70352547cf92c768c78614532c",
      "question": "What was Congress told?",
      "prediction": "The answer is: Congress told the National Electric Reliability Council (NEC) that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability, but Congress was misled.",
      "ground_truths": [
        "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,"
      ],
      "em": 0,
      "f1": 0.6111111111111112
    },
    {
      "id": "e747846267684ddeb167b085777bbd21",
      "question": "TVA supplies power to how many Americans?",
      "prediction": "The answer is, the government's electricity supply to the United States.",
      "ground_truths": [
        "almost 9 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4308b5c4c2af4b51b0d454474e139318",
      "question": "What does the TVA do?",
      "prediction": "The TVA's answer is: \"Not fully implementing appropriate security practices to protect the control systems used to operate its critical infrastructures, leaving vulnerable to disruption,\" with the following key points:",
      "ground_truths": [
        "supplies power to almost 9 million Americans,"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "9a9585ebb3ef47898a0e074a4ab0d6b5",
      "question": "Who caused the blackouts?",
      "prediction": "The answer is the Tennessee Valley Authority (TVA).",
      "ground_truths": [
        "a skilled hacker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7084bfe9b1b64089b8c1b38b38226424",
      "question": "What did representative say?",
      "prediction": "The answer is: \"Congress was misled. The result is no clear picture of how vulnerable utilities are to cyber attacks, and the electric reliability corporation is the only manufacturer of hardware to close the Aurora vulnerability.  The Congress will require new powers to require utilities to take corrective actions to close cyber security gaps and the federal government may need to ask for these powers to the Federal Energy Regulatory Commission.\"",
      "ground_truths": [
        "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\""
      ],
      "em": 0,
      "f1": 0.11627906976744186
    },
    {
      "id": "832db3f0a7264d689b6e7c8ca7869e9f",
      "question": "What percentage of utilities were fixed to combat attacks?",
      "prediction": "The answer is **100**.",
      "ground_truths": [
        "75 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "371c170f87a74d4ca39cbaf126b5172d",
      "question": "When did they find him",
      "prediction": "The Drug Enforcement Administration (DEA) said it's considering tighter restrictions on propofol, a drug reportedly found after Michael Jackson's death in Holmby Hills, California, and that the DEA is considering a \"scheduled\" drug, which would tighten restrictions on its distribution and use.",
      "ground_truths": [
        "June 25."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2e96837b84e478c8c580e054383e693",
      "question": "What drug was found in Michael Jackson's home?",
      "prediction": "The Drug Enforcement Administration (DC) stated that it considered tighter restrictions on propofol, a drug reportedly found after Michael Jackson's death in the Holmby Hills, California mansion he rented, as a possible \"scheduled\" drug.",
      "ground_truths": [
        "propofol,"
      ],
      "em": 0,
      "f1": 0.06451612903225806
    },
    {
      "id": "8f5f6161afbd49f4aedb61e2aa24a75f",
      "question": "What kind of drug is Propofol?",
      "prediction": "The Drug Enforcement Administration (DEA) has determined that Diprivan is a scheduled drug. This is because DEA researchers would typically seek information about the drug's effects on users. The DEA's decision is based on the data gathered at the last drug-use test, which is considered to be over several weeks.",
      "ground_truths": [
        "powerful anesthetic and sedative."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "096b55148bea4c5c9368d672892e9447",
      "question": "What happened to him",
      "prediction": "The Drug Enforcement Administration (DEA) stated that Wednesday it is considering tighter restrictions on propofol, a drug reportedly found after Michael Jackson's death in the Holmby Hills, California mansion he rented.",
      "ground_truths": [
        "death of cardiac arrest"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "bc036900b2f747aa92b1cdf251b0a37c",
      "question": "What is propofol used?",
      "prediction": "The drug's use is determined by the DEA's decision to restrict its distribution and use, and the NAAC's consideration of it for its potential for abuse and its long-term impacts on the user.",
      "ground_truths": [
        "anesthetic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c23796c4d75440d5a6e1200deb8c14e8",
      "question": "How long has the DEA been looking into it?",
      "prediction": "The DEA has stated that it is considering tighter restrictions on propofol, and it is also considering a scheduled drug.",
      "ground_truths": [
        "for the past two years,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ba8b05dc3d54a498ab16e372ef71035",
      "question": "Has the DEA been looking into this?",
      "prediction": "The Drug Enforcement Administration (DEA) said it's considering tighter restrictions on propofol, and the DEA is looking at it for potential abuse.",
      "ground_truths": [
        "have"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3ec8eaa57d0418da0c66d7ad83761b3",
      "question": "What where the results of the autopsy?",
      "prediction": "The results of the autopsy at Nico's death at Sea World in San Antonio, Texas, indicate that Nico was found to have a bone disease, but his death is not the only cause of his death.",
      "ground_truths": [
        "inconclusive"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8ec79c66e284a45a63c7dcdcf4f9064",
      "question": "What is happening at the Georgia Aquarium?",
      "prediction": "The Georgia Aquarium is experiencing significant changes, including the death of Nico, followed by the death of Gaspar.",
      "ground_truths": [
        "undergoing renovation."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c6f0f2b83e546e39bc40d37b0ee9975",
      "question": "Where in the world did Nico die?",
      "prediction": "The answer is, the Georgia Aquarium in Atlanta.",
      "ground_truths": [
        "Atlanta, Georgia"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9759378e0cc44a1c839214bdaa26612d",
      "question": "Where did Lavau's car come to a rest?",
      "prediction": "The answer is the Los Angeles County Fire Department.",
      "ground_truths": [
        "500 feet down an embankment"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18cbdcbef93544acac9407596569a64f",
      "question": "Who began searching for Lavau?",
      "prediction": "The answer is **California Highway Patrol.**",
      "ground_truths": [
        "family"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "40f708bf9a744a90b62edfb3b44ad9cf",
      "question": "What did the California man survive on?",
      "prediction": "The California man survived on leaves and water from a creek in Lake Hughes.",
      "ground_truths": [
        "by eating leaves and drinking water from a creek,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4d9450e5a9ce4b2d93b7576b2dce59ce",
      "question": "Who searched for Lavau when he went missing?",
      "prediction": "The answer is **California Highway Patrol**.",
      "ground_truths": [
        "family"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e9b048c06dc45be97da6116056f74f6",
      "question": "What did Lavau's car fall into?",
      "prediction": "The car fell into a ravine.",
      "ground_truths": [
        "down a steep embankment in the Angeles National Forest"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49af423e020444c999bdbdaa7cc573bd",
      "question": "Who found David Lavau?",
      "prediction": "David J. Lavau was found in a ravine a week after losing control of his car on a rural road in Lake Hughes, California.",
      "ground_truths": [
        "Sean"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53278976c290499ab7f1decb9f7bf6a2",
      "question": "what did the chp say?",
      "prediction": "The chp said: \"The CHP said Lavau's accident and the one involving the dead driver are under investigation. CNN's Chelsea J. Carter contributed to this report.\"",
      "ground_truths": [
        "Lavau's accident and the one involving the dead driver are under investigation."
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "423f76b5864047f480599c7946546eb1",
      "question": "when did the family begin the search?",
      "prediction": "The family began the search for Lavau in Lake Hughes on September 23rd.",
      "ground_truths": [
        "he failed to return home,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b32d5172d0b3451b83352d24deab59e6",
      "question": "Who will report to quinns office?",
      "prediction": "The answer is: The Illinois Reform Commission.",
      "ground_truths": [
        "Reform Commission"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "1405700ba6ce4058a4fa68e3c3d680e3",
      "question": "What does pat quinn want?",
      "prediction": "Pat Quinn wants a review of state government practices in Illinois.",
      "ground_truths": [
        "a review of state government practices completed in 100 days."
      ],
      "em": 0,
      "f1": 0.631578947368421
    },
    {
      "id": "413339d30cc447a884eb601571769b06",
      "question": "Who will report to Quinn's office?",
      "prediction": "The answer is \"The Illinois Reform Commission,\" and the document does not provide a specific name or date.",
      "ground_truths": [
        "commission, led by former U.S. Attorney Patrick Collins,"
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "5372bffd269e45f5b80bfa12e294af8f",
      "question": "Who took over as Illinois governor?",
      "prediction": "The answer is:  The Illinois Reform Commission, led by former U.S. Attorney Patrick Collins, was elected in 2019.",
      "ground_truths": [
        "Pat Quinn"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "487cd2a2f5ee49bb96d33d0d56ea6a2b",
      "question": "What governor was ousted?",
      "prediction": "The answer is: The governor who was ousted from office was Bart Obama.",
      "ground_truths": [
        "Rod Blagojevich,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd58ed81f9ea41e2b803c29dc3d424d0",
      "question": "What character does he play on \"How I Met Your Mother\"?",
      "prediction": "NPH says he's trying to make viewers feel like they're in good hands with him, and he's probably hosting an awards show. He's also likely hosting an awards show. Neil Patrick Harris, one of the stars of \"How I Met Your Mother,\" is not dabbling in the extraordinary. He's probably hosting an awards show.",
      "ground_truths": [
        "Barney Stinson,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ad7bbb7ca4e4e89bafebeb59f8ae349",
      "question": "What show does Neil Patrick Harris star in?",
      "prediction": "Neil Patrick Harris starred in \"How I Met Your Mother,\" which starts its fifth season on September 21st.",
      "ground_truths": [
        "\"How I Met Your Mother,\""
      ],
      "em": 0,
      "f1": 0.4347826086956522
    },
    {
      "id": "ba7ed34f4a424c8c9878ba4048de02a1",
      "question": "How long will it  take for the new season of Mother?",
      "prediction": "The answer to the question is: \"It's just a list of key things,\" so there's no real answer to it.",
      "ground_truths": [
        "September 21."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a96963964374eddb50b591b59d53734",
      "question": "What show is Neil Patrick Harris on?",
      "prediction": "Based on the context, the answer is \"How I Met Your Mother,\" which translates to \"How I Met Your Mother. [N.P. Harris] hosts the 61st Primetime Emmy Awards.\"",
      "ground_truths": [
        "\"How I Met Your Mother,\""
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "0fafdbcc50b44df0aebc0d097d9e6541",
      "question": "What will he probably not do at the Emmys?",
      "prediction": "The answer provided is: NPH says he's trying to make viewers feel like they're in good hands with him as Emmy host, but he's probably hosting an awards show.",
      "ground_truths": [
        "dancing"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "928063d9906145b6b83392f5068efb4f",
      "question": "what is borwns new novel",
      "prediction": "The answer is: \"The Lost Symbol,\" by Robert Langdon.",
      "ground_truths": [
        "\"The Lost Symbol,\""
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "0ad40f362389498382471e1f6171f996",
      "question": "who wrote novel da vinci code",
      "prediction": "The answer is: \"The Lost Symbol\" is written by Robert Langdon.",
      "ground_truths": [
        "Dan Brown's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57c251b6218b4c1094f9dfb9f53f2a21",
      "question": "According to EW it doesn't equal his previous novel entitled what?",
      "prediction": "According to EW, \"The Lost Symbol\" is his first novel since the blockbuster \"The Da Vinci Code\" in 2003.",
      "ground_truths": [
        "\"The Da Vinci Code\""
      ],
      "em": 0,
      "f1": 0.3157894736842105
    },
    {
      "id": "b322fdc7819546138582ebc07e8aa7a8",
      "question": "Dan Brown's new novel is called what?",
      "prediction": "Answer: The Lost Symbol.",
      "ground_truths": [
        "\"The Lost Symbol,\""
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "aa4169b3de364c318562b0b09308caa0",
      "question": "who is dan brown",
      "prediction": "The answer is Dan Brown.",
      "ground_truths": [
        "rare thriller writer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc530a1b42fc45af9b9112f391db3504",
      "question": "How much did Avatar gross in the 9th weekend?",
      "prediction": "The answer is $22 million.",
      "ground_truths": [
        "$22 million,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bf7fc80bf1024d73b6f5cea672696789",
      "question": "will the film top $60 million by the time the holiday is over?",
      "prediction": "The film's success is likely to be a strong performance, and it will likely be in the $60 million range by the time the Presidents Day holiday weekend is over.",
      "ground_truths": [
        "is likely to"
      ],
      "em": 0,
      "f1": 0.21428571428571425
    },
    {
      "id": "4cff857ac2f74c6094f26f596ba0cb1f",
      "question": "avatar grossed how much in its 9th weekend?",
      "prediction": "The answer is $15.3 million.",
      "ground_truths": [
        "$22 million,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "619869505781451abc4517f031a8b857",
      "question": "how much did valentine's day gross?",
      "prediction": "The total gross of Valentine's Day was $193 million.",
      "ground_truths": [
        "$52.4 million"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "8a34a18a13e9487984c1d6839fed053e",
      "question": "How much did the film gross?",
      "prediction": "The film grossed $52.4 million, and it was likely to top $60 million by the time the Presidents Day holiday weekend is over.",
      "ground_truths": [
        "$52.4 million"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "b39eac495217430ebe3cfb37878882ee",
      "question": "Which movies are competing with \"Valentine's Day\"?",
      "prediction": "The movies competing with \"Valentine's Day\" are:",
      "ground_truths": [
        "\"Wolfman,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "efc438e4fd18417daa0e662499fbe956",
      "question": "How much will the film top?",
      "prediction": "The film's highest-grossing weekend was $193 million, with an estimated total gross of $193 million.",
      "ground_truths": [
        "$60 million"
      ],
      "em": 0,
      "f1": 0.13333333333333336
    },
    {
      "id": "edd8922fe3ec43be851dee74c97ea861",
      "question": "where are the shows taking palce",
      "prediction": "The answer is: **London's 20,000-capacity O2 Arena in London.**",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "a045cbc6199c4b9e8dbb8545febda365",
      "question": "When were the shows scheduled to start?",
      "prediction": "The context provides the following:",
      "ground_truths": [
        "10th July"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cc25c3729ab4627a29f3f11172af372",
      "question": "Several shows were postponed to which year because of because of \"sheer magnitude\"?",
      "prediction": "The answer is, **the year**.",
      "ground_truths": [
        "2010,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1703025033a448048d6f688f9321e2a2",
      "question": "Several shows have been postponed until when?",
      "prediction": "The answer is: \"Due to the magnitude of the concerts, the decision to move back a few of the opening shows was necessary to meet the challenges of the massive and complex show, and that was the last reason for the postponement.\"",
      "ground_truths": [
        "next year"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f022ca089d74d51a4be4625cb6191d5",
      "question": "what are the rumors",
      "prediction": "The answer is, \"Because Michael Jackson was in good health, and he was diagnosed with skin cancer, and the \"This Is It\" show was to begin July 8 at 20,000-capacity O2 Arena in London, the delay is due to the sheer magnitude of the concerts, the commitment to ensuring a top-quality, phenomenal show, and the challenges associated with a large and complex show.\"",
      "ground_truths": [
        "diagnosed with skin cancer."
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "77eaa1d176e14eb092f85c51020850ff",
      "question": "What was Michael Jackson scheduled to perform?",
      "prediction": "The answer is: \"Michael Jackson's series of London concerts.\"",
      "ground_truths": [
        "comeback concerts in London"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "c524775cfdfd474691b06c6c05dc4cd2",
      "question": "How many died during the killing?",
      "prediction": "The answer is 100,000.",
      "ground_truths": [
        "800,000 dead."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6143a2a8fba6403bac9b1a61c403d540",
      "question": "Which is the most stable nation in Africa?",
      "prediction": "The answer is Rwanda.",
      "ground_truths": [
        "Rwanda"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9b6459e60add477cb7b5c6df85dcc727",
      "question": "What is the hatred between?",
      "prediction": "The answer is **\"Confined to the core of the nation's psyche.\"** - This is a direct translation of the context.",
      "ground_truths": [
        "Hutus and Tutsis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e55899b8be04ddc97c6cae4f4d01699",
      "question": "What lasted 100 days?",
      "prediction": "Here's the answer, directly from the context:",
      "ground_truths": [
        "killing rampage."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97b0da00c8d64c7e9cf9ca56599a06e8",
      "question": "What is Rwanda considered?",
      "prediction": "Here's a concise answer based on the provided context:",
      "ground_truths": [
        "one of Africa's most stable nations."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2546878366e744218972a0eb6c7c650b",
      "question": "What caused genocide between Tutsi and Hutu in 1994?",
      "prediction": "The answer is 1994.",
      "ground_truths": [
        "the privileged ethnicity,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f40c50b5bb8e4c7b9eb852a860a71d2f",
      "question": "How long was the rampage?",
      "prediction": "The answer is 100 days.",
      "ground_truths": [
        "100-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e957032916604030a309b46e81fd7382",
      "question": "For how long killing were between Tutsi and Hutu?",
      "prediction": "The answer is, Rwanda officially declared a cease-fire in 1994.",
      "ground_truths": [
        "100-day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88289b7898cf44bdb3bd88d62391a03b",
      "question": "With what organization Al-Maliki has business talks?",
      "prediction": "Al-Maliki's answer to the CNN's questions about the U.S. presence in Iraq is based on a speech delivered at the U.S. Chamber of Commerce in Washington. The key is that he stated that the U.S. has increased its investment in Iraq, and that the Americans are trying to stimulate spending and investment in Iraq's infrastructure to help the citizens' lives improve. He also mentioned the increasing number of American businesses in Iraq, and that the Chamber of Commerce in Washington is working to support American businesses. He concluded with a message to American executives.",
      "ground_truths": [
        "U.S. Chamber of Commerce"
      ],
      "em": 0,
      "f1": 0.09195402298850576
    },
    {
      "id": "70903ce347bf4339a8eb3f17576ddf6f",
      "question": "What will build the U.S.-Iraq relationship?",
      "prediction": "The answer is: The U.S. is continuing to trail its trading partners with respect to investment and economic engagement in Iraq, and American workers, before the US military departure.",
      "ground_truths": [
        "end of U.S. military operations"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "59d1e4a40a3f4aad9a0e5ae95b711322",
      "question": "What does Al-Maliki say Iraq wants to see?",
      "prediction": "Al-Maliki's speech to American executives at the Chamber of Commerce in Washington in Washington, D.C. included:",
      "ground_truths": [
        "a much greater presence of U.S. companies in his country to help spur greater spending and investment on the country's infrastructure"
      ],
      "em": 0,
      "f1": 0.17647058823529413
    },
    {
      "id": "1ffcd2e9e37749d7ac35f3229327a97e",
      "question": "When were Iraqi contracts auctioned?",
      "prediction": "The answer to the question is: In the past year, the U.S Investment in Iraq had totaled $70 billion, and the Chamber of Commerce in Washington held its annual meeting at the U.S Chamber of Commerce in Washington.",
      "ground_truths": [
        "two years ago."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "936773dbfb6c42989c8090f992429d0d",
      "question": "Who talked business?",
      "prediction": "The answer is: The United States' economic and political engagement in Iraq has resulted in a significant increase in foreign direct investment in Iraq's economy.",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a06f4131fee84de1b4b796edbfc4dbfa",
      "question": "Who talked business with the U.S. Chamber of Commerce?",
      "prediction": "The answer is: The International Monetary Fund.",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a15cda812324bc0adabb898c0f18a04",
      "question": "Who said he wants to see more us companies in his country?",
      "prediction": "The answer is: \"Iraq's prime minister made a pitch to leaders of American businesses: Iraq is open for business.\"",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "42fc7a4cc2c24cdb843174c1f1314dab",
      "question": "He's been given what to travel?",
      "prediction": "Answer: He's been given what to travel.",
      "ground_truths": [
        "an 11-seat Challenger, courtesy of Steve Wynn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd9d78ffa4134b8a8a45c017e9706bd5",
      "question": "Who is coming out of retirement?",
      "prediction": "The answer is \"Garth Brooks.\"",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bbae818236954eddb4473d9278983e2d",
      "question": "who was the one who did that",
      "prediction": "The answer is \"He was a single man, an average guy, and a man who loved music. He's the best-selling solo artist of all time in any genre. He's an old-fashioned guy with old-fashioned values.\"",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68f5d6f4ac934efba4a292614334dc2b",
      "question": "When did he retire",
      "prediction": "The answer is \"When he retired, he said he wanted to be able to drive his kids to school every day until they go to college.\"",
      "ground_truths": [
        "2000."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52327052a18c48079fbc64f02e20a583",
      "question": "Who describes himself as an average guy?",
      "prediction": "The answer is \"An average guy. What I like, an average guy likes. It's that simple. The music that I love, I find that most guys around me love, too.\"",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c8a8b75eb7c441ec9caad21622b73079",
      "question": "Where was the attack?",
      "prediction": "The answer is the Montana side of Buckhorn Mountain. The Colorado side of the national forest is transferred to the Montana side of Buckhorn Mountain.",
      "ground_truths": [
        "remote part of northwestern Montana"
      ],
      "em": 0,
      "f1": 0.16000000000000003
    },
    {
      "id": "c3f8ce9a9cfb41bf92e4bfda01b3ecc5",
      "question": "Is the man alive?",
      "prediction": "The man who was killed was part of a hunting party of three men, and that was the grizzly bear that killed the hunter.",
      "ground_truths": [
        "was killed"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "7e9c99ffa50c40bf94f5306d8134e736",
      "question": "Who found the man?",
      "prediction": "The answer is the sheriff's office.",
      "ground_truths": [
        "two hunters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb267b92a44f481d81b2d7aa6606847a",
      "question": "What was he attacked by?",
      "prediction": "The answer is \"Bear,\" but the context provides no information on the creature's exact location.",
      "ground_truths": [
        "grizzly bear"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "80109609b1f24dc0859af11d2bf33ca9",
      "question": "Who will he donate to?",
      "prediction": "Answer: Lee Myung-Bak will donate his salary to the underprivileged.",
      "ground_truths": [
        "the underprivileged."
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "ba3edfff481042f2a104ffdc278727c0",
      "question": "What kind of company was he the CEO of?",
      "prediction": "The answer is \"the state news agency.\"",
      "ground_truths": [
        "engineering and construction"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf0512befa13468ca6a1763a6e032e82",
      "question": "Who is the South Korean leader?",
      "prediction": "The South Korean leader is Lee Myung-Bak.",
      "ground_truths": [
        "Lee Myung-Bak"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7f5733184c134247ad79a6d2bc6f0081",
      "question": "Who will donate his salary to help the poor?",
      "prediction": "The answer is Lee Myung-Bak, the South Korean president.",
      "ground_truths": [
        "Korea's new president"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "5bbab85f5ac243a9b98914f8c7af6eb4",
      "question": "What city was he mayor of?",
      "prediction": "The answer is South Korea.",
      "ground_truths": [
        "Seoul"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de36cde140ee4ceda620f1518dd104bc",
      "question": "who has he donated to before",
      "prediction": "The answer is Lee Myung-Bak.",
      "ground_truths": [
        "children of street cleaners and firefighters."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa9daba7638b4fa9b1472bb648fe964e",
      "question": "what is lee a former ceo of",
      "prediction": "Lee Myung-Bak, a former CEO of South Korea's engineering and construction company, pledged his salary to the underprivileged.",
      "ground_truths": [
        "an engineering and construction company"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "68ea789496074f56878ceddf5f0bb6c0",
      "question": "What position did Lee hold?",
      "prediction": "Answer: Lee Myung-Bak held the position of South Korean President of South Korea.",
      "ground_truths": [
        "CEO of an engineering and construction company"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "66d0e7da123d49fd8f8016dc27775052",
      "question": "Who did he donate to while mayor of Seoul?",
      "prediction": "Answer: Lee Myung-Bak, the South Korean president of South Korea.",
      "ground_truths": [
        "children of street cleaners and firefighters."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "8b49dc87c93b45c48eb62e4a0b8ac604",
      "question": "What will Lee Myung-Bak donate?",
      "prediction": "The answer is: Lee Myung-Bak will donate his salary to the underprivileged.",
      "ground_truths": [
        "his salary"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "b7f306b0682a46e6907b952794cca076",
      "question": "What is the third most popular religion in the US?",
      "prediction": "The answer is Buddhism.",
      "ground_truths": [
        "Buddhism"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "349c6fea61d94b258a2b0b58783d96c3",
      "question": "what are the first two most popular religions?",
      "prediction": "The answer is Buddhist.",
      "ground_truths": [
        "Christianity and Judaism,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f114f6b090aa4b81abf5a6ee02f81928",
      "question": "Who are being educated about meditation and yoga?",
      "prediction": "The answer is, \"Not explicitly stated in the context.  However, it is implied that the answer is \"Meditation and Yoga.\"",
      "ground_truths": [
        "inmates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "328149409f62438f88c73c618a418b3a",
      "question": "What are the programs educating inmates about?",
      "prediction": "The answer is, \"Meditation and Yoga are teaching inmates how to cope in prison,\" and many other.",
      "ground_truths": [
        "meditation"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "8acf07893149496b980ed1f5f5421820",
      "question": "What has meditation helped to do?",
      "prediction": "The answer is meditation.",
      "ground_truths": [
        "stay on track and get me through prison,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d291eb71b93347c584b6b6aa8826cfb5",
      "question": "What is the third most popular religion in the United States?",
      "prediction": "The answer is Buddhism.",
      "ground_truths": [
        "Buddhism"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "019a54c5394847ec81325e8bdaf8355f",
      "question": "what does the analyst say?",
      "prediction": "The analyst says Argentina's ability to withstand the global economic slowdown was a key election issue. This is seen as a broader effort by Cristina Kirchner to tax agricultural exports, which escalated tensions with angry farmers.",
      "ground_truths": [
        "that indicators such as inflation are underreported by as much as half."
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "61509191d23a45baa208be884903f3b7",
      "question": "Who has \"been weakened by this latest economic crisis\"?",
      "prediction": "The answer is likely **Argentina's ability to withstand the global economic slowdown.** This is because Argentina's economic indicators, including inflation, and the effort by Cristina Kirchner to tax agricultural exports to address the crisis, likely led to a significant decrease in the number of politicians losing their seats in the Congress, with only one candidate winning three seats.",
      "ground_truths": [
        "\"The Kirchners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f24c1ba50b04f8493f8663fd779846e",
      "question": "who stepped down?",
      "prediction": "Answer: Nestor Kirchner",
      "ground_truths": [
        "Former Argentine President Nestor Kirchner"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fea7e77ef2364dcdb4bbfa5d69b0c0ef",
      "question": "What party  lost majority in Argentina's Chamber of Deputies?",
      "prediction": "The party that lost majority in the Chamber of Deputies in Argentina's Congress lost its ability to form a new legislative party.",
      "ground_truths": [
        "Justicialist"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2bc1abc94f141399b0b004676f84171",
      "question": "Who steped down as leader of Argentine's ruling party?",
      "prediction": "The answer is: Kirchner.",
      "ground_truths": [
        "President Nestor Kirchner"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "aa7ddfbb8f3c403ba5b0a0ded76f2bff",
      "question": "where is this taking place",
      "prediction": "The answer is Argentina (Buenos Aires).",
      "ground_truths": [
        "Buenos Aires."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "b0d0fee178644809ac252c7347aac58b",
      "question": "who lost the majority?",
      "prediction": "The answer is **Argentorist Party** (also known as the IAC).",
      "ground_truths": [
        "The ruling Justicialist Party, or PJ by its Spanish acronym,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "1cfc4441a86445fab7952656df2d64a0",
      "question": "who have been weakened by the economic crisis",
      "prediction": "The answer is: Argentina's ability to withstand the global economic slowdown is a key election issue, and there is agreement among analysts that the indicators are underreported by as much as half, making it difficult for Cristina Kirchner to hold the top.",
      "ground_truths": [
        "\"The Kirchners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d3ffcfb5097482881bfb029456e8e66",
      "question": "how many integrated it?",
      "prediction": "The answer is:",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89be46d5676849c19dd4ce9d45bda064",
      "question": "what In 1957 school was all-white; today?",
      "prediction": "The answer is, it's a new generation walks the halls at Central High.",
      "ground_truths": [
        "Little Rock Central High"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "c53dc70e93264f18adcdcddb971ed893",
      "question": "what school was integrated?",
      "prediction": "The answer is \"Central High School,\" Arkansas.",
      "ground_truths": [
        "Little Rock Central High"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "32bcc94873a24815b3f84adbc715bdb5",
      "question": "How many people integrated Little Rock Central High School?",
      "prediction": "The answer is: 1957.",
      "ground_truths": [
        "Nine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a17fbfcea608490bad3b6a46e1127316",
      "question": "where LaNier hopes Americans will focus?",
      "prediction": "The answer is:  \"It's definitely difficult to think about who these students are, and it's hard for everyone to understand.  It's a new generation of students who walk the halls at Central High, and this place is so different from the inside, and it's hard for the outside to see.  It's hard for the internal color of the students to be seen as a mix of all the racial differences in the school. This is definitely hard for me to say, but it's a major step in the journey toward the President-elect's inauguration.\"",
      "ground_truths": [
        "got to get past just the color of our skins being newsworthy. It's really about all the things we knew we could do for this country and now we have the opportunity to show it and it's going to come through his leadership,\""
      ],
      "em": 0,
      "f1": 0.21666666666666667
    },
    {
      "id": "996996ce4b1347a6985e7e0351f40ee3",
      "question": "In what year was the school all-white?",
      "prediction": "The answer is: \"It shows how the 101st were on the grounds of the school,\" and \"It is difficult for me to walk inside the school every day and see how this place could have been such a hostile environment for nine African-American students.\"",
      "ground_truths": [
        "In 1957,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7464dad64f7e486ca7535d7e02f28586",
      "question": "who integrated the high school?",
      "prediction": "The answer to the question is: \"We were giving up something for a bigger cause. We were happy that we did it.\"",
      "ground_truths": [
        "Carlotta Walls LaNier and eight other members of the Little Rock Nine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19837fd3eddc462c9b178a83fddce010",
      "question": "What is the length of Susan Atkin's sentaence?",
      "prediction": "The answer is 11.",
      "ground_truths": [
        "life sentence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "87ac9682d8b541de8ef5f78175be9615",
      "question": "Who says Susan has brain cancer?",
      "prediction": "The answer is: **Atkins had been convicted in the 1969 \"Manson murders, and she was hospitalized for her recovery.**",
      "ground_truths": [
        "Her husband and attorney, James Whitehouse,"
      ],
      "em": 0,
      "f1": 0.1739130434782609
    },
    {
      "id": "5626902cc76741eb9f4075d0a042016c",
      "question": "What was Susan Atkin's role in the Manson family murders?",
      "prediction": "The answer is, Susan Atkin was led from a Los Angeles grand jury room after her indictment in 1969 \"Manson murders.\"",
      "ground_truths": [
        "slayings of actress Sharon Tate and four others."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "864fba7b5c2744b8ad73ded96b8fef81",
      "question": "Who is serving a life sentence?",
      "prediction": "The answer is \"Atkins is serving a life sentence in California.\"",
      "ground_truths": [
        "Susan Atkins"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "7e7da6521baa4d6bb18f6279522d0f43",
      "question": "Who has terminal brain cancer?",
      "prediction": "The answer is: Atkins has terminal brain cancer.",
      "ground_truths": [
        "Susan Atkins"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "d219a6074cc74aea842c1a2a383e208f",
      "question": "What decade the murders occur?",
      "prediction": "The answers are:",
      "ground_truths": [
        "1969"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d210306368a24de884557f43a1c4d48e",
      "question": "What is harder to control than food allergies?",
      "prediction": "The answer is: **Food allergies** because it is not always possible to predict when the next allergic reaction is going to be followed by a severe, potentially life-threatening anaphylaxis.",
      "ground_truths": [
        "Insect"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5f166a914df4c3b8f6afe0ec3462437",
      "question": "In rare cases, mosquito bites can lead to what?",
      "prediction": "The answer is:",
      "ground_truths": [
        "anaphylaxis,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d98750850d5249e09d7bfbb43313513f",
      "question": "If you are allergic to insects, you should carry what?",
      "prediction": "Answer: \"If you are allergic to insects, you should carry a antihistamine and an epinephrine auto-injector for emergencies, experts said.\"",
      "ground_truths": [
        "antihistamine"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "4ba7c330436f469b835be6ce879b08d6",
      "question": "How many people die in the U.S. from stings?",
      "prediction": "The answer to the question is 40.",
      "ground_truths": [
        "40"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "c4e4cc4c65dc49cb951576e334e7bf52",
      "question": "How many people die each year?",
      "prediction": "The answer is 50.",
      "ground_truths": [
        "At least 40"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7518999e54834c1389cf350e45975ed7",
      "question": "40 people in the U.S. die each year from what?",
      "prediction": "The answer is: 40.",
      "ground_truths": [
        "At least 40 people in the United States die each year as the result of insect stings,"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "af0cc33542ac4440a243c4807d87fd37",
      "question": "What should you carry if you are allergic to insects?",
      "prediction": "The answer is: Campers should carry an antihistamine and epinephrine auto-injector for emergencies.",
      "ground_truths": [
        "an antihistamine and an epinephrine auto-injector"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "41b01c0e5e764c59a65bfaace8628f0e",
      "question": "Who's death was declared a homicide?",
      "prediction": "The answer is: \"There is no concrete information about the death of Caylee Anthony, and her family is unaware of the final outcome of the investigation. The death was declared a homicide.",
      "ground_truths": [
        "Caylee Anthony,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "cfd17679a12d453f8449f8dacf19eef2",
      "question": "What did the medical examiner declare?",
      "prediction": "The medical examiner declared that Caylee Anthony was \"completely \"skeletonized.\"",
      "ground_truths": [
        "The cause of the child's death will be listed as homicide by undetermined means,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e907f75777b242b5b1ebbc4533d577c9",
      "question": "What was Casey Anthony notified about?",
      "prediction": "The answer is: Caylee Anthony was notified of the results of a DNA test, which confirmed she was dead.",
      "ground_truths": [
        "test results"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "cdbbd817090f476f8c805bc304c9ca46",
      "question": "What has Caylee's death been declared as?",
      "prediction": "Caylee Anthony's death has been declared as \"open wound in the community\" until trial.",
      "ground_truths": [
        "homicide"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a773d482def146618ecee4e25e39568a",
      "question": "What was found in the wooded area?",
      "prediction": "The answer is \"a small, bone-like mass of a child's skeleton, with no obvious trauma.\"",
      "ground_truths": [
        "\"significant skeletal remains\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "23ae0c6768224c3f92a336904773a74d",
      "question": "What dpes a woman organize for food allergy sufferers?",
      "prediction": "The answer is \"The Focus on Food and Drug Administration's (FADM) September 16 public hearing on food ingredient labels, specifically regarding the inclusion of milk, eggs, fish, shellfish, and peanuts in labeled foods, followed by a conference on anxieties about food allergies in restaurants.\"",
      "ground_truths": [
        "monthly meals"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6e7c609c8a824ca9bb3d4d55270d9441",
      "question": "Why are food allergies on the rise?",
      "prediction": "The answer is that, in recent times, food allergies are on the rise, and the answer is that people are actively trying to avoid them, and this is happening in many cities.",
      "ground_truths": [
        "no one is sure"
      ],
      "em": 0,
      "f1": 0.0606060606060606
    },
    {
      "id": "5c99e7f2414645c1a212ce119fc38ef8",
      "question": "How many Americans have food allergies?",
      "prediction": "The answer is 17 million.",
      "ground_truths": [
        "about 12 million in America,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "ac9462160d8c4827aa0820c4b03c441e",
      "question": "What kind of food allergies?",
      "prediction": "The answer is \"food allergy,\" as mentioned by the author.",
      "ground_truths": [
        "peanuts, nuts, shellfish and fish"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f7024ec6d9243b09541119fa1bfdb47",
      "question": "Who does she organize dinners for?",
      "prediction": "From the context, the answer is:",
      "ground_truths": [
        "people with food allergies."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9f572cc061d4613b4ff2c08addaf855",
      "question": "What is the leading food allergy?",
      "prediction": "The leading food allergy is yes, and it is currently in the list of the most common in the United States, which is not known.",
      "ground_truths": [
        "peanuts."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8de1e7f2f6a44cdd900cfcfc5ddbe016",
      "question": "What should you do before going to a restaurant if you have a food allergy?",
      "prediction": "The answer is: **confront the staff with special requests.**",
      "ground_truths": [
        "staff with special requests."
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "340d572d148d4c298fe8a75a61d1625f",
      "question": "who was arrested?",
      "prediction": "The answer is Iran's government's accusation.",
      "ground_truths": [
        "of the religious minority"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25dc8619c6e743be99772792c454db17",
      "question": "What did Bani Dugal say?",
      "prediction": "Bani Dugal stated, \"The allegations are not new, and the Iranian government knows well that they are untrue,\" and \"the documented plan of the Iranian government has always been to destroy the Baha'i community, and these latest arrests represent an intensification of this plan.\"",
      "ground_truths": [
        "\"The allegations are not new, and the Iranian government knows well that they are untrue,\""
      ],
      "em": 0,
      "f1": 0.5098039215686275
    },
    {
      "id": "4d0bbc08ef8a4c6984394b40103d47ec",
      "question": "The representative said the claim was what?",
      "prediction": "The representative's answer is \"utterly baseless.\"",
      "ground_truths": [
        "\"utterly baseless.\""
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "a19d2fabcf3a4dc29f67b43b120b932b",
      "question": "Bani Dugal said the government is trying to destroy who?",
      "prediction": "The answer is **Iran's government**.",
      "ground_truths": [
        "the Baha'i community,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b1f8415d12b4b6c8b6e8f180e50da20",
      "question": "Where were the minority leaders arrested?",
      "prediction": "The answer is: \"The Baha'i community, members of the International Community to the United Nations, called Iran's assertion 'utterly baseless' and the Islamic Republic News Agency's \"evidence of its persecution\" stated at Wednesday, news release issued by the Baha'i International Community to the United Nations.",
      "ground_truths": [
        "Iran's"
      ],
      "em": 0,
      "f1": 0.05128205128205127
    },
    {
      "id": "d0e9edf083cf46c58984f222be658296",
      "question": "What did Baha'i officials deny?",
      "prediction": "The answer to the question based on the context is: The Baha'i official denied that the six were held for security reasons, and because they were not religious.",
      "ground_truths": [
        "held for security reasons and not because of their faith."
      ],
      "em": 0,
      "f1": 0.42424242424242425
    },
    {
      "id": "0c163ddaaef945b7b3224f1a7c8eaac5",
      "question": "How many leaders were held for security reasons?",
      "prediction": "The answer is: Iran's government has accused the Baha'i community of being persecuted, and these are not new claims.",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2875b09841345acbc5c811ecc81947c",
      "question": "who is trying to destroy the community?",
      "prediction": "The answer is: Iranian government.",
      "ground_truths": [
        "President Mahmoud Ahmadinejad's government"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "261af3059f674136b75a9f1ffd6b0499",
      "question": "who denies nuclear program has military purposes?",
      "prediction": "Answer: Israel's vice prime minister, Silvan Shalom, denies that nuclear programs are intended for military purposes.",
      "ground_truths": [
        "Iran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21d5eb5ff6594bec8489a26dc30e5221",
      "question": "When did the vice prime minister speak?",
      "prediction": "The vice prime minister of Israel spoke at the site of a World War II death camp in Auschwitz.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab6ae284b7a3413791e69f7f5f368192",
      "question": "who speaks on Holocaust remembrance day?",
      "prediction": "The answer is: Israel.",
      "ground_truths": [
        "Silvan Shalom"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0760573d4a6644b5a9528bd111cc597a",
      "question": "what was iran's nuclear effort compared to?",
      "prediction": "Iran's nuclear effort compared to Nazi Germany was \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago.\"",
      "ground_truths": [
        "Nazi Germany"
      ],
      "em": 0,
      "f1": 0.14814814814814814
    },
    {
      "id": "04aba1b206a74b468046427c8ba664a9",
      "question": "What does Iran deny the nuclear program is to be used for?",
      "prediction": "Iran denies its nuclear program is intended for building bombs.",
      "ground_truths": [
        "denies its"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "74499298c6294de997f203b1415d21d4",
      "question": "What is Iran's nuclear effort being compared to?",
      "prediction": "Iran's nuclear effort is being compared to Hitler's efforts to acquire nuclear weapons, not to a bomb.",
      "ground_truths": [
        "Nazi Germany"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5504cb34682f415d83c81e33fbc13214",
      "question": "What does Iran deny?",
      "prediction": "Iran denies its nuclear program, but it states that Israel does not believe it.",
      "ground_truths": [
        "denies its nuclear program is aimed at building bombs,"
      ],
      "em": 0,
      "f1": 0.34782608695652173
    },
    {
      "id": "201020dcae5841dfacf83d089035f079",
      "question": "Who will speak on Holocaust remembrance day?",
      "prediction": "Answer:  The United States (US)",
      "ground_truths": [
        "Silvan Shalom"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30fa17868c604959b1bca6654cb7b6e2",
      "question": "What is the effort compared with?",
      "prediction": "The effort compared with what Hitler did to the Jewish people.",
      "ground_truths": [
        "Hitler did to the Jewish people just 65 years ago,\""
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "4c3cfd857b01453f9d193ba70e0f2257",
      "question": "Woods will make statement at PGA headquarters  in what city?",
      "prediction": "The answer is \"in New York City\" and not \"at the PGA headquarters in Ponte Vedra Beach, Florida.\"",
      "ground_truths": [
        "Ponte Vedra Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d082708c14574406931567132712d955",
      "question": "who is woods' wife?",
      "prediction": "According to the context, Woods' wife is Elin Nordegren.",
      "ground_truths": [
        "Elin Nordegren,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "873ea1af23e548efb10a54c97edcdb6b",
      "question": "What is Woods' wife name?",
      "prediction": "The answer is **Elin Nordegren**.",
      "ground_truths": [
        "Elin Nordegren,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2e71514053a447d5bbd3f9625420ca43",
      "question": "what will woods announce",
      "prediction": "Based on the provided text, the answer is:",
      "ground_truths": [
        "\"apologize for his behavior\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "415c375731994f69a116e415444abd99",
      "question": "what did steinberg state?",
      "prediction": "According to Steinberg, he stated that he would not answer questions, and he would focus on his apologies. He also did not know if his wife would appear at the PGA headquarters.",
      "ground_truths": [
        "Woods would not answer questions."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "1667240d3dd740ffa3c317106730a441",
      "question": "ho will not answer question?",
      "prediction": "The answer is yes, only True.",
      "ground_truths": [
        "Woods"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ffa9e00578043369a677dc8661bfc96",
      "question": "what does woods plan to discuss",
      "prediction": "The answer is: Woods plans to discuss his past and future, and he intends to apologize for his behavior.",
      "ground_truths": [
        "his past and his future"
      ],
      "em": 0,
      "f1": 0.4347826086956522
    },
    {
      "id": "d9f2855c8621457fb68bac161b7f811b",
      "question": "Will Woods will  answer questions?",
      "prediction": "The answer is: YES.",
      "ground_truths": [
        "would not"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bdb76a5fa0754cb68f9f2388fad40bca",
      "question": "Espacio Escult√≥rico has a collection of what items?",
      "prediction": "The answer to the question is: \"Espacio Escult√≥rico at the UNAM offers a collection of sculptures by famous Mexican artists.\"",
      "ground_truths": [
        "sculptures"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "cdaaca079e2b4eef92a36b61ffa527d1",
      "question": "Zinco bar is on what street?",
      "prediction": "Based on the context, the answer is **Zinco bar**, located on Cinco de Mayo Street in downtown Mexico City, Mexico City.",
      "ground_truths": [
        "Cinco de Mayo"
      ],
      "em": 0,
      "f1": 0.2727272727272727
    },
    {
      "id": "f3fbc3504f744c3f971dfd3ce6e0dcf8",
      "question": "What kind of music is played at the Zinco bar?",
      "prediction": "The answer to the question is \"Not really. Just stick to the tacos and tequila as if you're anywhere in Mexico City.\"",
      "ground_truths": [
        "jazz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68b543f7a3aa4839a39e514e4f56f83b",
      "question": "Zinco bar has great what kind of music?",
      "prediction": "Here's the answer based on the context:",
      "ground_truths": [
        "jazz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e5412d7cf02047c3949b854b18c01770",
      "question": "On what street is the Zinco bar?",
      "prediction": "The answer is, Mexico City is one of the world's important cultural centers, and it is one of the most populated cities in the world, and one of the richest cities in the world.",
      "ground_truths": [
        "Cinco de Mayo"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aaa26fff7be41698806089edb69a0b9",
      "question": "Who is filing for divorce?",
      "prediction": "The answer is Ronald Cummings, the father of Haleigh Cummings, and Misty Cummings, the last person known to have seen Haleigh.",
      "ground_truths": [
        "Ronald Cummings"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "6b27b26ced424638a3b9c75a67d6f49d",
      "question": "Who plans to file for divorce?",
      "prediction": "The answer is Ronald Cummings.",
      "ground_truths": [
        "Ronald Cummings"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2866afbb73c248e7aadb04390c57124d",
      "question": "Where in Florida did the girl go missing?",
      "prediction": "The answer is Florida.",
      "ground_truths": [
        "her father's home in Satsuma,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f08712b06d7408189a0e846377feb85",
      "question": "Who went missing in Florida?",
      "prediction": "The answer is Misty Cummings.",
      "ground_truths": [
        "Haleigh Cummings,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "977a85dceaff4e9995034d133961a1ce",
      "question": "What do investigators think?",
      "prediction": "The answer is: **Investigators think that Misty Cummings continues to hold important answers in the case.**",
      "ground_truths": [
        "not feel Misty Cummings has told them everything she knows."
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "af5a0954d5ee4800bfc755fed87274db",
      "question": "What do investigators say?",
      "prediction": "The answer is: investigators say they don't feel Misty Cummings told them everything she knows.",
      "ground_truths": [
        "they"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "20fa1f36b57b47da812735a529e4117d",
      "question": "Who went missing?",
      "prediction": "The answer is Misty Cummings.",
      "ground_truths": [
        "Haleigh Cummings,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "d274da2cc20d4759b02ef68cab5a2b5c",
      "question": "Who is Haleigh Cummings?",
      "prediction": "Answer: Haleigh Cummings is the father of Haleigh Cummings, a Florida girl who disappeared in February.",
      "ground_truths": [
        "a Florida girl"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "497147b475924c7381b92e475b7de2c8",
      "question": "Who is Misty Cummings?",
      "prediction": "The answer is Misty Cummings.",
      "ground_truths": [
        "his wife,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d14218e29634731bab87948106fd070",
      "question": "Who pays for all their clothes?",
      "prediction": "The answer is, all the clothing from both Michelle Obama's and Grace Brown's outfits, and the cost of clothing, respectively. The answer is all of these.",
      "ground_truths": [
        "Both women"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "97c580ecb105499e9fd68b24fb82db8b",
      "question": "Who wore J.Crew at the England summit?",
      "prediction": "The answer is **j.crew**.",
      "ground_truths": [
        "Michelle Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e821c5211ebf41148fd5e9db16e605d0",
      "question": "Who is Gordon Bronw's wife?",
      "prediction": "The answer is Sarah Brown.",
      "ground_truths": [
        "Sarah,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "ba173ea4f5ba499585c7ccd85a75e3ca",
      "question": "Whose wife is Sarah?",
      "prediction": "The answer is **Michelle Obama**.",
      "ground_truths": [
        "British Prime Minister Gordon Brown's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88822c4e2acc4ea4a5f713386de7ded3",
      "question": "Who wore J. Crew?",
      "prediction": "The answer is: Buttons, the American-born Lintner, a spokeswoman for 10 Downing Street.",
      "ground_truths": [
        "Michelle Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "decd6f636da04b8ca01d4353ece9323e",
      "question": "What did Michelle Obama wera?",
      "prediction": "The answer is, she wore a J.Crew dress, a $158 green skirt and $298 bead and rhinestone cardigan, and a pair of Astley Clarke earrings, according to a spokesperson for 10 Downing Street.",
      "ground_truths": [
        "J.Crew,"
      ],
      "em": 0,
      "f1": 0.0689655172413793
    },
    {
      "id": "c38b1d313cba4a1c9dd244d09624945a",
      "question": "What type of clothes does Sarah wear to greet dignitaries?",
      "prediction": "Sarah Brown, Michelle Obama, and J.Crew.",
      "ground_truths": [
        "outfit from designer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "074bf4298dbc432dbe7989c913948a6c",
      "question": "What does Michelle Obama wear?",
      "prediction": "Michelle Obama's outfit is described as \"a pair of J.Crew, $158 green skirt and $298 bead and rhinestone cardigan,\" with a $158 green skirt and $298 bead and rhinestone cardigan.",
      "ground_truths": [
        "J.Crew,"
      ],
      "em": 0,
      "f1": 0.0689655172413793
    },
    {
      "id": "9e24cdb1d7df47b2ae96758ad816aab7",
      "question": "How much is the course?",
      "prediction": "The answer is:  \"The course is a year-round, full-day program at Sedona Spirit Yoga & Hiking, offering all-inclusive, outdoor-focused yoga hikes through red rock wilderness, Yellowstone territory, and the vibrant red rocks of Sedona. It is offered at four-day retreats at four-night lodges in Sedona, Arizona.\"",
      "ground_truths": [
        "$75 for full-day class,"
      ],
      "em": 0,
      "f1": 0.04444444444444444
    },
    {
      "id": "c63aa88af92d4f87b216f060cad550a7",
      "question": "where in mexico it offers cooking classes Seasons of My Heart",
      "prediction": "The answer is Sedona, Arizona.",
      "ground_truths": [
        "school in the Oaxacan countryside of southern"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1139bdf97f14af2b46dee7b9a3c52fa",
      "question": "Where is the school located?",
      "prediction": "The answer is **Sedona, Arizona**.",
      "ground_truths": [
        "in the Oaxacan countryside of southern Mexico"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33e668576ef44beb833c68a31b422aab",
      "question": "that place offers Mexican cooking classes in Oaxaca",
      "prediction": "The answer is:  Sedona Spirit Yoga & Hiking offers four-day retreats in Sedona, Arizona, where participants will learn about Sedona's unique spiritual and natural activities, including hiking, yoga, and meditation, and are encouraged to explore the red rocks of Sedona to find their \"path in Arizona.\" They also provides lodging and breakfast, including vegan and raw options in a red-tiled, and offer a bridge-out class for 30 hours. The answer is at the Red Rocks, in Sedona, Arizona, a town steeped in Native American culture, with a focus on the \"path to enlightenment.\"",
      "ground_truths": [
        "Seasons of My Heart,"
      ],
      "em": 0,
      "f1": 0.02298850574712644
    },
    {
      "id": "bdd5472a6ccc4d3c92a719454775639d",
      "question": "Who was recognized as a Top 10 CNN Hero of 2009?",
      "prediction": "The answer is: **Forester.**",
      "ground_truths": [
        "Roy Foster's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de6f95a86fe94cc0963cb044ffef3523",
      "question": "was Roy Foster recognized as a Top 10 CNN Hero of 2009?",
      "prediction": "The answer is: yes, Roy Foster was recognized as a Top 10 CNN Hero of 2009 for his efforts to help male veterans with homelessness and addiction.",
      "ground_truths": [
        "at \"CNN Heroes: An All-Star Tribute\" as a"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "e80f59eb7ee4467ea2a693cc03f7fedd",
      "question": "Who did the organization expand to help?",
      "prediction": "The organization expanded to help veterans and their families in Palm Beach, Florida.",
      "ground_truths": [
        "women."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83374abdcff843938dfe31e262431577",
      "question": "What do police say about it?",
      "prediction": "The police say that \"The Angels family has suffered a tremendous loss today,\" and \"We are deeply saddened and shocked by this tragic loss.\"",
      "ground_truths": [
        "a red minivan ran a red light and struck two vehicles at an intersection,"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "5d85c0bd310045578980993d8fe218ab",
      "question": "Where was the fatal crash?",
      "prediction": "The fatal crash occurred at UC Irvine Medical Center in Fullerton, California, in the afternoon of Thursday, August 16th.",
      "ground_truths": [
        "Fullerton, California,"
      ],
      "em": 0,
      "f1": 0.21052631578947367
    },
    {
      "id": "054f3f1eb5554431a707e3f984c4364e",
      "question": "what team is he on?",
      "prediction": "The team is on the Angels.",
      "ground_truths": [
        "Los Angeles Angels"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "55a60815fccf47c2826b21d56c3a1923",
      "question": "Who pitched six scoreless innings?",
      "prediction": "The team pitched six scoreless innings in their fourth MLB start.",
      "ground_truths": [
        "Nick Adenhart"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e96066c9b99a4c9c8f6e37cc96a91e8d",
      "question": "Where was the venue of the crash?",
      "prediction": "The answer is Los Angeles, California.",
      "ground_truths": [
        "Fullerton, California,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "300da5c6b3624ba18d2945c0a64130ea",
      "question": "what was the score?",
      "prediction": "The score is **6-4**.",
      "ground_truths": [
        "scoreless six innings,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86da104406044f35822517a0c9c1b1ba",
      "question": "what are Americans, South Africans investigating?",
      "prediction": "American",
      "ground_truths": [
        "allegations that a dorm parent mistreated students at the school."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0445da1d201e4e53af90522a804d3c73",
      "question": "Dorm parent accused abuse where at?",
      "prediction": "The answer is \"Nostalgia,\" as it is described by Hine.",
      "ground_truths": [
        "school"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86078e9a4b7f4384a221fa81c4fca7b6",
      "question": "where has dorm parent been accused of abuse?",
      "prediction": "The answer is \"the dormitory parent did not treat the girls the way she likes,\" adding that the investigation was completed after a team of American experts hired by Winfrey, including Robert Farley, determined that the dormitory parent had not treated the girls as they did.",
      "ground_truths": [
        "school in South Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "569e3465485f4597a4aadeaf1d8b4379",
      "question": "Who are investigation the allegations?",
      "prediction": "The investigations conducted at the school were officially opened after a team of American experts, including Robert Farley, had previously issued a statement.",
      "ground_truths": [
        "South African police"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2142d71d782c4fc784cf0e3fe37758e0",
      "question": "What did one parent call Winfrey?",
      "prediction": "The answer is: \"Hine's children's status as the \"poest of the poor\" was clear-cut: Hine supports the five people who lived in her small two-bedroom Soweto home on the $50 a week, and Hine also cares for two other orphans as well as Palesa and Alebohang.\"",
      "ground_truths": [
        "\"Oprah is an angel, she is God-sent,\""
      ],
      "em": 0,
      "f1": 0.04347826086956522
    },
    {
      "id": "d4e76b13326f4512a6e0c8b7c7552a8d",
      "question": "what does one mother say about Winfrey?",
      "prediction": "One mother says the talk-show host heaven-sent, despite allegations of abuse, is God-sent.",
      "ground_truths": [
        "\"Oprah is an angel, she is God-sent,\""
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "db15047ec8e348f18e46a99e55e3ddec",
      "question": "What did one parent claim was going on at the school?",
      "prediction": "The answer is Oprah Winfrey.",
      "ground_truths": [
        "abuse"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a30b126060c4496aaae5e5ae4c37976",
      "question": "What did the judge order?",
      "prediction": "The judge ordered the release of the four men, Jesus Ortiz, Stalin Felipe, Kevin Taveras, and Rondell Bedward.",
      "ground_truths": [
        "the release of the four men"
      ],
      "em": 0,
      "f1": 0.4210526315789474
    },
    {
      "id": "3073f61a2dcf4d5c9032ae4dcc47f8c1",
      "question": "What was the judge's verdict?",
      "prediction": "The judge's verdict was dismissed.",
      "ground_truths": [
        "dismissed all charges"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "8a87319c8e2f48808dd4c3934cf0c53e",
      "question": "What will the DA do?",
      "prediction": "The DA will act on the charges of Jesus Ortiz, Stalin Felipe, Kevin Taveras, and Rondell Bedward, and will also release the release of the New York metropolitan area's officers.",
      "ground_truths": [
        "her office has launched a criminal investigation into the statements and reports given by the woman."
      ],
      "em": 0,
      "f1": 0.05128205128205129
    },
    {
      "id": "8d39120fa12b4f63a63e4a352b43a18e",
      "question": "What did the judge do?",
      "prediction": "The judge did not make any decisions.",
      "ground_truths": [
        "dismissed all charges"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dcc6d8c18da44839a470a13ab7556100",
      "question": "What did the Hofstra student say?",
      "prediction": "The Hofstra student said that she had been lured from a club, forced into a men's bathroom at a university dormitory, assaulted, and then called the university public safety office, which alerted local police.",
      "ground_truths": [
        "claims that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0.2926829268292683
    },
    {
      "id": "6f4d684ebab5450da56c94bf1f930cd7",
      "question": "What did the student claim?",
      "prediction": "The student claimed that she was lured from a club, forced into a men's bathroom at a university dormitory, assaulted, and then called the university public safety office.",
      "ground_truths": [
        "that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0.411764705882353
    },
    {
      "id": "e62b6e9dfead44c3881da06f669aa294",
      "question": "What were investigators told?",
      "prediction": "The answers to the questions in the context are:",
      "ground_truths": [
        "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19e435b52d654888be8a2dfa6b343e64",
      "question": "What is the Hofstra student claiming?",
      "prediction": "The Hofstra University student claims that she was lured to a dorm, assaulted in a bathroom stall, and then confessed that she was consensual.",
      "ground_truths": [
        "that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0.6875000000000001
    },
    {
      "id": "cc436260a2524c7097fd044bbc5b40c6",
      "question": "What did the woman tell investigators?",
      "prediction": "The woman told investigators that she was lured from a club, forced into a men's bathroom at a university dormitory, assaulted, and then called the university public safety office, which alerted local police.",
      "ground_truths": [
        "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted."
      ],
      "em": 0,
      "f1": 0.5909090909090908
    },
    {
      "id": "bdbc4d71d79846079ed0526ba43af217",
      "question": "What is Mohammed al-Qahtani  accused of?",
      "prediction": "The answer is:  Mohammed al-Qahtani was accused of helping to plan the September 11, 2001 attacks, and possibly seeking to participate.",
      "ground_truths": [
        "helping to plan the September 11, 2001, terror attacks,"
      ],
      "em": 0,
      "f1": 0.5185185185185185
    },
    {
      "id": "6c98a5be3830481da066866354dee5fe",
      "question": "What did the judge refuse to do?",
      "prediction": "The judge at Guantanamo Bay, Cuba, refused to refer the case of Mohammed al-Qahtani to prosecutors because of the assessment of torture.",
      "ground_truths": [
        "to refer the case of Mohammed al-Qahtani to prosecutors"
      ],
      "em": 0,
      "f1": 0.5925925925925926
    },
    {
      "id": "1b391680d94f4505905a8d575648f3d9",
      "question": "Which paper had the story?",
      "prediction": "The paper had the story of Mohammed al-Qahtani, a Saudi national, who was convicted of torture at Guantanamo Bay, Cuba.",
      "ground_truths": [
        "Washington Post"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13f53b184a1d491ca79ab04591a29369",
      "question": "What was the White houses response?",
      "prediction": "The White House's response was to state that it has no policy to torture al-Qahtani.",
      "ground_truths": [
        "\"It has never been the policy of this president or this administration to torture."
      ],
      "em": 0,
      "f1": 0.3703703703703704
    },
    {
      "id": "c40eca727a4340e4b4e66130b55e3704",
      "question": "What did Crawford say?",
      "prediction": "The answer is, \"We tortured (Mohammed al-) Qahtani. His treatment met the legal definition of torture. And that's why I did not refer the case for prosecution.\"",
      "ground_truths": [
        "\"We tortured (Mohammed al-) Qahtani,\""
      ],
      "em": 0,
      "f1": 0.3448275862068966
    },
    {
      "id": "783dc9ff141f43229e359bd53b0f7f22",
      "question": "What did the judge do?",
      "prediction": "The judge at Guantanamo Bay, Cuba, did not refer the case of Mohammed al-Qahtani to prosecutors because of this assessment.",
      "ground_truths": [
        "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,"
      ],
      "em": 0,
      "f1": 0.6451612903225806
    },
    {
      "id": "f5f1913991544a3db364369d36b14e47",
      "question": "What did Susan Crawford say?",
      "prediction": "Susan Crawford's answer, based on the context, is \"We tortured (Mohammed al-) Qahtani, and that is why I did not refer the case for prosecution.\"",
      "ground_truths": [
        "\"We tortured (Mohammed al-) Qahtani,\""
      ],
      "em": 0,
      "f1": 0.3571428571428571
    },
    {
      "id": "457356915ea04301a7384118b59ea88d",
      "question": "What was al-Qahtani accused of?",
      "prediction": "The answer to the question is: \"Al-Qahtani was accused of helping to plan the September 11, 2001 attacks and participating, potentially as the 20th hijacker. The U.S. had sought the death penalty for him and other men.",
      "ground_truths": [
        "helping to plan the September 11, 2001,"
      ],
      "em": 0,
      "f1": 0.3243243243243243
    },
    {
      "id": "21a75de34c68413c9e418decc5c71d2f",
      "question": "What items were thrown?",
      "prediction": "The answer is: \"The mood is charged with a mixture of fear, anger, and a sense of oppression, with the militants, the government, and the military openly expressing their anger, frustration, and a degree of cooperation with the anti-government protests. The fire at the Tahrar (a large government building) in Cairo, and the use of Molotov cocktails, rocks, and glass, all of which was likely a part of the military's crackdown on similar protests, have also contributed to this.",
      "ground_truths": [
        "Molotov cocktails, rocks and glass."
      ],
      "em": 0,
      "f1": 0.14492753623188406
    },
    {
      "id": "ee7adf8b775344429dbd224b9e06c7d3",
      "question": "how many people died",
      "prediction": "The answer is \"at least 4,\" and the context explicitly states that \"six people were killed and at least 300 were injured.\"",
      "ground_truths": [
        "Six"
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "e09dea004dc14ca394d37b263e129ac3",
      "question": "what was thrown?",
      "prediction": "The answer is: \"A separate team was at the morgues, followed by people who were thrown Molotov cocktails and glass,\" while \"A Ministry of Transportation building was burned nearby and tents in a protest encampment caught fire.\"",
      "ground_truths": [
        "Molotov cocktails, rocks and glass."
      ],
      "em": 0,
      "f1": 0.21621621621621623
    },
    {
      "id": "715592c05e2a41b8a221df4f7b12274b",
      "question": "Where did it happen?",
      "prediction": "The answer is Cairo.",
      "ground_truths": [
        "central Cairo,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "61df2ca95986481da92fc106c8a6a1f0",
      "question": "What caused the violence",
      "prediction": "The provided text does not contain the answer to the question. The text only provides a list of the final statements of the involved individuals and their previous statements. It does not answer the question in the real-time. It only tells the actions of the people that were involved.",
      "ground_truths": [
        "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\""
      ],
      "em": 0,
      "f1": 0.09999999999999999
    },
    {
      "id": "dace90eb46944bc7aa77627525f618d9",
      "question": "How many people were injured?",
      "prediction": "Based on the provided text, the answer is: Cairo.",
      "ground_truths": [
        "at least 300"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6ee51c48856447ebba5d4192903e56a",
      "question": "What sparked the violence?",
      "prediction": "The context indicates that Cairo (CNN) experienced significant violence, including the deaths of six people and the use of gun fire, which was accompanied by a separate team at the morgues. The national emergency room was also burned. It was also stated that the military's actions were a warning against the use of projectiles and water cannons.",
      "ground_truths": [
        "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\""
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "6e3f97b8799e442991ae2ec7b7090a71",
      "question": "How many were injured",
      "prediction": "Based on the provided text, the answer is:",
      "ground_truths": [
        "at least 300"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "918d39163c934fb89811db623983a76c",
      "question": "How many died?",
      "prediction": "Based on the provided text, the answer is: Cairo.",
      "ground_truths": [
        "Six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a766d005b5c240d1af21ccf7a2d0d0f1",
      "question": "What says only 9 percent of Turks had positive view of U.S. in 2007 ?",
      "prediction": "The answer is \"No, not all Turks had positive view of the US in 2007.\"",
      "ground_truths": [
        "Pew Research Center"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1411ab49be5d4637a1e2a46c8ea6a69e",
      "question": "What percentage of Turks had positive view of the US in 2007?",
      "prediction": "The question asks for the percentage of Turkish citizens who had positive view of the United States in 2007.",
      "ground_truths": [
        "9 percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cc4aa5f329344e8919635f28f2ca39b",
      "question": "When did Obama go to Turkey?",
      "prediction": "The answer is based on the context provided.",
      "ground_truths": [
        "April."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04fef76641e3400ba7c0c83195153a1c",
      "question": "President Obama tried to change what in April ?",
      "prediction": "Based on the provided text, the answer is **\"The United States is not and will never be at war with Islam.\"** This is a core statement of the President's actions and actions taken in recent years, and the question \"Did Obama's new brand of diplomacy work?\" requires a full answer to be answered.",
      "ground_truths": [
        "public opinion in Turkey.\""
      ],
      "em": 0,
      "f1": 0.04
    },
    {
      "id": "c6d9da1ec8ea4a86a2ff76bf9bc1855b",
      "question": "What was the main reason Obama`s visit to Turkey?",
      "prediction": "Based on the context, the answer is: **\"At the moment, there are strong indications that words alone won't be sufficient to transform years of deep suspicion into a long-term, positive change in U.S.-Turkish relations.**",
      "ground_truths": [
        "to remedy the situation of America wielding a big stick for the last eight years.\""
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "94a79e4abb8a4b2ebfec7589ba777553",
      "question": "What is percentage of Turks being positive towards U.S.?",
      "prediction": "The answer is: 43% of Turks.",
      "ground_truths": [
        "9"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd467f3a57e2462388848bfd1a495c4b",
      "question": "where has the power being restored?",
      "prediction": "The answer is Fargo, North Dakota.",
      "ground_truths": [
        "Fargo, North Dakota,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "a4979cd79024479abb28834bc6104114",
      "question": "How many sandbags did the National Guard fill?",
      "prediction": "The answer is 700,000.",
      "ground_truths": [
        "a million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5ba0e7bd271342958ca73b52e8358c05",
      "question": "When did the River expect to crest?",
      "prediction": "The answer is: The city of Fargo, North Dakota, largely completed flood preparations early Thursday as a surging Red River rose nearly 3 feet in 24 hours.",
      "ground_truths": [
        "this week,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd9d280b011a4bfb8e128428c0ab216c",
      "question": "how many sand bags did the national guard fill?",
      "prediction": "The answer is 700,000.",
      "ground_truths": [
        "a million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a0c35d82c8a40a0aac26bc6f94d00da",
      "question": "The river expected to crest at what height on Sunday?",
      "prediction": "The answer is: Fargo, North Dakota.",
      "ground_truths": [
        "38 feet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec3dcf0931384721a5e7a6525b32ae0a",
      "question": "What does resident say?",
      "prediction": "In Fargo, North Dakota, the city's flood preparations were made early Thursday, as a surging Red River rose nearly 3 feet in 24 hours.",
      "ground_truths": [
        "\"Otherwise I believe we would have washed out a considerable amount of our town,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a461bec973b42b6b3e35a0dadd8dbdc",
      "question": "What did the volunteers do?",
      "prediction": "The volunteers worked with the National Guard in Fargo to fill sandbags, and they were also put in place in place.",
      "ground_truths": [
        "fill a million sandbags and place 700,000 around our city,\""
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "90c0b9ccd0c24d71a26ab4e175bca802",
      "question": "To what does Mandisa credit her weight loss?",
      "prediction": "Mandisa Hundley's weight loss is listed as 75 pounds.",
      "ground_truths": [
        "faith in God"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fd5919517f14c139d3c092367a33f5d",
      "question": "Amount of weight that Mandisa lost?",
      "prediction": "Mandisa Hundley lost 75 pounds, and her new album, \"Freedom,\" was released March 24th, after a significant loss.",
      "ground_truths": [
        "more than 75 pounds."
      ],
      "em": 0,
      "f1": 0.19047619047619047
    },
    {
      "id": "59795dd9895a4538bfb6a31e427ee527",
      "question": "When is the album due out>",
      "prediction": "The answer is, it's not until March 24th.",
      "ground_truths": [
        "March 24,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "c4f891cfa4e24a9f8c46748442169c71",
      "question": "What kind of music does Mandisa sing?",
      "prediction": "Mandisa Hundley's new album, \"Freedom,\" is a Christian pop song. It's called \"True Beauty,\" and it's full of catchy Christian pop tunes. It's said it's a testimony to her faith, and it's a personal story about her weight loss.",
      "ground_truths": [
        "catchy Christian pop tunes"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "5156a5e67de04c78a1a43f5ee9dd4347",
      "question": "When is Mandisa's second studio album out?",
      "prediction": "Mandisa Hundley's second studio album, \"Freedom,\" is out on March 24, 2024.",
      "ground_truths": [
        "March 24,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "38bc152a0b8e433084e206fbb785e239",
      "question": "When was Phagan murdered?",
      "prediction": "The answer to the question is: \"The story goes that no one in my family talked about it,\"",
      "ground_truths": [
        "April 26, 1913,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "565ba022cac0410e9e1ed653c54a25c7",
      "question": "Who was convicted of her murder",
      "prediction": "The answer is: **The Governor of Atlanta, and his administration.**",
      "ground_truths": [
        "Leo Frank,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe2e9d3e09e54b969381827d1eddd4ff",
      "question": "Who was murdered in 1913?",
      "prediction": "The answer is: \"Mary Phagan Kean was murdered in 1913.\"",
      "ground_truths": [
        "Mary Phagan"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "63dba556c6ea430890e1549f442c456e",
      "question": "What year was Mary Phagan's murder",
      "prediction": "The answer is: \"The story goes that no one in my family talked about it,\" as Smithline said.",
      "ground_truths": [
        "1913,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c3f78e8e52f4dd988ad0b7f4aaa9e3b",
      "question": "Who was convicted?",
      "prediction": "The answer is \"The governor commuted his sentence in 1915, about 25 men abducted Frank, 31, from the state prison and hung him from a tree in Marietta, Georgia.",
      "ground_truths": [
        "Leo Frank,"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "fef9627cdad84ad8ab8e138012e3e0ca",
      "question": "What hot buttons did the case hit",
      "prediction": "The answer is \"We will never talk about this again,\" and the concluding sentence, \"We will never talk about this again,\" suggests that the answer is a possible \"family embarrassment\" and \"confidentiality\" based on the initial information.",
      "ground_truths": [
        "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21dbbe66a7b649ea945b8b3d2a717063",
      "question": "How long was the boy with autism left in a cubicle?",
      "prediction": "The answer is: Garrett Peck's cubicle was left with a magical marker.",
      "ground_truths": [
        "at least two and a half hours."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41b02a0a6bcb4004ae5396399dd3395f",
      "question": "What was the outcome of the practices?",
      "prediction": "The outcome of the findings was that the GAO report found that special-needs students in U.S. schools are often subjected to policies of restraint, which results in lethal consequences for these students. It is concluded that, in most states, special-need children are isolated for hours at a time, and, in some cases, that the policy of isolation, and the resulting consequences are used to compel the students to follow instructions. In those cases, the consequences are often described as \"killer punishment\" by teachers, and parents report that when the teacher had a complaint, the child became unable to express her frustration. The GAO's findings",
      "ground_truths": [
        "some deaths"
      ],
      "em": 0,
      "f1": 0.021276595744680847
    },
    {
      "id": "018cfb1991114779a856fcf97c8de8be",
      "question": "What report uncovered information on the discipline of children?",
      "prediction": "The report uncovered widespread abuse of special-education students in the U.S. schools, including cases of children being held face-down on the ground.",
      "ground_truths": [
        "Government Accountability Office"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ee8dc50324b435c8634db82fc95f921",
      "question": "For what is Buster Keaton saved by in \"Steamboat Bill Jr\"?",
      "prediction": "Based on the context, the answer is **Steamboat Bill Jr.**.",
      "ground_truths": [
        "an open window"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5088d10d4c834617b64b3b952a76df5f",
      "question": "What is Buster Keaton saved by in \"Steamboat Bill Jr.\"?",
      "prediction": "The answer is:  \"Never.\"",
      "ground_truths": [
        "open window"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fef813f4b22a44e6a416d35ae878afd8",
      "question": "Where are they choosing the best stunts?",
      "prediction": "The answer to the question is \"SoundOff Box.\"",
      "ground_truths": [
        "\"The Screening Room\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4cbbf897c5a4ae58c7a3c50c0d257ec",
      "question": "Which Keaton film was chosen?",
      "prediction": "The answer is: \"Steamboat Bill, Jr.\" (Charles Reisner)",
      "ground_truths": [
        "\"Steamboat Bill, Jr.\""
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "59566aea9daa470f97c3d205d76f4a39",
      "question": "What does the Screening Room choose?",
      "prediction": "The Screening Room chooses to choose a stunt double between Terry Leonard and Jake Canutt.",
      "ground_truths": [
        "the best stunts ever pulled off"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e8c9439ad7b44b493274c5859920620",
      "question": "Which James Bond film was chosen?",
      "prediction": "The answer is \"Sharky's Machine\".",
      "ground_truths": [
        "\"GoldenEye\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18fc2fa9f7a24777954eab16627d62dd",
      "question": "You could say bungee jumping is an art form thanks to whom?",
      "prediction": "The answer is \"I am not able to provide a definitive answer to this question, as I am not able to provide a single, definitive answer. It is a question that is not asked with a single answer, as it asks for the answer. Therefore, I would not try to give a definitive answer.",
      "ground_truths": [
        "Stuntman: Wayne Michaels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a8f8b27573d46b7bd7256a603268a87",
      "question": "Who met at white house?",
      "prediction": "The answer is **Brazil**.",
      "ground_truths": [
        "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d24662abff4b4fcc8032254ae8e9b527",
      "question": "What did the CIA memo say about the Brazilian general?",
      "prediction": "The CIA memo said, \"Brazil will be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind. It is unlikely that Brazil will openly intervene in its internal affairs, but the regime will not be above using the threat of intervention or tools of diplomacy and covert action to oppose leftist regimes, to keep friendly governments in office, or help place them there.\"",
      "ground_truths": [
        "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\""
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "8c651c2134a6460db75b7805f0b7badf",
      "question": "When were the documents declassified?",
      "prediction": "The answer is **Washington (CNN)**.",
      "ground_truths": [
        "in July"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba0dc5621c94472a9de2afe8c0c3cc7b",
      "question": "How many leaders met at the White House?",
      "prediction": "The answer is: Not explicitly stated in the context, but the \"declassified document\" mentioned in the \"National Security Archive\" memorandum, which describes the high-level discussions at the White House during the Nixon-Medici meeting of the two leaders, does not specify how many leaders were met.",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0.052631578947368425
    },
    {
      "id": "a10302d3d03a4562827d729c1284abc7",
      "question": "What did Nixon offer?",
      "prediction": "Nixon offered money or other discreet aid for the effort if it could be made available, the document shows.",
      "ground_truths": [
        "money or other discreet aid"
      ],
      "em": 0,
      "f1": 0.45454545454545453
    },
    {
      "id": "64307f80a70740eca2071c3fa6ac7e4a",
      "question": "What did the U.S. want Brazil to do?",
      "prediction": "The U.S. wanted Brazil to do what?",
      "ground_truths": [
        "overthrow the socialist government of Salvador Allende in Chile,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b46e3738dddb49d3b21ad3bdb678df23",
      "question": "What does document show?",
      "prediction": "Document showing, \"The declassified document, a previously top secret memo for Nixon's file written by National Security Adviser Henry Kissinger, detailed the possibility of a coup by the Chilean military with assistance from Brazilian military officers when Nixon said that it was \"very important that Brazil and the United States work closely in this field.\" It also discussed the creation of a back channel for direct communication outside normal diplomatic protocols.",
      "ground_truths": [
        "Nixon offered money or other discreet aid for the effort if it could be made available,"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "848250b6500548f9a8ff55cf6f657c45",
      "question": "Who believe Damas boarded a flight?",
      "prediction": "The answer is: \"I can't say for certain, but I'm not sure I can provide an exact answer.\"",
      "ground_truths": [
        "authorities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60e90f8b066f4921897a71ce5031b5ef",
      "question": "What day were the bodies found?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "870f2e0c2b614c81b6cdf70cddc89a1d",
      "question": "What did the sheriff say there was?",
      "prediction": "The sheriff's department, Capt. Chris Roberts, stated that they have recovered Mesac Damas' vehicle at Miami International Airport, and they believe he left on a flight to Haiti on Friday. He also mentioned that he collected and processed evidence for the potential to be found in a database.",
      "ground_truths": [
        "any indication of an individual out in the neighborhoods committing additional crimes or homicides,"
      ],
      "em": 0,
      "f1": 0.03571428571428572
    },
    {
      "id": "ade061d2af09443ca7431c42dc6d8bcd",
      "question": "What is Mesac Damas' age?",
      "prediction": "The answer is: Mesac Damas is 32 years old.",
      "ground_truths": [
        "33-year-old"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1ae542670bf40bfb26367e761d2acfa",
      "question": "On what day were the bodies found?",
      "prediction": "The context provided does not give me enough information to definitively answer the question. The answer is based on the information provided:",
      "ground_truths": [
        "Saturday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "56d7ce14a3d640b3ac4df6f37f97e668",
      "question": "Where were the bodies found?",
      "prediction": "The answer is: \"In Haiti, Macaeba.\"",
      "ground_truths": [
        "Naples home."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85fab2583c0348308eb98aa738583563",
      "question": "Who do police believe boarded a flight to Haiti on Friday morning?",
      "prediction": "The answer is \"we will look at every opportunity\" based on the context provided. The answer is directly stated.",
      "ground_truths": [
        "Mesac Damas,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afdf027bc3b546eca6940aebedfd3718",
      "question": "Where did Mesac Damas board a flight to?",
      "prediction": "The answer is:  The answers are:",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "043896b56ef744daa11eb4c6857bed35",
      "question": "Who could be reinstated?",
      "prediction": "The answer is the U.S. and many other nations.",
      "ground_truths": [
        "Honduran President Jose Manuel Zelaya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70ac70404d9342a58cf771326205dfa2",
      "question": "Did the negotiators reach a deal?",
      "prediction": "The answer to the question is: Yes, the negotiators reached a deal.",
      "ground_truths": [
        "reached an agreement late Thursday to form a government of national reconciliation."
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "c3101f302e714d2881aded1522335b6c",
      "question": "Who is Manuel Zelaya replaced with?",
      "prediction": "The answer is Manuel Zelaya.",
      "ground_truths": [
        "Roberto Micheletti,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cc2d08fa347241ec99e8c7120b839fde",
      "question": "For whom do the negotiators work?",
      "prediction": "The answer is the U.S. and many other nations.",
      "ground_truths": [
        "Zelaya and Roberto Micheletti,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "2451b810994d4f1a9a28cc97350262e4",
      "question": "Manuel Zelaya is the president of what country?",
      "prediction": "The answer is the U.S. The United States.",
      "ground_truths": [
        "Honduran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8ff7b13f4b943d89f09cf03cbde580a",
      "question": "which nations are at odds",
      "prediction": "Answers are as follows:",
      "ground_truths": [
        "British"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "441f524567e9477a9b0d07f4a0907622",
      "question": "what does desire say",
      "prediction": "Desire Petroleum says \"oil may be present in thin intervals but that reservoir quality is poor.\"",
      "ground_truths": [
        "\"oil may be present in thin intervals but that reservoir quality is poor.\""
      ],
      "em": 0,
      "f1": 0.896551724137931
    },
    {
      "id": "40d56efbcf5041ff9ab958e865e63171",
      "question": "What quality is the reservoir?",
      "prediction": "The answer is \"poor\".",
      "ground_truths": [
        "poor.\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e296d418c6614640b45afe738e0fe7ce",
      "question": "What is causing tension between the UK and Argentina?",
      "prediction": "The tension between the UK and Argentina is caused by the Argentine government's commitment to sovereignty, the Falklands, and the Argentine position as a \"national defence\" of the interests of Argentineans, not just the islands.",
      "ground_truths": [
        "gas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "412a163387474317b4eb02196506d1f4",
      "question": "What did the Falklands government say?",
      "prediction": "The Falklands government said that \"natural resources should be protected, and Britain must accept international resolutions labeling the Falklands as a disputed area.\"",
      "ground_truths": [
        "\"This has to do with the defense of the interests of Argentineans, not just about sovereignty,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2818d5e47a844bc9831496e2f587a803",
      "question": "Argentina is at odds with who over ownership of South Atlantic islands?",
      "prediction": "Argentina is at odds with who over ownership of South Atlantic islands.",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "435e49b5d4474366b64f5e29ee0c3f63",
      "question": "Who was accused of firing on Indian forces?",
      "prediction": "The answer is **The Indian Army**.",
      "ground_truths": [
        "separatist militants"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6bc25fd1669d435fbb8b7d5fb1f08bb0",
      "question": "What is the death toll in the fighting in Kashmir?",
      "prediction": "The death toll in Kashmir is at least 25.",
      "ground_truths": [
        "25 dead"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "5b31bab9ac6d462a8c65f38942f63649",
      "question": "What type of campaign has Kashmir been involved in?",
      "prediction": "The answer is \"the throes of a violent separatist campaign for nearly two decades during which authorities say 43,000 people have been killed.\"",
      "ground_truths": [
        "violent separatist"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "fa2a50d9d16a449b9ecf338dcedd85ab",
      "question": "What campaign has waged for two decades?",
      "prediction": "The campaign has been conducted in the Himalayas, resulting in at least 25 deaths.",
      "ground_truths": [
        "violent separatist"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94d420e9c7754759a7391868d14e81fc",
      "question": "Who were involed in the gunfights?",
      "prediction": "The answer is \"In the Himalayan region,\" but the CNN question asks for \"involves\" (meaning \"who*s* involved) in the gunfights. Thus, \"involves\" does not directly answer the question.",
      "ground_truths": [
        "the Indian army and separatist militants in Indian-administered"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "ab1542b2a7e64584ba41a72e10d3f1d4",
      "question": "Who was involved in the fighting in Kashmir?",
      "prediction": "The answer is the Indian Army.",
      "ground_truths": [
        "the Indian army and separatist militants"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "d548f4b0a8014fa3bdc21f5d4e65665c",
      "question": "What weapon was used in the fighting?",
      "prediction": "The answer is **\"utmost firmness.\"**",
      "ground_truths": [
        "helicopter gunships"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a24d5cc2a8f44819ca35879d229e8bc",
      "question": "Where was the tournament?",
      "prediction": "The answer is: Doral.",
      "ground_truths": [
        "Florida"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0161b1de8e9944f58ba3663305c9759a",
      "question": "what Swedish star strips down to the bare essentials to play?",
      "prediction": "The Swedish star strips down to the bare essentials to play.",
      "ground_truths": [
        "Henrik Stenson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c78e0c2903384595af84d1f0cf51a54f",
      "question": "Who create a stir with Striptease?",
      "prediction": "The answer is: \"Absolutely, you never know, after this I might have a new endorsement with PlayGirl or something like that.\"",
      "ground_truths": [
        "Henrik Stenson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cdb3be6523047f48d6ec5110de357e0",
      "question": "What did he strip down to?",
      "prediction": "The answer to the question is: Stenson removed his shirt, trousers, socks, and shoes to prevent a mud-covered ball from being dropped in the mud, which he could not afford to play in any of his clothes.",
      "ground_truths": [
        "a pair of white boxer shorts"
      ],
      "em": 0,
      "f1": 0.052631578947368425
    },
    {
      "id": "dafae23b13b148619784124572b26008",
      "question": "where Henrik Stenson creates a stir with 'striptease'?",
      "prediction": "The answer is:  \"Absolutely, you never know, after this I might have a new endorsement with PlayGirl or something like that.\"",
      "ground_truths": [
        "on the Blue Monster course at Doral"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad0e4b1a103d400f9ef4dd6827e215b9",
      "question": "where was tournament?",
      "prediction": "The answer is \"European Ryder Cup,\" and it is stated in the text.",
      "ground_truths": [
        "Blue Monster course at Doral"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7611c1e36e6d43a991ba213c40fe5178",
      "question": "What could the discovery of water boost?",
      "prediction": "The answer is, \"We used to think of the moon as a dead, unchanging place that the moon was a dead planet. ... There are changes that occur over the course of days and weeks and months, not over the course of thousands or millions of years. That's what the lunar crater observation and Sensing Satellite (L-CROSS) and its companion spacecraft's discovery of water in a dark and cold place. These discoveries are not just new information; they are new chapters that are written into the mind, with the understanding that we don't know the questions are yet to be explored. The Lunar Crater Observation and Sensing Satellite",
      "ground_truths": [
        "sustain future exploration of the moon and beyond."
      ],
      "em": 0,
      "f1": 0.0594059405940594
    },
    {
      "id": "fe08748775ae44a6ad801cbfba521bc4",
      "question": "What is the lunar orbiter taking pictures of?",
      "prediction": "The Lunar Crater Observation and Sensing Satellite, or L-CROSS, and its companion spacecraft crashed into a crater at the moon's south pole in October and discovered water in a very dark and cold place. L-CROSS researchers said about 25 gallons of water were detected in the crater, measuring about 60 feet deep, which measured about 60 feet wide by a few feet deep.",
      "ground_truths": [
        "the moon's surface"
      ],
      "em": 0,
      "f1": 0.033898305084745756
    },
    {
      "id": "a3d418c59fe644388fccb6a470ec954b",
      "question": "What does the space agency say?",
      "prediction": "The space agency says the missions are rewriting lunar science books and revolutionizing our understanding of Earth's closest neighbor.",
      "ground_truths": [
        "scientists know about Earth's closest neighbor."
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "591c9c6b06934a25ae6612de894283b6",
      "question": "What could the discovery of water in a lunar crater do?",
      "prediction": "The answer is: The discovery of water in a lunar crater, in a dark and cold place, means there are potential resources could sustain future exploration of the moon for future exploration.",
      "ground_truths": [
        "sustain future exploration of the moon and beyond."
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "f3392fcd067b4e50ba6b28a937389f6b",
      "question": "How many in numbers are there",
      "prediction": "The answer is 100,000.",
      "ground_truths": [
        "100,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cffde012e2ae45b5bc9c6cf569ce7657",
      "question": "What breeds quickly and travels quickly?",
      "prediction": "The answer is **the largest clutches found in the Everglades have contained 83 eggs.**",
      "ground_truths": [
        "Burmese python."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce020475bd42443c9a47498225879754",
      "question": "What does the expert patrol?",
      "prediction": "The expert patrol works in the Florida Everglades, investigating the presence of Burmese pythons, and the potential for new species of pythons to emerge from the Everglades. They also monitor the movement of the largest clutches of snakes, such as 83 eggs, and the drivers of the truck, which also means the problem in the local ecosystem, as these are often toxic to endangered species.",
      "ground_truths": [
        "the Florida Keys,"
      ],
      "em": 0,
      "f1": 0.03571428571428571
    },
    {
      "id": "ad293f2dfc814c049ffc3d774d15033d",
      "question": "Where is a perfect place for Burmese pythons to live?",
      "prediction": "The final answer is the same. The question is: Where is a perfect place for Burmese pythons to live?",
      "ground_truths": [
        "The Everglades,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9bdff30a8674448db3b028d425d6c16b",
      "question": "Where is this place",
      "prediction": "The answer is the Everglades, Florida, USA.",
      "ground_truths": [
        "Florida Everglades."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "0453e067ffbf4cf19accf2fb20bb7db0",
      "question": "What does the reptile expert do?",
      "prediction": "The reptile expert Joe Wasilewski does \"The Everglades, Florida's River of Grass,\" and he does:",
      "ground_truths": [
        "holds a Burmese python"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f5e5584880dc440c848418daf2ade992",
      "question": "what is happening with microsoft",
      "prediction": "The answer is: The European Commission fined Intel a record $1.45 billion.",
      "ground_truths": [
        "round of investigations"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "858e77b8d0564a3486c079418a48fb5f",
      "question": "Who else is under investigation of the EC?",
      "prediction": "The answer to the question is: The European Commission's ruling in the computer processing unit (CPU) market, which details the EU's anti-trust rules and the commission's findings, likely has implications for American companies accused of \"jurisdiction shopping\" to avoid anti-trust verdicts against them.",
      "ground_truths": [
        "Microsoft."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "59a6f7f87da84cccb0a6bae21feb8de1",
      "question": "what EC says Intel has been abusing its dominant market position?",
      "prediction": "The EC's response to the \"anti-trust case\" against Intel's computer chip company, is:",
      "ground_truths": [
        "anti-trust laws."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "2cf6668f6c3e413ab7da6c421b25b6b2",
      "question": "What was Intel fined for?",
      "prediction": "Based on the context, the answer is $1.45 billion.",
      "ground_truths": [
        "violating anti-trust laws."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09e6ad72a4334ed7a46d3a12d1363b67",
      "question": "how much was the fine",
      "prediction": "The answer to the question is $1.45 billion.",
      "ground_truths": [
        "$1.45 billion"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6afa42cb52a342458ab65f1c2235ea11",
      "question": "What company initiated the investigation into Intel?",
      "prediction": "The answer is: Intel. The European Commission fined it a record $1.45 billion for violating anti-trust laws.",
      "ground_truths": [
        "AMD, a competitor, launched this in Europe (and in Japan and South Korea)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c6c32cae71b496c819d415dbd625fb9",
      "question": "what has the EC said about Intel",
      "prediction": "The EC stated that Intel has abused its dominant position in the computer processing unit market for years, and has systematically given PC makers and stores rebates to keep its computers off the shelves.",
      "ground_truths": [
        "been abusing its dominant market position in semiconductors"
      ],
      "em": 0,
      "f1": 0.2564102564102564
    },
    {
      "id": "5bc0838118ce439fa15dc66813fed529",
      "question": "where did the election take place",
      "prediction": "The answer is Gabon. The election was held on Thursday, July 27, 2024, in Libreville.",
      "ground_truths": [
        "Libreville, Gabon."
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "047c102fd84d4e478afe36ce9a384265",
      "question": "when did voters go to the polls?",
      "prediction": "The answer is April 1, 2024.",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb49e4d87a4a4b5e8fbc099316b18e95",
      "question": "Who is Ali Bongo?",
      "prediction": "The answer is Ali Bongo.",
      "ground_truths": [
        "son of Gabon's former president"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6285e913cb0d4e5f9ab9b2f23383933e",
      "question": "who was declared winner of the election",
      "prediction": "The answer is **Ali Bongo**, 50, who won the presidential election.",
      "ground_truths": [
        "Ali Bongo"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "6e5fcb94e6a14465b0a57e26e693271e",
      "question": "who is the winner",
      "prediction": "The answer is: **Ali Bongo** won the presidential election.",
      "ground_truths": [
        "son of Gabon's former president was declared the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "869b6218682e464bae6c1328ea6b2a7e",
      "question": "who was elected?",
      "prediction": "The answer is Ali Bongo.",
      "ground_truths": [
        "son of Gabon's former president"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76d53c92aca14d1d9e78aa402b17ca18",
      "question": "who is Gabon's former president?",
      "prediction": "Answer: The son of Gabon's former president was declared the winner of the country's presidential elections on Thursday, sparking violent protests in some parts of West African nation.",
      "ground_truths": [
        "Omar Bongo,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0028e5ee411f4dd9837c601ad7f0cc4a",
      "question": "who is protesting?",
      "prediction": "The question is, what is the protest in Libreville?",
      "ground_truths": [
        "Opposition supporters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "61f45cf18f2c47f18ba19d8e971f5712",
      "question": "on what day did the election take place",
      "prediction": "The answer is **Thursday**.",
      "ground_truths": [
        "Thursday,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "204991cae42743b583fe6e336abb0b6a",
      "question": "who faces arrest?",
      "prediction": "The answer is:  The deposed president, Jose Manuel Zelaya, will face arrest for violating the constitution, corruption, and drug trafficking charges.",
      "ground_truths": [
        "Ousted Honduran President Jose Manuel Zelaya"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "fbca8a676a7449e8ad526ad6d5b8adb8",
      "question": "who do the unions support?",
      "prediction": "The key answer is that the U.S. State Department has reviewed its aid to Honduras, and the answer is: \"because of the U.S. State Department's review of Zelaya's ouster.\"",
      "ground_truths": [
        "Zelaya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ca42356781e422d89c3767c3d711844",
      "question": "when will Jose Manuel Zelaya return home?",
      "prediction": "The answer is on the U.N. General Assembly.",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab546d56ffdb4c989b8fc4f76950eb1a",
      "question": "What did she remember?",
      "prediction": "The answer is: Michelle Obama remembered that she had, during her presidency, a deep understanding of the issue of health care, and a willingness to work with people who had sick, and to help those who were sick. She also remembered that she was a daughter of an immigrant and felt the national conversation about health care was lacking in her memories.",
      "ground_truths": [
        "when daughter Sasha exhibited signs of potentially deadly meningitis"
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "abdf1b5352e3426891d05c65e282677c",
      "question": "What is the first lady speaking about?",
      "prediction": "The answer is, \"What she's doing is putting a personal and human face on the issue...\"",
      "ground_truths": [
        "health care"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f19508f628f4cecaa6b3db9c3ad1c56",
      "question": "What will the speech be about",
      "prediction": "The answer is: \"How health care impacts families.\"",
      "ground_truths": [
        "health care reform"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "7cfa3ba0cc72417a8103369446fcaa87",
      "question": "what is the issue",
      "prediction": "The answer is: \"What she's doing is putting a personal and human face on the issue...\"",
      "ground_truths": [
        "how health care can affect families."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2988ddd1e4943d3b0a35c070b6c2091",
      "question": "What did Obama call for",
      "prediction": "The answer is: \"The most urgent priority now is to support our Afghan partners in reversing what General Mcchrystal called the 'deteriorating' security situation.\"",
      "ground_truths": [
        "\"new chapter\" of improved governance in Afghanistan"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "156a8983efb5464299bb5ed262ae00e2",
      "question": "what has president obama called for",
      "prediction": "The answer is:  \"hard conversations about ensuring credibility and improving governance, addressing corruption in Afghanistan.\"",
      "ground_truths": [
        "\"new chapter\" of improved governance in Afghanistan"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "62d99a30c21e4402b596e8f2885a42e7",
      "question": "What is President Obama calling for?",
      "prediction": "The answer is: \"This has been a difficult election process for Afghanistan and lessons must be learned. It is a point in time in which we begin to write a new chapter based on improved governance, a much more serious effort to eradicate corruption, joint efforts to accelerate the training of Afghan security forces so that the Afghan people can provide for their own security.\"",
      "ground_truths": [
        "a \"new chapter\" of improved governance in Afghanistan now that Karzai's re-election as"
      ],
      "em": 0,
      "f1": 0.2285714285714286
    },
    {
      "id": "ce353d421545436daa25071d4ebdc870",
      "question": "Where should U.S. and Britain keep out of, warns Mugabe?",
      "prediction": "The answer is: \"Zimbabwe cannot be British. It cannot be American. Yes, it is African.\"",
      "ground_truths": [
        "election,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ad512845a8249638d9780e7d6b9caee",
      "question": "what did the president Mugabe likens?",
      "prediction": "The answer is: \"Zimbabwe cannot be British, it cannot be American. Yes, it is African.\"",
      "ground_truths": [
        "likening one American diplomat to a \"prostitute\""
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "84a848a62eed4fb7b121decd5db944d8",
      "question": "To what does Zimbabwe President Robert Mugabe liken U.S. diplomat?",
      "prediction": "The answer is: \"It is African.\"",
      "ground_truths": [
        "\"prostitute\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f078b8bb57d47119420af37d7529346",
      "question": "what was the warns provided by mugabe?",
      "prediction": "The answer is: \"This is a clear testimony of the callousness of this regime,\" and \"it is a clear testimony of the hearts dripping with blood, to vote him out of office.\"",
      "ground_truths": [
        "against outside influences in next month's run-off"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55b31d139f764bfe81829b76c62031a8",
      "question": "what does mugabe say",
      "prediction": "The answer is: \"This is a clear testimony of the callousness of this regime,\" and \"it is a clear testimony of the hearts dripping with blood, to vote him out of office.\"",
      "ground_truths": [
        "\"Zimbabwe cannot be British, it cannot be American. Yes, it is African,\""
      ],
      "em": 0,
      "f1": 0.10256410256410256
    },
    {
      "id": "323ba733ecc64d9f93cb6c466ccc6f91",
      "question": "who are accuced for the murder",
      "prediction": "The answer is: Tsvangirai.",
      "ground_truths": [
        "Mugabe's supporters."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91764d605f5f4795b322ed5e35bed029",
      "question": "For what did Tsvangirai accuse?",
      "prediction": "From the context, the answer is: \"No, Tsvangirai did not accuse the U.S. or Britain. He stated that the U.S. ambassador to Zimbabwe James McGee would be expelled if he \"persisted in meddling in Zimbabwe's electoral process.\"",
      "ground_truths": [
        "kidnappings, torture and other violence, including the deaths of opposition party members."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbb711ac029644ba8e66865d150f486c",
      "question": "Who attends funeral?",
      "prediction": "The correct answer is: Zimbabwe President Robert Mugabe.",
      "ground_truths": [
        "Tsvangirai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4515974f2776464ea560272d9179a08c",
      "question": "who is mugabe?",
      "prediction": "The answer is \"Robert Mugabe.\"",
      "ground_truths": [
        "Zimbabwe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a5e4762fc7642c8b6c0e3edb934091e",
      "question": "What was Cummins found alongside in a tub?",
      "prediction": "Answer: The Arkansas State Crime Laboratory found that Cummins was found with a blue and purple-colored head, a chain around his neck, and a Maumelle Police Department report that he was asleep next to the corpse, with his head lying behind his shoulder.",
      "ground_truths": [
        "man's lifeless, naked body"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b6cfe0943b2e41438f69abee84b49a47",
      "question": "Who received all the inquiries?",
      "prediction": "The Arkansas weatherman found last week in a hot tub alongside a man's lifeless body, hoping for autopsy results, and CCS (Arkansas State Crime Laboratory) to clear his name.",
      "ground_truths": [
        "\"The station"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fb8fd9786fa4bd38963ac00bde7e7eb",
      "question": "What was found in the tub a week ago?",
      "prediction": "The Arkansas state crime laboratory found that Williams' body was found in a empty tub, blue-purple and purple, with a chain around his neck.",
      "ground_truths": [
        "a man's lifeless, naked body"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "b8d9791f00d34f9c803ce6000b087b97",
      "question": "what job did Brett Cummins resign from",
      "prediction": "The Arkansas Weatherman's job was in a hot tub, with a man's lifeless body, and a naked body, hoping for autopsy results and other evidence, but hoping for a resignation.",
      "ground_truths": [
        "meteorologist at KARK,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af49f9dece07440492b976d619ab4709",
      "question": "what was Brett Cummins  found alongside",
      "prediction": "The Arkansas state crime lab found the following:",
      "ground_truths": [
        "man's lifeless, naked body"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3b3f6f5ac1642db8cb48c5539e1bc21",
      "question": "Who resigned as a meteorologist?",
      "prediction": "The answer is: The Arkansas State Crime Laboratory (State Crime Laboratory) changed its name to Maumelle Police Department.",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76d4150a11f94a0a9df3acb524379dc3",
      "question": "Who resigned on Friday",
      "prediction": "The answer is: The Arkansas State Crime Laboratory (Maumelle Police Department) made the decision to resign on Monday, after a final report was completed on Monday.",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b2ff0268cb64664b0b7a27b7f5355b8",
      "question": "Which day did Brett Cummins resign?",
      "prediction": "The answer is **Friday afternoon**.",
      "ground_truths": [
        "Friday"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "a2025daeb7184c2badca285a883ead1e",
      "question": "What is the name of the meterologist who resigned at KARK?",
      "prediction": "The answer is: The name of the meterologist who resigned at KARK is Lt. Jim Hansard.",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e6cad506fcab4d979e90f4a9176b1408",
      "question": "What number of searches are planned for Monday?",
      "prediction": "The number of searches planned for Monday are three.",
      "ground_truths": [
        "Three"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "9e32a1d3fd11455280d3aecde908c2e6",
      "question": "Where did the pilot and passengers take off from?",
      "prediction": "The answer is: **The pilot and passengers from the Dominican Republic.**",
      "ground_truths": [
        "Casa de Campo International Airport in the Dominican Republic"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "d83aff530fd447b49138953308883ac2",
      "question": "How many searches have been made?",
      "prediction": "The number of searches made is 3.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d1a6ab35fe14fdd8bbe88f884d82ce6",
      "question": "What did the Coast Guard find?",
      "prediction": "The Coast Guard found three male and two female passengers, and four HH-65 Dolphin helicopters from the Coast Guard Air Station Borinquen, and the Coast Guard Cutter Matinicus, in the waters off Quebradillas.",
      "ground_truths": [
        "debris"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e613fb291864bdd8a0198c5df5ae238",
      "question": "Where is the debris from?",
      "prediction": "The debris found at the site of the crashed Cessna 206 is located in the waters off Quebradillas.",
      "ground_truths": [
        "the single-engine Cessna 206 went down,"
      ],
      "em": 0,
      "f1": 0.21052631578947364
    },
    {
      "id": "62c441cc36584729bc6608d4f56ca4d6",
      "question": "When will the searches occur?",
      "prediction": "The answers are:",
      "ground_truths": [
        "Monday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e52126edb5a499381fb1f064dd4377c",
      "question": "How many searches have taken place?",
      "prediction": "The answer is three.",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b91f7ce859194bf3a952f9948618cfaf",
      "question": "Where might the debris come from?",
      "prediction": "The debris from the U.S. Coast Guard's search operations in Juanita, Puerto Rico, is located in the water off Quebradillas, Puerto Rico.",
      "ground_truths": [
        "the single-engine Cessna 206"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6973eb89fb1441328fc7a79248c2a0ec",
      "question": "What is planned for Monday?",
      "prediction": "The answer is **three searches are planned for Monday**.",
      "ground_truths": [
        "Search operations"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962fac5bf5424b35a4237c1af07ede3e",
      "question": "when did Wagner waited 4 hours to call the Coast Guard?",
      "prediction": "The answer is \"during the 1981 investigation.\"",
      "ground_truths": [
        "1981"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "77357032f0954a87b6f12d01f073907a",
      "question": "What happened to Natalie Wood?",
      "prediction": "The answer is: A Los Angeles County Sheriff's Department (LCSD) reopened the case, and the sheriff's spokesperson said, \"After the presser, we got a number of calls, and those calls were intriguing, and we're chasing those down now,\" according to the CSD spokesperson.",
      "ground_truths": [
        "drowned in the Pacific Ocean"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "39d86558a6424e1289089f1dc859b691",
      "question": "Who has not yet been interviewed by police?",
      "prediction": "The answer is, \"The investigation is not closed. It's possible we can yield new information that changes this case,\" and this is a part of the context.",
      "ground_truths": [
        "Dennis Davern,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6179293e102d4898bee708ec10582472",
      "question": "Who drowned off Catalina Island in 1981?",
      "prediction": "The answer is \"My sister was not a swimmer and didn't know how to swim.\"",
      "ground_truths": [
        "Natalie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1050ec903b444d02bff6c0c0111a8351",
      "question": "What did police receive?",
      "prediction": "The CNN reported that two months ago, the LAPSO (Los Angeles County Sheriff's Department) responded to calls, and a number of those calls were intriguing, and the investigation was followed down. They subsequently investigated a key witness, Dennis Davern, and subsequently interviewed Wagner. This led to a new account about the state of the investigation, and the possibility of new information to be discussed tomorrow. The official said that a new analysis of the case would likely change the entire investigation. The final response to the CNN report included a statement from the sheriff's department: \"This new information is substantial enough to make us want to take a",
      "ground_truths": [
        "additional information"
      ],
      "em": 0,
      "f1": 0.02173913043478261
    },
    {
      "id": "155783cc25514eb5b17116ed5eaed9c6",
      "question": "Whom have detectives yet to interview?",
      "prediction": "The answer to the question is \"the investigators haven't re-interviewed Wagner. \"",
      "ground_truths": [
        "Dennis Davern,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81d4be0064a24ca48f1bc8655f1d561c",
      "question": "What do police receive after a recent press conference?",
      "prediction": "The answer is: \"After the press conference, we have no new information about the investigation, and we're not able to answer the question. We're working on it and we are able to provide the most accurate information that comes to our attention.\"",
      "ground_truths": [
        "a number of calls,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e665fe3a57234e01ab82f67a205fd6fe",
      "question": "What was the reason for PlanetSolar to stops off in Hong Kong?",
      "prediction": "The answer is: It was a trip to the equator, powered entirely by the sun.",
      "ground_truths": [
        "as part of its 18-month journey around the world."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73ffe7e6a2624701aed02b60f8690bdd",
      "question": "Where did it stop off at?",
      "prediction": "The answer is Singapore.",
      "ground_truths": [
        "Hong Kong's Victoria Harbor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32d011bee7194d05b191109f63093a09",
      "question": "What can it produce?",
      "prediction": "The answer is approximately 94 kilowatts of power.",
      "ground_truths": [
        "on average 94 kilowatts of power"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "bbe5fe704e1147bf8ad9fd90297f22ac",
      "question": "What are the measure of the Solar yacht?",
      "prediction": "The answer is \"Mother Nature has proven to be a challenge, but one day, we will have no more oil anywhere.\"",
      "ground_truths": [
        "31 meters (102 feet) long and 15 meters (49 feet) wide,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1d31a7dd5fe48ec86396fe1c70594c8",
      "question": "What is the area of the boat?",
      "prediction": "The answer is 15.0 meters (49 feet) long and 15 meters (49 feet) wide.",
      "ground_truths": [
        "31 meters (102 feet) long and 15 meters (49 feet) wide,"
      ],
      "em": 0,
      "f1": 0.7500000000000001
    },
    {
      "id": "11199ef38b1946959b670f145a229301",
      "question": "What will they embrace?",
      "prediction": "The answer is: \"the sun, and solar power.\"",
      "ground_truths": [
        "technology"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9e49c54d5cc47829e27ab441af295f1",
      "question": "What has been hope from Captain of Planet Solar to?",
      "prediction": "The answer is that CaptainCristophe Domjan, the Swiss-born engineer who drove the catamaran, hopes the sun, and solar energy will be the answer to solve the world's climate crisis.",
      "ground_truths": [
        "and renewable energy at home everyday,\""
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "93df3e4cd7ed4859a3e67bb63fded5f5",
      "question": "Who was removed from duties?",
      "prediction": "The answer is \"I take full responsibility for what I did,\" and \"I know it's wrong,\" and the specific words used to denote the removal are:",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50929bed80564062ad70230ddb02f956",
      "question": "Who says he is in love with the woman?",
      "prediction": "The answer is: *\"I don't support the breaking of the celibacy promise.\"*",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c064d29477e543a8a9f10e1f063f08a8",
      "question": "Who was the woman?",
      "prediction": "The woman is a woman of faith.",
      "ground_truths": [
        "has not been publicly identified,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99320c1449e54556ba293ed05d7f040d",
      "question": "Who was removed from his duties?",
      "prediction": "The answer is: The Rev. Alberto Cutie was removed from his duties at St. Francis De Sales Catholic Church in Miami Beach, Florida, and on the Radio Paz and Radio Peace networks.",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 0,
      "f1": 0.1875
    },
    {
      "id": "e65e84946d1d4f99819210a08cf3fd8b",
      "question": "Who struggled with relationship?",
      "prediction": "The answer is Catholic.",
      "ground_truths": [
        "They have \"both struggled\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "216f9d42835642648e2f0a9ff9b922cd",
      "question": "Who says he is in love with the woman, considering his options?",
      "prediction": "The answer is: \"I have never had a sexual relationship with anyone other than the woman since I left the seminary 15 years ago.\"",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d98ca69eb4344f2b7c4d8eaa52e85e7",
      "question": "In which archdiocese was the priest removed?",
      "prediction": "The priest was removed from his duties at St. Francis De Sales Catholic Church in Miami Beach, Florida, after photos of his Catholic bathing-suit-clad woman, Abuela Alberto Cutie, were shown on the cover of last week's TV notes magazine.",
      "ground_truths": [
        "Miami Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.15
    },
    {
      "id": "abf52d1c0add48f28486a2a3840f760c",
      "question": "what did the rumours say",
      "prediction": "The provided text is a news article from CNN about a movie set in the USA where Kim Cattrall's co-star, Kristin Davis, alleges that \"Sex and the City's\" Kim Cattrall didn't get along with her co-star, Kristin Davis, while another would allege there were catfights on the set of the sequel.",
      "ground_truths": [
        "would insinuate that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,"
      ],
      "em": 0,
      "f1": 0.4666666666666667
    },
    {
      "id": "5ff5d8f1083b442cbf759f1980f8cd15",
      "question": "What did he co-write",
      "prediction": "The answer is: \"I am in shock now, Taz was one of my best friends.\"",
      "ground_truths": [
        "its signature song,\"The Devil Went Down to Georgia.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ded2ac8aff04250896711bfc070cd39",
      "question": "What is the name of the band?",
      "prediction": "The band's name is The Charlie Daniels Band.",
      "ground_truths": [
        "The Charlie Daniels"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fc16e9799a154e94b2bcade2e9eacd72",
      "question": "Is DiGregorio an original member of the band?",
      "prediction": "Yes, DiGregorio was an original member of The Charlie Daniels Band.",
      "ground_truths": [
        "keyboardist and"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "63b89182644f454c92026c4ca6747140",
      "question": "Did he write the song on his own?",
      "prediction": "Yes, DiGregorio wrote the song on his own.",
      "ground_truths": [
        "co-wrote"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "352bbdac5d414a65a714e30a0fd97f42",
      "question": "Who is an original member of the band?",
      "prediction": "The original member of the band is Taz DiGregorio.",
      "ground_truths": [
        "\"Taz\" DiGregorio,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "76c6e9ef00884f509bc625e25b15cc85",
      "question": "What was DiGregorio a member of",
      "prediction": "DiGregorio was 67.",
      "ground_truths": [
        "The Charlie Daniels Band,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3ff08899c474fcdaaae2dd2409ec7a0",
      "question": "what did the faa say",
      "prediction": "The answer is: The FAA said that \"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere,\" and that \"most of the reports came in about midday Sunday in Dallas, Texas.\"",
      "ground_truths": [
        "hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\""
      ],
      "em": 0,
      "f1": 0.5581395348837209
    },
    {
      "id": "5d464ecc16894c0cbc33ed95ac502289",
      "question": "what were US pilots told to watch for?",
      "prediction": "The answer is \"numerous\" and \"people on the ground from Dallas, Texas, south to Austin, Texas.\"",
      "ground_truths": [
        "\"falling space debris,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33d1e51367a3481086b0b8206010e16c",
      "question": "What types of sightings were reported?",
      "prediction": "The answer is: The FAA received no reports from pilots in the air of any sightings of ground strikes or interference with aircraft.",
      "ground_truths": [
        "booms and at least one fireball"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12cc92d88fd24d6697009b061d1769c2",
      "question": "When was the video shot?",
      "prediction": "The video was shot on Sunday morning in Austin, Texas, at approximately 11:00 AM.",
      "ground_truths": [
        "Sunday morning."
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "ed32ea00d52a492b93ce00ce06b93f4a",
      "question": "what collided in space?",
      "prediction": "The answer is: the space debris.",
      "ground_truths": [
        "two satellites"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22ec9df97ce34bd3a6d514c8c2fd3c13",
      "question": "what did the spokesman say",
      "prediction": "The answer is \"probably from NORAD, or the North American Aerospace Defense Command,\" and the caption, \"Most of the reports came in about midday Sunday in an area of Texas from Dallas to Austin.\"",
      "ground_truths": [
        "There were no reports of ground strikes or interference with aircraft in flight,"
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "ce37bce116da48499f15b84b3c158692",
      "question": "where was the video shot",
      "prediction": "The answer is: The video was shot in Austin, Texas.",
      "ground_truths": [
        "Austin, Texas,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "57351304b16347d58b309e84d19b79aa",
      "question": "What were pilots told to watch out for?",
      "prediction": "The answer is \"numerous\" and \"probably from NORAD,\" or the North American Aerospace Defense Command.",
      "ground_truths": [
        "\"falling space debris,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f126e81901054b3ba5fb4ce920b1df38",
      "question": "When will the book be released?",
      "prediction": "When will the book be released?",
      "ground_truths": [
        "April 13,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "92216f1e211c4ddf968b958588255feb",
      "question": "Who else has the author written biographies on?",
      "prediction": "The answer is \"The most widely read biographer of our era,\" as listed in the context.",
      "ground_truths": [
        "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3365b3e46e9240dca893042f6f34486f",
      "question": "What is the name of Oprah Winfrey's tell- all?",
      "prediction": "Oprah Winfrey's tell-all biography of talk show queen Oprah Winfrey is not released.",
      "ground_truths": [
        "\"Oprah: A Biography,\""
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "08b8fd7f109845eda0ffb6868479f8c2",
      "question": "its authorized or not?",
      "prediction": "The answer is \"It is not authorized.\"",
      "ground_truths": [
        "unauthorized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50e8ae7a28b347edbef6a9604da6802a",
      "question": "Who will write the biography?",
      "prediction": "The answer is \"Whitney reported the biography on April 13th, with a first printing of 500,000 copies.\"",
      "ground_truths": [
        "Kelley,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "803441a3a59d42b293810212bf2d6ced",
      "question": "What other biographies has Kelley written?",
      "prediction": "The answer is \"Chronicling the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.",
      "ground_truths": [
        "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan."
      ],
      "em": 0,
      "f1": 0.9189189189189189
    },
    {
      "id": "0dfeb9e7d43747bcac4e5ac700caec79",
      "question": "What was found capsized?",
      "prediction": "The U.S. Coast Guard's Sunday search for a missing sailor and the Texas A&M University crew members found four life jackets, and a boat with a missing crane. The boat was overturned at 5:15 p.m. Saturday, and the crew were found several hours later, with a U.S. Coast Guard helicopter in Galveston. The survivors, four university students and a safety officer, were not rescued. The search was focused on a 10-mile area south of Matagorda, Texas. The wreckage was found by a Marine Corps C-130, a Coast Guard cutter,",
      "ground_truths": [
        "sailboat"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0c1c6f6b44e3466eac1a68dadefd06ab",
      "question": "A sailboat matching the decription was found how?",
      "prediction": "The answer is:  \"The fuselages of the four university students and a safety officer told the Coast Guard that were forced off their sailboat after it took on water and capsized.  The flooded sailboat was found overturned about 5:15 p.m. Saturday.  The survivors were found several hours later, lifted to safety by a U.S. Coast Guard helicopter around 1 a.m. and taken to University of Texas Medical Branch in Galveston.  The crash involved a 38-foot boat.  The missing crew member, Roger Stone, was the second safety officer, and the boat was overturned",
      "ground_truths": [
        "overturned"
      ],
      "em": 0,
      "f1": 0.024999999999999998
    },
    {
      "id": "d4b5dfb2769d4208804f62b71b6e71c3",
      "question": "What caused the capsize?",
      "prediction": "The U.S. Coast Guard's Sunday search for a missing sailor in the Gulf of Mexico is searching for a missing sailor in Texas A&M University. The survivors, four university students and a safety officer, provided the Coast Guard with information about the boat's safe equipment, including emergency radios, life rafts, and ring buoys. The Coast Guard's search involved the area of approximately 10 miles south of Matagorda, Texas, and a Marine Corps C-130, a Coast Guard cutter, and the lost life jackets. The search was focused on an area of 10 miles south of",
      "ground_truths": [
        "took on water"
      ],
      "em": 0,
      "f1": 0.024390243902439022
    },
    {
      "id": "eb0682a6d1694acf886b84a407d68416",
      "question": "What was taking part in regatta?",
      "prediction": "The answer is: thefts from Texas A&M University students and a safety officer.",
      "ground_truths": [
        "The sailboat, named Cynthia Woods,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52e75f8056964c38b2a325a45841c089",
      "question": "Where was the sailboat headed?",
      "prediction": "The answer is *Texas A&M University* in Galveston, Texas.",
      "ground_truths": [
        "Veracruz, Mexico,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7002124cbe1043ecb021b8193037ab49",
      "question": "Who won the endorsement of South Carolina Gov. Nikki Haley?",
      "prediction": "The answer is **South Carolina Republican (SC Republican)**.",
      "ground_truths": [
        "Romney"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c9494792b6f44d0b86ee620dfee49502",
      "question": "Who praised Mitt Romneys credentials?",
      "prediction": "The key answer is \"Republican presidential nomination,\" and the paper's critique of Romney's campaign.",
      "ground_truths": [
        "The board"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4fa4d65d44e4af08a95506f60889546",
      "question": "Who won endorsement of South Carolina Gov,?",
      "prediction": "The answer is, the Republican presidential nomination is won by Mitt Romney.",
      "ground_truths": [
        "Mitt Romney"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "f7904039eab64b9b8cb6271e7b8bf1eb",
      "question": "What came two weeks before the Iowa caucus?",
      "prediction": "The answer is: Clinton.",
      "ground_truths": [
        "The public endorsement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba62f9a8709e43e6a4353462aed825f5",
      "question": "Bob Dole and The Des Moines Register's editoral praised who?",
      "prediction": "The answer is \"The Des Moines Register's editorial board.\"",
      "ground_truths": [
        "Mitt Romney"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c89b52721af4cfab0cb98a246f488eb",
      "question": "Whose endorsement did Romney win?",
      "prediction": "The answer is, \"In his advertisement, Dole said Romney is the GOP candidate is best suited to defeat President Barack Obama in 2012.\"",
      "ground_truths": [
        "Bob Dole,"
      ],
      "em": 0,
      "f1": 0.08695652173913042
    },
    {
      "id": "fabb462af78845fc8a68a0b2a7a85607",
      "question": "The endorsements came about two weeks before what?",
      "prediction": "The endorsed candidates came about two weeks before what?",
      "ground_truths": [
        "Iowa's critical presidential caucuses"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d73cd5a9e4d47929655d94692a79355",
      "question": "How many million internet users are in china?",
      "prediction": "The answer is 25.5 million.",
      "ground_truths": [
        "338 million Internet users in"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "d4a8128476ac4d929bb12460282c1ed8",
      "question": "how many users has china?",
      "prediction": "The answer is 67.",
      "ground_truths": [
        "338 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd14e1dc2f5544ec958f98850e4105bd",
      "question": "what did henry have removed",
      "prediction": "Based on the provided context, the answer is:",
      "ground_truths": [
        "cancerous tumor."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c61125f708f444a08a74f67c940ed77a",
      "question": "How many eggs did Henry and his near 80 year old mate Mildred produce?",
      "prediction": "Based on the provided context, the answer is: 11.",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "7e62f6dd07e44159a17dbf9e23722d70",
      "question": "What did Henry and Mildren produce?",
      "prediction": "The answer is:Eleven out of eleven hatchlings.",
      "ground_truths": [
        "11 healthy eggs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bba8beff7e84467798bcc8877a8f8f24",
      "question": "how many eggs did henry and mildred produce",
      "prediction": "Answer: 11 out of 11",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "88128dcfd0874369a2dee08ef86f6c1e",
      "question": "What did Henry do for the first time in forty years?",
      "prediction": "Answer: Henry did take about 110 years of life and some delicate surgery.",
      "ground_truths": [
        "mated"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b56f8165ffa44477b6e69deef3533885",
      "question": "what is the name of henry's mate",
      "prediction": "The answer is, it was about 110 years and some delicate surgery on his most private parts, but Henry is now a dad.",
      "ground_truths": [
        "Mildred"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ffe44836297740d7a182a1a1c22a8868",
      "question": "What changed things for Henry?",
      "prediction": "The answer is:  \"Eleven out of eleven,\" and \"Lily, a younger woman of about 80, in March, laid 11 healthy eggs and hatched all 11 of them, including the last one on Wednesday.",
      "ground_truths": [
        "cancerous tumor."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72753eabd02346189c361e07e5693b57",
      "question": "What amount are the stolen artworks worth?",
      "prediction": "The stolen artworks worth are $163 million (180 million Swiss francs).",
      "ground_truths": [
        "$163 million"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "8eb5cc20098f42998110b6ae272d3db5",
      "question": "what happened to picassos art",
      "prediction": "The answer is:  The Swiss art heist involves the theft of two paintings by Pablo Picasso, Bjoern Quellenberg, and a painted \"Horse's Head\" and \"Verre et Pichet\" by Picasso, and the paintings, sold to a German museum for $4.5 million, for a reward of $91,000.",
      "ground_truths": [
        "stole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc23ad5bd40f4ed8874ef74776638b12",
      "question": "What country had other recent thefts?",
      "prediction": "The answer is Switzerland.",
      "ground_truths": [
        "Switzerland"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1feb6429cf2b45d1818ea0a530e965e1",
      "question": "what was stolen",
      "prediction": "The answer is $163 million (180 million Swiss francs).",
      "ground_truths": [
        "famous paintings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7baeb822f72c486ab0ecda07d34f1a8a",
      "question": "what happened in zurich",
      "prediction": "The answer is: The Swiss art heist in Zurich, Switzerland, is the recovery of two paintings by Pablo Picasso, Bjoern Quellenberg, and a German museum's art dealer, and is a reward of $91,000 (100,00 Swiss francs) for the return of the paintings.",
      "ground_truths": [
        "Swiss art heist"
      ],
      "em": 0,
      "f1": 0.15789473684210528
    },
    {
      "id": "301de41a38b141ba8fa519b780069911",
      "question": "What artists are included in the haul?",
      "prediction": "The answer is: the Swiss art heist follows the recent theft of two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus, a major art museum in Zurich, and a German museum, \"Tete de Cheval.\"",
      "ground_truths": [
        "Paul Cezanne, Edgar Degas, Claude Monet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "26946b22375240aca1f36b8280a59a11",
      "question": "What did Obama promise the girls?",
      "prediction": "The answer is, the dogs would bring a new puppy to the White House in January.",
      "ground_truths": [
        "that they'd get to bring a new puppy with them to the White House"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "bb9aadd4bff44abd87cc35736d98cd41",
      "question": "What did Barack Obama promise the girls",
      "prediction": "The answer is:  Barack Obama promised his daughters Sasha and Malia that they would bring a new puppy with them to the White House in January.",
      "ground_truths": [
        "they'd get to bring a new puppy with them to the White House in January."
      ],
      "em": 0,
      "f1": 0.5555555555555555
    },
    {
      "id": "cfcb878ce5814bc9bc167d9c2984adba",
      "question": "What can lead to more safety and save cash?",
      "prediction": "Here's a concise answer based on the provided context:",
      "ground_truths": [
        "fuel economy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2032756f88a047c480c62dde0370296a",
      "question": "Which program is boosting economy and auto industry?",
      "prediction": "The answer is **Cash for Clunkers**.",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "5b8b16eec3b9472f93b5e0d16458de0b",
      "question": "What did Jack Hidary say about the Cash for Clunkers program ?",
      "prediction": "Jack Hidary said the Cash for Clunkers program is working in more than 10 countries around the world. In Germany, consumers have junked more than 1.2 million guzzlers in the last five months, significantly boosting the economy there. He also stated, \"This is a lot of stimulus for the dollar.",
      "ground_truths": [
        "promotes fuel economy and safety while boosting the economy."
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "581c4fe469a84f2f989aabaede600937",
      "question": "What is the name of the program?",
      "prediction": "The answer is Cash for Clunkers.",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "9abd942170454e78981c5d29d713f515",
      "question": "What does the  Cash for Clunkers program save?",
      "prediction": "The Cash for Clunkers program is working in more than 10 countries worldwide. It successfully pushed for the use of hybrid cars as taxis in New York City. The program's success is achieved by increasing consumer car sales in major U.S. markets, and it also helps autoworkers in the truck and auto supply chain. This is attributed to the program's emphasis on driving low-emission vehicles, promoting a more efficient, safe, and new-economy vehicle. The program also helps consumers pay for the cost of the new car, which provides a significant stimulus to the dollar. The program also has a strong",
      "ground_truths": [
        "fuel economy and safety while boosting"
      ],
      "em": 0,
      "f1": 0.02150537634408602
    },
    {
      "id": "57bffe27d20040699f85a6954fbb5bea",
      "question": "What is being saved?",
      "prediction": "The answer is $21 billion.",
      "ground_truths": [
        "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9206b026187f4230b610c9a05a62edad",
      "question": "What is the reason as to why Cash for Clunkers is saving jobs?",
      "prediction": "Cash for Clunkers is saving jobs in the auto supply chain, including dealers, assembly workers, and parts markers, by decreasing the number of dealerships in the last 18 months, and creating new sales by the program's funding.",
      "ground_truths": [
        "It's helping consumers move beyond these hard times and has reignited a whole industry."
      ],
      "em": 0,
      "f1": 0.043478260869565216
    },
    {
      "id": "9d71016ff1924e6287251e9ae657d1d8",
      "question": "What will decrease dependence on foreign oil?",
      "prediction": "The answer to the question is that Cash for Clunkers encourages consumers to bring the rest of the money to pay for the new car in the showroom.",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 0,
      "f1": 0.24000000000000002
    },
    {
      "id": "1082fd6599c245bba88c6c46ce86eef8",
      "question": "what is her age",
      "prediction": "Meira Kumar is a member of the \"untouchable\" Dalit class.",
      "ground_truths": [
        "64,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6cb904d598ce4f7ebb4e4283a24a2f63",
      "question": "Who is a member of the the Dalit class",
      "prediction": "The answer is **Meira Kumar**.",
      "ground_truths": [
        "Meira Kumar"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "728220fe1b5f4ce886273ec524c6972b",
      "question": "Meira is a member of what?",
      "prediction": "Meira Kumar is a member of the \"untouchable\" Dalit class.",
      "ground_truths": [
        "\"untouchable\" Dalit class,"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "53fe1484646648969f42e40f550ab7ef",
      "question": "What amount of members will she preside over?",
      "prediction": "The answer is: **543.**",
      "ground_truths": [
        "543 elected"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "79668b02828141b4bc20fd9faba0e2d9",
      "question": "What age is Meira Kumar?",
      "prediction": "Meira Kumar is 64.",
      "ground_truths": [
        "64,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d316c97abd1e4815a14a66b9b76df881",
      "question": "Mrs. Kumar will preside over how many members?",
      "prediction": "The answer is:",
      "ground_truths": [
        "543"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ff5deeb56834423b46cb59226137c85",
      "question": "What age is Mrs. Kumar?",
      "prediction": "According to the context, Mrs. Kumar was nominated by the ruling Congress party.",
      "ground_truths": [
        "64,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "669c0a0c0c0b49f4b8d647e1ae814cb9",
      "question": "What did Sanford do?",
      "prediction": "The answer is:  The State House, South Carolina, Congress, and the South Carolina Republican Party (in a letter to the Governor) both requested that the Governor resign.",
      "ground_truths": [
        "admitted to an extramarital affair"
      ],
      "em": 0,
      "f1": 0.08
    },
    {
      "id": "28ca5c9e1476480a998289afcd8e3990",
      "question": "Who is the GOP chairwoman?",
      "prediction": "The answer is: **The GOP chairwoman.**",
      "ground_truths": [
        "Karen Floyd"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f16043e697d41e185b901033f1fa369",
      "question": "Who may have to step down?",
      "prediction": "The answer is \"I would ultimately be a better person of more service in whatever doors God opened next in life if I stuck around to learn lessons rather than running and hiding down at the farm,\" and \"South Carolina can no longer be at a standstill for the next 18 months with a governor who ignores his job responsibilities while pursuing personal interests.\"",
      "ground_truths": [
        "Gov. Mark Sanford"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e45687f5ab9d4e7a8468c93bd2e555cc",
      "question": "Who is now calling on Sanford to resign?",
      "prediction": "The final answer is: **\"We want legislators to quietly, in private talks, ask him if he would please step down, and we'd only ask him to step down if he's willing to do so.**\"",
      "ground_truths": [
        "South Carolina Republican Party Chairwoman Karen Floyd"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e44e649e65d144f6b7f4570657697e64",
      "question": "Who called on Sanford to resign?",
      "prediction": "The answer is: **Congress of South Carolina**.",
      "ground_truths": [
        "South Carolina Republican Party Chairwoman Karen Floyd"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "a902053fd61745bdb4850afb62bd6e84",
      "question": "How many senators are calling for resignation?",
      "prediction": "The answer is 13. A large number of GOP senators are calling for resignation.",
      "ground_truths": [
        "13."
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "7d1e3748c47143648832d63acda6de2b",
      "question": "Who suggests Sanford may have to step down?",
      "prediction": "The answer is \"We learn that GOP Sens. Daniel Verdin, Shane Martin, Ronnie Cromer, and Wes Hayes joined the anti-Sanford chorus, bringing the total number of Republican senators calling for the governor's resignation to 13. There are 27 Republicans in the state Senate.",
      "ground_truths": [
        "Karen Floyd"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ceb8c6e838d347268492c88995a6fbc2",
      "question": "What was cancelled?",
      "prediction": "The answer is: \"We learn that GOP Sens. Daniel Verdin, Shane Martin, Ronnie Cromer, and Wes Hayes joined the anti-Sanford chorus Wednesday morning, bringing the total number of Republican senators calling for the governor's resignation to 13.  The John McConnell, Senate president pro tempore, didn't explicitly ask for Sanford to resign, but he did ask him to do the right thing for himself, his family, and our state.",
      "ground_truths": [
        "rally at the State House"
      ],
      "em": 0,
      "f1": 0.030303030303030304
    },
    {
      "id": "64575952b546437aa2013f3bd1aae455",
      "question": "What type of work are the firefighters doing?",
      "prediction": "The firefighters at CNN.com are digging ditches, referring to the work they perform, which includes not just clearing roads and fire trails. They are also working on logging and other land-related projects. The workers are known for their hard work, and they are the most valued local heroes in the forest. The firefighters do not seek recognition for their work as they are the guardians of their communities, and they work tirelessly to prevent all fires from destroying their homes and settlements.",
      "ground_truths": [
        "digging ditches."
      ],
      "em": 0,
      "f1": 0.05194805194805195
    },
    {
      "id": "238847addfa9408487251bff5d0e2c4a",
      "question": "What hour shifts do they do?",
      "prediction": "The answer is: \"We believe this unique reporting approach is worthy of sharing with our CNN.com readers.\"",
      "ground_truths": [
        "12-hour-plus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "40d046f0160c4124b666ebaeff5e2ead",
      "question": "Which group followed firefighters?",
      "prediction": "The correct answer is YES. The context provides no information on the group of firefighters. The Fire Department of Grayback Forest is a Denver-based, government-funded fire department in Brooklyn, New York, which is known for its dedicated work in fire, but the Colorado River has no similar dedicated firefighters. The city's \"Dashes\" is a name for what it is doing.",
      "ground_truths": [
        "VBS.TV"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f03ff04d77b4145a7f14d0613733117",
      "question": "What is the VBS following?",
      "prediction": "The VBS.TV note, \"Whenever someone at our office bitches about being overworked, our stock response is 'Beats digging ditches.',\" likely means the office's response to a news article about the VBS.TV's work, which is based on VICE's broadband television network.",
      "ground_truths": [
        "a crew of Grayback forest-firefighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ebe149a32c74c02aa0a03cb1e1c6dda",
      "question": "What does the VBS call it?",
      "prediction": "The VBS.TV post, \"The Staff at CNN.com has recently been intrigued by the journalism of VICE, an independent media company and website based in Brooklyn, New York, and Vice's broadband television network.\"",
      "ground_truths": [
        "\"Beats digging ditches.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37ccaff71d2844b6ab69084e87b8d658",
      "question": "What type of burn was this?",
      "prediction": "The answer is: \"the work is the hardest and least rewarding work we have ever tried to do.\"",
      "ground_truths": [
        "controlled"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa36a51494e24e6fa1703c519e13d09f",
      "question": "How long are the shifts?",
      "prediction": "The answer is: \"We have not been notified of any changes to the work schedule at VBS.TV.\"",
      "ground_truths": [
        "12-hour-plus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9eacf1402c884680aa535a49eb50ec62",
      "question": "What does the 12 hr shift involved?",
      "prediction": "The 12-hour shift involved heavy-duty labor in a forested area, likely involving a crew of grayback forest-firefighters, performing tasks such as cutting, chopping, and preparing for a controlled burn. The work was characterized by labor, hard work, and a focus on safety and conservation. The shift was a \"project work\" in the wilderness, with no personal recognition or praise.",
      "ground_truths": [
        "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "dd28d4ce2c5f4fac9c44006755c274ba",
      "question": "what is happening in brazil",
      "prediction": "The answer is that Brazil's response to the HIV/AIDS epidemic was widely praised and adopted as a model around the world.",
      "ground_truths": [
        "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world."
      ],
      "em": 0,
      "f1": 0.7096774193548386
    },
    {
      "id": "66aab61f3c5640b28c962d656127f1cf",
      "question": "what country has been hailed as a leader in the fight against the HIV/AIDS epidemic?",
      "prediction": "The answer is Brazil.",
      "ground_truths": [
        "Brazil's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2125661233694655a3ed2e4be0429f38",
      "question": "what happened in 1996",
      "prediction": "The answer provided is based on the context.",
      "ground_truths": [
        "Brazil"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4b8d0b8de71c4c72be5ee5c11a8b367c",
      "question": "when did the Government start offering free anti-retroviral treatment to its citizens ?",
      "prediction": "The answer is Brazil's response to the HIV/AIDS fight.",
      "ground_truths": [
        "1996"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50211fc1b1cf4a00a980fdfab4e4424a",
      "question": "what did the government start offering to its citizens in 1996?",
      "prediction": "The answer is that Brazil's response to the HIV/AIDS epidemic was a model for countries around the world.",
      "ground_truths": [
        "free anti-retroviral treatment"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72db4fe2adba411abff72eab29cfaf23",
      "question": "What was his reported biggest obstacle?",
      "prediction": "The biggest obstacle at which Kevin Evans had most difficulty weighting was managing his time. He was changed his diet, and he focused on his workouts and meals, not on diet. He also kept eating 1,800 calories a day, with a separate meal of protein shakes and eggs, and a few other items like turkey and vegetables.",
      "ground_truths": [
        "time."
      ],
      "em": 0,
      "f1": 0.037037037037037035
    },
    {
      "id": "3a1f5df0db9f4f55a0cd291ba708e0c2",
      "question": "What did Kevin Evans buy?",
      "prediction": "Kevin Evans bought a treadmill with an eight-week plan for low-calorie meals. He stayed focused on the program and the price of the treadmill. He also prioritized his gym membership.",
      "ground_truths": [
        "treadmill"
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "44ba26c0dbb346e2956a96d27ddb3ebf",
      "question": "Who bought a treadmill?",
      "prediction": "The answer to the question is \"I bought a treadmill to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.\"",
      "ground_truths": [
        "Evans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af7d61da98fa436da3f6524e2b864b08",
      "question": "What type of eating plan did he follow?",
      "prediction": "The answer is \"I was focused on the program by keeping the price of the treadmill in mind. I would focus on how much I paid for the treadmill and for that not to be wasted.\"",
      "ground_truths": [
        "low-calorie meals"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d29f96a620a43bd99661dbfb7072b59",
      "question": "How long did he work out daily?",
      "prediction": "From the context, \"Weight in January 1994: 315 pounds\" and \"Size 40 to size 32 pants, Size 40 to size 32 pants.\"",
      "ground_truths": [
        "45 minutes,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "129e731f7cf548eb8ade87485fd2cbd1",
      "question": "How long did he workout for?",
      "prediction": "Answer: 20 minutes.",
      "ground_truths": [
        "45 minutes, five days a week."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "736df4db1462436a97d65428d4f1ac9b",
      "question": "What actually happens with the news?",
      "prediction": "The answer is:  \"In Iran, authorities cracked down on online media, including social media networks, after the June 12 presidential election, with the arrest of journalists and the removal of online media. This is driven by the need to suppress dissent and, in some cases, to combat terrorism.\"",
      "ground_truths": [
        "control and censorship"
      ],
      "em": 0,
      "f1": 0.04347826086956522
    },
    {
      "id": "87aa1eb8cc90491c9b80a38c7751a6f9",
      "question": "What governments want to control?",
      "prediction": "The answer is: \"In the Middle East and North Africa, the Internet has offered many people access to information and the outside world that would have been unimaginable a few years ago.\"",
      "ground_truths": [
        "Web"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f99c8aa8238493d9c2cc5e49f4eb371",
      "question": "what gets out via Internet, social media, despite governments' efforts?",
      "prediction": "The answer is, \"Social media, social media, the Internet, and the global network of information.\"",
      "ground_truths": [
        "access to information and the outside world"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "b8320aeb13a4407eab7c1935e30ea4ee",
      "question": "where Governments seek to control news, sometimes by jailing?",
      "prediction": "The answer is: in Iran.",
      "ground_truths": [
        "Middle East and North Africa,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45aceb7efa1141cb833f1187994e4881",
      "question": "who is controling news by jailling journalists?",
      "prediction": "The answer is: \"Government control of the media remains tight in almost all countries, and censorship and self-censorship are prevalent throughout the region.\"",
      "ground_truths": [
        "government"
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "0cb0af83d0024e0b9ddbe7fb7ffaff26",
      "question": "What mentions the International Press Institute report?",
      "prediction": "The International Press Institute's report, \"In the Middle East and North Africa,\" detailed the challenges of government control and censorship in the Middle East and North Africa, focusing on the Internet's role in information flow.",
      "ground_truths": [
        "Review 2009 Focus on the Middle East and North Africa."
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "c9bcb9b8a8ba42f5bfd2538b4925a3ac",
      "question": "How much did South Africa beat Australia by?",
      "prediction": "South Africa defeated Australia by winning the first Test in Melbourne by 2-0.",
      "ground_truths": [
        "2-0"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "8ae5ae5b607b4dbc86f7da5147a095c7",
      "question": "Who is leading the series?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6019b29d903348c4a083f53c4551ac53",
      "question": "Who won the first Test of the series?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bb720c2091ee4a94a85eb81aa5a7acc3",
      "question": "Who beat Australia?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "679586e746bf437fb4372790a38aaa81",
      "question": "By how many wickets did South Africa win?",
      "prediction": "Answer: South Africa won the first Test in Australia by winning 65 runs, including 180 wickets.",
      "ground_truths": [
        "nine-wicket"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9adfdf882a9a4f8bb69b96506ce7471c",
      "question": "Who beat Australia by nine wickets?",
      "prediction": "The answer is South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6489bd7df2d04a8aa23561eaed38c793",
      "question": "What was the winning edge in the second Test?",
      "prediction": "The winning edge in the second Test was a single hit by JP Duminy, which followed by a similar score from Dale Steyn.",
      "ground_truths": [
        "2-0"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cceb8fa1b2a84757932a6f7ae86b3653",
      "question": "Who has the lead in the series?",
      "prediction": "The answer is **South Africa**.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1e7aa41fe6ff4d048cf6c314eaae526a",
      "question": "How much passengers had fallen?",
      "prediction": "The answer is: The American tourist died aboard the MS Veendam, owned by cruise operator Holland America, with 86 passengers fallen.",
      "ground_truths": [
        "86"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "faca8fa5d6724e3f95ea8550bbb1a3c8",
      "question": "Of what did the elderly woman suffered?",
      "prediction": "The elderly woman suffered from diabetes and hypertension.",
      "ground_truths": [
        "diabetes and hypertension,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "384383fa2ce544db8f6eaf79bed69215",
      "question": "Who were shot and killed?",
      "prediction": "The answer is *Brian Smith, a 12-year-old Texas state trooper who died at Parkland Hospital in Dallas County Medical Examiner's Office.*",
      "ground_truths": [
        "Two people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "716a88bdce814241afaf7287239b1e51",
      "question": "Who was hospitalized in serious condition after a suicide attempt?",
      "prediction": "The answer is **C, the former Utah state trooper**.",
      "ground_truths": [
        "Brian Smith"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dab8ffafa5e04822a7c0f1c2cfd68e6f",
      "question": "Who was tied to at least three of four Dallas shootings?",
      "prediction": "The answer is: **A three-person charge.**",
      "ground_truths": [
        "Brian Smith."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f31334cd5435401299951cd843c721a7",
      "question": "Who is the ex-Utah officer?",
      "prediction": "The answer is **KSL in Salt Lake City, Utah**.",
      "ground_truths": [
        "Brian Smith."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "656256192ad14bbf9f0cbb3ee3aa052d",
      "question": "What he painted in 1610?",
      "prediction": "The answer is \"a rather young looking 46, it has to be said,\" and \"cobbe portrait\" (a portrait of the Irish family that owns the painting).",
      "ground_truths": [
        "portrait"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "7a1c968d39644e7eb1f0a27e3f70c70f",
      "question": "For how long was the painting on display?",
      "prediction": "The answer is, the Cobbe portrait, \"A portrait of William Shakespeare,\" was painted for three years, when it was found in a lost British portrait painting in the Irish home of William Shakespeare.",
      "ground_truths": [
        "several months starting April 23."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8af8f27867bd48a2b337b97001163115",
      "question": "what hakespeare group unveil \"only\" portrait of playwright?",
      "prediction": "The answer is \"the Cobbe family,\" which, based on the text, is the only portrait painted during Shakespeare's lifetime.",
      "ground_truths": [
        "London's Shakespeare Birthplace Trust."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7027f9cf591c42c1aa99c460a581d1d1",
      "question": "In what year did Shakespeare die?",
      "prediction": "The answer is 1616.",
      "ground_truths": [
        "1616."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5c77d779830a4f93b3261e066c7ea67d",
      "question": "When was this painted?",
      "prediction": "The answer to the question is: \"It is believed to be the only painting of William Shakespeare created during his lifetime.\"",
      "ground_truths": [
        "400 years ago"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f083d754f3b54ab9bca200bfaa83b30d",
      "question": "What is the adventure about?",
      "prediction": "The adventure is about the relationship between Carl and Russell.",
      "ground_truths": [
        "a curmudgeonly senior citizen, Carl, tries to cope with the enthusiasm of Russell, a young boy."
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "9b8c9419cfef4133b494c825c3cf6dfd",
      "question": "Which company created the movie \"Up\"?",
      "prediction": "The answer is \"I don't know. I'm certain it was a \"Up\" movie. The \"Up\" theater, which is not the same as the original Pixar's \"Up,\" was a \"Down\" movie. The answer is \"I don't know.\"",
      "ground_truths": [
        "Pixar's"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "338c52f639b64b4697831b74cbca529f",
      "question": "What is unusual about the dog in the story?",
      "prediction": "The unusual thing about the dog in \"Up\" is that it's not a literal, discrete dog. It's a mix of canine and feline characters.",
      "ground_truths": [
        "talking"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "454cbae23b864073955a1003858a86b7",
      "question": "Who won from Pixar?",
      "prediction": "The answer is \"Russell,\" or \"Wilderness Explorer.\"",
      "ground_truths": [
        "\"Up,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a3561154a454ca597d3a9a636a95a93",
      "question": "What have great material for film?",
      "prediction": "Here's the answer based on the context:",
      "ground_truths": [
        "the action in-and-around the golf course"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6e1780414deb4cf9b49568f07b3bdc26",
      "question": "Which actors were on the list?",
      "prediction": "Answer: The actors who were on the list were Dean Martin, Katharine Hepburn, Spencer Tracy-Ford, Adam Sandler, Bill Murray, Chevy Chase, and Will Smith.",
      "ground_truths": [
        "Will Smith."
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "3a1ba8b914d141eaac80d5de56ed0114",
      "question": "What films are on the list?",
      "prediction": "CNN's best golf movies are:",
      "ground_truths": [
        "\"Follow the Sun,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f5855e26ffa944fab475fd077cb06a43",
      "question": "What sport has been good for film makers?",
      "prediction": "CNN's best golf movies have been chosen for their focus on the core of the game: the driving, competition, and the unique appeal of the game.",
      "ground_truths": [
        "golf"
      ],
      "em": 0,
      "f1": 0.0909090909090909
    },
    {
      "id": "b3d3a7f3fd00419c896e03a2dd70c2eb",
      "question": "Which two funny films lead the list?",
      "prediction": "The two funny films that lead the list include:",
      "ground_truths": [
        "\"Three Little Beers,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c564493e2442402ebb6d4a5af51f7c25",
      "question": "What has provided great material for film makers?",
      "prediction": "CNN's best ten golf movies of all time include:",
      "ground_truths": [
        "drama of the action in-and-around the golf course"
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "8b1f4118d8cc46719e7be2e78f5bc20f",
      "question": "Which people have all starred down the years?",
      "prediction": "CNN's best ten golf movies, from the iconic \"Stooges\" to the contemporary films, all starred down the years.",
      "ground_truths": [
        "Dean Martin, Katharine Hepburn and Spencer Tracy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc07de7e20dc420f8313f86757aafd8a",
      "question": "where does the correspondent go",
      "prediction": "CNN Moscow Correspondent at Star City, the Russian cosmonaut training facility.",
      "ground_truths": [
        "at Star City, the Russian cosmonaut training facility."
      ],
      "em": 0,
      "f1": 0.8235294117647058
    },
    {
      "id": "c3f5e33aaf4e4f49a9d196f9cbe600b8",
      "question": "Who rides the train from artic to black sea?",
      "prediction": "The answer is: CNN's Moscow-based Senior International Correspondent Matthew Chance.",
      "ground_truths": [
        "Senior International Correspondent Matthew Chance"
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "9e3d4569c31f4984af8ccbf4f789e438",
      "question": "Who features in the series?",
      "prediction": "CNN's Moscow-based international Correspondent Matthew Chance, with the help of a Russian-language team.",
      "ground_truths": [
        "Matthew Chance"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "34bd133cbb6c42f5acdc5c38eb205ddb",
      "question": "A correspondent rides a train between which two points",
      "prediction": "The answer is Moscow.",
      "ground_truths": [
        "the Arctic north of Murmansk down to the southern climes of Sochi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64a39248e7b54b1fbc995e26886a78b7",
      "question": "Which news channel launches week long  programming?",
      "prediction": "The answer is CNN Moscow.",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "a70868a801ca4ed4be0c1e27a1dbb40b",
      "question": "Which tv station is launching a week long series on modern russis",
      "prediction": "The answer is CNN.",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ff22a081cf304ecba18fc33376e6eb4c",
      "question": "Name to people who are said to feature in the series",
      "prediction": "CNN's Moscow-based international Correspondent Matthew Chance, who provides news and commentary on Russia's modern Russia, writes to viewers in English and Russian, inviting them to share their thoughts and feel the impact of Russia's vibrant, diverse culture and global interactions in his online video, \"Russia of Today.\" He also mentions his commentary on the influence of the Russian diaspora in the international community and the appeal of online analysis of Russia's \"news.\" He also highlights CNN's programming for a variety of languages. He concludes by stating that all comments are valid and that CNN will be posted online to be read and watched by CNN.",
      "ground_truths": [
        "Nikolai Valuev"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5fcc6fb126e14d65ba4d0d86bb28a8be",
      "question": "What Polo club's comment?",
      "prediction": "The Polo club's comment was: \"Everybody is kind of in shock and trying to figure out what happened at the International Polo Club Palm Beach in Florida. No incident in which many horses died at once.\"",
      "ground_truths": [
        "\"A total of seven died on our property,\""
      ],
      "em": 0,
      "f1": 0.0975609756097561
    },
    {
      "id": "054b7315353146d4b0ad624514005ef2",
      "question": "How many horses died?",
      "prediction": "14",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29cf64bece6c424f80f4a55c46fc523f",
      "question": "Where were the races held?",
      "prediction": "The answer is *West Palm Beach. Florida*.",
      "ground_truths": [
        "International Polo Club Palm Beach in Florida."
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "b75b64ca513e4ae3be64ff5d71a9b652",
      "question": "Where was the match set?",
      "prediction": "True. The answer is definitively true.  The context states that the match set was at the International Polo Club Palm Beach in Wellington, Florida.",
      "ground_truths": [
        "West Palm Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.24999999999999997
    },
    {
      "id": "61bcc7e8df9841939729131fba4cd039",
      "question": "What was the reason that horses die?",
      "prediction": "The answer is: \"A total of seven horses died on our property.\"",
      "ground_truths": [
        "has not been determined,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1054b70aff2947d58044fae8a9a048c0",
      "question": "How many horsed died?",
      "prediction": "There are 7 horses.",
      "ground_truths": [
        "Fourteen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c13da5dd5e2246d28ea136f9f7db0468",
      "question": "What show is Piers Morgan a judge on?",
      "prediction": "Boyle's answer is \"Britain's Got Talent\" at Queen Elizabeth II in the Royal Variety Show.",
      "ground_truths": [
        "\"Britain's Got Talent.\""
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "ff4db7c7ac2446b6939d36496da1f46f",
      "question": "What dance group won Britain's Got Talent?",
      "prediction": "The answer is \"Britain's Got Talent\" in England.",
      "ground_truths": [
        "Diversity,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "590494beebfa4403b5ce68447f0499d2",
      "question": "Who is a show judge on Britain's Got Talent?",
      "prediction": "The answer is:  Boyle is a Scottish singer who performed as a dance group in England.",
      "ground_truths": [
        "Piers Morgan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5117de4cfa6c461babc5ee0b87c08fbf",
      "question": "Who came in second?",
      "prediction": "The answer is:",
      "ground_truths": [
        "Susan Boyle"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d140704e3b3a4639a0e0317b3ac3e726",
      "question": "What is the name of the dance group that won Britain's Got Talent?",
      "prediction": "The answer to the question is \"Britain's Got Talent\".",
      "ground_truths": [
        "Diversity,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af168f17b4e54de8b67d526a69269767",
      "question": "How many people are in the dance group Diversity?",
      "prediction": "The answer is \"100,000 British pounds ($161,000) and will perform for Queen Elizabeth II in the Royal Variety Show.\"",
      "ground_truths": [
        "10-person"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c8529b037aee493898daea2b1947af2b",
      "question": "What did Diversity win?",
      "prediction": "The answer is: \"The best people won.\"",
      "ground_truths": [
        "\"Britain's Got Talent\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eddee4047dc14889afa69dd1bdd8b3b3",
      "question": "Who came in second to the dance group Diversite?",
      "prediction": "Answer: The team \"Diversite\" from Essex and East London, England, ranged in age from 12 to 25 years old.",
      "ground_truths": [
        "Susan Boyle"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8df607a89ef24a34a5e5820f199dbc87",
      "question": "What did Johnny Depp say?",
      "prediction": "Johnny Depp said he \"sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason.\"",
      "ground_truths": [
        "\"I always kind of admired him, oddly.\""
      ],
      "em": 0,
      "f1": 0.13793103448275862
    },
    {
      "id": "ed22ba9720d249ef8627f5038fb406aa",
      "question": "How many people did Dillinger's gang kill?",
      "prediction": "The answer is 10.",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8563cf12e0cc4059b56b8e37eb16e2d5",
      "question": "Who is the character Johnny Depp is playing?",
      "prediction": "The answer is Johnny Depp.",
      "ground_truths": [
        "John Dillinger,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "98b1ffba0a8244a3bbcdbe600a1f75b8",
      "question": "Who was John Dillinger ?",
      "prediction": "John Dillinger was a renowned robber, a former bank robber and a known figure in the Depression-era American Midwest. He was known for his violent actions, often involving robbing banks and killing 10 men.",
      "ground_truths": [
        "bank robber"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "f0906a40a6054651a230fdb9a5a557b1",
      "question": "Who played John Dillinger in Public Enemies?",
      "prediction": "Based on the provided context, the answer is \"John Dillinger.\"",
      "ground_truths": [
        "Johnny Depp"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86f88ef6c5b74a548b18d51a0c738a28",
      "question": "What is Depp playing?",
      "prediction": "Depp is back as a bank robber, John Dillinger, as he is an actor who has been drawn to John Dillinger's actions. He's known for his mild-mannered personality and his penchant for criminal antics.",
      "ground_truths": [
        "John Dillinger,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "504f34a652e6456e993944c8716a518a",
      "question": "What is the surge of refugees to Yemen attributed to?",
      "prediction": "The surge of refugees to Yemen is attributed to strife in Somalia, where riots continued in Mogadishu for the second day Tuesday.",
      "ground_truths": [
        "Intensifying"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "769f1ed90a044b4ebee75a26c7572ddb",
      "question": "What is the number of refugees that have come to Aden?",
      "prediction": "The number of refugees that have come to Aden to search for asylum in Yemen is 361.",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e01adc0042e419787e823ca9d4dbdb9",
      "question": "Where have the refugess come to?",
      "prediction": "The answer is Yemen. The context states that \"Inhants said Tuesday,\" and \"Somali refugees pay as much as $150 to cross the Gulf of Aden in search of asylum.\"",
      "ground_truths": [
        "Yemen,"
      ],
      "em": 0,
      "f1": 0.07407407407407407
    },
    {
      "id": "64bc1a2e9ed348e5abe5add0e6e7dcf0",
      "question": "How many refugees have come to port city of Aden?",
      "prediction": "The number of refugees to port city of Aden is 361.",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83b36cb41c7c4d748b38795c9e32c208",
      "question": "To what does the agency attribute a surging number of refugees?",
      "prediction": "The agency attributes a surging number of refugees to a surge in the \"sicker\" (Mourning Somalia) in the capital city of Mogadishu, due to the proximity of the war-torn country to the country of Somalis, which is a common destination for economic hardship, famine, and war-torn refugees.",
      "ground_truths": [
        "violence, food shortages and widespread drought"
      ],
      "em": 0,
      "f1": 0.045454545454545456
    },
    {
      "id": "39b831ae82844d62a7e981b94ddb37ef",
      "question": "How many refugees have come to Aden since January?",
      "prediction": "The answer is 361.",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1535aa296b8842b281ff835cdfd0fc20",
      "question": "What else has contributed to increase of refugees to Yemen?",
      "prediction": "Answer: The increasing number of Somalis to seek asylum in Yemen, fueled by a surge in food shortages and drought, and the presence of the United Nations' refugee agency's saying, as confirmed by the UNHCR, resulted in a significant increase in the number of Somalis in Yemen's port city of Aden, as compared to 7,166 in the first four months of 2007. The influx was driven by factors in Somalia, particularly the constant and intense clashes in Mogadishu's capital city, and the ongoing protests in the capital city of Mogadishu for the second day, Tuesday,",
      "ground_truths": [
        "New smuggling routes across the Red Sea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41807a040375438e8c42a2100c8aa8a3",
      "question": "What kind of new routes are contributing to the increase?",
      "prediction": "The new routes are contributing to the increase in Yemen's refugee numbers, which has led to a surge in Somali migrants seeking asylum in the US.",
      "ground_truths": [
        "smuggling"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4a543ad4a77747a29d7258fff909d9d9",
      "question": "What is also contributing to the increase?",
      "prediction": "The answer is \"A man brandishes a knife while others carry old notes during a demonstration against record-high inflation.\"",
      "ground_truths": [
        "violence, food shortages and widespread drought"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "58659365bec34d19ad985f98b70d61f7",
      "question": "What did the report say?",
      "prediction": "The report stated that the Secretary of Homeland Security apologized to the American Legion National Commander David Rehbein for a department assessment about combat veterans being recruited by right-wing extremist groups.",
      "ground_truths": [
        "returning combat veterans could be recruited by right-wing extremist groups."
      ],
      "em": 0,
      "f1": 0.37837837837837834
    },
    {
      "id": "5a8cae18097a4d7a995aa7f655f1d8af",
      "question": "Who did the apologising?",
      "prediction": "The answer is \"The American Legion National Commander,\" but the Google search \"American Legion National Commander\" doesn't find that.",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a25fd6c20c84bb1abdda2294af223ce",
      "question": "Who apologizes?",
      "prediction": "The answer is \"the American Legion National Commander.\"",
      "ground_truths": [
        "Department of Homeland Security Secretary Janet Napolitano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "780dff9431124442a4f67b48da10faaf",
      "question": "Who apologies?",
      "prediction": "The answer is \"I apologize for any offense to our veterans caused by this report.\"",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3016b4474ded45d1bbe6599cad972735",
      "question": "Which document was released too early?",
      "prediction": "The document was released too early.",
      "ground_truths": [
        "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30d529c2768142d09ec9ebf9e201e533",
      "question": "Who said document was released?",
      "prediction": "The document was released to law enforcement agencies.",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ea483ff8e95c4daea3dd389f1c3eb646",
      "question": "Who could be recruited by right-wing extremists?",
      "prediction": "The answer is \"the American Legion National Commander David Rehbein.\"",
      "ground_truths": [
        "returning combat veterans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29c6e634000b4e03af13cde0b5a8f09f",
      "question": "Wh says veterans could be recruited?",
      "prediction": "The answer is yes, Whit says veterans could be recruited.",
      "ground_truths": [
        "Department of Homeland Security Secretary Janet Napolitano"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f0a70a65608b43c79cf0a22d6d11a538",
      "question": "What were the men?",
      "prediction": "The answers to the question are:",
      "ground_truths": [
        "club managers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ed1845d695148d5b950471d255316d0",
      "question": "Where is the strip club?",
      "prediction": "The answer is Atlanta, Georgia.",
      "ground_truths": [
        "Atlanta"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d0a6861253464596b3e82ef0691023bb",
      "question": "Who dies after fistfight?",
      "prediction": "The answer is *Kandi Burruss*.",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47e4f33155cd4ec2818b85e57b859170",
      "question": "What is the strip club called?",
      "prediction": "The strip club is called \"Body Tap.\"",
      "ground_truths": [
        "Body Tap,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fc6e9bcb0b6b4d6dafef94a3ef4fcc0b",
      "question": "Who was Kandi Burrus's fianc√©?",
      "prediction": "The answer is Kandi Burruss.",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33d88df314284b8885c0815ebf219da2",
      "question": "What was he charged with?",
      "prediction": "The answer is:  Kandi Burruss, the new cast member of the reality show \"The Real Housewives of Atlanta,\" was mourning the death of her fianc√©, who died after a fistfight.",
      "ground_truths": [
        "voluntary manslaughter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "238882a90dce41f6a69e2963ef23bc60",
      "question": "What does the advocate say?",
      "prediction": "The advocate says: \"We all know what happens when urban sprawl gets out of control: Commutes back up, smog thickens, and concrete suburbs gobble up green spaces. The oceans are growing crowded, and governments are increasingly trying to plan their use.",
      "ground_truths": [
        "planning processes are urgently needed"
      ],
      "em": 0,
      "f1": 0.045454545454545456
    },
    {
      "id": "b6ff077e9f204c3a8abb57b3f5a5e51b",
      "question": "What do advocats say?",
      "prediction": "The aquagistad advocates for the implementation of comprehensive ocean plans to protect our public-owned seas from the pressures of \"ocean sprawl\" and unsustainable development. They believe that by focusing on \"ocean sprawl\" and avoiding \"ocean eco-economy\" (the losses of fisheries and whales), they can achieve this goal by:",
      "ground_truths": [
        "planning processes are urgently needed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6240a4476804dfa888951352348797c",
      "question": "What is growing crowded?",
      "prediction": "The answer is: \"cities, oceans, and the \"ocean sprawl.\"",
      "ground_truths": [
        "The oceans"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "00e7c10ad5a14793a91667fede21506b",
      "question": "What is the purpose of the task force?",
      "prediction": "The task force is intended to advise federal state governments on the issue of ocean planning.",
      "ground_truths": [
        "federal ocean planning."
      ],
      "em": 0,
      "f1": 0.35294117647058826
    },
    {
      "id": "5c1d81a2821841cd869fd127cb7374a4",
      "question": "WHo created the task force?",
      "prediction": "The answer is, with the aim of preventing our public-owned seas from becoming sprawling, \"The oceans are kind of the last frontier for use and development.\"",
      "ground_truths": [
        "The Obama administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4ce634f201d41d08fb34d4a94de2b0a",
      "question": "What did the advocate say about oceans?",
      "prediction": "The advocate said that \"ocean sprawl\" is a problem, and that \"the oceans, like the land, have gotten crowded, and now scientists and policy makers are looking for ways to plan ocean development with the aim of preventing our public-owned seas from turning into sprawling, watery versions of Houston, Atlanta, and Texas.",
      "ground_truths": [
        "growing crowded,"
      ],
      "em": 0,
      "f1": 0.04081632653061224
    },
    {
      "id": "f3081bac0c7b4c31b5761dc00653c83e",
      "question": "What does he say about Turkey?",
      "prediction": "The text from the transcript expresses the President's general sentiment about Turkey, stating that the U.S can learn much from Turkey's expertise on Afghanistan and Pakistan, and that the U.S cares for Turkey's European identity as a secular democracy. He also emphasizes the importance of the United States's ties to Europe, and the president's thought that Turkey's \"cultural identity\" is a secular democracy. He concludes by stressing the crucial importance of a \"key to helping\" these issues, despite the difficult path of relations. He also points to the U.S's increasing support for the Taliban in Pakistan,",
      "ground_truths": [
        "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "35a4d88131f24082abcbca3edd33d1cf",
      "question": "Who says Turkey can help rebuild Afghanistan?",
      "prediction": "The provided text does not provide a single, definitive answer to the question. Instead, it provides a series of comments and statements from the American President Obama, and the Turkish Minister of Foreign Affairs, who also mentioned the role of the U.S. in Afghanistan and Pakistan, as well as the ongoing efforts to build a strong, safe, and sustainable Afghanistan-Pakistan relationship.  The text also highlights the key aspects of Turkey's \"European identity\" and the United States' \"secular democracy\" as a key aspect of its foreign policy. Finally, the response to the question highlights the complex and challenging situation in Afghanistan and the need",
      "ground_truths": [
        "Zeyno Baran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a80dbed58b934f7b96ff0990a777255a",
      "question": "What did Zeyno Baran say about Obama's speech?",
      "prediction": "Zeyno Baran's note asks about the United States to learn more from Turkey's expertise on Afghanistan and Pakistan. The answer is that Turks greeted President Obama with big excitement, and the US shared the same sentiments. He did so in a clear signal that the administration recognizes Turkey's value and is serious about engaging with it, and said the United States can learn much from Turkey's expertise in Afghanistan and Pakistan. He also mentioned the \"European identity,\" as the U.S. is a secular democracy, which he said is important.",
      "ground_truths": [
        "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan."
      ],
      "em": 0,
      "f1": 0.26373626373626374
    },
    {
      "id": "6b7c6c083c484e6a9d3b260a2637ec7f",
      "question": "Whose speech shows deep understanding of Turkey?",
      "prediction": "The answer is yes, yes, the U.S. has demonstrated deep understanding of Turkey's complex issues and identities. In that speech, the U.S. did not merely address the challenges of Iraq, but understood the strong faith and cultural sensitivity of Turkey, and the complexities of its political systems, and the importance of European identity.",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f714562f1c5045cb99bb8335566a1697",
      "question": "What did Baran say would be a mistake?",
      "prediction": "The answer is: **\"victory\" does not exist.** Baran's comment was a statement about the perceived success of U.S.-Turkey relations and his own views, not about the country's leadership. He stated that \"the United States can learn much from Turkey's expertise on Afghanistan and Pakistan, and \"Turkish identity\" is a secular democracy.\"",
      "ground_truths": [
        "pulling Turkey into any kind of engagement with the Taliban -- either as part of NATO or bilaterally"
      ],
      "em": 0,
      "f1": 0.03278688524590164
    },
    {
      "id": "51f6054c927f41a586b38361fc290fd1",
      "question": "When was she last seen?",
      "prediction": "The answer is: She was last seen at the checkout at Festival Foods in Kansas City, Missouri.",
      "ground_truths": [
        "10:30 p.m. October 3,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7046b4ee351b4200a16ba68fe210a839",
      "question": "In which location was Lisa last seen on October 3?",
      "prediction": "The answer is \"in her crib,\" based on the context.",
      "ground_truths": [
        "asleep in her crib,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "991bac9dc88e4388a02e3be07df05727",
      "question": "what does Bill Stanton want?",
      "prediction": "In a final statement, Bill Stanton wanted to seek the truth.",
      "ground_truths": [
        "a \"happy ending\" to the case."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "b9e7d81f1bc443508c715a712692e8cb",
      "question": "according to a supermarket clerk what did the mom never seem",
      "prediction": "The mom never seemed unusually sad, her face smiled, like she always does when she comes in here.",
      "ground_truths": [
        "looked depressed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04cfc749138b4f3dbed1b1582beeac18",
      "question": "What is the name of the Investigator?",
      "prediction": "The answer is \"Bill Stanton,\" a former NYPD security consultant.",
      "ground_truths": [
        "Bill Stanton"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "8f5969735d204ef2ba9ccf9d3fb58164",
      "question": "What did the clerk say about her mom?",
      "prediction": "The answer is: \"She had a smile on her face, like she always does when she comes in here,\" (CNN)",
      "ground_truths": [
        "face, like she always does when she comes in here,\""
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "7d74cc069506478abb97e75ede635049",
      "question": "when was the missing child last seen",
      "prediction": "The answer is: the missing child last seen at 11 months old.",
      "ground_truths": [
        "10:30 p.m. October 3,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a633f3f6fcb4f12b1f8398070ac6c5f",
      "question": "What did Stanton say?",
      "prediction": "The answer is: \"I am here to seek the truth.\"",
      "ground_truths": [
        "he wants a \"happy ending\" to the case."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "6ad74641727d460e9865833b76de85eb",
      "question": "Who suspended the lower courts order?",
      "prediction": "The answer is the U.S. Consulate in Rio de Janeiro.",
      "ground_truths": [
        "judge"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8339e3b757714b379f7dd0b729529214",
      "question": "What happened after mom moved with son to Brazil in 2004?",
      "prediction": "The answer is: The mother moved with her son to Brazil in 2004.",
      "ground_truths": [
        "divorced Goldman and married a Brazilian lawyer."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fce650a72de4befaacbf85b7453ddde",
      "question": "Where did the mom move with the son?",
      "prediction": "The answer is: **The mom moved with her son, David Goldman, to the United States.**",
      "ground_truths": [
        "Brazil"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef3c40e233344e10bc154aa185f5463e",
      "question": "What did the Brazilian supreme court judge suspend?",
      "prediction": "The Brazilian supreme court judiciary suspended a lower court's order for the custody of a 9-year-old boy to the U.S. Consulate in Rio de Janeiro, where he was to be reunited with his American father.",
      "ground_truths": [
        "would have given custody of a 9-year-old boy to the U.S. Consulate in Rio de Janeiro, where he was to be reunited with his American father."
      ],
      "em": 0,
      "f1": 0.7777777777777777
    },
    {
      "id": "58c0270a69db44d9a44ddb594f43cd63",
      "question": "Where did the lower court order the son to be taken?",
      "prediction": "The answer is, at the U.S. Consulate in Rio de Janeiro.",
      "ground_truths": [
        "U.S. Consulate in Rio de Janeiro,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "72d750ca2345477a80129e6b3e240072",
      "question": "When did the mother die?",
      "prediction": "The mother died on September 30, 2004.",
      "ground_truths": [
        "September,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "77ef14762d78476ba0c20e11ea6184c1",
      "question": "When did mom die in childbirth?",
      "prediction": "The answer is 2004.",
      "ground_truths": [
        "September,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1c71fcc9f6a468c9468d13d69e711a7",
      "question": "which is the project name?",
      "prediction": "The project name is \"Orbiting Carbon Observatory.\"",
      "ground_truths": [
        "Orbiting Carbon Observatory,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "077524326f6a4e53a728f5341e89f79c",
      "question": "How much was spent on the study of greenhouse gases effects?",
      "prediction": "The answer is: \"We'll get back to flying at a pace that allows us to do so successfully,\" according to NASA's program manager, John Brunschwyler.",
      "ground_truths": [
        "$273 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e5d6b99f2e6d4efea5d77b6e01aa98a3",
      "question": "Where did the satellite crash?",
      "prediction": "The answer is NASA's California base in California.",
      "ground_truths": [
        "landed just short of Antarctica in the ocean.\""
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "1af7aafeb2e24d348d72b7559a2ce754",
      "question": "What was the purpose of the rocket?",
      "prediction": "The answer is: \"to measure carbon dioxide in the Earth's atmosphere.\"",
      "ground_truths": [
        "monitored greenhouse gases to study how they affect the Earth's climate,"
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "f523ba87f5714a108e9bae698f676063",
      "question": "What was the $273 million project?",
      "prediction": "The answer is:  The $273 million project is for the International Ocean Observatory, which will measure CO2 levels in the Earth's atmosphere and climate.",
      "ground_truths": [
        "satellite, called the Orbiting Carbon Observatory,"
      ],
      "em": 0,
      "f1": 0.08000000000000002
    },
    {
      "id": "5837e1a0fc834500856bc94d81b68ac2",
      "question": "when it launched?",
      "prediction": "The answer is NASA launches a rocket from California's Vandenberg Air Force Base on Tuesday.",
      "ground_truths": [
        "Tuesday."
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "a456ea0840534b98b49cd3ef388e3a2a",
      "question": "What year is the Bureau of Labor Statistics report?",
      "prediction": "The Bureau of Labor Statistics (BLS) reports the salary paid to all federal workers, including those in the United States, for 2008.  The most recent data is for the United States, with 30 occupations earning $80,000.",
      "ground_truths": [
        "(May 2008)"
      ],
      "em": 0,
      "f1": 0.06060606060606061
    },
    {
      "id": "097faabf28a9492ab39094254f9046ec",
      "question": "how much do physics teachers earn",
      "prediction": "The answer is $81,600.",
      "ground_truths": [
        "$81,880"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7274cc5f0de84f7e9665d3234563a498",
      "question": "how much Physics and post-secondary biology teachers can earn?",
      "prediction": "The answer is $81,000.",
      "ground_truths": [
        "$81,880"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c448b1bd5874c299526596d9f80641b",
      "question": "how many jobs are listed that pay at least 80,000",
      "prediction": "The answer is: $80,000.",
      "ground_truths": [
        "30 occupations"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e17f0ff7bed494b88dd631b6122c549",
      "question": "What does Obama revive?",
      "prediction": "The answer is: \"To bring the military commissions in line with the rule of law and provide \"more due process rights for detainees held at Guantanamo Bay, Cuba,\" to a legitimate legal framework and a robust, consistent, and quick justice system, which would restore the commissions as a legitimate forum for prosecution.",
      "ground_truths": [
        "system of military trials"
      ],
      "em": 0,
      "f1": 0.12244897959183675
    },
    {
      "id": "1eede7107c9e48419d330929137ee37b",
      "question": "What did the ACLU call the move?",
      "prediction": "The ACLU called the withdrawal of the military commissions \"striking blow to due process and the rule of law.\"",
      "ground_truths": [
        "\"a striking blow to due process and the rule of law.\""
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "3a866b6d298b41afb982db6c34e1cd12",
      "question": "What is the new system to include?",
      "prediction": "The answer is: The new system to include.",
      "ground_truths": [
        "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts."
      ],
      "em": 0,
      "f1": 0.05263157894736842
    },
    {
      "id": "27e4cdf5912b4f558979ab1c2427ed75",
      "question": "Who calls the move \"a striking blow to due process and the rule of law\"?",
      "prediction": "The answer is, the Washington (CNN) -- \"because of the modified, and more palatable, military commissions.\"",
      "ground_truths": [
        "The ACLU"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "474cf4bfc8ed41268f0e9d6b62bde784",
      "question": "What does ACLU call the move?",
      "prediction": "The answer is: the \"Military Commissions Act\" (a 2006 law).",
      "ground_truths": [
        "\"a striking blow to due process and the rule of law.\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "366758dc520c473db2b9157d39a2ad10",
      "question": "What did Obama revive of Bush's?",
      "prediction": "The answer is: \"To create a legal framework and ensure justice against those detained at Guantanamo Bay, Cuba, and other facilities in Iraq and Afghanistan, a similar, unconstitutional, and failed law, to protect the rights of detainees in the U.S. for which the U.S. military has not been responsible.\"",
      "ground_truths": [
        "administration's controversial system of military trials for some Guantanamo Bay detainees."
      ],
      "em": 0,
      "f1": 0.22222222222222218
    },
    {
      "id": "2a993576ba994b8da9b170bdd1613c12",
      "question": "What will the new system include?",
      "prediction": "The answer is that the system will include:",
      "ground_truths": [
        "give detainees greater latitude in selecting legal representation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4441ed963a074e97ac0045a1560efe5f",
      "question": "who are growing?",
      "prediction": "The answer is: The Calverts' friends and family.",
      "ground_truths": [
        "friends"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "0c8cd47c97e545fb9b691546e8497d0e",
      "question": "Where did the couple live?",
      "prediction": "The question asks for the location of the Calverts' dead house.",
      "ground_truths": [
        "Hilton Head Island"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ef710fca1ae4712878511bcdea0af51",
      "question": "when was the last seen of John and Elizabeth?",
      "prediction": "The last seen: John and Elizabeth Calvert were photographed at Hilton Head Island, South Carolina, on March 3, 2008.",
      "ground_truths": [
        "March 3,"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "d06b179a42ad4291bf19955332ada7b1",
      "question": "Who were they suspicious of?",
      "prediction": "The questions asked at the end of the context are:",
      "ground_truths": [
        "Dennis Ray Gerwing"
      ],
      "em": 0,
      "f1": 0
    }
  ]
}