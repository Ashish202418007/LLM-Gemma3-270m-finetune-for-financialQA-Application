{
  "model_path": "/media/ashish/Ashish202418007/LLM_finetune/outputs/gemma3_finetune/final_model",
  "data_path": "./data/newsQA_validation.jsonl",
  "metrics": {
    "exact_match": 0.46533713200379864,
    "f1": 0.6281815350652358,
    "total": 4212,
    "samples_per_second": 4.418487530337179,
    "total_time_seconds": 953.2673728466034,
    "avg_time_per_sample": 0.22632178842511952
  },
  "config": {
    "max_new_tokens": 128,
    "temperature": 1.0
  },
  "predictions": [
    {
      "id": "849240180c584f71bc56c0c92c5df5fd",
      "question": "What will be nominated?",
      "prediction": "Keep American Beautiful campaign",
      "ground_truths": [
        "three different videos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8da0d6902cab4114b140672981c41ece",
      "question": "What does the Harrison Ford video feature?",
      "prediction": "the plight of the environment",
      "ground_truths": [
        "getting his chest waxed,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb575e95e21843f9bc7ab8acdde72ee7",
      "question": "What videos will you send?",
      "prediction": "eco-awareness messages",
      "ground_truths": [
        "environmental"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc754599d5c048639de9864adbce649a",
      "question": "What is Ford getting waxed?",
      "prediction": "Chest",
      "ground_truths": [
        "his chest"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f7b2f89be1724a9c86cbcc347b0c4425",
      "question": "Who got his chest waxed?",
      "prediction": "Harrison Ford",
      "ground_truths": [
        "Harrison Ford"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "342cb6f69f6449318f4c4e5385f954fd",
      "question": "How do you send in your video?",
      "prediction": "iReport form",
      "ground_truths": [
        "Use the iReport form"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f2f07af9f9874dc992ca32c22063ffd4",
      "question": "What type of videos should you nominate?",
      "prediction": "eco-awareness",
      "ground_truths": [
        "think are the best."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "365c5a43bd704e5bb1bc9e49d28bcf24",
      "question": "What did Steve Bruce describe Amire Zaki as?",
      "prediction": "the two have patched up their differences",
      "ground_truths": [
        "unprofessional."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d37eef3f1e014b2ebbd84db0e0fd9012",
      "question": "Which team does Zaki play for?",
      "prediction": "Wigan Athletic",
      "ground_truths": [
        "Wigan Athletic"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ffef7cbb9e7e4526a9826d07cb730fcd",
      "question": "Which club did Amir Zaki fail to return to?",
      "prediction": "Twain Athletic",
      "ground_truths": [
        "Wigan Athletic"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "63fdd20036c74bc585e0a089ac9317df",
      "question": "What punishment will be meted out for his disappearance?",
      "prediction": "maximum allowed",
      "ground_truths": [
        "a fine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d57e430e4bd40e6ab55ba4940df4767",
      "question": "Who went missing for two weeks and said he was taking a break from football?",
      "prediction": "Adriano",
      "ground_truths": [
        "Adriano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cc0e6b7eaa7409bb2455f1780bb777b",
      "question": "Who said there is no immediate plans for deployment?",
      "prediction": "President Obama",
      "ground_truths": [
        "President Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "014e5f5a9a98493ba20e29a3a4c478a5",
      "question": "How many troops does Canada have in Afghanistan?",
      "prediction": "35,000.",
      "ground_truths": [
        "35,000."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a5ad768fcc80413387ef56a790b3940f",
      "question": "How many more troops is the US planning to send?",
      "prediction": "6,000",
      "ground_truths": [
        "6,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5c1f77d8de6c4a3886d9fa099e0bc0a0",
      "question": "What are the plans of Obama after this deployment?",
      "prediction": "consult with U.S. allies",
      "ground_truths": [
        "to commit more U.S. troops to the ongoing war in Afghanistan,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "9a9e86c0ad0a4a2db791d4f5fd0a8b9b",
      "question": "How many troops are being send to Afghanistan this year?",
      "prediction": "6,000",
      "ground_truths": [
        "6,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a442d4094129405fae0a7d5cdedc0055",
      "question": "Women who don't conform will risk spending how long in jail?",
      "prediction": "12 hours",
      "ground_truths": [
        "12 hours"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b6f4d1fecbb647b0a355efb95d0cd94b",
      "question": "What are the rules of the new order?",
      "prediction": "cover their bodies and heads from view,",
      "ground_truths": [
        "to close their shops during daily prayers,"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "d405d7f57c324ded9029516e71164cc1",
      "question": "What controls Baidoa?",
      "prediction": "Al-Shabaab,",
      "ground_truths": [
        "Al-Shabaab,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02cc59d96ff94449845329b25359703e",
      "question": "What can happen to the women who don't obey the order?",
      "prediction": "jail time,",
      "ground_truths": [
        "face jail time,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "acb7e1b2249c4df1a0a5d2b0b3b6ed58",
      "question": "Women's clothing must cover what?",
      "prediction": "bodies and heads from view,",
      "ground_truths": [
        "their bodies and heads"
      ],
      "em": 0,
      "f1": 0.6666666666666665
    },
    {
      "id": "bd69fc3cb5e442d2b54243f2104eb508",
      "question": "When is the order in effect?",
      "prediction": "Tuesday.",
      "ground_truths": [
        "Tuesday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8a34a81fd7b4ba19a20a3acf7557a24",
      "question": "What must the clothing cover?",
      "prediction": "their bodies and heads from view,",
      "ground_truths": [
        "their bodies and heads from view,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "237573dcd82d4e30aa08b8411250e3c4",
      "question": "Where was Forrest killed?",
      "prediction": "southwest Atlanta, Georgia,",
      "ground_truths": [
        "in southwest Atlanta, Georgia,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "14e1933785044461a23c36b064f1ca0c",
      "question": "What three men were accused in the death?",
      "prediction": "Charman Sinkfield, 20; and Jquante Crews, 25",
      "ground_truths": [
        "and Jquante Crews,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "69d160735c2c4ecf933d73105ba74079",
      "question": "Which boxing champion was killed?",
      "prediction": "Vernon Forrest,",
      "ground_truths": [
        "Vernon Forrest,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cedc58cf8fa3405e8612bbdef684dbcf",
      "question": "What was the age of Vernon Forrest at the time of his death?",
      "prediction": "38,",
      "ground_truths": [
        "38,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e562f0b103a4805aaa6b2e539c8fb97",
      "question": "What reward was offered?",
      "prediction": "$17,000",
      "ground_truths": [
        "$17,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8037e7835754ed6b00599221a3351b1",
      "question": "How many men were accused of murder?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d154c947704e497fb83fe117c589917e",
      "question": "What boxing champion?",
      "prediction": "Vernon Forrest,",
      "ground_truths": [
        "Vernon Forrest,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d9fb0513fd644a388198e6f6f37096f9",
      "question": "What day in July was Vernon Forrest killed?",
      "prediction": "25.",
      "ground_truths": [
        "25."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa73c8d3b86b4f759ac802b5a8ac1112",
      "question": "What charges included  murder?",
      "prediction": "murder, felony murder, aggravated assault with a deadly weapon",
      "ground_truths": [
        "aggravated assault with a deadly weapon and possession of a firearm by a convicted felon,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "76f6b677a98341849679aa51229f8d69",
      "question": "What football star cleared of charge?",
      "prediction": "Steven Gerrard",
      "ground_truths": [
        "Steven Gerrard"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "43d665c7425145ed99cce24c58a8ed91",
      "question": "What was Gerrard's rationale?",
      "prediction": "\"I'm glad to put this case behind me and I am really looking forward to the season ahead and concentrating on my football now.",
      "ground_truths": [
        "he believed he was about to be attacked himself."
      ],
      "em": 0,
      "f1": 0.0625
    },
    {
      "id": "45b1985c5f744c6e93be01993d95010d",
      "question": "What did Gerrard admitted?",
      "prediction": "throwing three punches",
      "ground_truths": [
        "throwing three punches"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9660cdcc3e504d37aca14ad746928287",
      "question": "What player has cleared waivers?",
      "prediction": "England international footballer Steven Gerrard",
      "ground_truths": [
        "Steven Gerrard"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "a01ebb0233ee46afb7dc72e7d146dc1e",
      "question": "What was the verdict of the jury?",
      "prediction": "taken a little over an hour",
      "ground_truths": [
        "not guilty"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32a86d07321142fab582b0976e876d03",
      "question": "What was the reason for the punishment?",
      "prediction": "affray",
      "ground_truths": [
        "threatening behavior."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a36774065e944cc9ba1782363f8a3e1b",
      "question": "Iran criticizes who?",
      "prediction": "U.S. President-elect Barack Obama",
      "ground_truths": [
        "U.S. President-elect Barack Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0238ec9386c44699afc3d8a32e0d29e1",
      "question": "What are US and Iran relations tensioned about?",
      "prediction": "Iran's development of a nuclear weapon",
      "ground_truths": [
        "nuclear program."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "95bb179172e5461b822f2955ab4ea7b3",
      "question": "Who said Obama should apply campaign message?",
      "prediction": "Ali Larijani",
      "ground_truths": [
        "Ali Larijani"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5bbabe002ae84993bf2d810e91f0e01e",
      "question": "What should Obama apply according to speaker?",
      "prediction": "his campaign message of change to U.S. dealings with Iran.",
      "ground_truths": [
        "his campaign message of change"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e54deaf6751846a48c8ce39e038ea2bf",
      "question": "US - Iran tensions are high over what?",
      "prediction": "Iran's development of a nuclear weapon",
      "ground_truths": [
        "nuclear program."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "e4825a716b7849688276489483e5ee23",
      "question": "Who was the President of the US at this time?",
      "prediction": "Barack Obama",
      "ground_truths": [
        "Barack Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0886bb0eeac143bcb37ebd0ecf3e17cd",
      "question": "Who has tensions over Tehran's nuclear ambitions?",
      "prediction": "United States and Iran",
      "ground_truths": [
        "U.S."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3861c1666a0d464f82eadeb063bcaf06",
      "question": "Who criticized Obama ?",
      "prediction": "Iran's parliament speaker",
      "ground_truths": [
        "Iran's parliament speaker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da1702ba63dc476a939605d8e7bac2f2",
      "question": "Parliamentary speaker says who should apply campaign message of change?",
      "prediction": "U.S. President-elect",
      "ground_truths": [
        "U.S. President-elect Barack Obama"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f9872fe4f82246de9a9ba8194206ef69",
      "question": "What kind of weapons are being discussed?",
      "prediction": "nuclear",
      "ground_truths": [
        "nuclear"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "36285cfb993846c9878f228ba737a589",
      "question": "Who criticized Obama for saying nuclear weapon development is unaccaptable?",
      "prediction": "Iran's parliament speaker",
      "ground_truths": [
        "Iran's parliament speaker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "be0561947e1d4aeeae20e0bacda01994",
      "question": "Who spent nine years in prison?",
      "prediction": "Tim Masters,",
      "ground_truths": [
        "Tim Masters,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14749a452bd346f9abbffc18ac1278e9",
      "question": "Who seeks a dismissal of Tim Masters murder case?",
      "prediction": "a judge",
      "ground_truths": [
        "Colorado prosecutor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32c38f892d754ad89a9cfffa3d6afd7d",
      "question": "What was Masters convicted of?",
      "prediction": "murder",
      "ground_truths": [
        "first-degree murder charge"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8cf9e5b352044c15a7004e39dae7b80b",
      "question": "When was Masters released following the toss of his conviction?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f1afe460fefa4535a3ff8d744086e5d7",
      "question": "Who seeks dismissal?",
      "prediction": "A Colorado prosecutor",
      "ground_truths": [
        "Colorado prosecutor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ac839a5d98d842e589329484a33d49b6",
      "question": "Who was released on Tuesday?",
      "prediction": "Tim Masters,",
      "ground_truths": [
        "Tim Masters,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4159f0217b3f48ddbdee10ee5f4377e6",
      "question": "What was released?",
      "prediction": "new DNA evidence",
      "ground_truths": [
        "new DNA evidence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b1f93e5763f3443e812bb02db0b34dff",
      "question": "What was Masters convicted of in 1999?",
      "prediction": "murder",
      "ground_truths": [
        "first-degree murder"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f9a189e7d22948c78ac64341a4cb221c",
      "question": "What happened to the U.N. compound?",
      "prediction": "was killed by an Israeli airstrike,",
      "ground_truths": [
        "hit and set on fire,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de2414f2ccc343d4997480eed8d54a02",
      "question": "What does the lawmaker say?",
      "prediction": "\"My grandmother was ill in bed when the Nazis came to her home town of Staszow. A German soldier shot her dead in her bed,\"",
      "ground_truths": [
        "Israeli military action in Gaza is comparable to that of German soldiers during"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "311e5b81a69a40898d8f648ae9db3d58",
      "question": "Who are Israel being asked to talk to",
      "prediction": "Hamas,",
      "ground_truths": [
        "Hamas,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "997f584b4f774501a7365cf12f2fdbee",
      "question": "What has the UK PM called indefensible",
      "prediction": "shelling of the compound",
      "ground_truths": [
        "the shelling of the compound"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "52c7ca5b6e494e4f908bf7d7c348d2f1",
      "question": "What type of choice has Hamas made",
      "prediction": "murder of Palestinians.\"",
      "ground_truths": [
        "step up attacks against innocent civilians.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb192579a2d0438c935e63a19e1ff1f1",
      "question": "The second shot hit what?",
      "prediction": "Grant",
      "ground_truths": [
        "struck Grant in the upper right arm,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "a5031ac1cc9049608f116e78492a2901",
      "question": "The man was rescued from what in northern Australia?",
      "prediction": "crawling waters of the Adelaide River,",
      "ground_truths": [
        "the jaws of a crocodile"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c78d7fec07b14ea4aab3bbc8cef4e0ff",
      "question": "The men were collecting what on the river bank in the Northern Territory?",
      "prediction": "crocodile eggs",
      "ground_truths": [
        "crocodile eggs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c45d887d2074c3cb9b5c03e58b4c0c5",
      "question": "where Man rescues co-worker?",
      "prediction": "Australia's Northern Territory",
      "ground_truths": [
        "northern Australia"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "08e1f398fc554a2b9cc745c89b8ddd1c",
      "question": "What did Giuliana Rancic do?",
      "prediction": "Two weeks after undergoing a double mastectomy and reconstructive surgery,",
      "ground_truths": [
        "undergoing a double mastectomy and reconstructive surgery,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "fccd16a6079343e5ad862a07f82e3f42",
      "question": "What surgery did Rancic have?",
      "prediction": "double mastectomy and reconstructive surgery,",
      "ground_truths": [
        "double mastectomy and reconstructive"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "842b90f71d4142229534788a21189892",
      "question": "She says it feels great to be what?",
      "prediction": "back at work,\"",
      "ground_truths": [
        "back at work,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ecba8f966e03447a940cfff69f9ba8e1",
      "question": "Giuliana Rancic was back on the set where?",
      "prediction": "\"E! News\"",
      "ground_truths": [
        "\"E! News\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f1dcb1b8e9745ddb92444ccacb4b1b8",
      "question": "Who was back on the set at E!?",
      "prediction": "Giuliana Rancic",
      "ground_truths": [
        "Rancic"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f5894ccfbf314a63baaca983e208a43b",
      "question": "Where was Giuliana Rancic?",
      "prediction": "\"E! News\"",
      "ground_truths": [
        "\"E! News\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0ef3a30427c4d66a516feb413c6b262",
      "question": "What did Rancic say about it?",
      "prediction": "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"",
      "ground_truths": [
        "was a wonderful homecoming,\""
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "8834ade537ca4faca9fb3bdd7efcb88b",
      "question": "Rancic, 37, had the surgery after lumpectomies failed to eradicate her what?",
      "prediction": "breast cancer.",
      "ground_truths": [
        "breast cancer."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3b8f128366fb4a669715a4b6cfc166cd",
      "question": "What did she say?",
      "prediction": "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"",
      "ground_truths": [
        "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6e27ab10dda34bac8d427647fb0a3ce4",
      "question": "Who was greeted in Seoul?",
      "prediction": "news of Kim Jong Il's death",
      "ground_truths": [
        "the announcement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ae9f4575fc1464fa18740502542d2f5",
      "question": "What happened in 1994?",
      "prediction": "Kim Il Sung died",
      "ground_truths": [
        "Kim Il Sung died"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8aa85c5fbf0b4e21ac1f88d9cfdaf370",
      "question": "What was the reaction of South Korean military?",
      "prediction": "raised its alert level,",
      "ground_truths": [
        "raising its alert level,"
      ],
      "em": 0,
      "f1": 0.75
    },
    {
      "id": "b3f08e2a732a4b8aac4bdb22efbb6a03",
      "question": "Who died in 1994?",
      "prediction": "Kim Il Sung",
      "ground_truths": [
        "Kim Il Sung"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c20046f2bae84d69a5bc57eb097ed474",
      "question": "When did Kim II Sung die?",
      "prediction": "1994",
      "ground_truths": [
        "1994"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6842c4d6b88841b88274405ab3720cfb",
      "question": "Who says most people in the south are calm about the situation?",
      "prediction": "Woosuk Ken Choi,",
      "ground_truths": [
        "Woosuk Ken Choi,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5fb124107f024c4592306989f02b53b5",
      "question": "What level was raised?",
      "prediction": "its alert",
      "ground_truths": [
        "alert"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5f9519908e5c44a48cfdea1bccf3d78c",
      "question": "What was the reaction in Seoul?",
      "prediction": "the announcement",
      "ground_truths": [
        "astonishment"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da62aa39055e43479d74e92e330cbdd9",
      "question": "There was general astonishment where?",
      "prediction": "news of Kim Jong Il's death filtered through.",
      "ground_truths": [
        "Seoul,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f687cfbb7bec408799b8b6f8285b5608",
      "question": "What did cast member A.J. Jewell's death cause?",
      "prediction": "Taping of \"The Real Housewives of Atlanta\" reunion special,",
      "ground_truths": [
        "of \"The Real Housewives of Atlanta\" reunion special,"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "3e5332718ede4bcb99b7c336ff05b9dd",
      "question": "Who is the former fiance of Kandi?",
      "prediction": "Ashley \"A.J.\" Jewell,",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "059055bc2ed24376a2667ee2496aeb0c",
      "question": "Who was the former fiance of Kandi Burruss?",
      "prediction": "Ashley \"A.J.\" Jewell,",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "241efa1cac394ac983b144420a13a162",
      "question": "What show was scheduled to tape its reunion special recently?",
      "prediction": "\"The Real Housewives of Atlanta\"",
      "ground_truths": [
        "\"The Real Housewives of Atlanta\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e365fba82bca48ad8b9ec4fbac8bd3b7",
      "question": "Whose death caused the postponement of taping?",
      "prediction": "Ashley \"A.J.\" Jewell,",
      "ground_truths": [
        "Kandi Burruss' former fiancé, Ashley \"A.J.\" Jewell,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "c4c3e45a17884ef78442ed584e442741",
      "question": "When was the reunion scheduled for?",
      "prediction": "last week,",
      "ground_truths": [
        "last week,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "828c73b72b124615b8ccc1cf1ab1014b",
      "question": "When was the \"Real Housewives of Atlanta\" scheduled to tape their reunion special?",
      "prediction": "October 29 and November 5.",
      "ground_truths": [
        "last week,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f92d606a35b4cccae34b03477b7f88b",
      "question": "Where did the deadly earthquake happen?",
      "prediction": "Haiti.",
      "ground_truths": [
        "Haiti."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3ae1b671353c41eb9fb31fc466e14155",
      "question": "Where did he lead the effort?",
      "prediction": "Haiti.",
      "ground_truths": [
        "in Haiti."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "67a5828b6f5c4b7cbffe811a9e18290d",
      "question": "Is the cause of ibs known?",
      "prediction": "The exact cause of IBS remains unknown,",
      "ground_truths": [
        "remains unknown,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "05fd8aeb14d24b5fbb59d596f4aebf8a",
      "question": "What percent of North Americans have ibs?",
      "prediction": "About 10 to 15",
      "ground_truths": [
        "10 to 15"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "4829965548f840809e931e6b957f085c",
      "question": "What does the study analysis say works?",
      "prediction": "soluble fiber,",
      "ground_truths": [
        "Peppermint oil, soluble fiber, and antispasmodic drugs can indeed help people with irritable bowel syndrome,"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "7e8255671f0a4fbd88c688b4e6b2bee8",
      "question": "Approximately how many people in North America have IBS?",
      "prediction": "10 to 15 percent",
      "ground_truths": [
        "10 to 15 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "739854dee1cc4fbc991ea07a0e593389",
      "question": "What treatments work for ibs?",
      "prediction": "a variety of",
      "ground_truths": [
        "including fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "914623f6bde244cab5e20f01a6d2560b",
      "question": "who  has filed suit with international court?",
      "prediction": "Australian",
      "ground_truths": [
        "Australian officials"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d974393cef5d43e791127eaed8ddb259",
      "question": "What does the moratorium allow hunting whales for?",
      "prediction": "scientific reasons.",
      "ground_truths": [
        "scientific reasons."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8294085ecc44a89bc6838c7ab93f1e0",
      "question": "who says decision to head to court \"regrettable\"?",
      "prediction": "Sobashima",
      "ground_truths": [
        "Japanese Foreign Ministry spokesman Hidenobu Sobashima"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "16083c1098654497a127f77ea191c127",
      "question": "What has Austrailia filed  suit over?",
      "prediction": "Japan's whale-hunting practices,",
      "ground_truths": [
        "to stop Japan from exploiting the research loophole."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3287651f8746453ba1943191b85095a9",
      "question": "who allows hunting whales for scientific reasons?",
      "prediction": "Japan",
      "ground_truths": [
        "Japanese officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64eba29acb64482196d9e21f5170039d",
      "question": "what resumes TNT?",
      "prediction": "\"The Closer.\"",
      "ground_truths": [
        "\"The Closer.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8603756cbaaa464fb160ab2901bfbb00",
      "question": "what is a sag award",
      "prediction": "Screen Actors Guild",
      "ground_truths": [
        "Screen Actors Guild"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1a6aa177d7f744399fc722a923f38660",
      "question": "When does TNT resume the series?",
      "prediction": "Monday night",
      "ground_truths": [
        "Monday night"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53c67cb7647d4cc4bb23eaf2d317008d",
      "question": "Who did Superman battle in \"Clan of the Fiery Cross\"?",
      "prediction": "men in white hoods.",
      "ground_truths": [
        "Ku Klux"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d34861cb019d4deaaeaa08250ca1b4fc",
      "question": "What have affected people in real life?",
      "prediction": "Klan",
      "ground_truths": [
        "comic book characters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fee8b22532204c5c87bb96182e128d56",
      "question": "What group did Superman battle in \"Clan of the Fiery Cross\"?",
      "prediction": "Ku Klux Klan,",
      "ground_truths": [
        "the Ku Klux Klan,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebc8ddb0bc9e409e88de995ce9a063ab",
      "question": "Who did Superman battle in the radio series?",
      "prediction": "The Daily Planet reporter Jimmy Olsen,",
      "ground_truths": [
        "Ku Klux Klan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3811aec484274bd99fb8231b001e9e02",
      "question": "Who blocked a scientist from getting a patent?",
      "prediction": "Kruyer",
      "ground_truths": [
        "Donald Duck"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ecfb1c98af64534bda5385ce6253550",
      "question": "What cartoon character blocked a scientist from getting a patent?",
      "prediction": "Donald Duck",
      "ground_truths": [
        "Donald Duck"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7c9c76f0655142e8b33e0448f3f94777",
      "question": "Where was the teenage boy shot?",
      "prediction": "Athen,",
      "ground_truths": [
        "Athens,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37b9919bd53e4cd8ad12b4a1d646bf36",
      "question": "How many civilians were injured?",
      "prediction": "34",
      "ground_truths": [
        "34"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e4c8a8c9e174680a247362b4b0fe77c",
      "question": "What caused protests to explode?",
      "prediction": "the boy's death",
      "ground_truths": [
        "killing of a 15-year-old boy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01ea084994484e5caa8c9cb317cb11a5",
      "question": "Authorities vow to do what in regards to the rioting?",
      "prediction": "re-impose order",
      "ground_truths": [
        "re-impose order"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8e54fc7e65e14768a22401ac648db015",
      "question": "Where is the rioting happening?",
      "prediction": "Th Thessaloniki and Athens,",
      "ground_truths": [
        "across Greece"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "194a8ecbcf0d4d17b469be92d066ad93",
      "question": "Number of civilians injure during riots?",
      "prediction": "34",
      "ground_truths": [
        "34"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "708781de86994f85a8cf27d7d4d9552a",
      "question": "How many civilians were injured in the rioting?",
      "prediction": "34",
      "ground_truths": [
        "34"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "815cf0a1a5dd4a31b9b6b94e62e31435",
      "question": "Where was the teenager shot at?",
      "prediction": "Th Thessaloniki",
      "ground_truths": [
        "Athens,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f9d580fe51264fe1948adba52d906ee5",
      "question": "Who is rioting?",
      "prediction": "demonstrators",
      "ground_truths": [
        "young self-styled anarchists"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5252a53cef7d419a8ea37a3beb22e2fa",
      "question": "What do authorities vow to re-impose?",
      "prediction": "order",
      "ground_truths": [
        "order"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4c426e2c625f463f858241fbb08db079",
      "question": "Who is suing the ICE?",
      "prediction": "American Civil Liberties Union",
      "ground_truths": [
        "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ffae4c1a213448c93aae455b39ac77b",
      "question": "WHAT HAVE 1073 DETAINEES HAD SINCE 2003?",
      "prediction": "psychotropic medications",
      "ground_truths": [
        "\"medical escorts\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d20e69e95bb407d8eb523d6cfeb4c45",
      "question": "Which senator vows to investigate the allegations?",
      "prediction": "Lieberman",
      "ground_truths": [
        "Sen. Joe Lieberman,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "93fd50e8402b4c80bc553d18f4d6e818",
      "question": "Who did the detainees sue",
      "prediction": "agency",
      "ground_truths": [
        "the government."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8914c97e8b7143239e4c0f5be45b7a7a",
      "question": "What number of detainees had medical escorts since 2003?",
      "prediction": "1,073",
      "ground_truths": [
        "1,073"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b9f576ae3984970ae5ee0cbfcbc2af0",
      "question": "What did they tell CNN?",
      "prediction": "they were injected with the drugs against their will.",
      "ground_truths": [
        "were injected with the drugs against their will."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "aa66675458b041748d8d40a8140f82cb",
      "question": "What is ICE alleged to have done to detainees?",
      "prediction": "forcibly injecting them with psychotropic drugs",
      "ground_truths": [
        "forcibly injecting them with psychotropic drugs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "106958546d0a4d9cb5effa643be73014",
      "question": "What news station interviewed the detainee",
      "prediction": "Sen. Joe Lieberman,",
      "ground_truths": [
        "CNN"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd85694b24f8489e98a5beeeeec3d12b",
      "question": "Who did the detainees file a suit against?",
      "prediction": "the agency",
      "ground_truths": [
        "Immigration and Customs Enforcement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32e19ff72cc948d5bba10d5b45071a03",
      "question": "what is the senator doing",
      "prediction": "to ensure that detainees are not drugged unless there is a medical reason to",
      "ground_truths": [
        "intends to follow up with ICE"
      ],
      "em": 0,
      "f1": 0.10526315789473684
    },
    {
      "id": "d9a79c663c2f4359bc85a93bd49e4f90",
      "question": "what were detainees allegedly injected with?",
      "prediction": "psychotropic drugs",
      "ground_truths": [
        "psychotropic drugs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9cdb364365444a79a58033a1879248ce",
      "question": "who is lieberman?",
      "prediction": "I-Connecticut,",
      "ground_truths": [
        "\"Senator"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a9b90b0d783413c9c6ff456e7dfe0ee",
      "question": "who had medical escorts?",
      "prediction": "1,073 immigration detainees",
      "ground_truths": [
        "1,073 immigration detainees"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2984697f6d3c4ac79d2c916f3496a4d2",
      "question": "What is Sen. Lieberman vowing?",
      "prediction": "to ensure that detainees are not drugged unless there is a medical reason to do so.\"",
      "ground_truths": [
        "detainees are not drugged unless there"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "cc289ea875ee4805a12afd70d6c50f6b",
      "question": "What number of detainees have had \"medical escorts\" since 2003?",
      "prediction": "1,073",
      "ground_truths": [
        "1,073"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b8d6ee43c6941e5a772b201f4865aa9",
      "question": "What is the ICE suit about?",
      "prediction": "forcefully injecting them with psychotropic drugs while trying to shuttle",
      "ground_truths": [
        "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation."
      ],
      "em": 0,
      "f1": 0.6666666666666667
    },
    {
      "id": "4bb35180c958469b818d1a49460ce8c0",
      "question": "who files suit?",
      "prediction": "Amadou Diouf of Senegal",
      "ground_truths": [
        "American Civil Liberties Union"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0197137fc90f4fe2a809e0651f965ac0",
      "question": "what did he say",
      "prediction": "he was injected with drugs by ICE agents against",
      "ground_truths": [
        "was injected with drugs by ICE agents against his will."
      ],
      "em": 0,
      "f1": 0.8421052631578948
    },
    {
      "id": "cdfa787715a043bca7f13b775f44949f",
      "question": "What dog breed is described as \"active athletes\"?",
      "prediction": "Portuguese water",
      "ground_truths": [
        "Portuguese water"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c4638c3ddb3e49bea26f3ec538123eec",
      "question": "What are Portugese water dogs like?",
      "prediction": "more likely to rip a couch into pieces than lounge on it,",
      "ground_truths": [
        "are \"active athletes,\" far from couch potatoes,"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "13dfe50cd8404e7a9ef7f8d3dccae6ee",
      "question": "Who is more likely to rip up the couch, than lounge on it?",
      "prediction": "Bo,",
      "ground_truths": [
        "Portuguese water dogs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c20f39bfa2d4fdca81983ab72b56537",
      "question": "What issue is the breeders concerned about?",
      "prediction": "not necessarily a thorough understanding of the dogs' needs,",
      "ground_truths": [
        "a thorough understanding of the dogs' needs,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "f031f8ed49f8468386dc99013ce53127",
      "question": "Who is Bo the dog?",
      "prediction": "nation's new \"first dog\"",
      "ground_truths": [
        "gift to the Obama girls from Sen. Ted Kennedy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c479c64936a2439f93eee746440725c3",
      "question": "What is his name?",
      "prediction": "Jeffrey Jamaleldine",
      "ground_truths": [
        "Jeffrey Jamaleldine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bcf61d09606341f89cbf4bf4e6a1c2b1",
      "question": "Where did he go to college?",
      "prediction": "Missouri",
      "ground_truths": [
        "in Missouri"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d388f477244347c793cd528aa6e03473",
      "question": "Soldier was one of more than 20,000 \"green-card warriors\"",
      "prediction": "Jeffrey Jamaleldine",
      "ground_truths": [
        "Jamaleldine"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8173dbf1042c40eeb456d8842e319da0",
      "question": "What do people still believe?",
      "prediction": "in that.",
      "ground_truths": [
        "\"You can go from rags to riches"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "88f9909fba5a41f0bbaddd549d1003bc",
      "question": "What type of soldier is this?",
      "prediction": "U.S. Army scout who proudly wears a Stetson hat and spurs on his boots,",
      "ground_truths": [
        "U.S. Army scout"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "6ebd5652000e4e6c9d486c4873e23e06",
      "question": "What does his dad wonder?",
      "prediction": "wondering why he went to Iraq war.",
      "ground_truths": [
        "Why he's more American than a German,"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "fa44913cfa244faa88e773dcc3045012",
      "question": "what's the cyclist's Olympic record?",
      "prediction": "gold",
      "ground_truths": [
        "fifth"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8caeb3cde3074e66af2795f258908f69",
      "question": "who was a three-time road race world champion by 1988?",
      "prediction": "Longo-Ciprelli",
      "ground_truths": [
        "Longo-Ciprelli"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f87ac51d4db74ff7b51a4c5ea00723e8",
      "question": "Where did they win medals?",
      "prediction": "Atlanta",
      "ground_truths": [
        "Atlanta,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f888582a9e324a639852992392b5a0cf",
      "question": "who won the record",
      "prediction": "Jeannie Longo-Ciprelli",
      "ground_truths": [
        "Longo-Ciprelli"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "aad4e0033fd1472a8b6b4c3d1eec2096",
      "question": "Who is the president?",
      "prediction": "Bush",
      "ground_truths": [
        "Bush"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "19f2498cca6d46d78d4e3f5bfda08cff",
      "question": "Will there be any restrictions on funding the wars",
      "prediction": "the supplemental spending bill provides nearly $162 billion in",
      "ground_truths": [
        "without the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aea4400d534b40b5926322dac2091f01",
      "question": "What victory did bush get",
      "prediction": "Supplemental spending bill",
      "ground_truths": [
        "the bill"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f72b2c5781a44d10a48345cabe13be80",
      "question": "What is the benefit for veterans?",
      "prediction": "a 13-week extension of unemployment",
      "ground_truths": [
        "education"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "71827c25726b4b9b863d47e26f55e597",
      "question": "What else does the bill contain?",
      "prediction": "provides nearly $162 billion in war funding",
      "ground_truths": [
        "that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest that have been hit by record floods."
      ],
      "em": 0,
      "f1": 0.13636363636363638
    },
    {
      "id": "55a7f38bc3564f9eb6657cad876e5228",
      "question": "What causes a victory for President Bush?",
      "prediction": "supplemental spending bill",
      "ground_truths": [
        "the bill"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1a380b010fa1471dbf4636c7d6ede83d",
      "question": "what does legislation fund",
      "prediction": "wars in Iraq and Afghanistan",
      "ground_truths": [
        "wars in Iraq and Afghanistan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8c3fb24a332840c2ba5589d8aabb4d8d",
      "question": "what does bill contains",
      "prediction": "$162 billion in war funding",
      "ground_truths": [
        "nearly $162 billion in war funding"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "d9fba116703f4e9c8fb2a6ac44b98d96",
      "question": "what did bush do",
      "prediction": "signed a bill",
      "ground_truths": [
        "signed a bill that will pay for the wars in Iraq and Afghanistan"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "faf67589ec1f4d73a282e7e977edfa4f",
      "question": "What is the budget for the spending bill?",
      "prediction": "nearly $162 billion",
      "ground_truths": [
        "$162 billion"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f9ef08e2693943069670c3b422f8513a",
      "question": "What did he sign?",
      "prediction": "a bill",
      "ground_truths": [
        "a bill that will pay for the wars in Iraq and Afghanistan"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "7e4328290edf4f2aac9f8988b5079fe1",
      "question": "Can non European players be in the squad?",
      "prediction": "Ronaldinho",
      "ground_truths": [
        "frees up a place for another non-European Union"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74f114ab49744e608eafab899371d913",
      "question": "Who is granted dual nationality?",
      "prediction": "Barcelona's Brazil forward Ronaldinho",
      "ground_truths": [
        "Ronaldinho"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "8eab99234f214cfeba8bd193043cc3b8",
      "question": "What does this move mean for the squad?",
      "prediction": "grants frees up a place",
      "ground_truths": [
        "frees up a place"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "b31c60bda086454a900948cde75eb053",
      "question": "Where was he given dual nationality?",
      "prediction": "Spain",
      "ground_truths": [
        "Spain"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68b7a9d693f84289b1d8cca76c5daf3b",
      "question": "What has Robaldinho been granted by Spain?",
      "prediction": "dual nationality",
      "ground_truths": [
        "dual nationality"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c17b16d7c4e24f608c9af17cde7c62fb",
      "question": "Who stars in \"The Da Vinci Code\"?",
      "prediction": "Tom Hanks,",
      "ground_truths": [
        "Ewan McGregor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d40b03e0f9144f2b023013be680715e",
      "question": "What is the name of the other actor?",
      "prediction": "Ewan McGregor",
      "ground_truths": [
        "Ayelet Zurer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bd66428a35a144249fb8f291f3a16519",
      "question": "What is the name of the lead actor in the movie?",
      "prediction": "Tom Hanks",
      "ground_truths": [
        "Tom Hanks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1abdfb6c7fd4491a8ccbf4ebc845ccae",
      "question": "The president of which body said  \"It's all a lie\"?",
      "prediction": "Bill Donohue,",
      "ground_truths": [
        "the Catholic League."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "96b7abd0798c466aa7fc3dcc599472ab",
      "question": "Which actor starred in both movies?",
      "prediction": "Tom Hanks",
      "ground_truths": [
        "Tom Hanks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "99943e6e0c334768bf246ebb304d242a",
      "question": "Who is Tom's famous cast mate?",
      "prediction": "Ewan McGregor",
      "ground_truths": [
        "Ewan McGregor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f003b5f374324c6389dffc812158ffa8",
      "question": "What was siad by the Catholic League president?",
      "prediction": "\"You've got [Dan] Brown, [Ron] Howard and [Tom] Hanks in the movie all alleging that the Illuminati",
      "ground_truths": [
        "\"I have a strong objection to the genre of mixing fact with fiction,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "07d741fb951242a08936d7b32f7d6235",
      "question": "Angels & Demons was the sequel to which other film?",
      "prediction": "\"The Da Vinci Code\"",
      "ground_truths": [
        "\"The Da Vinci Code\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2433c32ec61452a87d61970da188c26",
      "question": "What HBO show was he on?",
      "prediction": "\"The Sopranos,\"",
      "ground_truths": [
        "\"The Sopranos,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5df04119979040a09204fba80e103a01",
      "question": "Who did he meet with to discuss the issue?",
      "prediction": "the Obama and McCain camps",
      "ground_truths": [
        "Obama and McCain camps"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4d4955503d14398a933b782c9c3c7d0",
      "question": "Who did he meet with?",
      "prediction": "Obama and McCain camps",
      "ground_truths": [
        "Obama and McCain camps"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77d545d2f95f41769e14c9207ea9cba9",
      "question": "What does the adovocacy group promote?",
      "prediction": "mental health and recovery.",
      "ground_truths": [
        "mental health and recovery."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41f884e430af48e1bc0bd5948b694667",
      "question": "Who is the co-founder of the advocacy group No Kidding, Me Too?",
      "prediction": "Joe Pantoliano,",
      "ground_truths": [
        "Joe Pantoliano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0c13f8b533ca4107bff9421662d0354f",
      "question": "What did the actor act in before?",
      "prediction": "Ralph Cifaretto",
      "ground_truths": [
        "\"The Sopranos,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60bbae0fbec24bf2a8de8ac0b7076b24",
      "question": "Who can Dublin rival?",
      "prediction": "Silicon Valley.",
      "ground_truths": [
        "Silicon Valley."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7eb4ea422a0440ab1a2ea0d9a313198",
      "question": "Where are headquartered Google and Facebook?",
      "prediction": "Dublin.",
      "ground_truths": [
        "Dublin."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e1d564bb5eb4b1ba37db917c472a566",
      "question": "What companies have their headquaters in Ireland?",
      "prediction": "Twitter",
      "ground_truths": [
        "Facebook and Google,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19d523d3a0824aed8cc62eed5e24f1e4",
      "question": "What provides Dogpatch Labs Europe?",
      "prediction": "a space for aspiring entrepreneurs to brainstorm with like-minded people.",
      "ground_truths": [
        "a space for aspiring entrepreneurs to brainstorm with like-minded people."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e52f520bbf54c38a827ebb5d2069dd2",
      "question": "Who already has headquarters in Ireland?",
      "prediction": "Facebook",
      "ground_truths": [
        "Facebook and Google,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3e4f8451992547708f6dac3268baa850",
      "question": "Who provides space for entrepreneurs?",
      "prediction": "Dogpatch Labs",
      "ground_truths": [
        "Dogpatch Labs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2a7071c8d8c74dac901956228d59fdfb",
      "question": "What did Twitter announce?",
      "prediction": "its intention to set up headquarters in Dublin.",
      "ground_truths": [
        "its intention to set up headquarters in Dublin."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ccf8e9a161124b058363c911f27cc4bf",
      "question": "Which river has plunged to record low levels?",
      "prediction": "The Tigris",
      "ground_truths": [
        "Tigris"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1fed0c068f89431696e283427f59f39d",
      "question": "What does Iraq want Turkey to do?",
      "prediction": "increase the flow of water passing through its network of dams.",
      "ground_truths": [
        "to increase the flow of water passing through its network of dams."
      ],
      "em": 0,
      "f1": 0.9523809523809523
    },
    {
      "id": "f7776c027aca468e87d60716cfe3c1c3",
      "question": "What river has plunged to record lows?",
      "prediction": "Tigris",
      "ground_truths": [
        "Tigris"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed20dcee741347bfb50dce447e13bc9b",
      "question": "What happened to the Tigris River?",
      "prediction": "has plunged to record low levels,",
      "ground_truths": [
        "has plunged to record low levels,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3b6bbfa360e14c8ea4997b82981bf6f7",
      "question": "What country does Iraq and Syria want to increase its water flow?",
      "prediction": "Turkey,",
      "ground_truths": [
        "Turkey,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "72a47042051c41f19706f18a5021cae3",
      "question": "What did Turkey do?",
      "prediction": "increase the flow of water passing through its network of dams.",
      "ground_truths": [
        "provided Syria and Iraq 500 cubic meters of water a second,"
      ],
      "em": 0,
      "f1": 0.20000000000000004
    },
    {
      "id": "ab351f57bcf34ad6a535ed69e1eed04b",
      "question": "What have they withdrawn from each others' capitals?",
      "prediction": "ambassadors",
      "ground_truths": [
        "ambassadors"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0371094da0dc476383a99661fe910873",
      "question": "Where is the rivers' source located?",
      "prediction": "Turkey,",
      "ground_truths": [
        "Turkey,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e9afbf1faa445db8535a1693283216a",
      "question": "Is pre-marital sex legal in Saudi Arabia?",
      "prediction": "illegal",
      "ground_truths": [
        "is illegal"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4ad550b97d884435b955434af7338463",
      "question": "What did Jawad talk about?",
      "prediction": "discussion",
      "ground_truths": [
        "foreplay, sexual conquests and how he picks up women,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d23aa2169253403daacd4adcdb967b6f",
      "question": "For what reason did Mazen Abdul Jawad apologize?",
      "prediction": "he discussed foreplay, sexual conquests and how he picks up women,",
      "ground_truths": [
        "his comments while Saudi authorities discuss whether he should be charged with a crime,"
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "3e854a8400dc4ca088df653f568d22b5",
      "question": "What are Saudi authorities debating?",
      "prediction": "whether he should be charged",
      "ground_truths": [
        "whether he should be charged with a crime,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "d9c8cde9f1c348f1848e4fe717df53b0",
      "question": "What did Mazen Abdul apologize for?",
      "prediction": "bragging about his sex life on television",
      "ground_truths": [
        "bragging about his sex life on television"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "102cad20644846c3b2cdfc5747db597b",
      "question": "What show was Jawad on?",
      "prediction": "\"Red Lines,\"",
      "ground_truths": [
        "\"Red Lines,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b51dcde14bb747ce8f791ba4ac1bfb2a",
      "question": "North Korea recently threatened to \"wipe out\" what country  if provoked?",
      "prediction": "the United States",
      "ground_truths": [
        "United States"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c41bd9457ee40e9b264162b24aa4f4d",
      "question": "Who does Japanese media reporte North Korea may fire a missile at?",
      "prediction": "Hawaii",
      "ground_truths": [
        "Hawaii."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3a6cb4766efe4fa4b1914c9d24664bd5",
      "question": "Who did North Korea threaten to \"wipe out\" if provoked?",
      "prediction": "\"It's not particularly difficult to fire off\" short- and medium-range missiles.",
      "ground_truths": [
        "the United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aba879c643644d1998692eb72c3c14d6",
      "question": "U.S. does not believe who intends to launch long-range missile soon?",
      "prediction": "North Korea",
      "ground_truths": [
        "North Korea"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "385fa0ebb7d64fa8bd1df50624998ef7",
      "question": "Who was warned to be clear due to \"military firing exercise\"?",
      "prediction": "mariners",
      "ground_truths": [
        "mariners"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "163c4658e8ca400fa498b30004f817b6",
      "question": "What does US believe?",
      "prediction": "North Korea will \"continue its provocations.\"",
      "ground_truths": [
        "North Korea intends to launch a long-range missile in the near future,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "6e111544f05745d693b3849705dbca73",
      "question": "What did North Korea recently threaten?",
      "prediction": "\"wipe out\" the United States if provoked.",
      "ground_truths": [
        "\"wipe out\" the United States if provoked."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e746904f3d2148d599c71540581a6fbf",
      "question": "What may North Korea due to Hawaii on July 4?",
      "prediction": "short- and medium-range missile tests,",
      "ground_truths": [
        "fire a missile toward"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "f959d7fa37cb45e09337e1475d63528e",
      "question": "When did the japanese media report?",
      "prediction": "July 4.",
      "ground_truths": [
        "July 4."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "675dc33e5be84d6d924a01c09f145723",
      "question": "Who refuses to broadcast ad?",
      "prediction": "The BBC",
      "ground_truths": [
        "The BBC"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2aff93bef4404bea9b6fe5abdf14bcb3",
      "question": "What is BBC funded by?",
      "prediction": "an obligatory license fee",
      "ground_truths": [
        "an obligatory license fee paid"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "37e1274e4a2b4b0c950789fddf51921c",
      "question": "What is the name of the charity group?",
      "prediction": "Disasters Emergency Committee,",
      "ground_truths": [
        "British Red Cross, Oxfam, Save the Children and 10 other charities,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a21c75fc6865449c8002290923056925",
      "question": "What do the protesters occupy?",
      "prediction": "the foyer of the BBC building in Glasgow, Scotland",
      "ground_truths": [
        "Glasgow office"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "676b4a9625a446868ff66785db808efe",
      "question": "Where did protestors occupy?",
      "prediction": "BBC building in Glasgow, Scotland",
      "ground_truths": [
        "the foyer of the BBC building in Glasgow, Scotland"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "5af072c9fcd04a99b64b2546fb15390d",
      "question": "what is the ad about?",
      "prediction": "a major ongoing news story, in which humanitarian issues -- the suffering and distress of civilians and combatants on both sides of the conflict, the debate",
      "ground_truths": [
        "aid to Gaza,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46c8ca9995974e1086fae3543f6a8d4d",
      "question": "on which date disasters emergency committee will launch appeal?",
      "prediction": "Monday.",
      "ground_truths": [
        "Monday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "de5cb70961984db4a811b269f8257857",
      "question": "Carter's sentencing was postponed so he could get what?",
      "prediction": "some dental work done, including removal of his diamond-studded braces.",
      "ground_truths": [
        "some dental work done,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8637839ccc094e64977ef2984182f7d3",
      "question": "Rapper Dwayne Carter will be sentenced for what kind of conviction?",
      "prediction": "gun",
      "ground_truths": [
        "gun"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14e1e5a0902d46a3b256f6cf8bcf5df4",
      "question": "What does the work?",
      "prediction": "get some dental",
      "ground_truths": [
        "removal of his diamond-studded braces."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5249075dcf8646b19ea8e050a4cfc254",
      "question": "Who will be condemned?",
      "prediction": "Rapper Lil Wayne",
      "ground_truths": [
        "Rapper Lil Wayne"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c6a22911be0943c69c493ec69afa746d",
      "question": "Work includes removal of diamond-encrusted what?",
      "prediction": "braceals.",
      "ground_truths": [
        "braces."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60f6708411f94ae2980e44346f73c8e5",
      "question": "Which gene did the ALS association discover?",
      "prediction": "ALS6,",
      "ground_truths": [
        "ALS6,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1395b6cae1c4d4ebcf98b76d83b90db",
      "question": "How many people does Lou Gehrig's disease effect?",
      "prediction": "10 percent of those cases are hereditary.",
      "ground_truths": [
        "5,600"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "14c01f1d03094e0ea0dd5b59f604d270",
      "question": "What number of people get ALS each year?",
      "prediction": "5,600",
      "ground_truths": [
        "5,600"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0d1230a75b547ca9237bde1e5ad0bf6",
      "question": "What does the ALS call the gene discovery?",
      "prediction": "\"momentous discovery\"",
      "ground_truths": [
        "\"momentous discovery\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a9d8c69172e3474097e6ad2e51cdaed6",
      "question": "How many people has ALS or Lou Gehrig's disease?",
      "prediction": "5,600",
      "ground_truths": [
        "5,600"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03e5521db7994b1d9c01547a2ec51aac",
      "question": "How much was seized?",
      "prediction": "44 firearms, 650 pounds of marijuana, 435 pounds of methamphetamine and $7.8 million in cash",
      "ground_truths": [
        "of methamphetamine and $7.8 million in cash"
      ],
      "em": 0,
      "f1": 0.6363636363636364
    },
    {
      "id": "5ce2330296d142a2a154298ea4339e6c",
      "question": "How long was the investigation?",
      "prediction": "15-month",
      "ground_truths": [
        "15-month"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7f8d3c8ec8d14efb97d10bd29838df03",
      "question": "Where there any other drugs recovered?",
      "prediction": "4.5 pounds of heroin,",
      "ground_truths": [
        "123 pounds of cocaine and 4.5 pounds of heroin,"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "58066ea944fe443a9283d135d5dafaac",
      "question": "What else did the authorities recover?",
      "prediction": "than 200 arrests and the recovery of 123 pounds of cocaine and 4.5 pounds of heroin,",
      "ground_truths": [
        "123 pounds of cocaine and 4.5 pounds of heroin,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "e476ba8414ac461baf6b78958906f247",
      "question": "How much cash did the authorities seize?",
      "prediction": "$7.8 million",
      "ground_truths": [
        "$7.8 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4795a6f9e6c24cd194173fe0e0b8b607",
      "question": "How many pounds of marijuana?",
      "prediction": "650",
      "ground_truths": [
        "650"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dbd9d9e9ee1a4314b6e46179fe3db41f",
      "question": "What was the investigation dubbed?",
      "prediction": "\"Operation Crank Call,\"",
      "ground_truths": [
        "\"Operation Crank Call,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2a8e2d0e16d64cc2a03180e7dfb8d745",
      "question": "Where was the overcrowded ferry?",
      "prediction": "Bangladesh,",
      "ground_truths": [
        "Bangladesh,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "99331d54ed084ccf98837531ede526e3",
      "question": "Where were the people traveling to?",
      "prediction": "their homes in Bhola",
      "ground_truths": [
        "Bhola"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "72fa80f3ffa24cadbc2df7c806b0b59b",
      "question": "How many died in ferry capsize?",
      "prediction": "28 passengers,",
      "ground_truths": [
        "28"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fe231269e136464c857e4cedaa8a523e",
      "question": "Where did the ferry depart from?",
      "prediction": "Dhaka,",
      "ground_truths": [
        "Dhaka,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e8d8610abde45809ece08144c48d9e7",
      "question": "By how many was the boat overcrowded?",
      "prediction": "about 2,000 people,",
      "ground_truths": [
        "2,000 people,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "ef29092974ce40b980f963ac746247ec",
      "question": "How many people were on board the ferry?",
      "prediction": "about 2,000",
      "ground_truths": [
        "2,000"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "22389bcc734d49d98fdccaf2c36e5c7f",
      "question": "What was conveyed?",
      "prediction": "sincerity",
      "ground_truths": [
        "our sincerity"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4166369ecf494ae998597cee8f2be647",
      "question": "What did the president of Toyota say he takes full responsibility for?",
      "prediction": "safety issues in the company's cars",
      "ground_truths": [
        "cars"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "35d000cd64d14585bd474322dc4f4f70",
      "question": "What did the president of Toyota say?",
      "prediction": "takes full responsibility for safety issues in the company's cars and vowed to regain the",
      "ground_truths": [
        "he takes full responsibility for safety issues in the company's"
      ],
      "em": 0,
      "f1": 0.7272727272727274
    },
    {
      "id": "811443af769846bd88db9031899cf838",
      "question": "Who should be held responsible?",
      "prediction": "the chief executive officer,",
      "ground_truths": [
        "Akio Toyoda"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2efef198621244dc8f660e49bcb0b54e",
      "question": "Who should be responsible?",
      "prediction": "the chief executive officer,",
      "ground_truths": [
        "the chief executive officer,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ffa4d47207614c1a94eed9f7a08fc862",
      "question": "What did Toyota's president say?",
      "prediction": "\"I would like to really listen to the customers' voices, and together with the dealership, distributor, suppliers, we need to work together, and we would like to work together and to strive for regaining the trust once again from our customers.\"",
      "ground_truths": [
        "he takes full responsibility for safety issues in the company's"
      ],
      "em": 0,
      "f1": 0.0425531914893617
    },
    {
      "id": "7594504b380443f6920beb03b1379bd7",
      "question": "What are the safety issues?",
      "prediction": "sudden acceleration.",
      "ground_truths": [
        "related to sudden acceleration."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "eee8cb77eccf4a5bab0fb2fe51c21dbf",
      "question": "When was it isolated?",
      "prediction": "1983",
      "ground_truths": [
        "1983"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "63a371a61f894309b90bdfc835cee0e3",
      "question": "Who pioneered lithium treatment?",
      "prediction": "Dr. Cade",
      "ground_truths": [
        "Dr. Cade"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "85ea67cf87f543148a3400fbb552a06c",
      "question": "When was HIV isolated?",
      "prediction": "1977.",
      "ground_truths": [
        "1983"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8189238b8f3c401f81c92d4cf28e7324",
      "question": "Many gay rights activists applaud Obama's what?",
      "prediction": "commitment to equality,",
      "ground_truths": [
        "on supporting full marriage equality,\""
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "4c67b27ffb9d441c9aa84fd03b2de3f9",
      "question": "what did the president do",
      "prediction": "he is committed to equality,",
      "ground_truths": [
        "He acknowledged \"we have more work to do,\" including on the issue of bullying."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "dbb8fc455e94400d89d4d1542418e945",
      "question": "what will obama do",
      "prediction": "speeches on gay rights issues",
      "ground_truths": [
        "\"we have more work to do,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c35b0a8ddbc4c6a8e8cfd53cfa6717e",
      "question": "who do the gay rights activists applaud",
      "prediction": "Barack Obama,",
      "ground_truths": [
        "the administration's progress,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ee81fad60d2d42bcbe7813550fee6aaa",
      "question": "Who says he will continue to advocate for equality?",
      "prediction": "President Barack Obama,",
      "ground_truths": [
        "President Barack Obama,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8caac9743984aa4a1719e71cdf2c70c",
      "question": "who will continue to advocate for equality",
      "prediction": "Barack Obama,",
      "ground_truths": [
        "President Barack Obama,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f2bc0ffd037f4b9397e5d7eb0e547819",
      "question": "What are hot spots for drug use?",
      "prediction": "entertainment venues",
      "ground_truths": [
        "clubs and bars in Hong Kong and Shenzhen,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c84dbe83b4934408b32e1799184873fe",
      "question": "Where else are drug hotspots?",
      "prediction": "Hong Kong and Shenzhen,",
      "ground_truths": [
        "public toilets and playgrounds."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "84087ea7a70e4646ad86e4db8c4b7fa5",
      "question": "what types of drugs consumed",
      "prediction": "ketamine.",
      "ground_truths": [
        "ketamine."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a369001634a84cb9bf81e4922893fe0e",
      "question": "What is ketamine?",
      "prediction": "an animal tranquilizer, can put users in a dazed stupor for about two hours,",
      "ground_truths": [
        "an animal tranquilizer,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "1e01e6c6e9ce48b4be86ec2a5010c489",
      "question": "What is the top drug choice in Hong Kong?",
      "prediction": "ketamine,",
      "ground_truths": [
        "ketamine,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ae0a845a533f4a4f86efad0aebbfed7e",
      "question": "what includes an ice sculpture of the Grinch?",
      "prediction": "Holiday",
      "ground_truths": [
        "frozen world located in the Gaslight Theater."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02b909812ebd41d4a7dfd2ebc0bb1f35",
      "question": "What offers great shopping?",
      "prediction": "Opry Mills,",
      "ground_truths": [
        "Opry Mills,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc706646f61c4f908f968b4b3ef25b68",
      "question": "Where is the show ICE! being held?",
      "prediction": "Gaslight Theater.",
      "ground_truths": [
        "Gaslight Theater."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8cb7447b1d56403fb64de07874a3e854",
      "question": "when did the decorations start going up?",
      "prediction": "in July",
      "ground_truths": [
        "in July"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "66e9c731011d45de8ca1e7734c27a935",
      "question": "When did the decorations begin to go up?",
      "prediction": "in July",
      "ground_truths": [
        "July"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b0db31bdf7ee43fdaeb49ae9dab8b70f",
      "question": "When do the decorations go up?",
      "prediction": "July",
      "ground_truths": [
        "July"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c50c42865c444b5c95fa97a7db5244e4",
      "question": "length of time pilots to be treated",
      "prediction": "at least 12 months.",
      "ground_truths": [
        "at least 12 months."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "040fd7f085b64969856b71c49b068884",
      "question": "how long is the treatment",
      "prediction": "at least 12 months.",
      "ground_truths": [
        "12 months."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3fbd68661ad646c5a5e859934c9f9f79",
      "question": "what does faa say",
      "prediction": "biPs with mild to moderate depression will be allowed to fly while taking antidepressants if they can demonstrate they have been satisfactorily treated for at least 12 months.",
      "ground_truths": [
        "the new policy will improve safety by bringing to the surface pilots who either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly."
      ],
      "em": 0,
      "f1": 0.20338983050847456
    },
    {
      "id": "1e86e90fef4a4737b6e45530eabf0178",
      "question": "pilots must have been treated for at least 12 months for what reason?",
      "prediction": "better",
      "ground_truths": [
        "mild to moderate depression"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74bddc4c04a845bea80ae16c5bd1fd9b",
      "question": "What does de FAA says about the policy?",
      "prediction": "that they do not know the extent of depression among pilots but that pilots are probably representative of the larger population,",
      "ground_truths": [
        "\"absolutely\" improve safety,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce0c7cc0513b45b6848c3a9b8c01f813",
      "question": "what has become a way in which to emphasize ideas on Twitter?",
      "prediction": "hashtag",
      "ground_truths": [
        "the hashtag"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5d833472ffb541e59c52411eb2140a6c",
      "question": "What we can use to tag a name on twitter or Facebook?",
      "prediction": "\"@\"",
      "ground_truths": [
        "\"@\""
      ],
      "em": 1,
      "f1": 1
    },
    {
      "id": "edc8a0f319ed4a4a8933ef300cf3f290",
      "question": "how many were wounded",
      "prediction": "Four",
      "ground_truths": [
        "Four other people"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "df1a8977601b4977a5d12f0ecca70c50",
      "question": "What was the number of suicide bombers?",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7aeaaea83aa14a6cb839bffbb9f350b9",
      "question": "who said 2 suicide bombers carried out the attack",
      "prediction": "NATO's International Security Assistance Force",
      "ground_truths": [
        "NATO's International Security Assistance Force"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1fe204be06f04ab487f825e9b6e80984",
      "question": "What did Afghan authorities describe?",
      "prediction": "The killing represents the strategy of the Taliban to assassinate as many leaders as possible,",
      "ground_truths": [
        "The attacker hid the explosive device inside his turban,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9203243d8d274c259071d818041c57a5",
      "question": "how many bombers were there",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7437ff48e3e942d2b2473df4a6f891a8",
      "question": "who said the face of the peace initiative has been attacked",
      "prediction": "Gen. John R. Allen,",
      "ground_truths": [
        "Gen. John R. Allen, commander of ISAF,"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "1d435550c89f41d0b161b5443956aa1f",
      "question": "What did the commander say?",
      "prediction": "\"We know that is the campaign the insurgents are on. We've got to adjust to that and protect the leaders.",
      "ground_truths": [
        "\"face of the peace initiative has been attacked.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2aa976c49d943adafac0517e3471eec",
      "question": "who was attacked",
      "prediction": "Burhanuddin Rabbani,",
      "ground_truths": [
        "Burhanuddin Rabbani,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41d5431a031744a3891f2e86b168ffbe",
      "question": "how many people were wounded in the attack",
      "prediction": "Four",
      "ground_truths": [
        "Four"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "967b7a252c2c43e79ee8c94e5d8c7b81",
      "question": "What wa sthe name of the suspect?",
      "prediction": "Adam Yahiye Gadahn,",
      "ground_truths": [
        "Adam Yahiye Gadahn,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3d5a45a99a744852873f9eae4e7c9694",
      "question": "what does he say about his citizenship",
      "prediction": "\"Isn't it shameful enough for a person to carry the citizenship of America, the symbol of oppression and tyranny and advocate of terror in the world?\"",
      "ground_truths": [
        "of America, the symbol of oppression and tyranny and advocate of terror in the world?\""
      ],
      "em": 0,
      "f1": 0.7428571428571429
    },
    {
      "id": "cab69864c0bb431ab2098cac338ac9a1",
      "question": "who is in the video?",
      "prediction": "Adam Yahiye Gadahn, also known as Azzam the American,",
      "ground_truths": [
        "Adam Yahiye Gadahn, also known as Azzam the American,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "10a9ca4228244de18071f4fccf05b2f2",
      "question": "who criticizes obama",
      "prediction": "Adam Yahiye Gadahn,",
      "ground_truths": [
        "Adam Yahiye Gadahn,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d124064943a2444e86841bc34e25fa81",
      "question": "What did Adam Yahiye Gadahn say?",
      "prediction": "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"",
      "ground_truths": [
        "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c758a02e5942409599888906bba81b3e",
      "question": "Which area was the convert from?",
      "prediction": "Pakistan",
      "ground_truths": [
        "rural California,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e43326fdfa2e4fecb89bc5174d482072",
      "question": "Who says the man denied wife liberty of coming and going with face uncovered?",
      "prediction": "Eric Besson",
      "ground_truths": [
        "Immigration Minister Eric Besson"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "99a4611c0c0d4131b8813facc14de19b",
      "question": "What did the man deny his wife of?",
      "prediction": "wearing a full Islamic veil,",
      "ground_truths": [
        "liberty to come and go with her face uncovered,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e58f6701ee0a4015a8de2d1f53fd6d6c",
      "question": "Which country is scheduled to vote on whether to ban full veils?",
      "prediction": "France",
      "ground_truths": [
        "France"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6fd0d95815d34aaa9beee0f56085f012",
      "question": "What did France deny a woman?",
      "prediction": "citizenship",
      "ground_truths": [
        "naturalization request"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dce68f614d0d4c1fad1ff4986dcd45fb",
      "question": "Which country denied a Moroccan woman's naturalization request?",
      "prediction": "France's",
      "ground_truths": [
        "France"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8eb13902054c435786dbfb9d907718ff",
      "question": "what was denied",
      "prediction": "citizenship",
      "ground_truths": [
        "citizenship"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cb6855a7c4964e44aa7d97b6bef22d46",
      "question": "what was man denied",
      "prediction": "citizenship",
      "ground_truths": [
        "citizenship"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8c42e3acb5934672bfdc3c713a75d371",
      "question": "Who defeated Froch?",
      "prediction": "Kessler",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5d866dd01974343ba3475ba0ca2387c",
      "question": "Who defeated Carl Froch?",
      "prediction": "Mikkel Kessler",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3a63e60b8ef140d28f31325f8211cb10",
      "question": "How many rounds were there in the match?",
      "prediction": "12",
      "ground_truths": [
        "12"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28ae4259c5bd485a899ec68af2eb4264",
      "question": "Who defeats Carl to win?",
      "prediction": "Kessler",
      "ground_truths": [
        "Mikkel"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ace17f51ddc44d7db28c05c44bc5d658",
      "question": "Kessler secures unanimous points decision after what number of rounds?",
      "prediction": "12",
      "ground_truths": [
        "12"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aecbba5c017943a79d643d0cc71e57b7",
      "question": "Where are the Thai soldiers accused of crossing into?",
      "prediction": "Cambodian territory",
      "ground_truths": [
        "Cambodian territory"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab65135d9fc94f16a97555b6b3c54832",
      "question": "Who claimed the soldiers had crossed into the area?",
      "prediction": "Cambodian officials",
      "ground_truths": [
        "Cambodian officials"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8b627a95012d4f65a70aff1249d43d64",
      "question": "What temple is at the center of the debate?",
      "prediction": "Preah Vihear",
      "ground_truths": [
        "Preah Vihear"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2fcab3f4f6484d8992a6c2aa67aeec69",
      "question": "The Thai army said what?",
      "prediction": "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.",
      "ground_truths": [
        "soldiers had not gone anywhere they were not permitted to be."
      ],
      "em": 0,
      "f1": 0.6875000000000001
    },
    {
      "id": "a65e6461f74c47c9a5ffee9366dea0a4",
      "question": "What century is the Preah Vihear temple from?",
      "prediction": "11th",
      "ground_truths": [
        "11th"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0bae44433814b2da5d10519d7ea3695",
      "question": "What did Thai soldiers cross into?",
      "prediction": "Cambodian territory",
      "ground_truths": [
        "Cambodian territory"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cdeb2b345c247bfb083ab363bebd64c",
      "question": "Who is arsenal manager?",
      "prediction": "Arsene Wenger",
      "ground_truths": [
        "Arsene Wenger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "23f3abb43b8f4f74bffb018bb3943125",
      "question": "Who is the Arsenal manager?",
      "prediction": "Arsene Wenger",
      "ground_truths": [
        "Arsene Wenger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9eecb36f5cb44bb18d24b6efe6824a06",
      "question": "What does Luka Modric suffer from?",
      "prediction": "broken his leg",
      "ground_truths": [
        "a fracture to his right fibula,\""
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "bd356a60992b44f7840f99ffc390382c",
      "question": "Which team beat Arsenal 2-1?",
      "prediction": "Manchester United",
      "ground_truths": [
        "Manchester United."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "51ff7cdf97074a07862234487477ef4a",
      "question": "What did Wenger kick?",
      "prediction": "an empty water bottle",
      "ground_truths": [
        "an empty water bottle"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a9dff945426841f8b37069eefd82297b",
      "question": "Who does Luka Modric play for?",
      "prediction": "Tottenham",
      "ground_truths": [
        "Croatia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20c6639f94d24a91ac79c15024fcd5f7",
      "question": "who will receive an apology?",
      "prediction": "Arsene Wenger.",
      "ground_truths": [
        "Arsenal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e21128d6906243a58807600c9e0744b4",
      "question": "what was traveling?",
      "prediction": "2,000 people",
      "ground_truths": [
        "2,000 people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e04f0cda81094300afaba328511581fe",
      "question": "How many people were on board?",
      "prediction": "2,000",
      "ground_truths": [
        "about 2,000"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "66225a9def6a45ddaac68462170b34b7",
      "question": "How much was the ferry capable of carrying?",
      "prediction": "capacity of 1,500",
      "ground_truths": [
        "1,500"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7cb53bae73d64900a63fde04f93eceea",
      "question": "what did authorities recover",
      "prediction": "54 bodies",
      "ground_truths": [
        "54 bodies"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1fd96469f9a24b37814917f25f8060f2",
      "question": "what is eid al adha?",
      "prediction": "Muslim festival",
      "ground_truths": [
        "Muslim festival"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cf3bbf5197224180b1a20f44bba8ff58",
      "question": "what capacity did the boat have?",
      "prediction": "1,500",
      "ground_truths": [
        "1,500"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5b0c1a6b5824201baa94cf53c56f0c5",
      "question": "what was the boat capacity?",
      "prediction": "1,500",
      "ground_truths": [
        "1,500"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8e5c3c1a2b514ea2b0d9eb06879364c2",
      "question": "How many bodies were recovered?",
      "prediction": "54",
      "ground_truths": [
        "54"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7f2e1ae39b54396b93b90189b8ef3da",
      "question": "Where was the ferry headed?",
      "prediction": "to their homes in Bhola",
      "ground_truths": [
        "Bhola"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "a256f8f1dbd644c98a3bc79d072405fc",
      "question": "Taliban plans what month offensive?",
      "prediction": "winter",
      "ground_truths": [
        "January"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "23d9c3e938cd4e54a2917488a719b832",
      "question": "When do they feel strong",
      "prediction": "during the snowing season,\"",
      "ground_truths": [
        "during the snowing season,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2af6e73ac32145e9bb843506ce6c6a61",
      "question": "Who is the Taliban leader?",
      "prediction": "Hakeemullah Mehsud",
      "ground_truths": [
        "Hakeemullah Mehsud"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7c34ec6a706d4aaf96e4f3356fe2da7c",
      "question": "What day was the blast in Peshawar?",
      "prediction": "Monday's",
      "ground_truths": [
        "Monday's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d847d59c18884a8eaddd5fc55d047799",
      "question": "How many people were killed in Monday's blast?",
      "prediction": "Eleven",
      "ground_truths": [
        "Eleven"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c0d3770ca41e4463958dca0c88ce3979",
      "question": "How many were killed  in Peshawar?",
      "prediction": "Eleven people",
      "ground_truths": [
        "people"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9b5c249c46644e04a2d28d98bac5f951",
      "question": "Do you know how many were killed",
      "prediction": "Eleven people died and 36",
      "ground_truths": [
        "Eleven"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "ef9cc08de0734290840e0ef6219ddf7c",
      "question": "Who led 2-0 at halftime?",
      "prediction": "Ghana",
      "ground_truths": [
        "Ghana"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c248d72216c04f8db99757a1a844af49",
      "question": "Who are the four-time champions?",
      "prediction": "Brazil",
      "ground_truths": [
        "Brazil"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cdf886b5b0934fde8b0ced32a083b8f5",
      "question": "Who beat Costa Rica?",
      "prediction": "Brazil",
      "ground_truths": [
        "Brazil"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d293f30681d147138fcc8dd488a33e27",
      "question": "Where is the under-20 World Cup being held?",
      "prediction": "Egypt.",
      "ground_truths": [
        "Egypt."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec713cfc79cc4dbc8c43ba239a74954b",
      "question": "What was the score of Brazil vs Costa Rica?",
      "prediction": "1-0",
      "ground_truths": [
        "5-0,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b1fdfecae5dd453bbba66cbd21e38740",
      "question": "Who did Ghana beat?",
      "prediction": "Spain",
      "ground_truths": [
        "Hungary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3db5e82c065b473cbf5627e6433aa143",
      "question": "Who faces the 4 times champion?",
      "prediction": "Costa Rica",
      "ground_truths": [
        "Los Ticos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93d27c2a9ec54c57bc14faada354cf3b",
      "question": "What is the price of the Large Hadron Collider?",
      "prediction": "$10 billion",
      "ground_truths": [
        "$10 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cbc47d7cb9e54eddb352a78e9e2be929",
      "question": "how much are the cost of particle accelerator?",
      "prediction": "$10 billion",
      "ground_truths": [
        "$10 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3d5be1ac654b455db987abc9335857e8",
      "question": "What is the Hadron Collider?",
      "prediction": "The Large Hadron Collider",
      "ground_truths": [
        "the world's largest particle"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1173701731e4cfda4d99d5969214317",
      "question": "What does the machine do?",
      "prediction": "look at how the universe formed by analyzing particle collisions.",
      "ground_truths": [
        "look at how the universe formed by analyzing particle collisions."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3f3a8c8b40e4f26b7696777583b2a7c",
      "question": "Who worked alongside Tom Cruise?",
      "prediction": "Scott Altman",
      "ground_truths": [
        "Scott Altman"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7eee6e5786e5492ea402aedfbe21fe2d",
      "question": "Who did Altman work with in this film?",
      "prediction": "Tom Cruise",
      "ground_truths": [
        "Tom"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fa357141135a4d6abc8ead62870d7ffc",
      "question": "What profession did Scott Altman work for?",
      "prediction": "Navy F-14 fighter pilot",
      "ground_truths": [
        "retired Navy F-14 fighter pilot"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "c0ac9981ab774c97a9b9150d190a83fc",
      "question": "Was Scott Altman a fighter pilot?",
      "prediction": "navy F-14",
      "ground_truths": [
        "retired Navy F-14"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "d7462fe0cb384c81a672367fb1e3c26f",
      "question": "Which 1986 hit film did Altman perform a stunt double in?",
      "prediction": "\"Top Gun\"",
      "ground_truths": [
        "\"Top Gun\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0302f03e0726433ca89b90c145baf5e3",
      "question": "in which hospital the child receives tracheotomy?",
      "prediction": "St. Louis, Missouri.",
      "ground_truths": [
        "SSM Cardinal Glennon Children's Medical Center in St. Louis."
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "1fc8e1310fe949df9825c423cbbba725",
      "question": "where is the hospital?",
      "prediction": "St. Louis, Missouri.",
      "ground_truths": [
        "St. Louis, Missouri."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "71a97c5cc88b4e3987dd4a7804a32235",
      "question": "when baby joseph died?",
      "prediction": "Tuesday afternoon.",
      "ground_truths": [
        "Tuesday afternoon."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "baff036612634a0da96854a984416dec",
      "question": "that suffers Joseph Maraachli?",
      "prediction": "in his sleep at his Windsor, Ontario, home,",
      "ground_truths": [
        "a progressive neurological disease"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "300a7093186542179fccce8e030fcaa6",
      "question": "The infant received a tracheotomy at",
      "prediction": "St. Louis, Missouri.",
      "ground_truths": [
        "a children's hospital in St. Louis, Missouri."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "892e26a6bf9c448c87fadbcb4ec322e9",
      "question": "who suffered from a neurological disease?",
      "prediction": "Joseph",
      "ground_truths": [
        "Joseph"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d77c4d812fdf415bb9e09645c84f01bb",
      "question": "What is the reason that women have mammograms?",
      "prediction": "breast cancer.",
      "ground_truths": [
        "cancer."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2f25f23c8bbd4ccd9a3b05e025b7b3fd",
      "question": "what does the cycle have to do with it?",
      "prediction": "\"I think we have to rely on having a clinical breast exam once a year at a health care provider and doing your self-breast exam on a monthly basis.\"",
      "ground_truths": [
        "\"The best time of your cycle to do a mammogram is going to be when your period is over, maybe the week after your period is done when the breasts are not going to be tender.\""
      ],
      "em": 0,
      "f1": 0.07017543859649122
    },
    {
      "id": "0c31eb03ae2349f49cd9e8bfd1e46a8b",
      "question": "What did the man have a degree in?",
      "prediction": "MBA in finance",
      "ground_truths": [
        "MBA in finance"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3eaa727ee7b94eb6a501ddd864e02c9f",
      "question": "What did the man get stuck in?",
      "prediction": "a rabbit hole,",
      "ground_truths": [
        "a rabbit hole,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "932256c149594b6cb4ab990092a0e422",
      "question": "What was the man's name?",
      "prediction": "Karthik Rajaram",
      "ground_truths": [
        "Karthik Rajaram"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e23ad745e76a4d53b8402b1c7ef40617",
      "question": "What did the father have an MBA in",
      "prediction": "finance",
      "ground_truths": [
        "finance"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dcd553b5c56f4a3f9eae9e67807c7147",
      "question": "Who was a fulbright schokar",
      "prediction": "Karthik Rajaram,",
      "ground_truths": [
        "Krishna Rajaram,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "03a761e779124cef8d2d9ca903cb2344",
      "question": "Where was one of the son's attending school?",
      "prediction": "UCLA.",
      "ground_truths": [
        "UCLA."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7191ed1791dc4e33a7a6e9dbb5ea01e8",
      "question": "How many letters did the man leave",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1ae4333af7ca4c33953c8030c3aed22f",
      "question": "What are government troops and rebels battling for?",
      "prediction": "the remaining rebel strongholds in the north of Sri Lanka,",
      "ground_truths": [
        "strongholds in the north of Sri Lanka,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "c89d1c366c274f6e8d233be688759386",
      "question": "Number of civilians that are trapped according to aid groups?",
      "prediction": "as 250,000",
      "ground_truths": [
        "many as 250,000"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "bf9d67fa8b3849ceb410c4dfd7947e5f",
      "question": "How many civilians are trapped?",
      "prediction": "as",
      "ground_truths": [
        "as 250,000"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d5938c1e944d44f7ab3ace92abc7d452",
      "question": "How many civilians are trapped, according to aid groups?",
      "prediction": "250,000",
      "ground_truths": [
        "250,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed28bccab4dc46f0a12e98fd366f2a83",
      "question": "What country is being discussed here?",
      "prediction": "Sri Lanka's",
      "ground_truths": [
        "Sri Lanka,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3e4aa100c744486d883502dd35d3569c",
      "question": "How long have the Ethnic Tamil minority been fighting?",
      "prediction": "1983.",
      "ground_truths": [
        "since 1983."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e0ce4da6cbfe4f26b945f5555901e5c5",
      "question": "How many civilians are trapped due to this conflict?",
      "prediction": "250,000",
      "ground_truths": [
        "as"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "273733b869b741a7b9667927ea701789",
      "question": "What do the Tamils want?",
      "prediction": "surrender.",
      "ground_truths": [
        "an independent homeland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf16d51b5b9540ec83fba053bc26a8a9",
      "question": "What happend to the last functioning medical facility in the zone?",
      "prediction": "the closure of Pudukkudiyiruppu hospital",
      "ground_truths": [
        "The fighting has forced the closure of Pudukkudiyiruppu hospital in the Vanni region,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "c7c63ad1a040451eaf13ad8160295ba0",
      "question": "Year the ethnic Tamil minority have been fighting since?",
      "prediction": "1983.",
      "ground_truths": [
        "1983."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e75dd8d226914bd9b9070a34d57ff0c9",
      "question": "How long have the Tamil minority been fighting for independence?",
      "prediction": "1983.",
      "ground_truths": [
        "an independent homeland since 1983."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d8d1caff0b4e4c73937106e9fb6f0fc9",
      "question": "What is the date of the case?",
      "prediction": "March 22,",
      "ground_truths": [
        "March 22,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0fc806535774a2cb7fb586c9f87f1e5",
      "question": "Who will it be argued before?",
      "prediction": "a federal judge",
      "ground_truths": [
        "a federal judge in Mississippi"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6d36da6f7d6c4ae898960241cc99cd4c",
      "question": "Which group filed a motion?",
      "prediction": "American Civil Liberties Union",
      "ground_truths": [
        "The American Civil Liberties Union"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8542bff725c4264aaf360ad929a1807",
      "question": "When will the case take place?",
      "prediction": "March 22,",
      "ground_truths": [
        "March 22,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af77d75481ae4b0391017dfd394050f4",
      "question": "Who files against the school district?",
      "prediction": "American Civil Liberties Union",
      "ground_truths": [
        "rights group"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e111ffbe05e4468c85bfaeceec2ea821",
      "question": "Which school district is subject to the injunction?",
      "prediction": "Itawamba County",
      "ground_truths": [
        "Mississippi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0480646187a4af3ae3e844f18b1c10f",
      "question": "What did the group file with the court?",
      "prediction": "filed a motion for a preliminary injunction",
      "ground_truths": [
        "a motion for a preliminary injunction against a Mississippi school district and high school"
      ],
      "em": 0,
      "f1": 0.5000000000000001
    },
    {
      "id": "10c2bfdd1e4e4886bf3428f1753757c5",
      "question": "Who made passengers remove nipple rings?",
      "prediction": "TSA officers",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc9153acb2bd49ed8771b45f7d5359fc",
      "question": "What did the agency say?",
      "prediction": "\"TSA supports the thoroughness of the officers involved as they were acting to protect the passengers and crews of the flights departing Lubbock that day.\"",
      "ground_truths": [
        "airport appear to have properly followed procedures when they allegedly forced a woman to remove her nipple rings -- one with pliers -- but acknowledged the procedures should be changed."
      ],
      "em": 0,
      "f1": 0.0851063829787234
    },
    {
      "id": "6ffd10e8acea47be8588900424296e22",
      "question": "What backs officers who made passenger remove nipple rings?",
      "prediction": "the thoroughness of the",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "706a0b4a648c4aa2b6aa5dda328fdb55",
      "question": "Who found piercings at airport?",
      "prediction": "Mandi Hamlin",
      "ground_truths": [
        "Mandi Hamlin"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bb633b7dfbd4bfbb77b7820ce3448d9",
      "question": "Who says she heard male agents snicker?",
      "prediction": "Mandi Hamlin",
      "ground_truths": [
        "Mandi Hamlin"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41d4727416c1422db29a26d8d40104f0",
      "question": "What did the woman say?",
      "prediction": "she was humiliated by last month's incident, in which she was forced to painfully remove the piercings behind a curtain as she heard snickers from male TSA officers nearby. The incident occurred at the Lubbock, Texas, airport.",
      "ground_truths": [
        "she was humiliated"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "61a08597e485463f8836d68886a6b8f3",
      "question": "Who said procedures need to be changed?",
      "prediction": "TSA",
      "ground_truths": [
        "Transportation Security Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6c58352d743439daf313313a04ddcdb",
      "question": "What needs to be changed?",
      "prediction": "procedures",
      "ground_truths": [
        "the procedures"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "337fbef73fd746a9ae74ffced3e49223",
      "question": "who is suspended",
      "prediction": "a student",
      "ground_truths": [
        "in a campus library,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ebc818947b23459b8e2af504e3ec65e8",
      "question": "What was mocked at the party?",
      "prediction": "Black History Month",
      "ground_truths": [
        "Black History Month"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "96213a25c60849749fd38b5bbed07a1c",
      "question": "What was the rally opposing?",
      "prediction": "racial intolerance.",
      "ground_truths": [
        "racial intolerance."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e28e16059c5649a9a2b0b982135f91b7",
      "question": "how many people involved",
      "prediction": "hundreds",
      "ground_truths": [
        "hundreds"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b59ea8ed1f8477692529ed59e6ef224",
      "question": "when is the incident",
      "prediction": "two weeks after Black History Month",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "01519bad5ff04398b6ed82dbf5aeb781",
      "question": "was he punished",
      "prediction": "suspended",
      "ground_truths": [
        "suspended"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "35502eb79d91480880ac508d4e2cdb15",
      "question": "Where did the student hang the noose?",
      "prediction": "in the library,",
      "ground_truths": [
        "campus library,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ef43f4191c424f70aec3ed9eaa357cb0",
      "question": "what was student attempting to accomplish",
      "prediction": "hanging a noose",
      "ground_truths": [
        "intent to terrorize"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d99e4c4e3ee24996a4c2a5e08d669dfb",
      "question": "What did Bryan ride for more than 11,000 miles?",
      "prediction": "motor scooter",
      "ground_truths": [
        "a motor scooter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53cdb654e9454fc7b6d7539fbcfbe8d7",
      "question": "What did the activist's route form?",
      "prediction": "peace sign.",
      "ground_truths": [
        "a peace sign."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21d720c9825a49109f21240335bc38bd",
      "question": "What did the activist want to do?",
      "prediction": "demand a meeting with the president",
      "ground_truths": [
        "meeting with the president to discuss her son."
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "5519a7b4759c4dd087a442a571ef4cb9",
      "question": "How many miles did Bryan ride his scooter?",
      "prediction": "11,000",
      "ground_truths": [
        "11,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "37d0106de39b4828ab2db59814510282",
      "question": "When was the tribunal established?",
      "prediction": "late 1994.",
      "ground_truths": [
        "in late 1994."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "ad65a15e844a4f078e4598b8cfe6ab87",
      "question": "when The United Nations established the genocide tribunal?",
      "prediction": "late 1994.",
      "ground_truths": [
        "in late 1994."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "3c0bb931c2394da1bca011585f60753c",
      "question": "bagosora is charged with which crime?",
      "prediction": "for genocide,",
      "ground_truths": [
        "genocide,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "405919df50b648c3909720727e9d3f16",
      "question": "Who is the mastermind?",
      "prediction": "Theoneste Bagosora,",
      "ground_truths": [
        "Theoneste Bagosora,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc24f8ce81044ef588f7565d29afaa5d",
      "question": "When did massacres occur?",
      "prediction": "April 6, 1994",
      "ground_truths": [
        "1994"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "49c4d8a65f2c4ec7a74535b35335a4f5",
      "question": "how many people dead?",
      "prediction": "800,000",
      "ground_truths": [
        "800,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5fd77e9ba374d4d89e90f52b3aa681d",
      "question": "what country did massacre occur?",
      "prediction": "Rwanda",
      "ground_truths": [
        "Rwanda"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed37709577d24ab982fafc3b5eb21ba9",
      "question": "what position did bagosora hold?",
      "prediction": "\"mastermind\" of the Rwandan genocide",
      "ground_truths": [
        "a colonel in the Rwandan army,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "a3118a625a3b41c18a04780ff784465b",
      "question": "What have beer drinkers left?",
      "prediction": "glass shards",
      "ground_truths": [
        "glass shards"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "580cfb51fef84f5e9cb2de0be76eaa61",
      "question": "What kind of dogs are wearing protective shoes?",
      "prediction": "German Shepherds and Belgian Shepherds,",
      "ground_truths": [
        "police"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89b7733b0a994df7b4250953ccff9c24",
      "question": "What do dog shoes cost?",
      "prediction": "€60",
      "ground_truths": [
        "60 euros"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e6bd79cae1cb472c990b5226f2a87152",
      "question": "What are police dogs now wearing?",
      "prediction": "little blue booties.",
      "ground_truths": [
        "protective shoes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7b7a2491fe134532816141e702cb9b4a",
      "question": "What was the reason that the dogs wear protective shoes?",
      "prediction": "Too",
      "ground_truths": [
        "Too many glass shards left by beer drinkers in the city center,"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "dcb5b33f7e1848c08ba6d3842490e174",
      "question": "What is the reason for giving them shoes?",
      "prediction": "Too many glass shards",
      "ground_truths": [
        "Too many glass shards left by beer drinkers in the city center,"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "25eec8384d9d45b8b75affcc9f164be3",
      "question": "What do the shoes cost?",
      "prediction": "60 euros",
      "ground_truths": [
        "$89"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "745c742fa8d94abe88d1330cd4cdae1d",
      "question": "What are police dogs wearing?",
      "prediction": "little blue booties.",
      "ground_truths": [
        "protective shoes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b9b15aa02b54f8e82a61c149799963b",
      "question": "Who also wore dog shoes?",
      "prediction": "dog shoes?",
      "ground_truths": [
        "walk"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a6ced88638f4bf68c6bb3d7a979d48e",
      "question": "Who says America needs a leader who understands the future we seek?",
      "prediction": "Former Virginia Gov. Mark Warner",
      "ground_truths": [
        "former Virginia Gov. Mark Warner"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c5fc7fc4522b43b7a0802acbd3963efd",
      "question": "What political party is Mark Warner associated with?",
      "prediction": "Democratic",
      "ground_truths": [
        "Democratic"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5b5d465a7ab2427485a00f3638543996",
      "question": "Bush never asked Americans to do what?",
      "prediction": "step up.\"",
      "ground_truths": [
        "step up.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7bfe3ee0862a4bdbb5e06b894cfefd29",
      "question": "Much of Warner's address focused on what?",
      "prediction": "the kind of bipartisan rhetoric",
      "ground_truths": [
        "bipartisan rhetoric Obama has espoused"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "362b90d2f2de47e1a0d9be76d1383cd5",
      "question": "Warner says America need what?",
      "prediction": "president who understands the world today,",
      "ground_truths": [
        "a president who understands the world today, the future we seek and the change we"
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "ef32addb3b9b4c8ab122fd787122a657",
      "question": "Who, specifically, did Mark Warner criticize?",
      "prediction": "President Bush",
      "ground_truths": [
        "President Bush"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9bb1b01620ea4ed28be3d13447145272",
      "question": "What type of rhetoric did Warner focus on?",
      "prediction": "bipartisan",
      "ground_truths": [
        "bipartisan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ae35f52b8069416da505e192383e1cab",
      "question": "Bush never asked what?",
      "prediction": "us to step up.\"",
      "ground_truths": [
        "us to step up.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9f8bb4dd263246dbac8f331f745d2e5a",
      "question": "What did most of Warner's address focus about?",
      "prediction": "\"race for the future",
      "ground_truths": [
        "the kind of bipartisan rhetoric Obama has espoused on the campaign trail."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fee6642b071a46e9b1a005d63b58a98c",
      "question": "What kind of rhetoric did Mark Warner allude to frequently?",
      "prediction": "bipartisan",
      "ground_truths": [
        "bipartisan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bb1589db33fa4309b9b3f7fb5e4b3480",
      "question": "What did Bush never ask Americans to do?",
      "prediction": "step up.\"",
      "ground_truths": [
        "step up.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "64637235282a495585c4b3eca6f776f7",
      "question": "Who does One Laptop per Child target?",
      "prediction": "little girls.",
      "ground_truths": [
        "the world's poorest children."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47ca117f64cd4938bff6ff061e319282",
      "question": "What can laptops do?",
      "prediction": "intellectually grow in the privacy of their own homes.",
      "ground_truths": [
        "allow students to engage in learning differently, enjoy a customized approach and hone critical thinking skills,"
      ],
      "em": 0,
      "f1": 0.08695652173913045
    },
    {
      "id": "21888eb1c822480e8fe265e946838188",
      "question": "Who knows how preachy and awkward the movies get?",
      "prediction": "Tripplehorn,",
      "ground_truths": [
        "Tripplehorn,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "716617662a834ce9a662a4225ccac89e",
      "question": "Who directed the vignettes?",
      "prediction": "Kristin Hahn,",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "725e0fdb36d34af6af01fe99c713ec3b",
      "question": "What does Jeanne Tripplehorn know about the movies?",
      "prediction": "how preachy and awkward cancer",
      "ground_truths": [
        "can get."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d67036c2061345e880ebcd02862a2cf4",
      "question": "What story does \"Five\" tell?",
      "prediction": "of different women coping with breast cancer in five vignettes.",
      "ground_truths": [
        "stories of different women coping with breast cancer"
      ],
      "em": 0,
      "f1": 0.7777777777777777
    },
    {
      "id": "84fe1a95dbb449e88149df2e3034787e",
      "question": "Who each directed a vignette?",
      "prediction": "Kristin Hahn,",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93ccadfc45214e47a0e4ee569572379c",
      "question": "Who directed a Vignette?",
      "prediction": "Kristin [Hahn]",
      "ground_truths": [
        "Aniston, Demi Moore and Alicia Keys"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12719e1448b74b5d87b14208859f8438",
      "question": "What is the subject of the vignettes?",
      "prediction": "stories of different women coping with breast cancer",
      "ground_truths": [
        "different women coping with breast cancer"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "cb602f398bf64c0f8775ec73e80e2f0e",
      "question": "What is \"Five\" about?",
      "prediction": "tells stories of different women coping with breast cancer",
      "ground_truths": [
        "tells stories of different women coping with breast cancer in five vignettes."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "5664735693d848dbb85b97195420a5a5",
      "question": "What stories does Five tell?",
      "prediction": "of different women coping with breast cancer",
      "ground_truths": [
        "different women coping with breast cancer in"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "13ed8c21d84140549dba809bdca38f6f",
      "question": "What would the news outlet say happened?",
      "prediction": "that the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.",
      "ground_truths": [
        "series of summer concerts at the O2."
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "7c6b394f8ec74b8a8e93c3b31624e754",
      "question": "What musician has scheduled a new conference?",
      "prediction": "Michael Jackson",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d5532d378e5c463cb39c6e3ef8b47524",
      "question": "In which season will the concerts be held?",
      "prediction": "summer",
      "ground_truths": [
        "summer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2fe9ef278e794c7ab6cc6f78a8714a8b",
      "question": "What has ben the subject of rumors?",
      "prediction": "Pop star Michael Jackson",
      "ground_truths": [
        "Pop star Michael Jackson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53f1d8637f554a908d4c557f43441eb0",
      "question": "Where was the news conference held?",
      "prediction": "London's O2 arena,",
      "ground_truths": [
        "London's O2 arena,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "90ab85c979ac477ea5a1278034d98e35",
      "question": "What news outlet says he will hold a series of summer concerts?",
      "prediction": "outside Organisation.",
      "ground_truths": [
        "Britain's Sky"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "564bd8b4319347d3b2e3be1bc86e1046",
      "question": "In what city is the O2 arena?",
      "prediction": "London's",
      "ground_truths": [
        "London's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd2ab3f3020e49238dfe29398993c918",
      "question": "What has been the subject of rumors?",
      "prediction": "health and about a comeback.",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cbbd003f83bd456789b0c7a7b0c4ce8f",
      "question": "What do new materials do?",
      "prediction": "made the new truck safer,",
      "ground_truths": [
        "truck safer,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "75d42bea14c24d5aa35dd3a4af32755c",
      "question": "What did the Ford expert say?",
      "prediction": "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"",
      "ground_truths": [
        "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\" said Gerry Bonanni,"
      ],
      "em": 0,
      "f1": 0.9302325581395349
    },
    {
      "id": "c3b05d40bfde4641991d458532505bc2",
      "question": "What will insurers do?",
      "prediction": "try and reduce the cost of auto repairs and insurance premiums for consumers",
      "ground_truths": [
        "try and reduce the cost of auto repairs and insurance premiums for consumers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "96b41a2772e3491ca5a00d89a09f9712",
      "question": "What makes the vehicles safer?",
      "prediction": "ultra-high-strength steel and boron",
      "ground_truths": [
        "ultra-high-strength steel and boron"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e55d296f4a274313b1561742bdfe41c2",
      "question": "What does the Ford expert say?",
      "prediction": "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"",
      "ground_truths": [
        "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02efcca165d248af83c561cf383e6ef7",
      "question": "Who did UAE deny visa to?",
      "prediction": "Shahar Peer",
      "ground_truths": [
        "Israeli tennis player Shahar Peer"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "6663783e06e94c3398504e0521a86783",
      "question": "What company is no longer sponsoring the tournament?",
      "prediction": "The Wall Street Journal Europe",
      "ground_truths": [
        "The Wall Street Journal Europe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "682331495d45489ba6c412a767b47552",
      "question": "What is the sport the player is active in?",
      "prediction": "tennis",
      "ground_truths": [
        "tennis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2472a778b8a44ccba2aaf6e2c6c7b19",
      "question": "what sport did he play",
      "prediction": "tennis",
      "ground_truths": [
        "tennis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1444cd7b36cd495fb7e94b9e4212c5e3",
      "question": "where did this happen",
      "prediction": "in Dubai",
      "ground_truths": [
        "Dubai"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2ec113535ab74562b219d362bef03b2b",
      "question": "How many tips did the worker call in?",
      "prediction": "three times",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f2b9e8d1f4fe486d87a9d663503bef07",
      "question": "What child's remains were found?",
      "prediction": "Caylee Anthony",
      "ground_truths": [
        "Caylee Anthony"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "130f065449984159b589f2af234e3216",
      "question": "How long ago did the friend tell police to check the area?",
      "prediction": "month before the",
      "ground_truths": [
        "month before the meter"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e480eabdd5a84307a99dfd94a032efc2",
      "question": "Who went missing?",
      "prediction": "Caylee Anthony,",
      "ground_truths": [
        "Caylee Anthony,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8edf93c15f94d288bc4fb52fc9da573",
      "question": "What was found in the search area?",
      "prediction": "remains",
      "ground_truths": [
        "\"significant skeletal remains\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "b8cb96fe354b4069bf3175062a10fd03",
      "question": "Who called in several tips?",
      "prediction": "The meter reader",
      "ground_truths": [
        "meter reader"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f6f7ad6f6f6a4d0bacf185da89805c6f",
      "question": "Who told police to check the area five months ago?",
      "prediction": "Casey Anthony,",
      "ground_truths": [
        "KioMarie Cruz,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3ec09495f6a4147824e3c975da11da2",
      "question": "What news station reported on this case?",
      "prediction": "CNN",
      "ground_truths": [
        "CNN"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3cb7ed7443af4ee3a65dac3239ea38f2",
      "question": "Who crossed the Atlantic in a micro yacht?",
      "prediction": "Hugo Vihlen",
      "ground_truths": [
        "Hugo Vihlen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "298362fdf91747a582112a69b7df48bf",
      "question": "Who made evolutionary discoveries?",
      "prediction": "Charles Darwin",
      "ground_truths": [
        "Naturalist Charles Darwin"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "647d0669235a41a8a2fe0b37134a6832",
      "question": "What happened in 1620?",
      "prediction": "The Pilgrims sail to Plymouth Rock",
      "ground_truths": [
        "Pilgrims sail to Plymouth Rock"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "78b488ee4cbe43acb8e7824123baacc5",
      "question": "When did Darwin make his discoveries?",
      "prediction": "1831",
      "ground_truths": [
        "1831"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d1811934388341ae98b2cd3946c77b45",
      "question": "Where did the Pilgrim's voyage to?",
      "prediction": "Plymouth Rock",
      "ground_truths": [
        "Plymouth Rock"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1c128e3cbdcd4b20bf65d89db284e474",
      "question": "When was the Beagle voyage?",
      "prediction": "1831",
      "ground_truths": [
        "1831"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aef0cd2a0a8342dfb8c1d1d4b5911e47",
      "question": "When was the Mayflower voyage?",
      "prediction": "1620",
      "ground_truths": [
        "1620"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5953190ee0eb4ada8a3f788a60ba875a",
      "question": "Who did they back?",
      "prediction": "Dalai Lama",
      "ground_truths": [
        "the Dalai Lama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14fda3c8f7d2443aa2732bd885c8f79a",
      "question": "Who wnated Tibet's independence?",
      "prediction": "Some have",
      "ground_truths": [
        "small minority"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "706ce26401d54a5d94687fb541f7350c",
      "question": "What does Lama seek?",
      "prediction": "autonomy.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5a1406f34ca94051b34e3b4a769669b2",
      "question": "What does a small minority demand for Tibet?",
      "prediction": "independence,",
      "ground_truths": [
        "independence,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "989fede52bf840609bb69d7ba143301b",
      "question": "What do Tibetian leaders back?",
      "prediction": "the Dalai Lama's current \"middle way approach,\"",
      "ground_truths": [
        "the Dalai Lama's current \"middle way approach,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "16e5c8bb8ce841de8613411fcbcfedad",
      "question": "What does the Dalai Lama seek from Bejing?",
      "prediction": "autonomy.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d3be7c50925040dca87dc6d971145ccb",
      "question": "What does Dalai Lama seeks?",
      "prediction": "autonomy.",
      "ground_truths": [
        "autonomy."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8aec4e2910254ab29fd7579a418fe40c",
      "question": "What does the minority want?",
      "prediction": "Tibet's independence,",
      "ground_truths": [
        "Tibet's independence,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4491a921e450412fafa90ae9c4a2ce61",
      "question": "What do the Tibetan exile leaders back?",
      "prediction": "a potential new approach to Tibet's decades-long struggle",
      "ground_truths": [
        "\"middle way approach,\""
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "2da01c4994c34acdb27bf9bedabae346",
      "question": "who designed courses in Eastern Europe?",
      "prediction": "Gary Player",
      "ground_truths": [
        "Gary Player"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8869c54796a0408cb95c46e9542cbed7",
      "question": "where has 6 courses opened",
      "prediction": "Bulgaria",
      "ground_truths": [
        "on the Black Sea coast in Bulgaria."
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "69e835bab41c46bf95d6616907f2ca10",
      "question": "what has Player designed",
      "prediction": "two courses on the Black Sea coast in Bulgaria.",
      "ground_truths": [
        "two courses on the Black Sea coast in Bulgaria."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "18760a916f444d95af7e2f7213887025",
      "question": "Who has been designer for Golf in several Eastern European countries?",
      "prediction": "Gary",
      "ground_truths": [
        "Gary Player"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "628f1b28c94042b8bf488ef3ebeb70dd",
      "question": "How many courses have opened?",
      "prediction": "two",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5927d25618045649eb93255907dd1fe",
      "question": "what has made progress",
      "prediction": "the game in the region.",
      "ground_truths": [
        "development of two courses on the Black Sea coast in Bulgaria."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "47a07984eeb64685b92b0c51f5a57a21",
      "question": "All together, how many Golf developments Bulgaria has?",
      "prediction": "two courses",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "143d368c91d54e6987caebfed87a0f6a",
      "question": "Who is the  Aztecas street gang affiliated with?",
      "prediction": "the Juarez drug cartel.",
      "ground_truths": [
        "cartel."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "06efd77e664b4991bb1be5c082b3d0b9",
      "question": "Who is the suspect?",
      "prediction": "Ricardo Valles de la Rosa,",
      "ground_truths": [
        "Ricardo Valles de la Rosa,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e353df5892514b8f87494942da1c02bc",
      "question": "What is Ricardo Valles de la Rosa's age?",
      "prediction": "42 years old",
      "ground_truths": [
        "42 years old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "50d55f0e179142b88ec8fa428537ca76",
      "question": "Who arrested a suspect on Friday?",
      "prediction": "Mexican military",
      "ground_truths": [
        "Mexican military"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c00ec98cc0ad43dcbe419f9a64923f56",
      "question": "Who arrested the suspect Friday?",
      "prediction": "Mexican",
      "ground_truths": [
        "Mexican military"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "382d699060ef477cbeb6a709468cb4fc",
      "question": "Who is the new suspect sought in connection with the slaying?",
      "prediction": "Ricardo Valles de la Rosa,",
      "ground_truths": [
        "Ricardo Valles de la Rosa,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bbffd4ef5094b518beaab077ebf5a50",
      "question": "When was the Mexican military arrested?",
      "prediction": "this month",
      "ground_truths": [
        "Monday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aa93a690e904c89b259f98500106b69",
      "question": "what is Bin's first name?",
      "prediction": "Omar",
      "ground_truths": [
        "Omar"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6e38539cc9944d6fa54d4c503a83644a",
      "question": "Omar bin Laden split with whom in 2000?",
      "prediction": "Osama",
      "ground_truths": [
        "al Qaeda."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0a42325e79534043a17bcbb10e870779",
      "question": "Who is Bin Laden?",
      "prediction": "Omar",
      "ground_truths": [
        "the most-wanted man in the world"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f6249167ca14f0c966477580281ef72",
      "question": "When did he split from his father?",
      "prediction": "2000",
      "ground_truths": [
        "in 2000"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4bbe5fb171c64a7c903b6d71e61d52db",
      "question": "What is the name of Bin Laden's son?",
      "prediction": "Omar",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3000fad50c3e4d449a952b1a0b9f7f4f",
      "question": "Bin Laden's son was named who?",
      "prediction": "Osama",
      "ground_truths": [
        "Omar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e6e822a6b904dc8afab7de215c4e5e5",
      "question": "when was 9/11?",
      "prediction": "2001.",
      "ground_truths": [
        "September 11, 2001."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "04110e98bb844515b66207809954c750",
      "question": "Who is CNN's Baghdad correspondent?",
      "prediction": "Michael Ware",
      "ground_truths": [
        "Michael Ware"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "29e4bdfd3cc242578410c942a502c7bb",
      "question": "Who is the leader of the Iranian-backed militia?",
      "prediction": "Muqtada al-Sadr,",
      "ground_truths": [
        "Muqtada al-Sadr,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9044663edf944402bc008c70542c6baa",
      "question": "what did CNN Baghdad say?",
      "prediction": "\"The U.S. subcontracted out an assassination program against al Qaeda",
      "ground_truths": [
        "correspondent Michael Ware cast doubt on Woodward's assertion"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "00d946fd84644d3c9d2f45ba3b5d95fe",
      "question": "What team does Bob Woodward credit?",
      "prediction": "U.S. subcontracted out an assassination program against al Qaeda",
      "ground_truths": [
        "\"fusion teams,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd193591d3ee4f23b6fc3febb87671e9",
      "question": "What day will the winners be announced?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "92586185a2144c5ba7648b93c8033212",
      "question": "when is going to be announced the winner?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2e72578e8af74acab4e428e48b6f5ef0",
      "question": "whats the name of the colombian senator?",
      "prediction": "Piedad Cordoba,",
      "ground_truths": [
        "Piedad Cordoba,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "de80bc2bd912402ba58f27bff9e98925",
      "question": "how many contenders are for nobel peace prize?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3402c594c0cd444091e58f8e217dcffa",
      "question": "When is the winner going to be announced?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0446a0bd16cf4eea818d7dbb932edf35",
      "question": "How many top contenders are there for the Nobel Peace Prize?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "32942a78bfb84a1195aaa580a8bd9fb1",
      "question": "How many years until another hearing for Atkins?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "174f57168a9d482cb523b60f23c48c18",
      "question": "Who held Sharon Tate down and stabbed her 16 times?",
      "prediction": "Atkins,",
      "ground_truths": [
        "Susan Atkins,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "44eed06f9e1e49bdbcca72ca2836d01c",
      "question": "When did the panel set another hearing for?",
      "prediction": "in three years,",
      "ground_truths": [
        "in three years,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c0aaf4ae674846c3a7ec5bc71c4ba4c0",
      "question": "What happened at Susan Atkins parole hearing?",
      "prediction": "denied",
      "ground_truths": [
        "denied"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9cfdac35dca045f5ac4cdc0aa14a317a",
      "question": "Did Sharon Tate's murderer get parole?",
      "prediction": "denied",
      "ground_truths": [
        "denied"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3520396bebc1406382993de89ae175ce",
      "question": "How many times was Sharon Tate stabbed?",
      "prediction": "16",
      "ground_truths": [
        "16"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a544b20196e94d7fad2e68b70300054f",
      "question": "What is Atkins battling?",
      "prediction": "terminal brain cancer.",
      "ground_truths": [
        "terminal brain cancer."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a8ce2d5f5adf469da7c2544958d7b33d",
      "question": "Where did the dogs detect her scent?",
      "prediction": "near the George Washington Bridge,",
      "ground_truths": [
        "near the George Washington Bridge,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c5af90443fde4a6f9f8feba49b266725",
      "question": "What is the disappeared woman's name?",
      "prediction": "Liza Murphy",
      "ground_truths": [
        "Liza Murphy"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5347843098a3485eaeef3dc00d24aae7",
      "question": "What number do you call if you have information?",
      "prediction": "201-262-2800.",
      "ground_truths": [
        "201-262-2800."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0c7279a28507495abc478963bd318914",
      "question": "When did the woman disappear?",
      "prediction": "August 19, 2007.",
      "ground_truths": [
        "August 19, 2007."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f640e6ad69b54fcd9b606186fb712e07",
      "question": "What did the husband try after wards?",
      "prediction": "to take his own life",
      "ground_truths": [
        "to take his own life"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b0e63f428d16422db0687b63cd4d96e2",
      "question": "where was a man killed ?",
      "prediction": "the middle of the street",
      "ground_truths": [
        "Port-au-Prince, Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1acd5b23dbcd4af28709c20565a87a36",
      "question": "What was the cause of death?",
      "prediction": "a pool of blood",
      "ground_truths": [
        "\"The people kill him with the blocks,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cc72beb75ea497e930d8667dff597c0",
      "question": "What did the photos show?",
      "prediction": "the man facing up, with his arms out to the side. He is wearing socks but no shoes.",
      "ground_truths": [
        "the man facing up, with his arms out to the side."
      ],
      "em": 0,
      "f1": 0.72
    },
    {
      "id": "819e9d1061c54fd4bd76023e08d14667",
      "question": "what did the man say?",
      "prediction": "he was a thief trying to steal people's money Friday amid the chaos from last week's earthquake.",
      "ground_truths": [
        "\"This is robbery. He went to rob the people. He went to steal money -- American dollars,\""
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "93ee454cabff4b79ad4f77b8ac4660ca",
      "question": "what showed the gruesome scene?",
      "prediction": "Photos",
      "ground_truths": [
        "photos"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "faf5c483ab2d4cf9b3a8bbecc5442cdb",
      "question": "Who provides humanitarian aid to the Somalis?",
      "prediction": "A growing percentage of the Somali population",
      "ground_truths": [
        "Daryeel Bulasho Guud"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2e10abef8034c62b1e1030158e4195c",
      "question": "What is the DBG agency and where do they operate?",
      "prediction": "Somalia",
      "ground_truths": [
        "Nairobi, Kenya,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba96798c3ba0464e9436aa0d0ea49872",
      "question": "What nationalities were the aid workers?",
      "prediction": "Somali",
      "ground_truths": [
        "Somalis"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a82c0cb9a3ab4baca2ccd215c7d6a5c6",
      "question": "What aid agency suspended operations?",
      "prediction": "U.N. Development Program.",
      "ground_truths": [
        "DBG,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "826da1e9a1c04a958f5f314498d08d0d",
      "question": "What are some reasons why Somalis depend on humanitarian aid?",
      "prediction": "severe famine",
      "ground_truths": [
        "severe famine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ae7f52f9387841b99305f07a288e38ba",
      "question": "What has the aid agency done?",
      "prediction": "He was shot by three gunmen",
      "ground_truths": [
        "suspend all"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8c42925b30c84c02b35aff4b9357010d",
      "question": "Since when do Somalis depend on humanitarian aid?",
      "prediction": "1991-1993,",
      "ground_truths": [
        "1991-1993,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ede6871c829b4a80ac507536b3a6d14f",
      "question": "Who was shot in Somalia?",
      "prediction": "Three aid workers",
      "ground_truths": [
        "Three aid workers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bd35d7a651974bb181849a4486692264",
      "question": "How long have Robinson and Bridges been dating?",
      "prediction": "two years,",
      "ground_truths": [
        "two years,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6254496b903a42e680fd882f3283fb49",
      "question": "how long have they been dating",
      "prediction": "two years,",
      "ground_truths": [
        "two years,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b476d07cc2e747d0bad28a795661540d",
      "question": "how many children does he have",
      "prediction": "second child",
      "ground_truths": [
        "second child"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9a0b3de216b6498699f9a1745ef51447",
      "question": "Who is having a baby?",
      "prediction": "Rockwell Hudson's ex, Black Crowes rocker Chris Robinson,",
      "ground_truths": [
        "Chris Robinson and girlfriend Allison Bridges"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "3ca19860d31d40d58c3d6018498bbc5e",
      "question": "How long has Robinson and girlfriend been dating?",
      "prediction": "two years,",
      "ground_truths": [
        "two years,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dbc0bbc541124ed0843db76f2e3a1581",
      "question": "Who is Robinson's ex-girlfriend?",
      "prediction": "Allison Bridges",
      "ground_truths": [
        "Kate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f092922c6854e3f8bdf22cc50cf71a9",
      "question": "What is the 5 1/2 year old's name?",
      "prediction": "Ryder Russell,",
      "ground_truths": [
        "Ryder Russell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45c6b42c27e84dc48bc37bccd095980b",
      "question": "What is the name of her show ?",
      "prediction": "\"The Rosie Show,\"",
      "ground_truths": [
        "\"The Rosie Show,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "51a0d143d66d4312b89e4e00929ee837",
      "question": "What O`Donnell said about her love life?",
      "prediction": "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to",
      "ground_truths": [
        "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\""
      ],
      "em": 0,
      "f1": 0.9743589743589743
    },
    {
      "id": "4b7de03eac3642c5aad653d3fe1600dc",
      "question": "What did O'Donnell say recently?",
      "prediction": "\"My gaydar was way off!\"",
      "ground_truths": [
        "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\""
      ],
      "em": 0,
      "f1": 0.08000000000000002
    },
    {
      "id": "0633af8dabeb4925bf0fb0110cc54247",
      "question": "Where  was O'Donnell  attracted to Rounds  ?",
      "prediction": "Starbucks",
      "ground_truths": [
        "Starbucks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "241ae106b1f7409d82a03f8296c8a6ab",
      "question": "Who recently said she's in love ?",
      "prediction": "Rosie O'Donnell",
      "ground_truths": [
        "O'Donnell,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d94300f7464c401db93e557fec221320",
      "question": "What is the name of her new show?",
      "prediction": "\"The Rosie Show,\"",
      "ground_truths": [
        "\"The Rosie Show,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6905efa62e924f7c9960bc701d1025bb",
      "question": "On what O`Donnell has been attracted to?",
      "prediction": "Michelle Rounds",
      "ground_truths": [
        "Michelle Rounds"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "160bd2f1f99b4697a5945323087874cb",
      "question": "When did the Rosie Show debut on the OWN network?",
      "prediction": "Monday",
      "ground_truths": [
        "Monday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce867b37ae1d4bc6b35988966e721575",
      "question": "Who was O'donnel attracted to after seeing her in starbucks?",
      "prediction": "Michelle Rounds",
      "ground_truths": [
        "Michelle Rounds"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "93f2aed21ef74156be3d976b205a907e",
      "question": "Who was absent from the trial?",
      "prediction": "Gov. Rod Blagojevich",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b40aa09142ee437bac25b93f6300e20a",
      "question": "who is absent from trial?",
      "prediction": "Gov. Rod Blagojevich",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b80ad96c0e04a61bf5221c06c14a9a4",
      "question": "Who heard the recorded phone calls?",
      "prediction": "state senators",
      "ground_truths": [
        "state senators"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc9d87c351b84540981e32798a4bdc37",
      "question": "Who is holding interviews?",
      "prediction": "CNN's Campbell Brown",
      "ground_truths": [
        "Gov. Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fdf3652f9965474eb63b7c5b227983c0",
      "question": "Who did the senators hear from?",
      "prediction": "FBI Special Agent Daniel Cain,",
      "ground_truths": [
        "Rod Blagojevich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cc048131d7e410fa3083078bb1fff5a",
      "question": "From who was the testimony?",
      "prediction": "F.A. Daniel Cain,",
      "ground_truths": [
        "FBI Special Agent Daniel Cain,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0ec4f738dc3143a7bf541019bfe9c705",
      "question": "Who heard recorded phone calls?",
      "prediction": "Senators",
      "ground_truths": [
        "state senators"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a52bd9f9d68446088b731bfd750f6820",
      "question": "What did commentators focus on?",
      "prediction": "the idea that the Richmond students did nothing because of the \"bystander effect\":",
      "ground_truths": [
        "\"bystander effect\":"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "b36738aa32d44026be321cdf04b08d39",
      "question": "what are commentators focused on?",
      "prediction": "the idea that the Richmond students did nothing because of the \"bystander effect\":",
      "ground_truths": [
        "\"bystander effect\":"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "016465fabbcb45038bfc279cb8190345",
      "question": "what do the studies show?",
      "prediction": "has taught us a lot about how to prevent such school violence.",
      "ground_truths": [
        "students often know ahead of time when and where violence will flare up on campus."
      ],
      "em": 0,
      "f1": 0.07692307692307691
    },
    {
      "id": "98141f5f55af44a589cad71ac101cc96",
      "question": "What crime do students not report?",
      "prediction": "gang rape",
      "ground_truths": [
        "gang rape"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5bf5a22842224fe4952ea03b53d31fd3",
      "question": "What do studies show?",
      "prediction": "students often know ahead of time when and where violence will flare up on campus.",
      "ground_truths": [
        "that students often know ahead of time when and where violence will flare up on campus."
      ],
      "em": 0,
      "f1": 0.967741935483871
    },
    {
      "id": "f380a5932d4b4a43969a84c3338edae0",
      "question": "Who is Robert Kimmitt?",
      "prediction": "Deputy Treasury Secretary",
      "ground_truths": [
        "Deputy Treasury Secretary"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "46f896a31c254fb2bf5b68c060edb437",
      "question": "What does MME talk to UAE's Minister of Foreign Trade about?",
      "prediction": "future relations",
      "ground_truths": [
        "Sovereign Wealth Funds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c67f7f0037b4ab1bb510cba499d11a4",
      "question": "Name of the Deputy Treasury Secretary?",
      "prediction": "Robert Kimmitt.",
      "ground_truths": [
        "Robert Kimmitt."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d83d3eda31c14835877587e71ae8f858",
      "question": "Who is the U.A.E.'s Minister of Foreign Trade?",
      "prediction": "Lubna Al Qasimi,",
      "ground_truths": [
        "Sheikha Lubna Al Qasimi,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "6fe2385a3a1f4e7c8b1982cad82c939d",
      "question": "What was the purpose of Brown's Gulf tour?",
      "prediction": "to secure more funds",
      "ground_truths": [
        "in an attempt to secure more funds from the region."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6c8e1e1b56bc4eb5b99b83ce6c5f3afd",
      "question": "Where did UK Prime Minister Gordon Brown tour?",
      "prediction": "the Gulf",
      "ground_truths": [
        "the Gulf"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9f1d761a46c64578849ff7408c6351b2",
      "question": "What did Robert Kimmitt discuss?",
      "prediction": "future relations with Washington.",
      "ground_truths": [
        "Sovereign Wealth Funds"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e66e99bd83d4ae9abaa6e14c89cddb9",
      "question": "Name of the prime minister of the UK?",
      "prediction": "Gordon Brown",
      "ground_truths": [
        "Gordon Brown"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d23a30ecc5b8469c8e6324ff5d42409b",
      "question": "What position does Robert Kimmitt hold?",
      "prediction": "Deputy Treasury Secretary",
      "ground_truths": [
        "Deputy Treasury Secretary"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1699130630f2472cacfcbfe214d40416",
      "question": "what is missing?",
      "prediction": "Pablo Picasso sketchbook",
      "ground_truths": [
        "A Pablo Picasso sketchbook with 33 pencil drawings"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "fef91dcc799042b59201fc0f963a8b17",
      "question": "what does it look like?",
      "prediction": "A Pablo Picasso sketchbook",
      "ground_truths": [
        "The sketchbook has a red varnished cover with the word \"Album\" inscribed on"
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "34a877cef8f4456197484f09d1643e87",
      "question": "how many drawings missing",
      "prediction": "33",
      "ground_truths": [
        "33 pencil"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0e497d4f0b4642248a1e1e4b25960ce0",
      "question": "The artist used the sketchbook when?",
      "prediction": "between 1917 and 1924",
      "ground_truths": [
        "between 1917 and 1924"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77dc5fcd3f654630a9a8ec90c9375b45",
      "question": "when was the notebook used?",
      "prediction": "between 1917 and 1924",
      "ground_truths": [
        "between 1917 and 1924"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cdec18921af845c694384dea037eb6ec",
      "question": "What color is the cover?",
      "prediction": "red",
      "ground_truths": [
        "red"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6392f94b4c9f4fd38b24b66e0f2aead8",
      "question": "Who was killed in the hijacking?",
      "prediction": "Reggae legend Lucky Dube,",
      "ground_truths": [
        "Reggae legend Lucky Dube,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "76bde369225e4bbda0147da9063ff142",
      "question": "Who has killed in the attempted hijacking?",
      "prediction": "Lucky Dube,",
      "ground_truths": [
        "Reggae legend Lucky Dube,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7c59243c55f6421f8119754dd28c0766",
      "question": "Does this affect the upcoming World Cup?",
      "prediction": "cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby",
      "ground_truths": [
        "his death cast a shadow over festivities"
      ],
      "em": 0,
      "f1": 0.4444444444444444
    },
    {
      "id": "9dd205a2702747e18373629a67b11baf",
      "question": "Where did the hijacking take place?",
      "prediction": "Johannesburg",
      "ground_truths": [
        "Johannesburg"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "63eaf8aa6eda439cb304bb18e12eea91",
      "question": "What did the hijacker try to steal?",
      "prediction": "his car,",
      "ground_truths": [
        "car,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "be51da2808964196bf2f456a76db8be9",
      "question": "In what city was Dube killed?",
      "prediction": "Johannesburg",
      "ground_truths": [
        "Johannesburg"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c6f1257958664760877836b38f68af88",
      "question": "When was Dube killed?",
      "prediction": "8 p.m. local time Thursday",
      "ground_truths": [
        "around 8 p.m. local time Thursday"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "f79b0a4362b34592be510c1391e9f64a",
      "question": "Did hijacker try to steal a car?",
      "prediction": "reggae star Lucky Dube was killed in Johannesburg",
      "ground_truths": [
        "his"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c974615345cf4c53b0db7413aa72bcfe",
      "question": "What country was reggae legend Lucky Dube from?",
      "prediction": "South Africa.",
      "ground_truths": [
        "South Africa's"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cd7cad43372b4947834b1f92e929ba96",
      "question": "Who was killed in an attempted hijacking?",
      "prediction": "Reggae legend Lucky Dube,",
      "ground_truths": [
        "Lucky Dube,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c09ddd59a8b14c9281b874e24a2053e4",
      "question": "Did he during an attempted carjacking",
      "prediction": "was shot.",
      "ground_truths": [
        "Dube, 43, was killed"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "5ff70db3c11f4c09b46ec32324df940a",
      "question": "Where did 1 million gather?",
      "prediction": "Angola",
      "ground_truths": [
        "in Angola"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "263ac68d0d564969908cdec34ad10b8d",
      "question": "What is the name of the Pope?",
      "prediction": "Benedict",
      "ground_truths": [
        "Benedict XVI"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4bd6129a79ac4ba3ae46913fb8243d9c",
      "question": "Amount of people gathered to hear the Pope?",
      "prediction": "estimated 1 million",
      "ground_truths": [
        "1 million"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "dc1bcf2ea7d34043b9337438ed4acbf4",
      "question": "where was the event",
      "prediction": "Angola",
      "ground_truths": [
        "Angola"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3eb511ac77664650b1b55a19c630b375",
      "question": "What does the Pope express?",
      "prediction": "\"deep sorrow\" at the death of two women killed in a stampede",
      "ground_truths": [
        "Benedict also expressed \"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola on Saturday,"
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "b83cfc8d017346568f72d0993f32a5bd",
      "question": "how many dead?",
      "prediction": "two women",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ff45f51c967447bbafe8da76140a1a0f",
      "question": "Where was Mass?",
      "prediction": "Angola",
      "ground_truths": [
        "in Angola,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d40052ef1ed24b12a67167b232d8d8e1",
      "question": "What continent is the Pope visiting?",
      "prediction": "Africa",
      "ground_truths": [
        "Africa."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1eb7d127003f4185bd7ffa3cc441685e",
      "question": "Which airline has lighting that subtly shifts throughout the day?",
      "prediction": "Virgin America",
      "ground_truths": [
        "Virgin America"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b71158f3ac6c4380aa5d0ca50b47f0dc",
      "question": "Which airline has in-cabin lighting?",
      "prediction": "Virgin America",
      "ground_truths": [
        "Virgin America"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88d0ad4c27184b2886a42cd1aa074e19",
      "question": "How many driverless pods were there?",
      "prediction": "18",
      "ground_truths": [
        "18"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "513409fabe5141609eff1756de476af0",
      "question": "Where are the driverless pods being tested",
      "prediction": "at London Heathrow's Terminal 5.",
      "ground_truths": [
        "at airports"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "1df83e933ab24ff0b54b67700f2cfa1b",
      "question": "How many pods are being tested?",
      "prediction": "18",
      "ground_truths": [
        "are"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e349996be424e28a5b92a8b05a0d198",
      "question": "What shifts throughout the day?",
      "prediction": "cabin lighting",
      "ground_truths": [
        "in-cabin lighting system"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "aa3e80c7f0fc47e49f2db9651533f6f7",
      "question": "In which country is Madonna helping orphans?",
      "prediction": "Malawi.",
      "ground_truths": [
        "Malawi,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1d7d93b9022543dca084d0fa1c352457",
      "question": "Who discovers the passion of the Angolan footballers?",
      "prediction": "David McKenzie",
      "ground_truths": [
        "David McKenzie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ceb298b8c9064b5d94824599481fdef8",
      "question": "Who discovered the passion of the Angolan football squad?",
      "prediction": "David McKenzie",
      "ground_truths": [
        "David McKenzie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "409da2ac9b274f3ca50de2b2fcf4d564",
      "question": "Alina Cho speaks to who about her efforts to help other Malawi's orphans?",
      "prediction": "singer",
      "ground_truths": [
        "Madonna"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7e5dd4ef82641c6bebdd9a9c4a7ebb0",
      "question": "Who is helping Malawian orphans?",
      "prediction": "Madonna",
      "ground_truths": [
        "Madonna"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0b7ae048102f4dcda239ce5315eb5e72",
      "question": "What sport does the World Cup legend play?",
      "prediction": "football",
      "ground_truths": [
        "football"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "274afac6641443f7b27638c937cdb43e",
      "question": "What is the name of the World Cup legend?",
      "prediction": "David McKenzie",
      "ground_truths": [
        "Diego Maradona"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6340a3591404e35b566ee5b14cf49bd",
      "question": "Inside Africa catches up with a World Cup legend spreading football excitement in which country?",
      "prediction": "South Africa.",
      "ground_truths": [
        "South"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9c4e8842976246d0adeceba704a8ac8c",
      "question": "Who speaks to Madonna?",
      "prediction": "David McKenzie",
      "ground_truths": [
        "Alina Cho"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1364ba1827604428a0d1e1ce8fd8fdc9",
      "question": "Where did he go?",
      "prediction": "a gym",
      "ground_truths": [
        "drove to a gym"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "327e68cc519349b695720f801ef03267",
      "question": "where were the bodies found?",
      "prediction": "bedrooms of their two-floor home",
      "ground_truths": [
        "the bedrooms of their two-floor home"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a94e90ab1f0e4ffa8760e52e917216a9",
      "question": "what city did slayings occur in?",
      "prediction": "Columbia, Illinois,",
      "ground_truths": [
        "St. Louis,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7944ad8c802d417fb16189db00b2dcd3",
      "question": "What was on the walls of the home?",
      "prediction": "threatening messages",
      "ground_truths": [
        "threatening messages"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0e370c4ffa374f42ba74df68937b9f6f",
      "question": "Who will decide whether to file charges?",
      "prediction": "state's attorney",
      "ground_truths": [
        "state's attorney"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5043888646d5421d81fc3b47f8aef68e",
      "question": "Where did the husband say he was?",
      "prediction": "gym",
      "ground_truths": [
        "gym to work out."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "707ff5fcde5548819e3c5f3c95a12beb",
      "question": "Who made the documentary?",
      "prediction": "Sabina Guzzanti",
      "ground_truths": [
        "Sabina Guzzanti"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b9f6b0c738be48b5a1fc57326e3a87ba",
      "question": "Documentary was screened where?",
      "prediction": "Cannes Film Festival,",
      "ground_truths": [
        "Cannes Film Festival,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0fa068fc67094f12bd7bd1147fa6f3ca",
      "question": "What was the name of the documentary?",
      "prediction": "\"Draquila",
      "ground_truths": [
        "\"Draquila"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "950b433fc59c497e93d4e7e4fcb5438c",
      "question": "Who boycotted the film?",
      "prediction": "Mediaset TV network.",
      "ground_truths": [
        "Sandro Bondi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21abaf846e2a489a8d36c0c21e27358d",
      "question": "The film, \"Draquila,\" takes issue with the way the prime minister handled what earthquake?",
      "prediction": "L'Aquila",
      "ground_truths": [
        "L'Aquila"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bdc2f07e48814dec8eb597d9b77a8d11",
      "question": "Italian Culture Minister boycotted Cannes  why?",
      "prediction": "denouncing the film as \"propaganda\"",
      "ground_truths": [
        "Bondi issued a statement, dismissing the documentary as \"propaganda\" and saying it \"offends the truth and all of the"
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "721e1c8fa27e45dbbbc5261cd2c3d9d4",
      "question": "What is the documentary about?",
      "prediction": "The Italian government has responded angrily to the film, whose title is a combination of the words \"Dracula\" and \"L'Aquila.\"",
      "ground_truths": [
        "prime minister's handling of the L'Aquila earthquake,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "af32ee59935e4a90b57a6e01d8ae0ced",
      "question": "What is the filmmaker's name?",
      "prediction": "Sabina Guzzanti",
      "ground_truths": [
        "Sabina Guzzanti"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "82210515b3ad4b2bbdad50b7b0f7faff",
      "question": "Who was the documentary about?",
      "prediction": "Italian leader Silvio Berlusconi.",
      "ground_truths": [
        "Silvio Berlusconi."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "46c47691cf5b44489b589d6a42b0e438",
      "question": "What is open 24/7?",
      "prediction": "The control room",
      "ground_truths": [
        "The control room"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8b9bcb9933c4334a5e15347af4e6404",
      "question": "what year was H1N1",
      "prediction": "2009",
      "ground_truths": [
        "2009"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "26768a2b3ffd462aa76c0814a0fb67b6",
      "question": "Who needs to use caution?",
      "prediction": "Australian ships",
      "ground_truths": [
        "those traveling near the Somali coast"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f6fd7c37e3c4224892dcacfcdad4a68",
      "question": "Where will passengers fly?",
      "prediction": "over the Gulf of Aden,",
      "ground_truths": [
        "over the"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "321880cf525f47328e5f457f4b2386c4",
      "question": "Where do passengers have to fly to continue their journey?",
      "prediction": "Dubai",
      "ground_truths": [
        "Dubai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d658d3578ccb4607bdbc7b6585bc9a14",
      "question": "Who issued a travel warning?",
      "prediction": "German Foreign Ministry,",
      "ground_truths": [
        "German Foreign Ministry,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "787e42ec72df45998c4507345004caea",
      "question": "How many passengers were mentioned as travelling in this situation?",
      "prediction": "246",
      "ground_truths": [
        "246"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "99b4eebe743f4bcabce1ec386e81793e",
      "question": "Which countries advise extreme caution while traveling near the Somali coast?",
      "prediction": "U.S. State Department and British Foreign Office",
      "ground_truths": [
        "U.S. State Department and British Foreign Office"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b244e94cae0f40d2a5ef6d39687b8eea",
      "question": "Where will passengers fly to continue their journey?",
      "prediction": "Dubai",
      "ground_truths": [
        "Dubai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2a8589ed0aa4c0e9cfb1446827b09ab",
      "question": "How many passengers does it involve?",
      "prediction": "246",
      "ground_truths": [
        "246"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "793f58910e644f078c60b96a422b9dcc",
      "question": "Which ship was the incident aboard?",
      "prediction": "MS Columbus,",
      "ground_truths": [
        "MS Columbus,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e006b8380d04df7ac129af93547dc5f",
      "question": "Where did the US and UK say that people should avoid?",
      "prediction": "the Somali coast",
      "ground_truths": [
        "the Somali coast"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b454ee0cee8452986cf0b57342e8534",
      "question": "Where was Omar previously denied?",
      "prediction": "Britain.",
      "ground_truths": [
        "asylum in Britain."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "540ee9edf9fd4bfc981390d172704bbc",
      "question": "Did Spain give a reason for turning down the asylum?",
      "prediction": "no",
      "ground_truths": [
        "was given"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fae579ad240d4227b1301d52e1de5ff3",
      "question": "Who was denied asylum in Britain?",
      "prediction": "Osama bin Laden's sons",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "654cd3a33c974ab989777ea8cd69c130",
      "question": "Where did Omar bin Laden first try to flee to?",
      "prediction": "Madrid's Barajas International Airport",
      "ground_truths": [
        "Britain."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c9d87584b86413d86f28eb40a1af7cf",
      "question": "Who did Spain deny asylum to?",
      "prediction": "Osama bin Laden's sons",
      "ground_truths": [
        "Omar bin Laden,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "ed2ba8953b6a49bb8f961969603a956f",
      "question": "Who was denied asylum?",
      "prediction": "One of Osama bin Laden's sons",
      "ground_truths": [
        "Omar bin Laden"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "5ed1ab8a035e44ec9200692431509884",
      "question": "Who didn't give a reason?",
      "prediction": "The Interior Ministry,",
      "ground_truths": [
        "U.N. High Commissioner for Refugees"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "548b12b048354fa198f25caa234a0081",
      "question": "What family member of Omar bin Laden was associated with terrorism?",
      "prediction": "Bin Laden's sons",
      "ground_truths": [
        "his father"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ecb8eb62bc3d4ae280ea3151211089a3",
      "question": "What did he want his father to abandon?",
      "prediction": "terrorism.",
      "ground_truths": [
        "terrorism."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "00adadfa907f41a0b1bb4e8e4fd98a81",
      "question": "Mugabe and Tsvangirai have signed agreement paving way for what?",
      "prediction": "power-sharing talks",
      "ground_truths": [
        "power-sharing talks to take place in the next few weeks."
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "e370ff23a5e849569a688e4159142601",
      "question": "Whose government was called to be illegitimate?",
      "prediction": "Mugabe",
      "ground_truths": [
        "Zimbabwean"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9542c8bd1b494422b21cbbbd76fa9ea0",
      "question": "What does order expand?",
      "prediction": "U.S. sanctions",
      "ground_truths": [
        "U.S. sanctions against"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "cd7056ccab0749d7930b3a75760ff7d8",
      "question": "What did Bush calls Robert Mugabe's government?",
      "prediction": "\"illegitimate.\"",
      "ground_truths": [
        "\"illegitimate.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4fedeb2052db4dcbb256d6bd31be5c5d",
      "question": "Who signed the order?",
      "prediction": "President Bush",
      "ground_truths": [
        "President Bush"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "83524294a9eb41f5acc541555457dcae",
      "question": "What two countries vetoed UN resolutions relevant to this situation?",
      "prediction": "Russia",
      "ground_truths": [
        "Russia and China"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ab4547eb5b754a788730ac927811a4a3",
      "question": "Who is Tsvangirai?",
      "prediction": "opponents",
      "ground_truths": [
        "opposition candidate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6e0c91b398b44fea89c823558223b92",
      "question": "What did President Bush sign order to expand?",
      "prediction": "U.S. sanctions",
      "ground_truths": [
        "U.S. sanctions against"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "0715d4474ede4876903c8b68ce6c897a",
      "question": "Who signed an agreement paving way for power-sharing talks?",
      "prediction": "Mugabe and Tsvangirai",
      "ground_truths": [
        "Mugabe and Tsvangirai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da070cef91304c029a6edab81dec8e02",
      "question": "What country does Robert Mugabe lead?",
      "prediction": "Zimbabwean government",
      "ground_truths": [
        "Zimbabwe,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62f85a5490834ba294bd772557571fca",
      "question": "What country are the sanctions against?",
      "prediction": "Zimbabwe",
      "ground_truths": [
        "Zimbabwe,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2a65bbb2d8e4767ac2fb9423e8c685b",
      "question": "Who vetoed the UN resolution?",
      "prediction": "Russia and China",
      "ground_truths": [
        "Russia and China"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2058b653142d477caffeab8b3e29dbcd",
      "question": "What did tthe CEO say?",
      "prediction": "\"several pieces of aircraft equipment were at fault or had broken down.\"",
      "ground_truths": [
        "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "758df4eb23104c66950c66ce7d3a78c9",
      "question": "What encountered heavy turbulence?",
      "prediction": "The mysterious disappearance of Flight AF 447",
      "ground_truths": [
        "the Airbus A330-200"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3617db7062854907aa937695b18e8696",
      "question": "What did the A330-200 encounter?",
      "prediction": "heavy turbulence",
      "ground_truths": [
        "heavy turbulence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7047a1d5e4564611903f19acd9f2e764",
      "question": "What did the Airbus A330-200 encounter",
      "prediction": "heavy turbulence",
      "ground_truths": [
        "heavy turbulence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e271a1cce4441378acdeabc701dcb04",
      "question": "How many people were on the flight?",
      "prediction": "228",
      "ground_truths": [
        "228"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "97f7bf68a05f48958ef3062ec906f6cb",
      "question": "How many people where onboard the flight",
      "prediction": "228",
      "ground_truths": [
        "228"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b832de505fb545e2995dfc6e50284d8f",
      "question": "What did some consider to be a possible cause",
      "prediction": "lightning strike",
      "ground_truths": [
        "lightning strike"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "27447b04916e4932aa885dc445aa0f68",
      "question": "What did Obama said?",
      "prediction": "\"In the 21st century, we know that the future of our economy and national security is inextricably linked with one challenge: energy,\"",
      "ground_truths": [
        "\"We've seen Washington launch policy after policy, yet our dependence on foreign oil has only grown, even as the world's resources are disappearing,\""
      ],
      "em": 0,
      "f1": 0.04761904761904762
    },
    {
      "id": "b2e3e2f0039645cbb9bbef363bb33901",
      "question": "How much will be invested?",
      "prediction": "$150 billion over 10 years",
      "ground_truths": [
        "$150 billion"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "77fc4a206e1b4992a867608d3e9cd208",
      "question": "How much would Obama invest?",
      "prediction": "$150 billion over 10 years",
      "ground_truths": [
        "$150 billion over 10 years"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba8c3e4c32eb4c6c8513fe3ffb6cd12f",
      "question": "Who was named secretary of energy?",
      "prediction": "Steven Chu",
      "ground_truths": [
        "Steven Chu"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "660fdaeb1a134f3782b6737147b03bfd",
      "question": "Who named Secretary of energy?",
      "prediction": "Barack Obama",
      "ground_truths": [
        "Steven Chu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b29f0f79a6e24fa695af9ea0ca9818bf",
      "question": "Who created climate policy?",
      "prediction": "Carol Browner",
      "ground_truths": [
        "Carol Browner"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "85f8058231ee479b8af6624c13a1993f",
      "question": "Who hosted \"Star Search\"?",
      "prediction": "Ed McMahon",
      "ground_truths": [
        "Ed McMahon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "97547bbeb3614747bdb52be1d90c7bc2",
      "question": "Who hosted \"The Tonight Show\"?",
      "prediction": "Carson",
      "ground_truths": [
        "Carson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "be20cc1e18b54ecc87e4a1e34d45bff6",
      "question": "What tv shows hosted McMahon?",
      "prediction": "\"The Tonight Show,\"",
      "ground_truths": [
        "\"TV's Bloopers and Practical Jokes,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f40951a7d86e43dcb35ade73d74a48a4",
      "question": "What is McMahon most famous for?",
      "prediction": "Johnny Carson sidekick",
      "ground_truths": [
        "longtime pitchman and Johnny Carson sidekick"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e82fa7a590004a5091ed4ff9d55156b5",
      "question": "what health problems suffered McMahon?",
      "prediction": "pneumonia",
      "ground_truths": [
        "pneumonia and other medical"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "64395311a5d54d79baae78a2162cf8b9",
      "question": "Which TV programs did McMahon host?",
      "prediction": "\"Star Search\"",
      "ground_truths": [
        "\"TV's Bloopers and Practical Jokes,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "097639dba05445eba050b1141ea106b9",
      "question": "New book quotes Reid discussing what?",
      "prediction": "Obama's race in 2008.",
      "ground_truths": [
        "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\""
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "fc1195a9859b43be9b77eff8e9ea98f4",
      "question": "What does Congressional Black Caucus reject?",
      "prediction": "calls for Reid's dismissal.",
      "ground_truths": [
        "calls for Reid's dismissal."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "deabb360cc7344ff91443e4ecc8cb960",
      "question": "What does GOP chair say about the Senate majority leader's language?",
      "prediction": "\"Racism and racist conversations have no place today in America.\"",
      "ground_truths": [
        "\"embarrassing and racially insensitive,\""
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "9937f92b0d734539ae1d70d9845d5ed4",
      "question": "Congressional Black Caucus rejects calls for?",
      "prediction": "rejection",
      "ground_truths": [
        "him to step down as majority leader."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c2ad6efb4e64e0f98d96ec7c5d3ba0b",
      "question": "What are training dogs for?",
      "prediction": "trainers to sniff out cell phones.",
      "ground_truths": [
        "to sniff out cell phones."
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "482ca94d43ea43c481d3e179d4c613bf",
      "question": "What are dogs trained to do?",
      "prediction": "sniff out cell phones.",
      "ground_truths": [
        "sniff out cell phones."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a22abca53c0c4cbea72109ec7e09b0f3",
      "question": "What clears Texas Legislature?",
      "prediction": "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.",
      "ground_truths": [
        "that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b05edac93854de7bfc7f66698634f62",
      "question": "What does the bill crack down on?",
      "prediction": "convicted caught with phones",
      "ground_truths": [
        "convicts caught with phones"
      ],
      "em": 0,
      "f1": 0.75
    },
    {
      "id": "82f836841fb745ef972057edb01a3892",
      "question": "What are sniffer dogs in Texas prisons looking for?",
      "prediction": "cell phones.",
      "ground_truths": [
        "sniff out cell phones."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "47c655c75bad4ccb96e18b573d3ab162",
      "question": "What do inmates in Texas have?",
      "prediction": "Hundreds of contraband cell phones",
      "ground_truths": [
        "cell phones"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "7c24847ff8924ded8c7f14ae191cb0f5",
      "question": "What is texas?",
      "prediction": "among a growing number of state governments going after them.",
      "ground_truths": [
        "state"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "2712e3b995a849349843ff869f4c5820",
      "question": "What does the book include?",
      "prediction": "a paragraph about the king and crown prince",
      "ground_truths": [
        "a paragraph about the king and crown prince"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "27ba4dcff67c42e4bd660ed84520cc9d",
      "question": "who was arrested?",
      "prediction": "Harry Nicolaides",
      "ground_truths": [
        "Harry Nicolaides,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5579074640614bacbdd03ed34a3ddf08",
      "question": "what did authorities consider it as",
      "prediction": "defame, insult or threaten the crown.",
      "ground_truths": [
        "illegal to defame, insult or threaten the crown."
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "b7be73a8ada1408b95b476b80880309d",
      "question": "When was Harry Nicolaides arrested over his 2005 book?",
      "prediction": "August",
      "ground_truths": [
        "August 31."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e854d8deb6cf4af4ac58d44cb3c027c3",
      "question": "what age was Harry Nicolaides",
      "prediction": "41,",
      "ground_truths": [
        "41,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "171414c078ce452d9374be8989aa0147",
      "question": "what does book include?",
      "prediction": "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.",
      "ground_truths": [
        "a paragraph about the king and crown prince"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "fe0121ef5b874df0bfca5369f2253f03",
      "question": "what did Man's lawyer say",
      "prediction": "\"He is obviously very relieved and grateful that the pardon was granted,\"",
      "ground_truths": [
        "he was released Friday and taken to the Australian embassy"
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "6a80ae4b171345c3ba541b8aa343b1be",
      "question": "what did authorites deem?",
      "prediction": "a violation of a law that makes it illegal to defame, insult or threaten the crown.",
      "ground_truths": [
        "a violation of a law that makes it illegal to defame, insult or threaten the crown."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5436abb6b9514525b7f31b9e1b4935b7",
      "question": "Where does the husband tweet from?",
      "prediction": "Twitter",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eab5c3e8c5b7496084b5bce4d50369f8",
      "question": "from where Troy issues tweets?",
      "prediction": "that alternated between raw descriptions and expressions of hope:",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a951f069f82b478a9ec9d63a07844610",
      "question": "who are missionary couple with houseful of children?",
      "prediction": "Tara and Troy Livesay",
      "ground_truths": [
        "Tara and Troy Livesay"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ee882bdef05c416d8da08f506b7f0abb",
      "question": "When did wife blog?",
      "prediction": "Wednesday morning.",
      "ground_truths": [
        "Wednesday morning."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8145ebc165f4377b68d53d8e91debdf",
      "question": "Where does Troy issue tweets from?",
      "prediction": "World Wide Village,",
      "ground_truths": [
        "Haiti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e89f382568a945d8bed79a01ca36defc",
      "question": "What are the couple?",
      "prediction": "Amerman missionary family",
      "ground_truths": [
        "country directors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7fc7459e586348a193b39ef43b3e57bd",
      "question": "who is reputed leader of klan?",
      "prediction": "Raymond \"Chuck\" Foster",
      "ground_truths": [
        "Raymond \"Chuck\" Foster"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da428c5bcb424a7d895a78bed863d644",
      "question": "Where is Cynthia Lynch from?",
      "prediction": "Tulsa, Oklahoma.",
      "ground_truths": [
        "Tulsa, Oklahoma."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "206318a41226426c9daef8ac3ed2ceca",
      "question": "What is the KKK?",
      "prediction": "Sons of Dixie,",
      "ground_truths": [
        "Ku Klux Klan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "51ef7c72120942d287ad51db83ccd02b",
      "question": "What are Americans doing as a way to cope with tough economic times?",
      "prediction": "bartering",
      "ground_truths": [
        "bartering"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1ff3f2ec9a814df7b4dfaea27fcdd29c",
      "question": "When does bartering rise dramatically?",
      "prediction": "during an economic downturn.",
      "ground_truths": [
        "\"When the economy turns unfriendly,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db9d9939947742aaa3a4f8c7cd946cef",
      "question": "Who is the barter network president?",
      "prediction": "Susan MacTavish Best.",
      "ground_truths": [
        "Michael Krane,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9bb38adb3e7e4ee2ba2c7cfbe892ab34",
      "question": "What does bartering involve?",
      "prediction": "not exchanging money",
      "ground_truths": [
        "trading goods and services without exchanging money"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "2f0f14b7f5824a639aa29f309aa3e903",
      "question": "What does barterng involve?",
      "prediction": "trading goods and services without exchanging money",
      "ground_truths": [
        "trading goods and services without exchanging money"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "82675ed3f91146cda526915da1211f18",
      "question": "What are Americans doing to cope with tough economic times?",
      "prediction": "bartering",
      "ground_truths": [
        "bartering"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "58b38431dd6a401194b71fafd8326ed9",
      "question": "who is not  considered an African-American woman for high court?",
      "prediction": "Leah Ward Sears,",
      "ground_truths": [
        "Elena Kagan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e772aa8945a4d19aa6b80ab41e15ef1",
      "question": "what are they ticked off ab out",
      "prediction": "a number of leaders I've talked with over the last several days.",
      "ground_truths": [
        "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul"
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "6c064ea2080147e797731b9e36ad5834",
      "question": "What has ticked off a number of leaders?",
      "prediction": "nominations of Elena Kagan",
      "ground_truths": [
        "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "a3961cb61ec74837b21fa52a4f6b518b",
      "question": "what woman has he considered",
      "prediction": "Leah Ward Sears,",
      "ground_truths": [
        "Elena Kagan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d98133709d87454b8868e46a888416b4",
      "question": "What does the draft report say about Iran?",
      "prediction": "nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,",
      "ground_truths": [
        "could be secretly working on a nuclear weapon"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "ff6c6eecb3414370b325b900070c2711",
      "question": "What could Iran be secretly working on?",
      "prediction": "nuclear weapon",
      "ground_truths": [
        "nuclear weapon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28c41ce5339b42849560e5551160bf9a",
      "question": "Who said the report is a major development?",
      "prediction": "Faried Zakaria.",
      "ground_truths": [
        "Fareed Zakaria."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "536a0720343f436abf39e0409ac9beae",
      "question": "what did zakaria say",
      "prediction": "A new report saying that Iran could be secretly working on a nuclear weapon is a major development,",
      "ground_truths": [
        "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\""
      ],
      "em": 0,
      "f1": 0.06451612903225808
    },
    {
      "id": "168b2329b8504ce2a0640c61e14ff536",
      "question": "What is not on the side of the Iranian regime?",
      "prediction": "of a potential military strike",
      "ground_truths": [
        "Iran's Green Movement of protesters"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "2a38dff494494797839d2adfe6875577",
      "question": "What does he talk of against Iran?",
      "prediction": "nuclear weapon",
      "ground_truths": [
        "military strike"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d197ff9d87cc47c4ae12db152196fa35",
      "question": "what was the report about",
      "prediction": "Iran could be secretly working on a nuclear weapon",
      "ground_truths": [
        "Iran could be secretly working on a nuclear weapon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f05df570a93435fb2fe25016fa26166",
      "question": "When was Ma Khin Khin Leh sentenced to life in prison?",
      "prediction": "July 1999,",
      "ground_truths": [
        "July 1999,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "12039f9c08bf4751afd6c8396ffecaf4",
      "question": "How many prisoners were freed from Myanmar?",
      "prediction": "Nineteen",
      "ground_truths": [
        "Nineteen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "efefcf436f894cbd94bff1c8b03a7458",
      "question": "Where was leader Aung San Suu Kyi confined to?",
      "prediction": "Her last house arrest began in 2003 and has been periodically renewed.",
      "ground_truths": [
        "in her home"
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "bd01ef65a2bd4c56accb1f2446318bf9",
      "question": "Number of politicl prisoners freed in Myanmar?",
      "prediction": "Nineteen",
      "ground_truths": [
        "Nineteen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "54e2bcd8dc47489a988fd7647b6c6126",
      "question": "Where were the school teacher and 18 other political prisoners freed from?",
      "prediction": "Myanmar",
      "ground_truths": [
        "Myanmar"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7f334092620f45bba5fa99140c443e3c",
      "question": "Who is still confined to home?",
      "prediction": "Aung San Suu Kyi",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2e45c3ad14df4921bb75492b6ec4275c",
      "question": "Who was confined to his home?",
      "prediction": "Aung San Suu Kyi",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1f4d2e02ed614e1ba05e280a9d2dd39a",
      "question": "Why was Ma Khin Leh sentenced to life?",
      "prediction": "her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,",
      "ground_truths": [
        "because her husband, a student activist, had helped plan a protest demonstration in Bago in July 1999,"
      ],
      "em": 0,
      "f1": 0.9655172413793104
    },
    {
      "id": "3c473008c4bb49e4b6e79330fe92c32d",
      "question": "Who is the pro-democracy leader confined to her home in Myanmar?",
      "prediction": "Aung San Suu Kyi",
      "ground_truths": [
        "Aung San Suu Kyi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "17d54228f388411d864fcd28aaa0da8a",
      "question": "Who wa sentenced to life?",
      "prediction": "Ma Khin Khin Leh,",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8557bf45257f477c9da97b4c04560acd",
      "question": "Who was sentenced to life?",
      "prediction": "Ma Khin Khin Leh,",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "094d74e7455d401f84ca459c40084cb5",
      "question": "Who is among the 19 prisoners?",
      "prediction": "Ma Khin Khin Leh,",
      "ground_truths": [
        "Ma Khin Khin Leh,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e1dac33266d048c89894eab0b0dc1c2a",
      "question": "What is being planned?",
      "prediction": "Kingdom City",
      "ground_truths": [
        "tallest building,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9e3b57d9763d4f1da04cc784d5944be2",
      "question": "What will reach over a kilometer in height?",
      "prediction": "the world's tallest building,",
      "ground_truths": [
        "tallest building,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "695bcb8dd90046bba77ed4fe322db935",
      "question": "a new generation of what?",
      "prediction": "innovative, exciting skyscrapers",
      "ground_truths": [
        "innovative, exciting skyscrapers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "66671cea0dee4e95ace4c4ccbdcc094b",
      "question": "Which towers have good designs in the US?",
      "prediction": "Chicago Spire",
      "ground_truths": [
        "highlight of the rebuilt World Trade Center."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9d30d695b524637976deff70f6a7afd",
      "question": "What height will Kingdom City be?",
      "prediction": "over a kilometer (3,281 feet) high.",
      "ground_truths": [
        "over a kilometer (3,281 feet)"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "e3ece1d381234fa9a9402f702b38c5b5",
      "question": "What will the height of Kingdom City be?",
      "prediction": "over a kilometer (3,281 feet)",
      "ground_truths": [
        "over a kilometer (3,281 feet) high."
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "8b0e7ccc65914a2c8d7cbc43b114b06b",
      "question": "How many hostages did the Farc hold?",
      "prediction": "750",
      "ground_truths": [
        "750"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8586cbf2042d473196cb9cd0622be798",
      "question": "Who was kidnapped August 4?",
      "prediction": "Oscar Tulio Lizcano",
      "ground_truths": [
        "Oscar Tulio Lizcano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b7e8fbd8eb7489983dbfac313c5124e",
      "question": "What does FARC stand for?",
      "prediction": "Revolutionary Armed Forces of Colombia,",
      "ground_truths": [
        "Revolutionary Armed Forces of Colombia,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f4dae4387c854583a99555dab762a1db",
      "question": "Who kidnapped the ex-congressman?",
      "prediction": "FARC",
      "ground_truths": [
        "leftist rebels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46fd622e5ff544c29074182e84798594",
      "question": "In what country are the estimated 750 hostages being held?",
      "prediction": "Colombia.",
      "ground_truths": [
        "Colombia."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7ccb38cca004c16af0889a565db7996",
      "question": "When was Lizcano kidnapped?",
      "prediction": "August 4, 2000",
      "ground_truths": [
        "August 4, 2000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d93fb638308e48419553c46f6fd4582e",
      "question": "How many hostages does the FARC hold?",
      "prediction": "750",
      "ground_truths": [
        "750"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b69d29e4720248f283d9ab2913ce47e1",
      "question": "Lizcano fled how long ago?",
      "prediction": "three days",
      "ground_truths": [
        "about three days"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "9f6d3bd15c0c4bdf963d31cccb7a8b7b",
      "question": "Who was kidnapped on August 4, 2000?",
      "prediction": "Oscar Tulio Lizcano",
      "ground_truths": [
        "Oscar Tulio Lizcano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7648c8b476e4482a7fa9a8d4584c551",
      "question": "What was the value of the fortune Daniel Radcliffe received when he turned 18?",
      "prediction": "£20 million",
      "ground_truths": [
        "£20 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8047bb5d810b4c2f967c45b876ff4bb8",
      "question": "What age is Daniel?",
      "prediction": "18",
      "ground_truths": [
        "18,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "db63ac356f6a4df8b59087b055a45357",
      "question": "Where was the money held?",
      "prediction": "in a trust fund",
      "ground_truths": [
        "in a trust fund"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f3fc1b6ee3b24e47856d9fe7730350a2",
      "question": "What is he going to do?",
      "prediction": "get into a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\"",
      "ground_truths": [
        "have some sort of party,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eff7102be794409081a5e3b3ef3c9518",
      "question": "What happens when Daniel Radcliffe turns 18?",
      "prediction": "the money won't cast a spell on him.",
      "ground_truths": [
        "gains access to a reported £20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2747da8e1e1416cb057ee8651131dd8",
      "question": "Which earnings were held?",
      "prediction": "in a trust fund",
      "ground_truths": [
        "£20 million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ba2b3c2aeb464d8f8a949fc75944bcbd",
      "question": "Who stars in Harry Potter?",
      "prediction": "Daniel Radcliffe",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f5b923f78d642ebb61f535595115ceb",
      "question": "What day is Daniel Radcliffe's birthday?",
      "prediction": "Monday,",
      "ground_truths": [
        "Monday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3bb0f77a9cbc44fab12fdc8005e69f91",
      "question": "Who inherits £20M on Monday?",
      "prediction": "Daniel Radcliffe",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dfc54f2afa2849f4a1d9d7095fa37f8d",
      "question": "What is he saving for?",
      "prediction": "\"I won't cast a spell on him.",
      "ground_truths": [
        "books"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a168c89e395546be9605e969f682e554",
      "question": "What happened to all the money Radcliffe made from the Harry Potter movies?",
      "prediction": "has no plans to fritter his cash away on fast cars, drink and celebrity parties.",
      "ground_truths": [
        "held in a trust fund"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "69bda7a3b8cb4e2baecd4fedbe7458ce",
      "question": "Where did the money in the trust fund come from?",
      "prediction": "first five Potter films",
      "ground_truths": [
        "the first five Potter films"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b6ae22e3a9d04cfbaf6206a39dd062da",
      "question": "What is Radcliffe's net worth?",
      "prediction": "£20 million ($41.1 million)",
      "ground_truths": [
        "£20 million"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b18ad0a41f164debb57278760e011fd1",
      "question": "What are his plans with the money?",
      "prediction": "not cast a spell on",
      "ground_truths": [
        "books"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25b9ecd1aefd493eae1547302afed49b",
      "question": "Who is the star of Harry Potter?",
      "prediction": "Daniel Radcliffe",
      "ground_truths": [
        "Daniel Radcliffe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f4ef4313787d49d7851ad2a6c66cf163",
      "question": "What amount of money is Daniel getting?",
      "prediction": "£20 million ($41.1 million)",
      "ground_truths": [
        "£20 million"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "eb6590ce2f134b889d853696a1519eca",
      "question": "what did the young actor say?",
      "prediction": "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.",
      "ground_truths": [
        "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\""
      ],
      "em": 0,
      "f1": 0.04761904761904762
    },
    {
      "id": "08ff89d3a48f43799ba35c485995ff19",
      "question": "What is search of the temple expected to turn up?",
      "prediction": "possible victims of physical and sexual abuse.",
      "ground_truths": [
        "possible victims of physical and sexual abuse."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6919a1b049514471b42c88f1e89ac267",
      "question": "Where is Jeffs in jail awaiting trial?",
      "prediction": "Arizona",
      "ground_truths": [
        "Texas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fb052cb92b64f6eadd68c2a9d16b0bb",
      "question": "What is the number of children removed from ranch?",
      "prediction": "183",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "df799a737f6343a8b4776b01076e3a3c",
      "question": "In which state is Jeffs jailed while awaiting trial?",
      "prediction": "Arizona",
      "ground_truths": [
        "Arizona"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cdd10c1af31544cdbe58e6e3edd89bde",
      "question": "How many children were removed?",
      "prediction": "183 people, including 137",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "0dbf719fe2f54987b1118862ace2d1fb",
      "question": "What is the number of people removed from ranch?",
      "prediction": "183",
      "ground_truths": [
        "183"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "90d73aa610df4fadad68bf28ed91ac0f",
      "question": "How many children were removed from the ranch?",
      "prediction": "183",
      "ground_truths": [
        "137"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "104a4d520b854b7aaae7f12b77c8891a",
      "question": "How many people were removed?",
      "prediction": "183",
      "ground_truths": [
        "183"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5406135ecb9b4257807d6a25521cc062",
      "question": "How many years was Jeffs sentenced to prison last year?",
      "prediction": "10",
      "ground_truths": [
        "10"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3db78d9616ac4d99986d3c580ad26549",
      "question": "What state jail is Jeffs waiting trial in?",
      "prediction": "Arizona",
      "ground_truths": [
        "Utah"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "10ca15fbc0e84072a3b293df8aef3e99",
      "question": "What instrument does she play?",
      "prediction": "piano",
      "ground_truths": [
        "piano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4a225586c76c416faf13ff63eeac09c8",
      "question": "Whose new album is she producing?",
      "prediction": "Barbara Streisand's",
      "ground_truths": [
        "\"Quiet Nights,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb54542b6bc748c89b36807ab7b082b2",
      "question": "What is Diana Krall's new album?",
      "prediction": "\"Quiet Nights,\"",
      "ground_truths": [
        "\"Quiet Nights,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a25ba84fca504b2aa4e126007a22a14f",
      "question": "what bridge did Monet draw?",
      "prediction": "Waterloo",
      "ground_truths": [
        "London's Waterloo"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3febc40bd3ee4d4e9226aa424b60123c",
      "question": "what room did Monet stay in at the Savoy?",
      "prediction": "618",
      "ground_truths": [
        "618"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "63fc6e7a70aa4d60a8cab579a90afb9e",
      "question": "From which room was Monet's Waterloo Bridge painted?",
      "prediction": "618 at the hotel",
      "ground_truths": [
        "618"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4fc9afe35d8c4cdf8115fe9faf6ac96a",
      "question": "what hotel did Monet stay at?",
      "prediction": "Savoy",
      "ground_truths": [
        "Savoy"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "400ab40069334ea6a12b1085941a9880",
      "question": "Whose drawing is on display at the Savoy?",
      "prediction": "Claude Monet",
      "ground_truths": [
        "Claude Monet"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b0bc5b996f8c43a4a27f7a0a92100227",
      "question": "When did the crash happen?",
      "prediction": "Monday",
      "ground_truths": [
        "Monday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "721498d8723844d5b1ab0d15eb8e7699",
      "question": "what is the number of passengers?",
      "prediction": "82",
      "ground_truths": [
        "82"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45e285d6497d478b835ec6f97fe0edb8",
      "question": "what amount of bodies have been found so far?",
      "prediction": "14",
      "ground_truths": [
        "14"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8b75f189ac042c9a664c7bc13bcb936",
      "question": "Where was black box located?",
      "prediction": "1,300 meters in the Mediterranean Sea.",
      "ground_truths": [
        "at a depth of about 1,300 meters in the Mediterranean Sea."
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "cbb5d9c625b74ae1b3426d2a185549c4",
      "question": "What was found?",
      "prediction": "flight data recorder",
      "ground_truths": [
        "flight data recorder from an Ethiopian Airlines plane"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "614b047fc1144246af208be2a508d5ce",
      "question": "How many bodies were recovered?",
      "prediction": "At least 14",
      "ground_truths": [
        "14"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8dcf91cc88c7478fb336f2b856b1f9b8",
      "question": "Where was the flight going?",
      "prediction": "Addis Ababa,",
      "ground_truths": [
        "Addis Ababa,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e45fbf36c6d64467b16f13324d22a4c2",
      "question": "where was the back box found?",
      "prediction": "at a depth of about 1,300 meters in the Mediterranean Sea.",
      "ground_truths": [
        "at a depth of about 1,300 meters in the Mediterranean Sea."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ae06e8228ca4ca89224a8598de5e6d9",
      "question": "What did 26 year old return from doing?",
      "prediction": "a former world No. 1",
      "ground_truths": [
        "after giving birth to baby daughter Jada,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47e690d71a1648aba0e8ee4f18ede013",
      "question": "How long did the final last and what was the score",
      "prediction": "58 minutes.",
      "ground_truths": [
        "6-2 6-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f764f3b545744048cf82108c4bb5157",
      "question": "What did Belgian defeat American in?",
      "prediction": "tennis",
      "ground_truths": [
        "final of the Sony Ericsson Open"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5bdc71bbfc6e4934aa337ab24559eb6c",
      "question": "Who did Kim demolish?",
      "prediction": "American third seed Venus Williams",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "354948edd4c94ed49d3c744a8213b0e4",
      "question": "who beat williams",
      "prediction": "Clijsters",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fed7ee44ec9407ebf2f05e33d2f6d54",
      "question": "what was the score",
      "prediction": "6-2 6-1",
      "ground_truths": [
        "6-2 6-1"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec0a4212a5df47f682fbba53188e7af8",
      "question": "How many titles has the winner of the tournament collected on the tour",
      "prediction": "2005",
      "ground_truths": [
        "third"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0178feda5ef74636ba6e5ba11135ceb6",
      "question": "Which player did Venue Williams lose to in the Miami final",
      "prediction": "Clijsters",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff83ee0ffed94025b218bc5444b14880",
      "question": "who had a baby",
      "prediction": "Clijsters",
      "ground_truths": [
        "Kim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b509458380a84ed5bdbaa84a6a4d1b86",
      "question": "who is al-moayad",
      "prediction": "Yemeni cleric",
      "ground_truths": [
        "a Yemeni cleric"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "57973e7ba373436eb1374990545bcf11",
      "question": "What did the court say?",
      "prediction": "overterrorism convictions for a Yemeni cleric and his personal assistant,",
      "ground_truths": [
        "not receive a fair trial."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8c03f10b27045d89e068c461ea3d69c",
      "question": "Who did al-Moayad boast about giving money to?",
      "prediction": "al Qaeda leader Osama bin Laden.",
      "ground_truths": [
        "al Qaeda leader Osama bin Laden."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9d53777d8a5b407f9330e6fdd7e43214",
      "question": "who supported terrorism?",
      "prediction": "a Yemeni cleric and his personal assistant,",
      "ground_truths": [
        "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "25223706065646aebcebc831d267c30d",
      "question": "Who gave money to Osama bin Laden?",
      "prediction": "Mohammed Ali al-Moayad and his personal assistant,",
      "ground_truths": [
        "al-Moayad"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "43204be4d77d421990db36f2d9951e63",
      "question": "What denied the pair a fair trial?",
      "prediction": "certain pieces of evidence presented by prosecutors",
      "ground_truths": [
        "prosecutors"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "ca8d39a6862e49b58a03a0da02a7bede",
      "question": "what does court say",
      "prediction": "they did not receive a fair trial.",
      "ground_truths": [
        "they did not receive a fair trial."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc5ecf6df2414809b12dd513fdfa7935",
      "question": "Who was convicted of supporting terrorism?",
      "prediction": "Yemeni cleric and his personal assistant,",
      "ground_truths": [
        "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "afe6d5bb126344f7aed2447a11235964",
      "question": "who is Khalid Sheikh Mohammed ?",
      "prediction": "mastermind behind the September 11, 2001,",
      "ground_truths": [
        "the mastermind behind the September 11, 2001, terrorist attacks on the United States."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6f7b064fa9fe497abdc626bc2f315179",
      "question": "What happened to Khalid Sheikh Mohammed?",
      "prediction": "waterboarding",
      "ground_truths": [
        "was waterboarded 183 times in a month,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "87fae21d4ba64ce39473e53303b9a010",
      "question": "What did former CIA officer say?",
      "prediction": "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.",
      "ground_truths": [
        "Kiriakou said Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day."
      ],
      "em": 0,
      "f1": 0.9473684210526316
    },
    {
      "id": "10711b8a6a60450f97f1d8c41c04b6a1",
      "question": "Who denounced the decision?",
      "prediction": "Hayden",
      "ground_truths": [
        "Hayden"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "24b16656c4004ad897fb617b063b9d13",
      "question": "who did denounces decision to release memos?",
      "prediction": "Hayden",
      "ground_truths": [
        "Hayden"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2b5d2e3dba846f2b501f617fbda65eb",
      "question": "Where did the passenger purchase airline tickets from?",
      "prediction": "Expedia.",
      "ground_truths": [
        "Expedia."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0b6a31e91402412db37498cbbf45f68c",
      "question": "What did the passenger purchase?",
      "prediction": "two tickets",
      "ground_truths": [
        "two tickets to Italy"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "50df16ae2a574f7c9140cd8242b98a90",
      "question": "What travel company issued a corrected ticket?",
      "prediction": "Expedia.",
      "ground_truths": [
        "Expedia."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af70919c8e954162a4fed9325ffff0d4",
      "question": "What was misspelled when received the tickets?",
      "prediction": "his wife's name,",
      "ground_truths": [
        "his wife's name,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e089e57101b24a09865fd079c8a853be",
      "question": "Who faces 22 felony counts in connection with sex tape?",
      "prediction": "Chester Arthur Stiles",
      "ground_truths": [
        "Chester Arthur Stiles,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b267cf430ab441cc8fdb4fe5b62f89ae",
      "question": "Who discovered the tape?",
      "prediction": "A man",
      "ground_truths": [
        "Darrin Tuck,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d22bea148e76450886164c2c2b46c92c",
      "question": "What is the evidence that shows a girl under 3 being assaulted?",
      "prediction": "tape",
      "ground_truths": [
        "videotape"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8eb2b0d07894b3ca6980fd85b5c1815",
      "question": "How many felony counts does Stiles face?",
      "prediction": "22",
      "ground_truths": [
        "22"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2f13bf99fea7424189c6428348a832bb",
      "question": "What group of people know about the case and hae strong feelings about it?",
      "prediction": "Jurors",
      "ground_truths": [
        "a jury"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb6fb9e713e94b0c85efed7896bb364b",
      "question": "What did the tape show?",
      "prediction": "images of the small girl being sexually assaulted.",
      "ground_truths": [
        "images of the small girl being sexually assaulted."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "04b44e8378454db2b175693882d6f37e",
      "question": "How many felony counts is Arthur Stiles facing?",
      "prediction": "22",
      "ground_truths": [
        "22"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "efca00cb58c04369b44b7edd3d9ab167",
      "question": "Who was charged with slaying the woman?",
      "prediction": "Philip Markoff,",
      "ground_truths": [
        "Philip Markoff, a pre-med student at Boston University"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "2c4feb40f8d84a8abd45dbd71fe093e4",
      "question": "who makes public plea?",
      "prediction": "Davis and Conley",
      "ground_truths": [
        "Boston Police Department,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da43ca226cb640228c78e6bec528b1fe",
      "question": "who was charged",
      "prediction": "Philip Markoff,",
      "ground_truths": [
        "Philip Markoff,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cda5c1184ed6422c9a1e554586f6befd",
      "question": "what was he charged with",
      "prediction": "murder",
      "ground_truths": [
        "murder"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b1ae9084cebb44b48a69c1c52e6408aa",
      "question": "What else was the suspect charged with?",
      "prediction": "the armed robbery and kidnapping of another victim,",
      "ground_truths": [
        "the armed robbery and kidnapping of another victim,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "075c87d32f284998931c5db62c495931",
      "question": "what did the police say",
      "prediction": "Brisman, a model, offered massages via Craigslist,",
      "ground_truths": [
        "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "46e63673f28e492f89d73d10df691d85",
      "question": "whom did police charge?",
      "prediction": "Philip Markoff,",
      "ground_truths": [
        "Philip Markoff,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b82861fa9c7a4b6ab291c2ec24540604",
      "question": "The suspect is how old?",
      "prediction": "22-year-old",
      "ground_truths": [
        "22-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "05528848df284bf1bf0221430f701239",
      "question": "What does the columnist describe draper as?",
      "prediction": "complicated and deeply flawed man who",
      "ground_truths": [
        "complicated man"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ea4395e24790427eb9016fdc41d97557",
      "question": "What did columnist describe Draper as?",
      "prediction": "a complicated man beneath a confident exterior,",
      "ground_truths": [
        "sexy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f07632e29e364346a67e7e83b6ba57c5",
      "question": "what is frustrating?",
      "prediction": "the refusal or inability to \"turn it off\"",
      "ground_truths": [
        "the refusal or inability to \"turn it off\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "24fbb9ffcb9f4ac89a61f53531515f7b",
      "question": "What year was Bhutto's father hanged?",
      "prediction": "1979",
      "ground_truths": [
        "1979"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c204c9bbf71046878a9b4ea3c122cd9b",
      "question": "What happened to Bhutto in October?",
      "prediction": "She filed a nomination paper for a parliamentary seat on November 25",
      "ground_truths": [
        "narrowly escaped injury"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4df5296566b641b0adc46f5ee2458181",
      "question": "What happened to her father?",
      "prediction": "was hanged in 1979",
      "ground_truths": [
        "hanged in 1979"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "de80389f9b874879bd8503f3a8f9d0d9",
      "question": "Who attempted to assassinate Bhutto in October?",
      "prediction": "Ansar",
      "ground_truths": [
        "suicide bombing"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1f9377b97fc04b368c8a59a88819df6b",
      "question": "Who was the first female prime minister of a Muslim country?",
      "prediction": "Benazir",
      "ground_truths": [
        "Bhutto,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f0cb1630307f4842b035bb378e164d3e",
      "question": "Who was hanged in 1979?",
      "prediction": "Zulfikar Ali Bhutto,",
      "ground_truths": [
        "Zulfikar Ali Bhutto,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8415726510464e069a79294889f5578c",
      "question": "What did Bhutto refuse to allow?",
      "prediction": "assassins",
      "ground_truths": [
        "assassins"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3707d857e2c84a94ba2aa8f965ebe359",
      "question": "What is Georgia supposed to join?",
      "prediction": "NATO",
      "ground_truths": [
        "NATO"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4fe29932e9544f579ad4de4582172344",
      "question": "what do european countries do",
      "prediction": "have expressed concerns about the missile defense system.",
      "ground_truths": [
        "expressed concerns about the missile defense system."
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "ad4834d86052486e93a0267fa03a013d",
      "question": "what did bush say",
      "prediction": "\"NATO is an organization that's peaceful.",
      "ground_truths": [
        "A planned missile defense system in Eastern Europe poses no threat to Russia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f7bda2de72f480fad80c4e296528d29",
      "question": "What do European countries share?",
      "prediction": "Russian concerns that the defensive shield could be used for offensive aims.",
      "ground_truths": [
        "Russian concerns that the defensive shield could be used for offensive aims."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4bd6887183bd4b53aaf174debcb3b3f1",
      "question": "to where they carried the injured?",
      "prediction": "Nasser Medical Institute in Cairo,",
      "ground_truths": [
        "Nasser Medical Institute in Cairo,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a36a9969dee14e6f90fc1785e4d8151a",
      "question": "How many tourists are injured?",
      "prediction": "19 American",
      "ground_truths": [
        "19"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0f98e25c885246da8bf351307de869b3",
      "question": "What is \"A Lion Among Men,\" about?",
      "prediction": "the Cowardly",
      "ground_truths": [
        "-- the motherless cub defended by Elphaba in \"Wicked.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb216937fe904ea8bd27fa37bed51ea0",
      "question": "What did Maguire write for 14 years?",
      "prediction": "children's books",
      "ground_truths": [
        "children's books"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e2d378b24594539b665d54846855743",
      "question": "How many Maguire books of Wicked has he sold?",
      "prediction": "2.5 million",
      "ground_truths": [
        "2.5 million copies,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "c3d122f0ff5048a9ae7793e6253fab48",
      "question": "How long has Maguire been writing childrens's books?",
      "prediction": "14 years",
      "ground_truths": [
        "14 years"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c523aea52aa481e9ce5bb6248498fde",
      "question": "What is the name of Gregory's new book?",
      "prediction": "\"A Lion Among Men,\"",
      "ground_truths": [
        "\"A Lion Among Men,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2297d8f94e90417abf39aea108b1b108",
      "question": "How many years did Maguire write children's books?",
      "prediction": "14",
      "ground_truths": [
        "14"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd23f19adb1c4bfb8e746e18835fb514",
      "question": "How many copies has \"Wicked\" sold?",
      "prediction": "2.5 million",
      "ground_truths": [
        "2.5 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "63ac58f147fb47969c3b84de7e517fd8",
      "question": "How many times did the novel 'Wicked' sell?",
      "prediction": "2.5 million copies,",
      "ground_truths": [
        "\"Wicked,\" has sold more than 2.5 million copies,"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "561ae3af3c364b27a82dd5560b22555e",
      "question": "What is Gregory Maguire's new book about?",
      "prediction": "\"A Lion Among Men,\"",
      "ground_truths": [
        "tells the story of the Cowardly Lion"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "794ebaee3f574f5097807ca0e53abceb",
      "question": "Who is wanted for questioning in the death of a two-year-old girl?",
      "prediction": "Arthur E. Morgan III,",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "dcd806e325b74372a9b842d8cddb0281",
      "question": "where was the child's body found?",
      "prediction": "Shark River Park in Monmouth County",
      "ground_truths": [
        "in a stream in Shark River Park in Monmouth County"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e6a8249c106042fba956548da2b42160",
      "question": "Who is wanted for questioning in the death of a two year old girl?",
      "prediction": "Arthur E. Morgan III,",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "466e7f5760a1471b923c3d19b6587f40",
      "question": "What is he wanted for",
      "prediction": "questioning",
      "ground_truths": [
        "questioning in the death of a two-year-old girl,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "413b110ca14c475ea7cf2363ab405203",
      "question": "Where was the body discovered",
      "prediction": "stream in Shark River Park in Monmouth County",
      "ground_truths": [
        "in a stream in Shark River Park"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "16f229e67d1b4807bd7d75bf021fb6b2",
      "question": "The child was last seen with who?",
      "prediction": "her mother.",
      "ground_truths": [
        "Arthur"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d8bba20206e4231bde074aad69904b4",
      "question": "who is victor?",
      "prediction": "Victor Manuel Mejia Munera",
      "ground_truths": [
        "Manuel Mejia Munera was a drug lord with ties to paramilitary groups,"
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "9cba4af1e5634c5096943b274940a517",
      "question": "Who did the defense minister issue a warning to ?",
      "prediction": "drug traffickers",
      "ground_truths": [
        "drug traffickers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8ea1edfd43d41c3bf0bc5317a11b7d5",
      "question": "What was found with a dead drug lord ?",
      "prediction": "the brothers",
      "ground_truths": [
        "short- and long-range weapons"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4034aa4e80a64b048d230e50b51bbe69",
      "question": "Where was Manuel Munera on the wanted list ?",
      "prediction": "Colombian",
      "ground_truths": [
        "Colombia's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da732bb77e624a11a31f1146aeabdd2c",
      "question": "What did the government find?",
      "prediction": "Victor Manuel Mejia Munera and two bodyguards were killed Tuesday when police tracked them down on a farm in the northwestern province of Antioquia,",
      "ground_truths": [
        "identity documents"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "907592fbdbc74aef8328e9af8a826aeb",
      "question": "What title did Eikenberry hold whilst he served time in Kabul",
      "prediction": "a general,",
      "ground_truths": [
        "U.S. security coordinator"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4f21b85d55c34ce88a2df48a109f1a9c",
      "question": "When did Eikenberry advise the secretary of defense?",
      "prediction": "March.",
      "ground_truths": [
        "for strategy, plans and policy on the Army staff."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ba1bd3f1dc54887910a00880e903024",
      "question": "After how many years service did Eikenberry retire from the army",
      "prediction": "40",
      "ground_truths": [
        "nearly 40"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1abb7082fac043689955b298c4454bd9",
      "question": "What did Eikenberry do?",
      "prediction": "sent private cables to Obama last week, urging the president not to rush to send more troops",
      "ground_truths": [
        "sent private cables to Obama last week,"
      ],
      "em": 0,
      "f1": 0.6086956521739131
    },
    {
      "id": "ab977ab989cf498ea08ccbf421490729",
      "question": "what did he advise US secretey of defense",
      "prediction": "China, Taiwan, Hong Kong and Mongolia,",
      "ground_truths": [
        "China, Taiwan, Hong Kong and Mongolia,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "467580cb39bf4886805860713e5a2ccc",
      "question": "Name the four countries that he advised the US secretary of defense on",
      "prediction": "China, Taiwan, Hong Kong and Mongolia,",
      "ground_truths": [
        "China, Taiwan, Hong Kong and Mongolia,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a92b72e3800a4803afb6e038e0b3d9fb",
      "question": "who retired from the army",
      "prediction": "Eikenberry",
      "ground_truths": [
        "Eikenberry"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "47159fd3a28d47b38edc4ec20f19f4d6",
      "question": "What is Apples iphone 4s voice assistant feature called?",
      "prediction": "Siri.",
      "ground_truths": [
        "Siri."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a42a58af650a4ec3b6948870d36e95cd",
      "question": "What was siri based on?",
      "prediction": "many years of extensive technical research.",
      "ground_truths": [
        "artificial intelligence."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc2454ca01d44e4e9965d90d5b9f024d",
      "question": "What is siri based on",
      "prediction": "artificial intelligence.",
      "ground_truths": [
        "onstage demos."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e72ba2f7e7f54f4d921c0c7049f1c42f",
      "question": "When did apple acquire siri",
      "prediction": "April 2010.",
      "ground_truths": [
        "April 2010."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "33e4070dbdca45ebb435ba3b44c71771",
      "question": "What is the name of the voice-assistant?",
      "prediction": "Siri",
      "ground_truths": [
        "Siri"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c21991db995941fa91aa5f02a615503e",
      "question": "What station did the train leave from?",
      "prediction": "Liverpool Street",
      "ground_truths": [
        "Liverpool Street"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91ff6f650b2944f3b449902d2bbb724a",
      "question": "What number of suspects are in the videos?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7ae0c09062ab4d87baaf4edd736aba4e",
      "question": "Where does the train leave?",
      "prediction": "Liverpool Street Station",
      "ground_truths": [
        "Liverpool Street Station"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c95da94fd3d949e7ac8a4f73bd5e4452",
      "question": "What did the jurors see?",
      "prediction": "Videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to",
      "ground_truths": [
        "transit bombings"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "8f6bdcf0c3ed4a21a0e54b26d5186bb4",
      "question": "What did videos also show?",
      "prediction": "of the chaos and horrified reactions after the July 7, 2005, London transit bombings",
      "ground_truths": [
        "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East."
      ],
      "em": 0,
      "f1": 0.15384615384615383
    },
    {
      "id": "d802582c938f4c4589efd03f9b80490f",
      "question": "What does the video show?",
      "prediction": "surroundings at Liverpool Street Station",
      "ground_truths": [
        "transit bombings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be0883a19590447b8c005d99288c16c1",
      "question": "What will the jurors see?",
      "prediction": "Videos of the chaos and horrified reactions",
      "ground_truths": [
        "Videos of the chaos and horrified reactions after the July 7, 2005, London"
      ],
      "em": 0,
      "f1": 0.7058823529411764
    },
    {
      "id": "736a2ec3de8641228ec051fcc9db843f",
      "question": "What number of men were charged with conspiracy?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f05dc53dcc24ed1a59fdd5d950966d0",
      "question": "What does the painting show?",
      "prediction": "Picasso's muse and mistress, Marie-Therese Walter.",
      "ground_truths": [
        "Picasso's muse and mistress, Marie-Therese Walter."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4d50fbdd13184f64aa70b8e68ea872d8",
      "question": "What was the previous record?",
      "prediction": "$104,327,006",
      "ground_truths": [
        "$104,327,006"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c8c2af2e7b14a84acf550c9253961d8",
      "question": "How much did the painting sell for?",
      "prediction": "nearly $106.5 million",
      "ground_truths": [
        "$106,482,500"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57b5639ab5b441d9956c8bb4d17cf3e2",
      "question": "What painting sold for $106.482,500?",
      "prediction": "\"Nude, Green Leaves and Bust\"",
      "ground_truths": [
        "\"Nude, Green Leaves and Bust\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2d0189ba8fd449889044fecb68c28b2",
      "question": "Who was Picasso's muse?",
      "prediction": "Marie-Therese Walter.",
      "ground_truths": [
        "Marie-Therese Walter."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "637157837144436a8998cd271d8babda",
      "question": "Where is the facility?",
      "prediction": "south of Atlanta",
      "ground_truths": [
        "in Salt Lake City,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93ade19467b641139f41f9fe6e0eb251",
      "question": "What did FAA say about the situation?",
      "prediction": "pretty much resolved,\"",
      "ground_truths": [
        "\"The"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "daa4cf0cb9dd4a64892c8decb071a087",
      "question": "where is the problem",
      "prediction": "Atlanta's Hartsfield-Jackson International Airport",
      "ground_truths": [
        "a Federal Aviation Administration facility,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6974b36e9aa949de8791f1545e7bef62",
      "question": "where is all flight-plan information processed",
      "prediction": "through a facility in Salt Lake City, Utah,",
      "ground_truths": [
        "through a facility in Salt Lake City, Utah,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "db07c15f3b7a40c7b61fa05120bfede2",
      "question": "what does faa say",
      "prediction": "Flights at Atlanta's Hartsfield-Jackson International Airport were delayed Tuesday afternoon.",
      "ground_truths": [
        "\"The"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cdc323e4b9f94668b7099f2f958f2a29",
      "question": "Where is flight plan information processed?",
      "prediction": "through a facility in Salt Lake City, Utah,",
      "ground_truths": [
        "Salt Lake City, Utah,"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "cf3cb36658104db4b9fc76212bd43ef7",
      "question": "What is the problem the facility is having?",
      "prediction": "communications breakdown",
      "ground_truths": [
        "processing data,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f67479f04dd14ff9829ca588bfb8ebb0",
      "question": "What is there an unknown number of?",
      "prediction": "hundreds.",
      "ground_truths": [
        "flights affected"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6079453c108242a4a62fbc139b352026",
      "question": "What problem were FAA having?",
      "prediction": "communications breakdown",
      "ground_truths": [
        "communications breakdown"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b5f0520db0c94594949ebeb81146b062",
      "question": "What has deteriorated this year?",
      "prediction": "relations with Israel",
      "ground_truths": [
        "the peace with Israel"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b72bdadb5821459ead2f35971941243a",
      "question": "In what year was the president murder?",
      "prediction": "1973",
      "ground_truths": [
        "1981,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "575ab7bd4c394bc6b01b31d572258129",
      "question": "Who was assasinated?",
      "prediction": "President Mohamed Anwar al-Sadat",
      "ground_truths": [
        "President Mohamed Anwar al-Sadat"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "decfea1e7c1f4fdb89e62f3edf078428",
      "question": "What happened in October 1981?",
      "prediction": "remains etched in the minds of Egyptians",
      "ground_truths": [
        "assassination of"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "638d58cec45e4bea83844a2c8acd2a87",
      "question": "Who has Sadt's daughter implicated?",
      "prediction": "Egypt",
      "ground_truths": [
        "Mubarak,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97d117cc5c8e4480af9110fd44b1ef22",
      "question": "What amount many members of Zoe's Ark are under arrest?",
      "prediction": "103 children",
      "ground_truths": [
        "Six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d50f3c55c0c433c87aa649e7ef23e74",
      "question": "What president wants the journalists and flight crew released",
      "prediction": "Chadian",
      "ground_truths": [
        "Chadian"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ccae12e191874f8c8c929fa1b9b197f4",
      "question": "WHAT DOES THE CHADIAN PRESIDENT WANT?",
      "prediction": "the journalists and flight crew will be freed,",
      "ground_truths": [
        "journalists and the flight crew will be freed,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d4f4cf4d890b4190a37e0234f229bc62",
      "question": "Who was arrested in Chad?",
      "prediction": "Hundreds of women protest child trafficking and shout anti-French slogans",
      "ground_truths": [
        "Six members of Zoe's Ark"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "50adaceccbd944119768b4f3f954bda3",
      "question": "Who is under arrest in Chad?",
      "prediction": "Six members of Zoe's Ark",
      "ground_truths": [
        "But the four women and three men are"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c867c284d21e46119e6538baa179a82c",
      "question": "What is happening in Chad?",
      "prediction": "attempted to take to France from",
      "ground_truths": [
        "Hundreds of women protest"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b33fd5b1bd34682b58b350be208abf2",
      "question": "What is the Chadian president's reaction?",
      "prediction": "hopes the journalists and the flight crew will be freed,",
      "ground_truths": [
        "Idriss Deby hopes the journalists and the flight crew"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a53ebe1ca9c84930b1eeb03a7a143cb1",
      "question": "Where are the children from?",
      "prediction": "Chadian villages along Chad's border with Sudan.",
      "ground_truths": [
        "Chad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6174cd572d34786b0bdd775d4e52439",
      "question": "Who wants journalists, flight crew released?",
      "prediction": "Chadian President Idriss Deby",
      "ground_truths": [
        "Chadian President Idriss Deby"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8be8b34d71a948568beed0e595760a0b",
      "question": "What number were arrested in Chad",
      "prediction": "Six",
      "ground_truths": [
        "Six members"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "669f5a064f5640bfa06abf444e42d066",
      "question": "few were arrested in chad?",
      "prediction": "Six members of Zoe's Ark",
      "ground_truths": [
        "Six members of Zoe's Ark"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a4a58940ed9d46289d94011a45d3194d",
      "question": "What is the Chadian president asking for?",
      "prediction": "help journalists and the flight crew will be freed,",
      "ground_truths": [
        "journalists and the flight crew will be freed,"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "d56bdbbd91ce456c86ea9670fc3712ea",
      "question": "Where did they try to fly from",
      "prediction": "Chad",
      "ground_truths": [
        "Chad"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "72ebeac43a1048c393dc6283c0242f33",
      "question": "Who interviewed the children that tried to fly out of Chad?",
      "prediction": "the agencies",
      "ground_truths": [
        "The Red Cross, UNHCR and UNICEF"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "77b5870674e94f139fe4160ebc755ecc",
      "question": "What does the president want?",
      "prediction": "the journalists and the flight crew will be freed,",
      "ground_truths": [
        "hopes the journalists and the flight crew will be freed,"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "5e923fe0adac4d1bb02e2207f78e5d2f",
      "question": "Which charity organisations have been interviewing children?",
      "prediction": "Zoe's Ark",
      "ground_truths": [
        "Red Cross, UNHCR and UNICEF"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d593f719345401e85e2baa15492fa88",
      "question": "WHO ARE UNDER ARREST IN CHAD?",
      "prediction": "Six members of Zoe's Ark were arrested last week as they tried to put the children on a plane to France,",
      "ground_truths": [
        "Three French journalists, a seven-member Spanish flight crew and one Belgian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "604c65f6b6044a8bb7c85d5c6fe63ff9",
      "question": "where was the blast",
      "prediction": "Baghdad's Sadr City,",
      "ground_truths": [
        "a municipal building in Baghdad's Sadr City,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "90e20a8ca8b8461994b6731460d74160",
      "question": "Who was killed in the Mosul suicide bomb?",
      "prediction": "four Americans",
      "ground_truths": [
        "two soldiers and two civilians from the Defense and State departments"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1582018cb34e4a81a4e3bd6c09c30e80",
      "question": "How many americans died",
      "prediction": "Four",
      "ground_truths": [
        "Four"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2db4a1fc8aa479c81d9546911c79dfe",
      "question": "How many were killed in the bombing?",
      "prediction": "Six Iraqis and wounded 10 others,",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "45f8a09242f04f1a804f6fce37d3d020",
      "question": "Who was killed in Mosul?",
      "prediction": "two civilians",
      "ground_truths": [
        "two soldiers and two civilians from the Defense and State departments"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "d042832ba94a4e10a544d91ee12fa061",
      "question": "Who did the victim work for?",
      "prediction": "Defense and State departments",
      "ground_truths": [
        "U.S. Defense Department"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "fa95ad070965475088a5826805ab3ef9",
      "question": "Who was shot in California?",
      "prediction": "Samuel Herr,",
      "ground_truths": [
        "Samuel Herr,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f12397bd3304231a5a0145051286d5c",
      "question": "Where is Daniel Wozniak held without bail?",
      "prediction": "Costa Mesa",
      "ground_truths": [
        "California"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3a33265c80d4541aed7148b15ff4f15",
      "question": "Police says he shot Herr at California training base for what?",
      "prediction": "to cover up his crime,",
      "ground_truths": [
        "financial gain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd00fcba4eea469799b95defa152dba8",
      "question": "Who is charged with murder?",
      "prediction": "Daniel Wozniak,",
      "ground_truths": [
        "Daniel Wozniak,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "66e898eab8f3429cb1d5b893a6348670",
      "question": "Who is charged with two counts of murder?",
      "prediction": "Daniel Wozniak,",
      "ground_truths": [
        "Daniel Wozniak,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a5e2ed2c52bf4df8a8cad864e6057bf0",
      "question": "What is Daniel Wozniak charged with?",
      "prediction": "two counts of murder.",
      "ground_truths": [
        "two counts of murder."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "11d608c95915496db587a48b7d7e448a",
      "question": "Wozniak is being held where?",
      "prediction": "without bail",
      "ground_truths": [
        "Costa Mesa Police Department"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8e7e230cb20c4bbc9a8c6deb8edcd62f",
      "question": "Where did Daniel Wozniak shoot Samuel Herr?",
      "prediction": "Los Alamitos Joint Forces Training Base",
      "ground_truths": [
        "Los Alamitos Joint Forces Training Base"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f510e4b92495473db9b86dd2d5b6eb18",
      "question": "Who got engaged to Ryan Adams?",
      "prediction": "Mandy Moore",
      "ground_truths": [
        "Mandy Moore"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1d9dc72255774f95b5e274139d8ea9bd",
      "question": "What is Moore famous for?",
      "prediction": "\"License to Wed\"",
      "ground_truths": [
        "role as a bride in the 2007 movie \"License to Wed\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e7abf6763c7d425c8577ed893a0cd182",
      "question": "What is Moore better known for?",
      "prediction": "a recording artist",
      "ground_truths": [
        "success as a recording artist"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "35190f405eb4417497a797f137fb3f1b",
      "question": "Who is she engaged to?",
      "prediction": "Ryan Adams.",
      "ground_truths": [
        "Ryan Adams."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aee9e1f69d5048c2a687b4a5acc056be",
      "question": "where Site raised $17,000 before crashing on Tuesday due to high volume?",
      "prediction": "Los Angeles",
      "ground_truths": [
        "City of Los Angeles' Web"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "b8093ba1236f482a9191dd552f526ec2",
      "question": "How many police were on hand?",
      "prediction": "Three thousand",
      "ground_truths": [
        "Three thousand"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "59cee8c898e5429f952a3af057e0d383",
      "question": "Costs include what?",
      "prediction": "police on the streets, trash pickup, sanitation, traffic control and more",
      "ground_truths": [
        "putting extra police on the streets, trash pickup, sanitation, traffic control and more for the Tuesday event,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "67bee8ca8bf642238b43fd87d1a5405f",
      "question": "How many cops were at the event?",
      "prediction": "Three thousand police officers",
      "ground_truths": [
        "Three thousand"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "04948ff5f6ba4caea0a805f6c1596d56",
      "question": "What did costs include?",
      "prediction": "putting extra police on the streets, trash pickup, sanitation, traffic control",
      "ground_truths": [
        "putting extra police on the streets, trash pickup, sanitation, traffic control and more"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "a04a4431e605471480fb64662b38504d",
      "question": "Site raised how much money before crashing?",
      "prediction": "hundreds of donors contributed more than $17,000 through the Web site.",
      "ground_truths": [
        "$17,000"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "c5dc840ded554956be6daa4c6631deb3",
      "question": "what City set up Web page asking Jackson fans to donate money?",
      "prediction": "Los Angeles'",
      "ground_truths": [
        "Los Angeles'"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "959464bf0c7c44ba8f2b21448867e4ad",
      "question": "How much money was raised?",
      "prediction": "$17,000",
      "ground_truths": [
        "$17,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02a006ec43e94d8a8fec1ab448fa5e00",
      "question": "Which political party did Nepal belong to?",
      "prediction": "Communist",
      "ground_truths": [
        "Communist"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "652962af37ce4c5799789678fa661553",
      "question": "What was Nepal's old job?",
      "prediction": "General secretary of the Communist Party,",
      "ground_truths": [
        "former general secretary of the Communist Party,"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "d5500a7bfa4d4b1683dc90808707e306",
      "question": "When did Pushpa Kamal Dahal resign ?",
      "prediction": "May 4",
      "ground_truths": [
        "May 4"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4740a5c55c1f47bf93a59c65525f43e9",
      "question": "What is Nepal's age?",
      "prediction": "56,",
      "ground_truths": [
        "56,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1b2178b59534886b5fab6b21bb11662",
      "question": "Who was the only candidate?",
      "prediction": "Madhav Kumar Nepal",
      "ground_truths": [
        "Madhav Kumar Nepal"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed00bbd0da8841fa9bb15f15859a9876",
      "question": "What is the age of Madhav  Kumar Nepal?",
      "prediction": "56,",
      "ground_truths": [
        "56,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6e0186a28acb4427aedaee03db16e623",
      "question": "Who resigned as Prime Minister?",
      "prediction": "Pushpa Kamal Dahal,",
      "ground_truths": [
        "Pushpa Kamal Dahal, the Maoist chairman,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "a0b2f384132d475d9b375afdc5343fb2",
      "question": "When did the Maoist chairman resign?",
      "prediction": "May 4",
      "ground_truths": [
        "May 4"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f002078c2ea2435db1e405606b820e19",
      "question": "What age is Madhav Nepal?",
      "prediction": "56,",
      "ground_truths": [
        "56,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0d56e8c7e4b6449696113a82ec0da19e",
      "question": "What is the Palm Jumeirah?",
      "prediction": "A huge man-made island shaped like a date palm tree",
      "ground_truths": [
        "A huge man-made island"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "f97696deb9704d2f8ae5834f9fcdd88c",
      "question": "Where is Palm Jumeirah island?",
      "prediction": "off the coast of Dubai",
      "ground_truths": [
        "off the coast of Dubai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "07f2d15033cf4fb6916bed5d56dcb8f5",
      "question": "Who will be at the opening party?",
      "prediction": "including Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York",
      "ground_truths": [
        "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson"
      ],
      "em": 0,
      "f1": 0.782608695652174
    },
    {
      "id": "97034e6a207f4a238a49bf7babb932e9",
      "question": "When was the opening party?",
      "prediction": "Thursday",
      "ground_truths": [
        "Thursday night."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "dd5a9212340046e2a340103c45fd7f17",
      "question": "Where is Dubai?",
      "prediction": "United Arab Emirates",
      "ground_truths": [
        "Arab Emirates"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "9186ab50536b4241b5033a6f1bfa8c9b",
      "question": "How much has the addition of the man-made island increased the Dubai coastline?",
      "prediction": "100 percent",
      "ground_truths": [
        "100 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1646e87aca7b4f078ae816044f0f5955",
      "question": "What is the name of the man-made island?",
      "prediction": "Palm Jumeirah",
      "ground_truths": [
        "Palm Jumeirah"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ff0076b7fe74648806429e78621f265",
      "question": "Where is the island located?",
      "prediction": "off the coast of Dubai",
      "ground_truths": [
        "off the coast of Dubai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6455065759ff4462a651dca3f2bdc610",
      "question": "How many died in mall shooting?",
      "prediction": "eight.",
      "ground_truths": [
        "eight."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "43bd6876bebc480d80d49b28c9f6ba5d",
      "question": "What are malls expected to assess?",
      "prediction": "security measures in light of Wednesday's shooting,",
      "ground_truths": [
        "their emergency plans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81d4deec3837435fb36d6d66a0514010",
      "question": "How many people did the gunman kill?",
      "prediction": "eight.",
      "ground_truths": [
        "eight."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3b0d0110d85b4616b9554ee617750926",
      "question": "What day did the shooting occur?",
      "prediction": "Wednesday.",
      "ground_truths": [
        "Wednesday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "492061a0ce704e7790ebe5e33f796052",
      "question": "How many people did the gunman shoot?",
      "prediction": "eight.\"",
      "ground_truths": [
        "eight.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c723a032ea4e457b8fac1068352121c9",
      "question": "What do security experts say abut such incidents?",
      "prediction": "keep malls safe",
      "ground_truths": [
        "it's a matter of money."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "266894f86fe6470bac36f71baa2b9ea9",
      "question": "What is impossible to anticipate?",
      "prediction": "\"This is not something that anybody can reasonably anticipate,\"",
      "ground_truths": [
        "murderous rampage"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6634ef970c9342ab9fe5f7a6ef1d72f7",
      "question": "Where was the shooting?",
      "prediction": "Omaha, Nebraska, shopping mall",
      "ground_truths": [
        "Westroads Mall in Omaha, Nebraska,"
      ],
      "em": 0,
      "f1": 0.6666666666666665
    },
    {
      "id": "9f4b7421498641fe88cfb553ed3fa756",
      "question": "What are malls expected to do in wake of Wednesday shooting?",
      "prediction": "review their emergency plans and consider additional security measures",
      "ground_truths": [
        "review their emergency plans and consider additional security measures"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2ff5cf3e188e4e37ac835adc32165b87",
      "question": "What did a gunman and Omaha Nebraska mall do?",
      "prediction": "he immediately began firing, Warren said.",
      "ground_truths": [
        "murderous rampage"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d8a6ddd00ff46a4a68fb56507cd76ce",
      "question": "Who opened civil rights investigation?",
      "prediction": "FBI's Baltimore field office",
      "ground_truths": [
        "FBI's Baltimore field office"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8b7525de970c4f40865c6e5c3dc89500",
      "question": "Who is being held for the death of a police officer?",
      "prediction": "Ronnie White,",
      "ground_truths": [
        "Ronnie White,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b5ffedbec441477fa239e782fb628236",
      "question": "What type of investigation did the FBI open?",
      "prediction": "civil rights",
      "ground_truths": [
        "civil rights"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "287c319b65094d23809ba1563ec3e70e",
      "question": "Who has been held following the death of police officer?",
      "prediction": "Ronnie White,",
      "ground_truths": [
        "Ronnie White,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "16210271258f4a84b1dd5f0e13449d6d",
      "question": "What did Ronnie White die of?",
      "prediction": "strangulation and asphyxiation",
      "ground_truths": [
        "strangulation and asphyxiation"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9da90ceaca2f41d6a8aab4bf02a599df",
      "question": "Where was Ronnie White?",
      "prediction": "Prince George's County Correctional Center,",
      "ground_truths": [
        "Prince George's County Correctional Center,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "67f03659bf904f96b7c75c168e76bbc1",
      "question": "Where was White being held?",
      "prediction": "Prince George's County Correctional Center,",
      "ground_truths": [
        "Prince George's County Correctional Center,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "116ec128ff1b4e0e9affb80d35b16123",
      "question": "What is the name of the defendant?",
      "prediction": "Casey Anthony,",
      "ground_truths": [
        "Casey Anthony,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "817ffee256de400687ad09739aec7dbd",
      "question": "What doesn't help the mother's defense?",
      "prediction": "The remains were found last week in woods about a half-mile from Anthony's parents' house and identified through DNA testing.",
      "ground_truths": [
        "her alibi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8d626720877446678ac80b14af871812",
      "question": "What could hinder the prosecution?",
      "prediction": "lack of a cause of death and the absence of any soft tissue",
      "ground_truths": [
        "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains"
      ],
      "em": 0,
      "f1": 0.846153846153846
    },
    {
      "id": "44a83f25d8814eb99197b7348039eab0",
      "question": "When did the toddler vanish?",
      "prediction": "last summer.",
      "ground_truths": [
        "last summer."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f0cb5ab79e1347979a1260074cf92df4",
      "question": "What is the cause of death?",
      "prediction": "homicide.",
      "ground_truths": [
        "no evidence"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c723a72b2904344bc7a7ba2c7d6de33",
      "question": "Who vanished last summer?",
      "prediction": "Caylee Anthony's",
      "ground_truths": [
        "Caylee Anthony's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b46d98652ce4436ac3994b2f43b44d5",
      "question": "What is the mothers name?",
      "prediction": "Casey Anthony,",
      "ground_truths": [
        "Casey Anthony,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "239cee8d0f52431bb19648a0518dc8b7",
      "question": "Who started as a tie salesman?",
      "prediction": "Ralph Lauren",
      "ground_truths": [
        "Ralph Lauren"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2e5c5a41f9e4b0a9784f7c5cafde0c4",
      "question": "For how long has he designed high fashion?",
      "prediction": "decades",
      "ground_truths": [
        "four decades"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f11480c8747745c892004397ecff2180",
      "question": "What did Lauren start as?",
      "prediction": "a few people questioned if it was named after Marco Polo",
      "ground_truths": [
        "tie salesman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d9017d072a945659b3a7f10694ad4d8",
      "question": "What makes clothing glamorous?",
      "prediction": "effortless luxury",
      "ground_truths": [
        "Polo because \"it was the sport of kings."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bde9e44277a3414a9ebe14c35375d069",
      "question": "Lauren sold what?",
      "prediction": "designer logo exemplifies aspiration in the home of the free and the brave like the mallet-wielding guy on the pony,",
      "ground_truths": [
        "ties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fd24a6d670df4f6db5f4c1e6fec0f867",
      "question": "Who began as a tie salesman?",
      "prediction": "Ralph Lauren",
      "ground_truths": [
        "Ralph Lauren"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "132f427910cd43b983e06b7156df6cf4",
      "question": "Where was Ralph Lauren from?",
      "prediction": "Bronx.",
      "ground_truths": [
        "Bronx."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c6dff9d75f984f36a1ebd1fd5a3e31c1",
      "question": "What was his first design?",
      "prediction": "polo",
      "ground_truths": [
        "ties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b85480c5b66345d685f83715df02faf6",
      "question": "What was Ralph Lauren's first job?",
      "prediction": "salesman from the Bronx.",
      "ground_truths": [
        "former tie salesman"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "65323f0418c6473882bb7078b52e8568",
      "question": "what did he design for four decades?",
      "prediction": "Ralph Lauren,",
      "ground_truths": [
        "way American men and women dress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6869198945d042f0b076c7d26debcaef",
      "question": "What did Ralph Lauren originally do?",
      "prediction": "name his company Polo",
      "ground_truths": [
        "tie salesman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8409d0d4207f4d84a097738867f2b082",
      "question": "What did he design?",
      "prediction": "Ralph Lauren,",
      "ground_truths": [
        "ties,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "147beb71e56b4756af9790dcedf83b13",
      "question": "where did Lauren began as tie salesman?",
      "prediction": "Bronx.",
      "ground_truths": [
        "the Bronx."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0be6356817ca41d59b0a75655fca24b9",
      "question": "What city has closed the airport?",
      "prediction": "Oklahoma",
      "ground_truths": [
        "Oklahoma"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5aaeceef16f34275b07bef56b93edddc",
      "question": "when close oklahoma city?",
      "prediction": "2:30 p.m.",
      "ground_truths": [
        "2:30 p.m."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "20e0dcefdff242ebb51ce89c34182814",
      "question": "Where is the storm going?",
      "prediction": "east,",
      "ground_truths": [
        "Virginia, West Virginia, the Carolinas, Tennessee, Kentucky and Arkansas."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b47d1c59552a453ba57c2121f029242d",
      "question": "When will Oklahoma city close runways?",
      "prediction": "2:30 p.m.",
      "ground_truths": [
        "2:30 p.m."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e83d3398a8a4e26a98fab7f85c8cb59",
      "question": "what is in nashville and georgia mountains?",
      "prediction": "large accumulations of ice",
      "ground_truths": [
        "large accumulations of ice"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4a93c5ba8a9f4632b9fd8768d008ea73",
      "question": "What weather will other's get?",
      "prediction": "major winter storm with heavy snow and ice",
      "ground_truths": [
        "winter storm"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "fcc39f65d75847b79baf078313a37bdb",
      "question": "What was said at the university?",
      "prediction": "\"I never thought any of this",
      "ground_truths": [
        "\"I'm just getting started.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e44335869d29465f972c14a36ada5ad1",
      "question": "What did he say at the univerisity?",
      "prediction": "\"I never thought any of this was going to be easy,\"",
      "ground_truths": [
        "\"I never thought any of this was going to be easy,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a72a835834ab424e9ecc30a0c106d0a2",
      "question": "Where did the 4 hijackers board the Maersk Alabama?",
      "prediction": "off Somalia's coast.",
      "ground_truths": [
        "Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bbae459788f3428fb4d9941e806c07e2",
      "question": "What did the crew say?",
      "prediction": "the crew had captured one of four pirates who hijacked their vessel, but they released him in a failed hostage trade.",
      "ground_truths": [
        "had captured one of four pirates who hijacked their vessel, but they released him in a failed hostage trade."
      ],
      "em": 0,
      "f1": 0.972972972972973
    },
    {
      "id": "46bcda0c67f849ca91cbd6707073d260",
      "question": "Where was the Alabam located?",
      "prediction": "off Somalia's coast.",
      "ground_truths": [
        "off Somalia's coast."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc05642a8ec84226979d1a3aa29edea6",
      "question": "Who did Alabama elude?",
      "prediction": "pirates",
      "ground_truths": [
        "The ship had eluded the pirates for more than three hours off the coast of Somalia before being boarded,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "b03261d3d4be4b90886641c1832f0083",
      "question": "What did Maersk Alabama elude for more than three hours?",
      "prediction": "the pirates",
      "ground_truths": [
        "eluded the pirates"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "36117da6832d406fa33aa93e477dfcb5",
      "question": "Who did a crew member call with details?",
      "prediction": "CNN",
      "ground_truths": [
        "CNN"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "278313c771714829ac3610569a889fbc",
      "question": "Where did the hijackers board Maersk Alabama?",
      "prediction": "off Somalia's coast.",
      "ground_truths": [
        "off Somalia's coast."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b55c21723801443a97f7eecb8d39fdd0",
      "question": "Who boarded the Alabama?",
      "prediction": "four pirates",
      "ground_truths": [
        "four pirates"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eabc6d504d3546a3b1d46f161e8e0981",
      "question": "What happened in 1975?",
      "prediction": "He checked himself into a Los Angeles mental institution",
      "ground_truths": [
        "he checked himself into a Los Angeles mental institution"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c2e58ec6e792484f80ff5002eab4c4f5",
      "question": "Who produced the 3 classic albums?",
      "prediction": "Iggy Pop:",
      "ground_truths": [
        "David Bowie,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a6fd87b6e1c24d9595f8fc12a3c1be71",
      "question": "Who co-wrote and produced Iggy's The Idiot and Lust for Life albums?",
      "prediction": "Bowie",
      "ground_truths": [
        "David Bowie,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "03d55bea2f16415f8a57e00d81d30218",
      "question": "In what year did Iggy check himself into a mental institution?",
      "prediction": "1975",
      "ground_truths": [
        "1975"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8feaa5ac0b5746d6a145102e2da94175",
      "question": "When did the group reunite?",
      "prediction": "1975",
      "ground_truths": [
        "1973's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7ae978d0f8154c7694f562ef5cf6b4ef",
      "question": "What was the reason for FARC to target the specific group that they did?",
      "prediction": "because the Indians don't want to get involved in the armed struggle",
      "ground_truths": [
        "collaborating with the Colombian government,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f04922a1c8b04513b654d344038b5ef1",
      "question": "Who said that rebels tortured some Indians before killing them?",
      "prediction": "The Human Rights Watch organization",
      "ground_truths": [
        "The Human Rights Watch organization"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fda78fbbc7184f34b3c4dcbe111f593a",
      "question": "Who took responsibility for the slayings?",
      "prediction": "the Revolutionary Armed Forces of Colombia, better known as FARC,",
      "ground_truths": [
        "Marxist guerrillas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33fefd93a2694bb8a3566caa9acbc60c",
      "question": "What does FARC stand for?",
      "prediction": "Revolutionary Armed Forces of Colombia,",
      "ground_truths": [
        "Revolutionary Armed Forces of Colombia,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6810a483fad34323be263d443696b0e7",
      "question": "Number of Indians that FARC slayed?",
      "prediction": "27",
      "ground_truths": [
        "at least 27"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "15b6eb5fb08b4a7da535ea7047d26794",
      "question": "How many people did FARC kill?",
      "prediction": "at least 27",
      "ground_truths": [
        "at least 27"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "85c04b3d7724447880a73aad3a81da3b",
      "question": "Why were the victims executed?",
      "prediction": "for collaborating with the Colombian government,",
      "ground_truths": [
        "collaborating with the Colombian government,"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "b6614f09b982496697e987427eaaa1ae",
      "question": "Who was tortured?",
      "prediction": "some of the Awa",
      "ground_truths": [
        "some of the Awa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "52748255a17b442c8ee693520c697af1",
      "question": "What did FARC cite as the cause for executions?",
      "prediction": "the Indians were gathering information about the rebels to give to the Colombian military.",
      "ground_truths": [
        "because the Indians were gathering information about the rebels to give to the Colombian military."
      ],
      "em": 0,
      "f1": 0.9565217391304348
    },
    {
      "id": "6039c005852044ee8cd9760fe3d91e1b",
      "question": "What is the cost of the conveyor belts?",
      "prediction": "roughly $5.5 billion",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e0b6b807c35b498b9c57d7c05813c554",
      "question": "How much did the plant cost to build?",
      "prediction": "$5.5 billion",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "42923549ccb94a399374e526f6651aa7",
      "question": "Which company is reducing emissions at its steel plant in Dangjin, South Korea?",
      "prediction": "Hyundai Steel",
      "ground_truths": [
        "Hyundai"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "39cb589172e84effa1e2d22eb157da0f",
      "question": "Does the company recycle?",
      "prediction": "its byproducts emitted during the process of burning and melting raw materials.",
      "ground_truths": [
        "100% of its byproducts"
      ],
      "em": 0,
      "f1": 0.39999999999999997
    },
    {
      "id": "908f4a95e90647a78fd238a2d96f1d2e",
      "question": "Who says it recycles all of its byproducts and monitors its emissions 24 hours a day?",
      "prediction": "Hyundai Steel's",
      "ground_truths": [
        "Dangjin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ea575fda1ae4b3785ebcb9cc2ebc1ad",
      "question": "Where is Hyundai reducing their emissions at?",
      "prediction": "Dangjin plant,",
      "ground_truths": [
        "Dangjin plant,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "791f7cd96b1b452a90b44d16904a868c",
      "question": "How much did the conveyor belts cost to build?",
      "prediction": "$5.5 billion",
      "ground_truths": [
        "$5.5 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "efacc04e1812429f83fd4409ea5093de",
      "question": "The company recycles what 24 hours a day?",
      "prediction": "100% of its byproducts",
      "ground_truths": [
        "100% of its byproducts"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "47ec654cee204c2ca4aabd88438e35e2",
      "question": "Who has a steel plant in Dangjin?",
      "prediction": "Hyundai",
      "ground_truths": [
        "Hyundai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "36c255f124334df8baabb2e913aefc6f",
      "question": "What is a test of obamas policy?",
      "prediction": "of pirate attacks",
      "ground_truths": [
        "the piracy incident"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0412b5c73f744608255bd12ebf67d3b",
      "question": "Who cared about Somalia?",
      "prediction": "United States",
      "ground_truths": [
        "William Jelani Cobb"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38af62bfb6eb48a8b4937f1d7ca7ac09",
      "question": "What did Cobb say?",
      "prediction": "Somalia's piracy problem was fueled by environmental and political events.",
      "ground_truths": [
        "Somalia's piracy problem was fueled by environmental and political events."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "283449da34f24389b1967779d0ed4dfd",
      "question": "What is said to have caused piracy?",
      "prediction": "environmental and political events.",
      "ground_truths": [
        "environmental and political events."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02236d41505743b7995ad3ffa3fd1318",
      "question": "in what year its revolution?",
      "prediction": "1979",
      "ground_truths": [
        "1979"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c90c0b26990c48c3b78097fd570a807b",
      "question": "What may Iran find difficult this year?",
      "prediction": "to put a lid on the marking of Ashura",
      "ground_truths": [
        "put a lid on the marking of Ashura"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "cbef062d89b944c5acd16f892a79611c",
      "question": "What falls one week to the day after the death of Grand Ayatollah?",
      "prediction": "Ashura",
      "ground_truths": [
        "Ashura."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0811fa0aee1141f0a91351c897beb555",
      "question": "What drugs are marketed without approval?",
      "prediction": "unapproved pain-relief",
      "ground_truths": [
        "A form of liquid morphine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a85d330e9b948fda0c33536760ff0b9",
      "question": "What drug had its usage extended?",
      "prediction": "morphine sulfate oral solution",
      "ground_truths": [
        "morphine sulfate oral solution 20 mg/ml."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "96ec04ebdb2b47d58ed22d148137caf1",
      "question": "What is the top priority until alternatives are developed?",
      "prediction": "to ensure there",
      "ground_truths": [
        "ensure there is no shortage of the drug while patients wait for an approved product to take its place."
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "5ffa044924924886bb08d1d71a406cac",
      "question": "What would pulling the drug cause?",
      "prediction": "hardship for terminally ill patients and their caregivers,",
      "ground_truths": [
        "hardship for terminally ill patients and their caregivers,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3692805baa804755bab5c37bc3240fd4",
      "question": "what was the estimation",
      "prediction": "estimates there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.",
      "ground_truths": [
        "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country."
      ],
      "em": 0,
      "f1": 0.9696969696969697
    },
    {
      "id": "7dc45360651d4f84a2648e9b0993c895",
      "question": "what did hospital groups say",
      "prediction": "expressed concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers,",
      "ground_truths": [
        "expressed concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f4e04bf1629c41f38e15786299a33202",
      "question": "What do treasure hunters hope to find?",
      "prediction": "Nazis Gold and possibly the legendary Amber Room",
      "ground_truths": [
        "lost Nazi gold."
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "f13af4c8e3574745a176421c2ada029d",
      "question": "What is called off?",
      "prediction": "the hunt for Nazi Gold and possibly the legendary Amber Room",
      "ground_truths": [
        "the hunt for Nazi Gold"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "a463b547f86240248cbeca805df114fb",
      "question": "Where is the dig?",
      "prediction": "Deutschneudorf",
      "ground_truths": [
        "Deutschneudorf,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e1fdd740b9bb490ebdabb2590fbeb38d",
      "question": "What may resume in two weeks?",
      "prediction": "digging",
      "ground_truths": [
        "digging"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f9546b34ca734a26ba7075fd2f064404",
      "question": "Where is the location of the dig?",
      "prediction": "Deutschneudorf,",
      "ground_truths": [
        "village Deutschneudorf,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "89c493d17fad4483a54447872ff7fe6d",
      "question": "What do treasures hope to find?",
      "prediction": "Nazi Gold",
      "ground_truths": [
        "Nazi Gold"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7def903743ac43748483019d8b62d815",
      "question": "What was the coldest spot in the US?",
      "prediction": "Midwest",
      "ground_truths": [
        "Fargo, North Dakota,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57e62137793b4537bb7eb95d44481eb3",
      "question": "It was what temperature in Chicago?",
      "prediction": "10",
      "ground_truths": [
        "(3 degrees Fahrenheit),"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e436b3b625de462c800a23badd21bf28",
      "question": "Where is the icy cold front pushing into?",
      "prediction": "Southeast,",
      "ground_truths": [
        "the Southeast,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2711d5ffc59a43ecae9d0ce410db0d93",
      "question": "What was the feels-like temp in Mpls.?",
      "prediction": "40 below",
      "ground_truths": [
        "40 below"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6401157d496747a092bf97a5f430d281",
      "question": "What was the temperature in Chicago?",
      "prediction": "10",
      "ground_truths": [
        "10 below"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ff5f25fc4c9b4354b49fc603df373ee0",
      "question": "What was the actual temp in Chicago?",
      "prediction": "10 below",
      "ground_truths": [
        "Illlinois."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a7fcc1296b646bc86439722d2a7ac81",
      "question": "The cold front is pushing where?",
      "prediction": "Southeast,",
      "ground_truths": [
        "Southeast,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b4e39f9e15734b6a82739950c34aa8ee",
      "question": "How cold did it feel in Minneapolis?",
      "prediction": "40",
      "ground_truths": [
        "felt like 40 below"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "e08b24a8d41544e38f2489b4e32e2764",
      "question": "How long will this cold weather last, possibly?",
      "prediction": "Friday.",
      "ground_truths": [
        "through the weekend,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "00388805ff934f4a9f6c374dd50be557",
      "question": "What did it feel like because of the wind chill?",
      "prediction": "0 (3 degrees Fahrenheit),",
      "ground_truths": [
        "40 below"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f1fca269781c4454a9fbcf23844c79dd",
      "question": "It felt like 40 below in Minneapolis because of what?",
      "prediction": "the wind chill,",
      "ground_truths": [
        "of the wind chill,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "c2259f85230746369c272c79f271f791",
      "question": "Where is the sanctuary located?",
      "prediction": "rural Tennessee.",
      "ground_truths": [
        "rural Tennessee."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d3ae3c08aad44cbda0ba4bf1b82fca1e",
      "question": "Where is the Elephant Sanctuary?",
      "prediction": "in rural Tennessee.",
      "ground_truths": [
        "rural Tennessee."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "3040bd06744c435a980080d34055f300",
      "question": "is the site open to the public?",
      "prediction": "Visitors aren't allowed onto the property",
      "ground_truths": [
        "Visitors aren't allowed"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "d6c2f7abf2414aa78c2c2bf6213d7d29",
      "question": "What takes in old, injured, and abused animals?",
      "prediction": "The Elephant Sanctuary.",
      "ground_truths": [
        "The Elephant Sanctuary."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cae2fe78dae544d3a5bf2c2eb3919562",
      "question": "What size is the facility?",
      "prediction": "2,700-acre sanctuary",
      "ground_truths": [
        "2,700-acre"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a2fec7feaca844cc8b0c63711b785de5",
      "question": "When was the facility opened?",
      "prediction": "1995",
      "ground_truths": [
        "1995"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88cb2f147775410ab3e7470896b76c80",
      "question": "Where did the new face come from",
      "prediction": "donor cadaver.",
      "ground_truths": [
        "a donor cadaver."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "465e12aff2ba4eac863fa56232c3b64b",
      "question": "What is as important as the face transplant itself?",
      "prediction": "Social reincorporation",
      "ground_truths": [
        "Social reincorporation"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "982a9f6440124bf585f35e72281200d5",
      "question": "who recivied new face?",
      "prediction": "The patient,",
      "ground_truths": [
        "woman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e337e8458d6241c3a340dbd46d16208a",
      "question": "Who can now eat solid foods?",
      "prediction": "the patient",
      "ground_truths": [
        "The woman"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "588aca70e4b94ed6a92633e34ef2c5f5",
      "question": "What can face transplant recipient do now",
      "prediction": "breathe through her nose, smell, eat solid foods and drink out of a cup,",
      "ground_truths": [
        "breathe through her nose, smell, eat solid foods and drink out of a cup,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1102fa5e20584c7ab36afa172f64709c",
      "question": "Who received her new face in one graft from a donor cadaver?",
      "prediction": "The patient,",
      "ground_truths": [
        "The patient,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab72decc14e541329832a35eb36c9fa4",
      "question": "What did the police chief say?",
      "prediction": "Seven members of a banned militant group with strong ties to al Qaeda and the Taliban were arrested Sunday along with suicide vests and a large quantity of ammunition,",
      "ground_truths": [
        "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,"
      ],
      "em": 0,
      "f1": 0.058823529411764705
    },
    {
      "id": "8e083432032f4c0c9df404f7db7478dd",
      "question": "What did the arrest preventing happening in Karachi",
      "prediction": "massacre",
      "ground_truths": [
        "numerous suicide attacks,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2e9097aa9a8438c820acdc165cc2785",
      "question": "How many suspects are there?",
      "prediction": "13",
      "ground_truths": [
        "At least 13"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "14f15d96977a488fbc42bccc615d586a",
      "question": "Who was assassinated?",
      "prediction": "former Pakistani Prime Minister Nawaz Sharif",
      "ground_truths": [
        "Mehsud"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d9042f11cbc7409aaf5e7364dd1358fc",
      "question": "What sports team was attacked?",
      "prediction": "cricket",
      "ground_truths": [
        "Sri Lankan cricket"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "59f797b748f94ba48ef4d452bc78310b",
      "question": "Three Pakistan Tailban members where arrested wearing what",
      "prediction": "suicide jackets and explosives",
      "ground_truths": [
        "people carrying suicide jackets and explosives"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "83740e4a867446d486f86cba14a2e433",
      "question": "Where were attacks averted?",
      "prediction": "Karachi",
      "ground_truths": [
        "southern port city of Karachi,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "e6f16711704b482a82373c1fdb384783",
      "question": "Who attached the cricket team?",
      "prediction": "Taliban militants",
      "ground_truths": [
        "Lashkar-e-Jhangvi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5e5d9c40e1545c4b6ba1881756b297f",
      "question": "who testifies about exposure in Iraq?",
      "prediction": "Russell Powell,",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c101f1e6013417689d6efbc66d716f2",
      "question": "What was stationed at base when he was born?",
      "prediction": "his parents",
      "ground_truths": [
        "his parents"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "08e3cb915245438d8034ea7139799ab6",
      "question": "who was exposed to chemicals?",
      "prediction": "Camp Lejeune,",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5f99cc82a7b48289ba83b2ed51ffc4e",
      "question": "What type of cancer did the man experience after the exposure?",
      "prediction": "breast",
      "ground_truths": [
        "breast"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4074ae28c4a74172ae8d9d74a8877e8a",
      "question": "What type of exposure occured in Iraq?",
      "prediction": "chemicals",
      "ground_truths": [
        "by military personnel to hazardous materials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab324f250d0b4f4e8dfa7c59b237a270",
      "question": "Who was exposed to chemicals in tap water?",
      "prediction": "at Camp Lejeune,\"",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce887cd5e83841a18f14a0a15043df12",
      "question": "who was diagnosed with breast cancer?",
      "prediction": "Michael Partain,",
      "ground_truths": [
        "Michael Partain,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "afbbc141253142fab6d2b697ae4ee6fe",
      "question": "What weakened us security?",
      "prediction": "the use of torture and indefinite detention",
      "ground_truths": [
        "the use of torture and indefinite detention"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "766f91b668a7407690874162a7f1bf7c",
      "question": "What did John Kerry say?",
      "prediction": "Barack Obama sent a message that fight against terror will respect America's values.",
      "ground_truths": [
        "Barack Obama sent a message that fight against terror will respect America's values."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ca71c2aabe54b088ede58a422a56ec6",
      "question": "what was the law",
      "prediction": "American",
      "ground_truths": [
        "the rule of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa17174e9fd54409b297cb9ea0371333",
      "question": "What was president Obama's message?",
      "prediction": "that fight against terror will respect America's values.",
      "ground_truths": [
        "that fight against terror will respect America's values."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1ade6f519404356a8aa9a474c20374e",
      "question": "what was the decision",
      "prediction": "closed of Guantanamo Bay prison and CIA \"black site\" prisons,",
      "ground_truths": [
        "eventual closure of Guantanamo Bay prison"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a58732fa10294a8fb02303af8a2b4553",
      "question": "Who highlights public spirited people?",
      "prediction": "Kenneth Cole",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "15a23c40a7ff4d308287523820b86539",
      "question": "What is the name of the fashion designer?",
      "prediction": "Kenneth Cole",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "61fd162dc6174559989603efdfadb330",
      "question": "What use of humor leavens passion?",
      "prediction": "help",
      "ground_truths": [
        "And I think we also have these walls around us, and when people disagree, we're inclined not to listen, but to a degree you can break through that wall often"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2819a55322f74b0d9bbaaca2e4bf5671",
      "question": "What does Kenneth Cole highlight?",
      "prediction": "In war is it who's right, or who's left?\"",
      "ground_truths": [
        "social issues"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "993381382d96468e85b18ca37eada8f3",
      "question": "What fashion designer admires use of humor?",
      "prediction": "President-elect Barack Obama's",
      "ground_truths": [
        "Kenneth Cole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b76d9a4d357146cf8761f216db62cf32",
      "question": "What level is the Mongolian?",
      "prediction": "sumo's most successful ever grand champion,",
      "ground_truths": [
        "most successful ever grand champion,"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "a8606273fab14a929fde3fa46e58300c",
      "question": "Why was he banned?",
      "prediction": "for pulling on the top-knot of an opponent,",
      "ground_truths": [
        "using recreational drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "be35f44aabff441fa8c79147a8c67371",
      "question": "Who feigned illness?",
      "prediction": "Asashoryu",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5c79509d21e1437a949f0a5b481b26ad",
      "question": "What is the wrestler accused of?",
      "prediction": "bribing other wrestlers to lose bouts,",
      "ground_truths": [
        "to lose bouts,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "24eee9ea8c97417fa7655ea0363e789c",
      "question": "What is he accused of?",
      "prediction": "bribing other wrestlers to lose bouts,",
      "ground_truths": [
        "bribing other wrestlers to lose bouts,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41ee31948eb0428294b160f5ee76e85c",
      "question": "He is a grand champion what?",
      "prediction": "sumo's most successful ever",
      "ground_truths": [
        "sumo wrestling"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6152a8e8d0a480681f85f240c9bf762",
      "question": "Who reinvigorated the sport?",
      "prediction": "Asashoryu's",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3c63a50e5cf94374a3f70b850a9c90e1",
      "question": "Who is one of the sumo's most successful ever?",
      "prediction": "Asashoryu",
      "ground_truths": [
        "Asashoryu"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8467d64414a246cdbee345b883085de2",
      "question": "What is the name of the Mongolian sumo champ?",
      "prediction": "Dolgorsuren Dagvadorj,",
      "ground_truths": [
        "Dolgorsuren Dagvadorj,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8023d1f9e86e4b7c8a862887e3d021d4",
      "question": "How old is Robert Mugabe?",
      "prediction": "84-year-old",
      "ground_truths": [
        "84-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e1681c57e4347889007a5d3bb1bcf87",
      "question": "Millions of extra what were printed?",
      "prediction": "ballots",
      "ground_truths": [
        "ballots"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b532cee5e3c849fe998c2889e1bfab8f",
      "question": "How long has Mugabe ruled the country?",
      "prediction": "28 years",
      "ground_truths": [
        "28 years"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "00a9927209044872a29d9e1379e3e241",
      "question": "Who is the president?",
      "prediction": "Robert Mugabe",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9123f89740f0448f9249410770e1780c",
      "question": "What will Mugabe face on Saturday?",
      "prediction": "election",
      "ground_truths": [
        "elections"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbb08647ed16423eb66dc10023c88798",
      "question": "What official is suspicious of rigged elections?",
      "prediction": "Zimbabwe's main opposition party",
      "ground_truths": [
        "President Robert Mugabe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d6337c3b3858489ca8604ee85b5c11dc",
      "question": "How old is Mugabe?",
      "prediction": "84-year-old",
      "ground_truths": [
        "84-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cac535b4f8a844b48303a5a809a5c76e",
      "question": "Who suspects elections will be rigged?",
      "prediction": "President Robert Mugabe",
      "ground_truths": [
        "Human Rights Watch"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16be475fe48941c3b9848826b416940b",
      "question": "Where did the Movement for Democratic Change secretary suspect will happen in the elections?",
      "prediction": "Zimbabwe's",
      "ground_truths": [
        "President Robert Mugabe intends to rig"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca30d55d5a03439a985864d0da100ef6",
      "question": "How long has Robert Mugabe ruled Zimbabwe?",
      "prediction": "28 years of rule.",
      "ground_truths": [
        "28 years"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "506ee84329dd45168c9ad80ccb2c4890",
      "question": "What year is the World Championships?",
      "prediction": "2009",
      "ground_truths": [
        "2009"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1c0731c5f9444bef8b93d3b5e81579ea",
      "question": "Where were the 2009 World Championships held?",
      "prediction": "Scots village of Strachur",
      "ground_truths": [
        "Scotland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0476dedf5ec040b1b0ff875bb6891391",
      "question": "Who started Swamp Soccer?",
      "prediction": "Jyrki Vaananen",
      "ground_truths": [
        "cross-country skiers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b07600ccc1b34b68af3d83f3e558516e",
      "question": "What was started by cross-country skiers?",
      "prediction": "the game",
      "ground_truths": [
        "\"Swamp Soccer\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c67aaa46c3bf4d2c81e972b4b41905a9",
      "question": "Who makes wheelchairs?",
      "prediction": "Prisoners at the South Dakota State Penitentiary",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2f106c4d596a41a393aeb1a49f3d396b",
      "question": "Where are wheelchairs made that are delivered by the U.S. military?",
      "prediction": "South Dakota State Penitentiary",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "65669ddf87c8475386a72370395b8a22",
      "question": "What did the Dad of three disabled children say?",
      "prediction": "\"I am sick of life --",
      "ground_truths": [
        "can I"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "569caf1ecad34c36b3aa62a28faffb74",
      "question": "What was the vision of Brad Blauser?",
      "prediction": "to make life a little easier for these families",
      "ground_truths": [
        "organizing the distribution of wheelchairs,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "adf40d194a474f1881d407c00bdb62db",
      "question": "What was the vision of American contractor Brad Blauser?",
      "prediction": "try to make life a little easier for these families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchairs for Iraqi Kids.",
      "ground_truths": [
        "make life a little easier"
      ],
      "em": 0,
      "f1": 0.2758620689655173
    },
    {
      "id": "1e851401e19a42e1a155231b92e63312",
      "question": "Who make the wheelchairs in South Dakota?",
      "prediction": "prisoners",
      "ground_truths": [
        "prisoners"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef6f214df0ae42cc83130d61fb329036",
      "question": "Who makes the wheelchairs distributed by the U.S. military?",
      "prediction": "Brad Blauser,",
      "ground_truths": [
        "prisoners at the South Dakota State Penitentiary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e17e9f25a202485c8d14b354a92e101d",
      "question": "Who beat Federer in the match?",
      "prediction": "Del Potro.",
      "ground_truths": [
        "Juan Martin Del Potro."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2a1d9283cc9e4b2c8254c54dc6ca538d",
      "question": "In what city did the tournament take place?",
      "prediction": "Genoa.",
      "ground_truths": [
        "Genoa."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fcaa65142b5e4b9f9131b1ff61bccbcc",
      "question": "How much was Federer fined?",
      "prediction": "$1,500",
      "ground_truths": [
        "$1,500"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ddf8070fbf124b54bfdca93791ef2056",
      "question": "Who did Federer confront?",
      "prediction": "a fan",
      "ground_truths": [
        "umpire Jake Garner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32e5e53144904525bdd91c2346ecdc21",
      "question": "what did Del Potro won?",
      "prediction": "his first grand slam,",
      "ground_truths": [
        "his first grand slam,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "75d472336f5146239acd2998b0eb0bd3",
      "question": "How many sets was the final match?",
      "prediction": "five-set",
      "ground_truths": [
        "five-set"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62747a42a9074e7da75c495cc4daca38",
      "question": "how much money did Roger Federed fined for his argument with umpire?",
      "prediction": "$1,500",
      "ground_truths": [
        "$1,500 fine"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "574519a5dbfc4727bd68c1f4f0ad92b0",
      "question": "who confronted Federer?",
      "prediction": "Louise Engzell.",
      "ground_truths": [
        "Jake Garner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30e3992d09454ab9ab96ec7a3ded0a29",
      "question": "Where was the Vancouver Olympic Torch lit?",
      "prediction": "ancient Greek site of Olympia",
      "ground_truths": [
        "of Olympia"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8ef8fd98f839468294d2d3ee05b6ac8e",
      "question": "Where will it be lit",
      "prediction": "on the green hillside at Olympia,",
      "ground_truths": [
        "of Olympia"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "16ee96b324b84dbb817b18b4861a48fa",
      "question": "When is the Olympic Flame due to arrive in Vancouver?",
      "prediction": "February 12",
      "ground_truths": [
        "February 12"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "118155e6c91348bb855638c1150b4cfb",
      "question": "What makes history",
      "prediction": "The torch for the 2010 Vancouver Olympics was lit in a ceremony at the ancient Greek site of Olympia",
      "ground_truths": [
        "the longest domestic torch relay in the games'"
      ],
      "em": 0,
      "f1": 0.19047619047619044
    },
    {
      "id": "ecb6fdce8c384ec4ab6816743369a33f",
      "question": "who was exposed to cancer-causing chemical at Iraq plant?",
      "prediction": "16 Indiana National Guardsmen",
      "ground_truths": [
        "16 Indiana National Guard soldiers"
      ],
      "em": 0,
      "f1": 0.6666666666666665
    },
    {
      "id": "0f6cc70366444535856b1723cb8a412f",
      "question": "What did the suit allege about KBR",
      "prediction": "knowingly exposed the soldiers to a cancer-causing toxic chemical.",
      "ground_truths": [
        "company knowingly exposed the soldiers to a cancer-causing toxic chemical."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "7e1c7da2d759475a884c617a527cb3a9",
      "question": "what company says it's not to blame for creating unsafe conditions at plant?",
      "prediction": "KBR",
      "ground_truths": [
        "Kellogg Brown and Root,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d90f411d7d14ea6997a41d9a95f785a",
      "question": "who knew of contamination threat and did nothing to protect soldiers?",
      "prediction": "KBR",
      "ground_truths": [
        "Houston-based company Kellogg Brown"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d0d71a5c8444f8f9af90158cc607d45",
      "question": "The suit says Guardsman wsa exposed to what",
      "prediction": "sodium dichromate,",
      "ground_truths": [
        "cancer-causing toxic chemical."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dde0fd30f93b4eb8977287881411a2a2",
      "question": "Who did Condoleeza Rice say was used by non-state actors?",
      "prediction": "Pakistani territory",
      "ground_truths": [
        "Pakistani territory"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4063c87f2db432da6bea32bdbd9b4ca",
      "question": "What is Pakistan obligated to do according to Rice?",
      "prediction": "root out terrorists within its borders.",
      "ground_truths": [
        "root out terrorists within its borders."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "727e9e359e494729b6230a54f0a0e54d",
      "question": "What did Rice say about Pakistan's obligations?",
      "prediction": "to root out terrorists within its borders.",
      "ground_truths": [
        "is obligated to root out terrorists within its borders."
      ],
      "em": 0,
      "f1": 0.8750000000000001
    },
    {
      "id": "4c2b980aefd442bfaf443a14dfc548cf",
      "question": "Who did India blame for the Mumbai attacks?",
      "prediction": "Lashkar-e-Tayyiba (LeT),",
      "ground_truths": [
        "Lashkar-e-Tayyiba (LeT),"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2fa48dfd60ff499391b05561646ddf11",
      "question": "How many people died in the Mumbai attacks?",
      "prediction": "more than 170",
      "ground_truths": [
        "170"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e59c51ab66a74987991669ef16dec043",
      "question": "What position does Condoleezza Rice hold?",
      "prediction": "Secretary of State",
      "ground_truths": [
        "Secretary of State"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b25284d251ef491d9c0b6273f654d1c8",
      "question": "What did India allege?",
      "prediction": "they believe all the attackers were Pakistanis,",
      "ground_truths": [
        "terrorists operating within its borders."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a2d80efa9b7b479180a74423dfc26450",
      "question": "What does he say about humans?",
      "prediction": "will sense danger yet still walk right into it.",
      "ground_truths": [
        "will sense danger yet still walk right into it."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fbe0a747cc2143b493b0cb079d9364dd",
      "question": "When did the victims recall feeling uneasy?",
      "prediction": "weeks,",
      "ground_truths": [
        "noticed a UPS delivery box where it shouldn't be."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0924af12440f4773bc02e167ab91616f",
      "question": "What is the only animal who senses danger and walks into it?",
      "prediction": "humans",
      "ground_truths": [
        "humans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a42c52547f04e958698f5040284ad01",
      "question": "Who are the only animals who can sense danger?",
      "prediction": "humans",
      "ground_truths": [
        "humans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a316b27e5c304b808e861d89d8d72238",
      "question": "Who says fear can be a gift?",
      "prediction": "Gavin",
      "ground_truths": [
        "Gavin de Becker"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e58fb5569ab14dc18252ec9ebe19a0e4",
      "question": "Who felt uneasy?",
      "prediction": "Nicole",
      "ground_truths": [
        "Nicole"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bfbcdab7351542ffb580b819b8be0db7",
      "question": "What should you tell his mom?",
      "prediction": "mention how",
      "ground_truths": [
        "mention how"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a8288e34202a43ba912afef8cf20ca42",
      "question": "Post what to facebook to get back at your ex?",
      "prediction": "Crap E-mail From A Dude",
      "ground_truths": [
        "photo album full of pics of you looking smiley."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "edf07fccef7d409596c9a49cf47e207e",
      "question": "Where should you post \"pix\"?",
      "prediction": "Facebook photo album",
      "ground_truths": [
        "Facebook"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "42b41c9d7daa4fb89b93e94932183fa5",
      "question": "Where should you post pix?",
      "prediction": "Facebook photo album",
      "ground_truths": [
        "Facebook"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4509a472ffd6415aad18c8dc2da90e69",
      "question": "When should you tell he cheated on you?",
      "prediction": "It's OK to admit",
      "ground_truths": [
        "your ex's loved ones ask why"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72d5f68909b64c34894c5f1bd6766b19",
      "question": "What impeded rescue efforts?",
      "prediction": "flooding and debris",
      "ground_truths": [
        "flooding and debris"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cbec4bf4aa824ceaa082b7cf6640011e",
      "question": "What was impeding rescue efforts?",
      "prediction": "heavy flooding and scattered debris.",
      "ground_truths": [
        "heavy flooding and scattered debris."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "51a87a55365340bc864d259f036bd38d",
      "question": "What's the name of the tropical storm?",
      "prediction": "Ike",
      "ground_truths": [
        "Hurricane Ike,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0a6a960ca31344e69c202561b9687db7",
      "question": "Who rode out the storm at home?",
      "prediction": "Many people, like D.J. Knight",
      "ground_truths": [
        "D.J. Knight of Pearlman, Texas, decided to ride"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "17b6b8ced336420da3ab95bd9da12e60",
      "question": "How many were affected by the power outages?",
      "prediction": "20,000",
      "ground_truths": [
        "20,000 people"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d8ae61251f0d48168e7974d197c2353d",
      "question": "How many deaths were attributed to Ike?",
      "prediction": "no casualties",
      "ground_truths": [
        "four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f5fb5d4b20344b0971b7a250d8e452c",
      "question": "Where was the wereckage?",
      "prediction": "Galveston",
      "ground_truths": [
        "Galveston, Texas,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1d4fdb8572534e2e9f138fb02278083e",
      "question": "What storm caused four deaths in Texas?",
      "prediction": "Ike",
      "ground_truths": [
        "Ike"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc9a1c8de6ba491aa2161ec2236f2830",
      "question": "when was the attack?",
      "prediction": "November 26,",
      "ground_truths": [
        "November 26,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b9570d5dc5754c3da865896d3c701553",
      "question": "What did Kasak admit to?",
      "prediction": "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,",
      "ground_truths": [
        "he was one of 10 gunmen who attacked several targets in Mumbai"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "28c201d7c1a049e3a2f1b1d2058acf03",
      "question": "what did officials say",
      "prediction": "government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.\"",
      "ground_truths": [
        "Kasab had admitted he was one of 10 gunmen who attacked several targets in Mumbai on November 26,"
      ],
      "em": 0,
      "f1": 0.15789473684210525
    },
    {
      "id": "11c7856a8f4d4bb89c20bb2d212c120e",
      "question": "Who does the surviving attacker seek help from",
      "prediction": "Pakistani officials,",
      "ground_truths": [
        "Pakistani officials,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d59b5a2e398a4687b6ea3404b7ffa789",
      "question": "who wrote a letter",
      "prediction": "Mohammed Ajmal Amir Kasab,",
      "ground_truths": [
        "The suspect, Mohammed Ajmal Amir Kasab,"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "4141a787f20a49ff845742502d21baba",
      "question": "what did kasab admit",
      "prediction": "he and the others were trained for more than a year in Pakistan by Lashkar-e-Tayyiba,",
      "ground_truths": [
        "he was one of 10 gunmen who attacked several targets in Mumbai"
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "01110d58a77142f9a89705de4d78dbac",
      "question": "Who did  Kasak write a letter to?",
      "prediction": "Pakistan's High Commission in India",
      "ground_truths": [
        "Pakistan's High Commission in India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af6bf271717f4e8a8b0fec2b8a4120ef",
      "question": "who joins the attorneys",
      "prediction": "AbdulMutallab",
      "ground_truths": [
        "AbdulMutallab"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7739bb07f2e641089a5f30a0c540234f",
      "question": "What witness said a passenger yelled something",
      "prediction": "AbdulMutallab",
      "ground_truths": [
        "Michael Zantow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37b9ed4b33a7417593525ba9f53b5742",
      "question": "what does the prosecutor say of Abdulmutallab?",
      "prediction": "will represent himself.",
      "ground_truths": [
        "\"His sole reason for being on Flight 253 was to kill all of the passengers and himself. He thought he'd end up in heaven.\""
      ],
      "em": 0,
      "f1": 0.07692307692307691
    },
    {
      "id": "afdfa4bf455643d9b82c0305fef02b9f",
      "question": "what did the witness says?",
      "prediction": "\"His sole reason for being on Flight 253 was to kill all of the passengers and himself. He thought he'd end up in heaven.\"",
      "ground_truths": [
        "that AbdulMutallab was in the bathroom for about 15 to 20 minutes,"
      ],
      "em": 0,
      "f1": 0.2352941176470588
    },
    {
      "id": "79918fdc580d45b9a26512ae87fe4fca",
      "question": "What person thought they would end up in heaven",
      "prediction": "AbdulMutallab",
      "ground_truths": [
        "AbdulMutallab,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "26d739c555714488a425a4675291dcb9",
      "question": "What adult remained calm throughout",
      "prediction": "AbdulMutallab",
      "ground_truths": [
        "AbdulMutallab"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3997d6350e36484e8157c6fcee5d9523",
      "question": "What did Magee apologize for?",
      "prediction": "any abuse that occurred in his diocese.",
      "ground_truths": [
        "Catholic church sex abuse scandal,"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "3df1cf455a204c7ea4d7e9d3510f2073",
      "question": "Does Vatican accept Bishop John Magee's resignation ?",
      "prediction": "\"truly sorry\" for the abuse.",
      "ground_truths": [
        "it has been accepted,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6dd8ec6ba0d94b7b9b84f549ab000ca9",
      "question": "Who's resignation does the Vatican accept?",
      "prediction": "Bishop John Magee",
      "ground_truths": [
        "Bishop John Magee"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a62b164f214c4f1a829a822730275b14",
      "question": "How many irish bishops resigned since December ?",
      "prediction": "Two",
      "ground_truths": [
        "Four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18f073004adb4b668d69638bb2c2ede6",
      "question": "What will happen Sunday morning?",
      "prediction": "Another high tide",
      "ground_truths": [
        "Another high tide"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5dc60b23b934c858913fdb568523b0d",
      "question": "what does Thailand is dealing with ?",
      "prediction": "worst flooding since 1942.",
      "ground_truths": [
        "severe flooding"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "afbc7ecd775e4a99b99f7102082cbb8e",
      "question": "Where is the flood?",
      "prediction": "Bangkok",
      "ground_truths": [
        "Bangkok"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "70f8bc2a7e5749c4a8a759c2ee7c2cee",
      "question": "What country is suffering floods?",
      "prediction": "Thailand",
      "ground_truths": [
        "Thailand"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "558a58525a3c4d32a0a0ec255883368c",
      "question": "what is the forecast for Sunday morning?",
      "prediction": "another high tide",
      "ground_truths": [
        "Another high tide"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "56d8cd1f1cc14c38b975f03fee9d985c",
      "question": "What is the high tide forecast?",
      "prediction": "4 meters (13 feet)",
      "ground_truths": [
        "expected to reach about 4 meters (13 feet)"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d1dfcaa42fb54127ac19e50f2c2adc21",
      "question": "Where is the largest center for breeding and exporting terrorism?",
      "prediction": "Pakistan's",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "10a96cc02bef41ff853195cc0c11ac24",
      "question": "Who makes allegations without proof?",
      "prediction": "Afghan lawmakers",
      "ground_truths": [
        "Afghan lawmakers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7a8456d57d4f496e8d4ee674693d34a4",
      "question": "What country is \"largest center for breeding and exporting terrorism\"?",
      "prediction": "Pakistan's",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1a0471461fcf40ee98f9639a39a73de6",
      "question": "What kind of militants are attacking?",
      "prediction": "Islamic",
      "ground_truths": [
        "Islamic"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dd12c78d2b504fae811d89ba33fa9a61",
      "question": "Is Pakistan the leading terrorist country?",
      "prediction": "the largest center for breeding and exporting terrorism.\"",
      "ground_truths": [
        "\"the largest center for breeding and exporting terrorism.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "753c70ba368244869149d377ee86e4af",
      "question": "What has killed scores of people in Afghanistan?",
      "prediction": "a series of deadly attacks",
      "ground_truths": [
        "suicide bomber"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21f10f73f5d94cce841b6f93f68bb9bf",
      "question": "Who accused of orchestrating attacks?",
      "prediction": "Pakistan's intelligence agency",
      "ground_truths": [
        "Pakistan's intelligence service"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "db16711529134ff3ba582b3aa4971373",
      "question": "Where is the killing the worse?",
      "prediction": "Afghanistan,",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc0659b5894a487fa98b941caa1486b1",
      "question": "Who suffered first defeat of the tournament in New Zealand?",
      "prediction": "France",
      "ground_truths": [
        "Ireland"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "950471917d1b470d8bdb484f1a4ec3ab",
      "question": "Where did the tournament take place?",
      "prediction": "Auckland,",
      "ground_truths": [
        "New Zealand"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a40f6724f3804b098d5497661da35619",
      "question": "When did Les Bleus last lose to England?",
      "prediction": "2007",
      "ground_truths": [
        "2007"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0d5463f68f9491a9266e492a057d428",
      "question": "Who won the game 19-12 in the World Cup quarterfinals?",
      "prediction": "England",
      "ground_truths": [
        "Les Bleus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "815441b0e2b74f1b89e8d877d64e842a",
      "question": "What was the score in semifinal game of Wales-Ireland?",
      "prediction": "22-10.",
      "ground_truths": [
        "22-10."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7c2744f9a22d4fcb8921360fc515824d",
      "question": "What country defeated England in quarterfinals?",
      "prediction": "New Zealand",
      "ground_truths": [
        "France's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fda9c59e8374ccda356e7822945c769",
      "question": "when was she found",
      "prediction": "nine months later,",
      "ground_truths": [
        "nine months later,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b65b25205b634394ac78d0413098bd39",
      "question": "Who was charged with kidnapping Elizabeth Smart from her bedroom?",
      "prediction": "Brian David Mitchell,",
      "ground_truths": [
        "Brian David Mitchell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "00d92a1afce34a8196d1c7e27038014b",
      "question": "what age was smart when she was kidnapped",
      "prediction": "14,",
      "ground_truths": [
        "14,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b2f02c184e7f4d4f91bc1f7b87260e56",
      "question": "Who agreed to cooperate against her husband?",
      "prediction": "Barzee,",
      "ground_truths": [
        "Barzee,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60aaf4fca83f4fd9ac3dd152fff0de6d",
      "question": "What age was Smart?",
      "prediction": "14,",
      "ground_truths": [
        "14,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7586c657762a4899b2a03fa3a483f26c",
      "question": "who was charged",
      "prediction": "Brian David Mitchell,",
      "ground_truths": [
        "Brian David Mitchell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1272079830e045288cf3d0752a6fa3d7",
      "question": "when was T.I arrested?",
      "prediction": "weekend",
      "ground_truths": [
        "Saturday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "391a7fdd51944e06afa3c7fe338dea81",
      "question": "Where was T.I. arrested?",
      "prediction": "midtown Atlanta.",
      "ground_truths": [
        "midtown Atlanta."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed0bb059347f4fb38dcec73cb6fc8553",
      "question": "Who won awards?",
      "prediction": "T.I.",
      "ground_truths": [
        "Harris"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e983455b5be4ee788c28ee5b889972c",
      "question": "When will the bond hearing take place?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3af5fd79dfe743be9e29cec27572fe13",
      "question": "What was the rapper T.I. arrested for on Saturday?",
      "prediction": "gun charges,",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e0906b93e02441783d5b68582e59d81",
      "question": "What weapons was T.I carrying?",
      "prediction": "machine guns and two silencers",
      "ground_truths": [
        "three machine guns"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d571d248d30b4ecaba93f1eb28f88abd",
      "question": "what did TI do?",
      "prediction": "arrested without incident in midtown Atlanta.",
      "ground_truths": [
        "Harris provided the bodyguard $12,000 to buy the weapons,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b3812b214fc04fad833518e375f34e58",
      "question": "When was T.I. arrested",
      "prediction": "Saturday",
      "ground_truths": [
        "Saturday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a6a3831e65bd4a06aa984a16c845edd5",
      "question": "What is the rapper charged with?",
      "prediction": "gun charges",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "49e37f2ab89a42948fbd2249c10c2ddb",
      "question": "What is T.I. in custody for?",
      "prediction": "gun charges",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0d0a9395b6cb46db9f64e6acad197fd6",
      "question": "What did T.I. got arrested for",
      "prediction": "gun charges,",
      "ground_truths": [
        "gun charges,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af1df0d16de14e6187d0a6906505ac65",
      "question": "Who was arrested on weapons charges?",
      "prediction": "T.I.",
      "ground_truths": [
        "Clifford Harris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "13990b7394164f97b4c35cc400d63d5e",
      "question": "Did the rapper win any awards at the BET show?",
      "prediction": "won two",
      "ground_truths": [
        "Harris won two"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2cf921f1a4854045bd615640a37b2bad",
      "question": "What is happening to T.I?",
      "prediction": "will remain in custody until a bond hearing Friday,",
      "ground_truths": [
        "will remain in custody"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "48cc49890e394c47ab4443d4f2e65e46",
      "question": "Who remains in custoday until Friday?",
      "prediction": "Rapper T.I.",
      "ground_truths": [
        "T.I., whose real name is Clifford Harris,"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "3afe86583cc04c64ad908b3b2ae889e1",
      "question": "When is T.I.s bond hearing scheduled for",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ae2b7dd03781453d9e0ef25d421d808d",
      "question": "when was arrested the raper?",
      "prediction": "Saturday",
      "ground_truths": [
        "Saturday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9c46fbc04a814abc9146acfa990a1b4e",
      "question": "When is the bond hearing?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cff10f8297c64447bf388d45ed577e95",
      "question": "When does the agreement expire?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4790b60660da497bb390ed9bc5996973",
      "question": "What number of troops will remain?",
      "prediction": "100 to 150",
      "ground_truths": [
        "a dozen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dcd06efd15d8483f9ae99008aae4b8e6",
      "question": "Who are being pulled out of Iraq?",
      "prediction": "British troops",
      "ground_truths": [
        "Almost all British troops"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2b41e95bcc554f69b8d594bc3d05bf1a",
      "question": "Which troops are being pulled out",
      "prediction": "British",
      "ground_truths": [
        "British"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1c3202898a2340d8b9ee33b3d2feb483",
      "question": "What day does the agreement for them to be there expire?",
      "prediction": "Friday,",
      "ground_truths": [
        "Friday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5b7236c16734b2388cf37cfd5cbf27b",
      "question": "Which countries troops are being pulled out?",
      "prediction": "Iraq",
      "ground_truths": [
        "Britain's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f86df4ac102d4a058ac5a346fed44de9",
      "question": "What team is associated with Diego Maradona?",
      "prediction": "United",
      "ground_truths": [
        "Argentine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "435fd88dd9034cf1bd211e03bab2bed5",
      "question": "What position does Carlos Tevez play?",
      "prediction": "striker",
      "ground_truths": [
        "striker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a14a2ea82a1544a19f7c45cbf7f82f6e",
      "question": "Who urges Carlos Tevez to quit Manchester Utd at end of season?",
      "prediction": "Diego Maradona",
      "ground_truths": [
        "coach Diego Maradona"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "48685e35e9f24e6f94468729a1571a18",
      "question": "What bench was Tevez on?",
      "prediction": "despite a rousing reception",
      "ground_truths": [
        "Manchester United"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f64621c513a84c33a48b8f024d190371",
      "question": "Who is the Argentine coach?",
      "prediction": "Diego Maradona",
      "ground_truths": [
        "Diego Maradona"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "308816ab37144748816002e1b38d2e0b",
      "question": "Where was Tevez when Maradona watched United beat Chelsea ?",
      "prediction": "Old Trafford",
      "ground_truths": [
        "on the bench"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8bf4e9d3157446bd95bbea12227c100b",
      "question": "Who believes striker would be better off joining Italy's Inter Milan?",
      "prediction": "Maradona",
      "ground_truths": [
        "Argentine coach Maradona"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "06631314756e4d9fa7eb5f040d4d9da4",
      "question": "When will Tevez quit?",
      "prediction": "season and head for Italy.",
      "ground_truths": [
        "end of the season."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "4cedc34c6b2744e5b9385e0a2c2e7208",
      "question": "Who is at the top of Isreal's threat list?",
      "prediction": "Iran",
      "ground_truths": [
        "Iran"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2df5a7b010e644369ace3fe64a1d8fa8",
      "question": "What does an analyst think of Iran developing a nuclear bomb?",
      "prediction": "within the next year.",
      "ground_truths": [
        "could develop a"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b825f11fc8544f39b67d9e0e276ed3a",
      "question": "Who believes Iran still has secret sites?",
      "prediction": "Israel",
      "ground_truths": [
        "Israel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c74ea36d83af4441be3167fda8bee5c6",
      "question": "What was man accused of?",
      "prediction": "gunned down four Lakewood, Washington, police officers",
      "ground_truths": [
        "gunned down four Lakewood, Washington, police officers Sunday."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "bb8c1477740b4bd18455b3055f5bb98a",
      "question": "When did Huckabee commute the sentence?",
      "prediction": "2000",
      "ground_truths": [
        "In May 2000,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4eabcc0952174c2aa97df657e13776b1",
      "question": "What percentage of requests were denied?",
      "prediction": "10½ years",
      "ground_truths": [
        "\"Ninety-two percent"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85eb4e6d469f4dc3aa73c89a9f8c13e3",
      "question": "on what date the event occurred",
      "prediction": "Sunday.",
      "ground_truths": [
        "May 2000,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "956f69c5b64e48238441d49ffa32e523",
      "question": "When did the surgery occur?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "646c226fc7de4bf8ab2940795db0b2c5",
      "question": "Who gave Cole a new kidney?",
      "prediction": "OneLegacy,",
      "ground_truths": [
        "someone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f66c93d5e36e45edbf7f8e1dda09156c",
      "question": "When did she have her surgery?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ff7c4097100d487aa28a0e5482de7448",
      "question": "Where did the kidney come from?",
      "prediction": "organ procurement agency in Southern California,",
      "ground_truths": [
        "a deceased organ donor,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "5a3bf32e88574c78a5ef3fcb1756502f",
      "question": "What did the singer receive when on CNN?",
      "prediction": "a kidney transplant",
      "ground_truths": [
        "new kidney"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a2a1fb5d4f984d009cf64e937c5df55d",
      "question": "When was her surgery?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f014120341b4f609cf55f752f70e1ae",
      "question": "Who received a new kidney?",
      "prediction": "Natalie Cole",
      "ground_truths": [
        "Natalie Cole's"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8e6a38ca6c644c0e8fc56f5779009ab2",
      "question": "What newspaper says about landing?",
      "prediction": "the trip had caused fury among some in the military who",
      "ground_truths": [
        "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0.45000000000000007
    },
    {
      "id": "42fb458e399b41f9b78a83d4b30d27b0",
      "question": "What Ministry of Defense says about the operation?",
      "prediction": "part of a planned training exercise designed to help the prince learn to fly in combat situations.",
      "ground_truths": [
        "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations."
      ],
      "em": 0,
      "f1": 0.9375
    },
    {
      "id": "bd0f2b0fc40146edb228ac6be2fd9e5a",
      "question": "What did Prince William use to visit a girlfriend?",
      "prediction": "Royal Air Force helicopter",
      "ground_truths": [
        "Royal Air Force helicopter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a71f1a9b718241dd890fd8feb1be3ab8",
      "question": "What did the Ministry of Defense say about it?",
      "prediction": "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.",
      "ground_truths": [
        "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d21a2587e9e64ae08a9243d3f950521b",
      "question": "who says operation was training procedure?",
      "prediction": "British military officials",
      "ground_truths": [
        "The Ministry of Defense"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9fb85b5dcad34ba497f04a41616f6d0c",
      "question": "What did the media say about  it?",
      "prediction": "as a waste of time and money",
      "ground_truths": [
        "The paper said the trip had caused fury among some in the military who saw"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "502a0737d1ac4ce58ae0a8e0098dfa91",
      "question": "what caused the accident",
      "prediction": "two Metro transit trains",
      "ground_truths": [
        "The train in front had stopped"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cc168d69c69444cb1abd767d8fd7e37",
      "question": "A former commanding general of what was among those killed?",
      "prediction": "District of Columbia National Guard,",
      "ground_truths": [
        "of Columbia National Guard,"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "1e765f40aab74bffb1a8a5fc1ab4f90d",
      "question": "where was the killing",
      "prediction": "Washington.",
      "ground_truths": [
        "Washington."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8e5f4f90422c425bb047dd335d4a5598",
      "question": "Who said shots were fired?",
      "prediction": "Israeli Navy",
      "ground_truths": [
        "Hanin Zoabi,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "956c96514760435e81db606781abcb67",
      "question": "When were the shots fired?",
      "prediction": "days before.",
      "ground_truths": [
        "five minutes before commandos descended"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "342f15ed9e814127b49f3340dead119c",
      "question": "How many dead bodies did Zoabi see?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "49d05065daf847d7b985d4252b97a7cf",
      "question": "What was placed over a witness's head?",
      "prediction": "bag",
      "ground_truths": [
        "bag"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e6bceecd0180492db6f7d9f4df16e546",
      "question": "How many dead bodies were there?",
      "prediction": "nine",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc4dab450d5b4a6e9d009d7c5f2a9e4a",
      "question": "Whose forces started shooting?",
      "prediction": "Israeli",
      "ground_truths": [
        "Israeli"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "531226f1c86f49a3bee6638692fedd34",
      "question": "How many say things are going well for them, personally?",
      "prediction": "21 percent",
      "ground_truths": [
        "three out of four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70ff5cd057104a24a7de546f6a0a93e2",
      "question": "What are Americans angry about?",
      "prediction": "the way things are going in the country.",
      "ground_truths": [
        "the way things are going in the country."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "388dcae5aa9248318cd8d493a4c3519b",
      "question": "Who ran this poll?",
      "prediction": "CNN/Opinion Research Corporation",
      "ground_truths": [
        "CNN/Opinion Research Corporation"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da630eb21dea4abe920e8253d4fd6a34",
      "question": "What positive sentiment did a lot of Americans echo?",
      "prediction": "\"There is a tiny sliver of good news",
      "ground_truths": [
        "that things are going well for them personally."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "692084604fa74539aa32ec6abec75341",
      "question": "What out of four people say things are going well for them?",
      "prediction": "A deputy oversees an eviction in Lafayette, Colorado, last week.",
      "ground_truths": [
        "three out of four questioned say that things are going well for them personally."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "851dc215cd4f478ea5bc5be433bb650b",
      "question": "How many say things are going badly in the country?",
      "prediction": "Seven-Thirty percent",
      "ground_truths": [
        "Nearly eight in 10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91ef74a5a47a442aaa5af41b3d564051",
      "question": "Who says things are going well?",
      "prediction": "Three out of four Americans",
      "ground_truths": [
        "21 percent suggesting that"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b99c50bfd5f849afb601bd8ac95b0bf4",
      "question": "Number of people in 10 that say thing are going badly in country?",
      "prediction": "Nearly eight in 10",
      "ground_truths": [
        "Nearly eight"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6d8f6111e0f2484f961308f6dc5270b3",
      "question": "What proportion of Americans think things are going badly domestically?",
      "prediction": "Nearly eight in 10",
      "ground_truths": [
        "Nearly eight in 10"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bd3693226104fdeb8577e678c8560e1",
      "question": "Who celebrates Thanksgiving and an important religious occasion on the same day?",
      "prediction": "Muslim Eid-ul-Adha",
      "ground_truths": [
        "Muslims"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5dee8293fd7e42a1ba9896818b5aea22",
      "question": "Who celebrates Thanksgiving?",
      "prediction": "Muslims",
      "ground_truths": [
        "millions of Americans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dfdebae12b1448ce8bb75f3c7e8029d7",
      "question": "Who played a key role in civil rights movement?",
      "prediction": "Jewish",
      "ground_truths": [
        "Muslim revolutionary named Malcolm X"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d41593a1eb0e4edd8e8941e625eb8b8f",
      "question": "Who played roles in the civil rights movement?",
      "prediction": "Jewish",
      "ground_truths": [
        "Martin Luther King Jr."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c9cf44b1466b49388a43f7ded1ec0cba",
      "question": "Who does Obama hold formal talks with?",
      "prediction": "Chinese President Hu Jintao",
      "ground_truths": [
        "Chinese President Hu Jintao"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2ec1cc208a0d4795a63e4b5fcb1f15a2",
      "question": "Who is Obama holding formal talks with?",
      "prediction": "President Hu Jintao",
      "ground_truths": [
        "Chinese President Hu Jintao"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "42bcbd0ec2d340968120e2ce262ed8a9",
      "question": "How much does the US import in Chinese products every year?",
      "prediction": "billions",
      "ground_truths": [
        "billions of dollars"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "210c806253b1415999dcfd89f23c9d8e",
      "question": "What does the new poll show?",
      "prediction": "71 percent of Americans consider China an economic threat",
      "ground_truths": [
        "71 percent of Americans consider China an economic threat to the United States,"
      ],
      "em": 0,
      "f1": 0.8421052631578948
    },
    {
      "id": "cc4d7cb20065488ea6e8440d75e36b8a",
      "question": "How much does the US import?",
      "prediction": "billions",
      "ground_truths": [
        "billions of dollars in Chinese products each year,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "baeddc270ac04d039a6f83b9ccef8627",
      "question": "how many dollars does US import in Chinese products each year?",
      "prediction": "billions",
      "ground_truths": [
        "billions of"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2e3e4e90adb64e86bf0e6ccf5499126b",
      "question": "Which days are declared holidays?",
      "prediction": "Monday and Tuesday",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8283aed3c37f45439d976f9d7fd3851f",
      "question": "Who switched from driving on the left to the right?",
      "prediction": "Samoa",
      "ground_truths": [
        "Samoa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f955cb3e7bda4a93a867b361b7b96327",
      "question": "Which countries switched from driving on left side to the right?",
      "prediction": "Samoa",
      "ground_truths": [
        "Samoa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "644155df8e7446718cc1d9ca4d2f4c05",
      "question": "Who rejected constitutional challenge to change?",
      "prediction": "People Against Switching Sides (PASS)",
      "ground_truths": [
        "People Against Switching Sides (PASS)"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5010c8ecc7584e2d86bbb2ab8acfa878",
      "question": "What days were declared holidays?",
      "prediction": "Monday and Tuesday",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c0d5e844424d4d9bb84902a8ce411cc2",
      "question": "What was declared holidays for people to adjust?",
      "prediction": "and Tuesday",
      "ground_truths": [
        "Monday and Tuesday"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "808b1f032cab4da587ca8837b7ee6686",
      "question": "What types of ships are trapped?",
      "prediction": "30 to 40",
      "ground_truths": [
        "passenger"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04676862004644f18aa94ff54107047c",
      "question": "What does the spokesman expect?",
      "prediction": "The extra help was expected to arrive around midnight (6 p.m. ET),",
      "ground_truths": [
        "that most of the ships would be freed by Friday."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "31d748672420422aa724d2b6db6b1d18",
      "question": "What is being used to free the ships?",
      "prediction": "additional ice breakers.\"",
      "ground_truths": [
        "additional ice breakers.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "893449002d894f7695b2a32aa7713cb5",
      "question": "few ships trapped in the ice of the Sea?",
      "prediction": "Thirty to 40",
      "ground_truths": [
        "to 40"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e48a0f7a1c464cc2836d315091387b4f",
      "question": "What was packed?",
      "prediction": "the National Mall",
      "ground_truths": [
        "the Mall"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "dc9ba1c066e849778a296e23ea424da0",
      "question": "What is the wait time for some?",
      "prediction": "hours",
      "ground_truths": [
        "several hours"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "72e756b7dce94c2e9107e7efc71ea29e",
      "question": "What city is this?",
      "prediction": "Washington",
      "ground_truths": [
        "Washington"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4fbaacdd8faf49ed89f14aabbfd62193",
      "question": "What time does the mall close off?",
      "prediction": "9 a.m.,",
      "ground_truths": [
        "9 a.m.,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c13625cdb6614fe9914a3eaca30ef8cf",
      "question": "What is the change in threat level?",
      "prediction": "made some readjustments to inauguration security",
      "ground_truths": [
        "did not"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b955e270519147ad8c4ef7356d7a4011",
      "question": "when Zeiger became Larry King?",
      "prediction": "1959,",
      "ground_truths": [
        "1957,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a4ee6e421be40b2afc83dd19d87ac7f",
      "question": "Who is Larry Zeiger?",
      "prediction": "former on his WKAT radio program.",
      "ground_truths": [
        "\"Larry King"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74cabd6142eb4bab88050888d5f4f324",
      "question": "What name did Zeiger use?",
      "prediction": "Bobby Darin,",
      "ground_truths": [
        "Larry King"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44037ae7db3d4534986eb3db347727de",
      "question": "Who became the King?",
      "prediction": "Bobby Darin,",
      "ground_truths": [
        "Larry King?\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef078e968b6942d18ad8d2e5f4104222",
      "question": "Where did Larry Zeiger  move?",
      "prediction": "went to stay with my Uncle Jack in Miami Beach.",
      "ground_truths": [
        "Miami Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "716b2f16ce0645bf89407581a31ee24b",
      "question": "What happened the first time Larry was on the air?",
      "prediction": "The Larry King Show.\"",
      "ground_truths": [
        "couldn't speak."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f29f74ee9dbd4a8bb20bbebd4e7aa566",
      "question": "who moved from Brooklyn to Miami?",
      "prediction": "Larry Zeiger",
      "ground_truths": [
        "Larry Zeiger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c5e69a8c19ec4fadb153de7ed8d06b9f",
      "question": "What time is the lift off scheduled for?",
      "prediction": "9:20 p.m. ET",
      "ground_truths": [
        "9:20 p.m."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "0af880e8558a4fc3afbc9a8a25d081fd",
      "question": "When is the  space shuttle Discovery  scheduled for launch?",
      "prediction": "Wednesday.",
      "ground_truths": [
        "9:20 p.m. ET Wednesday."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "3645db7e841f4242ae0b3be674e8bc8a",
      "question": "When is the launch scheduled for?",
      "prediction": "Wednesday.",
      "ground_truths": [
        "9:20 p.m. ET Wednesday."
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "0104a4f929b64f0e84d574262a285e30",
      "question": "What is scheduled for launch on Wednesday?",
      "prediction": "space shuttle Discovery",
      "ground_truths": [
        "The space shuttle Discovery,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a9dae85ef8448b88947f67b7368cd84",
      "question": "How long is the mission?",
      "prediction": "14-day",
      "ground_truths": [
        "14-day"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c034e68af36c41d79e0782c060d7acbd",
      "question": "what time is the shuttle",
      "prediction": "9:20 p.m. ET",
      "ground_truths": [
        "9:20 p.m."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "48e5d37aaea9404a891d01584e699f18",
      "question": "What is the name of the space shuttle?",
      "prediction": "Discovery,",
      "ground_truths": [
        "Discovery"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3ac333098d74ad7acf2d20bbc261c9b",
      "question": "What is set to lift off?",
      "prediction": "shuttle Discovery",
      "ground_truths": [
        "space shuttle Discovery"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f1b9b4f3aeae403297d234cfe9a6adfc",
      "question": "what will Band partners do",
      "prediction": "with Keep America Beautiful,",
      "ground_truths": [
        "partnered with Keep America Beautiful,"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "2b483ffa8a594b72ada39a0d9adb43a2",
      "question": "What is the purpose of the band traveling  light?",
      "prediction": "cut down our diesel consumption.",
      "ground_truths": [
        "reduced their carbon footprint by 132 tons."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "83191d65ecfe4f649a4a2c61b6f99ee0",
      "question": "What did the guitarist say they wanted to do?",
      "prediction": "\"heeding local\"",
      "ground_truths": [
        "hire local,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8f8dc0c9c2264752b3f946c8e7d2cdcc",
      "question": "Where did the iReporter grow up?",
      "prediction": "New Delhi, India,",
      "ground_truths": [
        "Middle East,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b91bec646d641fab11061917d864c78",
      "question": "who says \"I remember growing up in the Middle East ... waiting for his albums,\"?",
      "prediction": "Sachina Verma",
      "ground_truths": [
        "Rany Freeman,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5faf2cc346b4c92a7258acfc94e389f",
      "question": "what Fans across the world remark on Jackson's music?",
      "prediction": "left hundreds of messages in languages ranging from French and Spanish to Japanese and Hebrew.",
      "ground_truths": [
        "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c027ab8bde24b2aaae43a127848342e",
      "question": "What did fans remark about?",
      "prediction": "Michael Jackson's death",
      "ground_truths": [
        "talked of an impromptu memorial for the late singer at the \"Stone Circle,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a0ce6a0b7664deca7f3454e5520195a",
      "question": "Where do the most serious attacks come from?",
      "prediction": "the sins of the members of the church,",
      "ground_truths": [
        "the sins of the members of the church,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d84a937c35bd44f9ba5d4952c3df1c34",
      "question": "where does the most serious attacks come from?",
      "prediction": "the sins of the members of the church,",
      "ground_truths": [
        "sins of the members of the church,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bd6ff486bd6d413baa8804b53f6e5382",
      "question": "Where was the Pope en route to?",
      "prediction": "Portugal,",
      "ground_truths": [
        "Portugal,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0ba1c2d49b54ca4adc7e5c38dbe942e",
      "question": "Which church does Pope Benedict lead?",
      "prediction": "Roman",
      "ground_truths": [
        "Roman Catholic"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d96314813ffe466c92d3778208871ade",
      "question": "Who says the scandal is terrifying?",
      "prediction": "Pope Benedict XVI",
      "ground_truths": [
        "Pope Benedict XVI"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0ab491f01e8b47e79001da10007967fe",
      "question": "what does Pope Benedict XVI say about child abuse?",
      "prediction": "\"terrifying.\"",
      "ground_truths": [
        "the reality he has seen is \"terrifying.\""
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "4ebc2de2a65e4549893bb0d0d4dbed4e",
      "question": "he was speaking en route to where?",
      "prediction": "Portugal,",
      "ground_truths": [
        "Portugal,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8fcc1e39bba146a08300eef42c641e71",
      "question": "Where is tennis player Carlo Moya from?",
      "prediction": "Spaniard",
      "ground_truths": [
        "Spaniard"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "27ed5ffe8231443985b27f0d9d605774",
      "question": "Who won their opening match at the Kooyong Classic?",
      "prediction": "Roger Federer",
      "ground_truths": [
        "Roger Federer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "30cbead889fc4c0783a1afd652cfb163",
      "question": "Who lost to Roger Federer?",
      "prediction": "Carlos Moya",
      "ground_truths": [
        "Carlos Moya"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5dc68d2932bb4b0f861bd4642c674c43",
      "question": "What country is Fernando Gonzalez from?",
      "prediction": "Chile",
      "ground_truths": [
        "Chile"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8c3c9f350c5469da18f367a6a250097",
      "question": "Who won his opening match?",
      "prediction": "Roger Federer",
      "ground_truths": [
        "Federer"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1cc48ae35d6a4676a0823fdf2550f2ea",
      "question": "Who is the worlds's number two tennis player?",
      "prediction": "Roger Federer",
      "ground_truths": [
        "Roger Federer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b6a4b25c0cf41d89f69fab7e636d836",
      "question": "Where is the Kooyong Classic Held?",
      "prediction": "Melbourne",
      "ground_truths": [
        "Melbourne"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3350d80e3c94368aed1d9a34577d519",
      "question": "Which competitor from Chile went through?",
      "prediction": "Fernando Gonzalez",
      "ground_truths": [
        "Fernando Gonzalez"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b94b8e734c3b40eaba7291c227e2edb7",
      "question": "Where is Carlo Moya from?",
      "prediction": "Spaniard",
      "ground_truths": [
        "Spaniard"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d6270ad7fc144895abf4552a2e0f8fc4",
      "question": "Who did Federer beat?",
      "prediction": "Carlos Moya",
      "ground_truths": [
        "Spaniard Carlos Moya"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "a78b494b73574ef89bc015078c11cafc",
      "question": "what was approved for use",
      "prediction": "cervical cancer vaccine,",
      "ground_truths": [
        "cervical cancer vaccine,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c7c4dbc9ee8445f9b784291cd63d7f6e",
      "question": "Who should get the vaccine?",
      "prediction": "girls around 11 or 12.",
      "ground_truths": [
        "girls around 11 or 12."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "308fd3471884450dbcfc690ab8dc5398",
      "question": "What can cause cervical cancer",
      "prediction": "sexually transmitted.",
      "ground_truths": [
        "HPV (human papillomavirus)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f0e6f73fe4c4e79b1f6c2ab05ae44e2",
      "question": "What position does Kgalema Motlanthe have?",
      "prediction": "Deputy President",
      "ground_truths": [
        "African National Congress Deputy President"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "74fe96e647704cca90c6b1e351132a0d",
      "question": "Who resigned in South Africa?",
      "prediction": "Ten",
      "ground_truths": [
        "Ten"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e45a17436bc24b3880cd99d05da8d4ca",
      "question": "What country is being discussed?",
      "prediction": "South Africa",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "728a6e1ff8d74b919e2cb64333781ad0",
      "question": "Who was sworn in on Thursday?",
      "prediction": "Kgalema Motlanthe,",
      "ground_truths": [
        "African National Congress Deputy President Kgalema Motlanthe,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "d299bbb3cb8c4cc586d87e04eedef293",
      "question": "What longtime rival did Mbeki's court case involve?",
      "prediction": "Zuma",
      "ground_truths": [
        "Zuma"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ac119b06a9954a51ab96864361e5f831",
      "question": "Who resigned with President Thabo Mbeki?",
      "prediction": "Ten South African ministers and the deputy",
      "ground_truths": [
        "Ten South African ministers and the deputy"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef88996852174dd8971704d9556057e9",
      "question": "Who was sworn in Thursday?",
      "prediction": "Kgalema Motlanthe,",
      "ground_truths": [
        "African National Congress Deputy President Kgalema Motlanthe,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "08cc57b5d9f342b489a0907d02617ab6",
      "question": "Who quit after court case?",
      "prediction": "Ten South African ministers and the deputy president",
      "ground_truths": [
        "Ten South African ministers and the deputy president"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f70ba795250b478498351d4a7e8c5154",
      "question": "Who is the rival of Mbeki?",
      "prediction": "Zuma",
      "ground_truths": [
        "Zuma"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f7cfe3979d14e35acb5cf8337396d2d",
      "question": "Name of the replacement that was sworn in Thursday?",
      "prediction": "Kgalema Motlanthe,",
      "ground_truths": [
        "Kgalema Motlanthe,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da4803aad9bd412f97dc1d468eb6fd52",
      "question": "what people used a surrogate to have a baby",
      "prediction": "Arnold and Klein,",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "aec05463fc614cf787003f33ba3101f7",
      "question": "what family was happy to visit the doctor",
      "prediction": "Arnold and Klein,",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "b481a8d55f2d45029107048c524decc5",
      "question": "Who suffer from skeletal dysplasia?",
      "prediction": "Dr. Jennifer Arnold and husband Bill Klein,",
      "ground_truths": [
        "Dr. Jennifer Arnold and husband Bill Klein,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ca166bde82644d2937a20a6c4eb3122",
      "question": "what men have skeletal dysplasia",
      "prediction": "Dr. Jennifer Arnold",
      "ground_truths": [
        "Bill Klein,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ee12be811354cf89be9edefeaad7dd0",
      "question": "What can pose health problems?",
      "prediction": "a pregnancy",
      "ground_truths": [
        "the challenges a pregnancy"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "531b2b755eb7418b91795b6546663372",
      "question": "What lost the couple?",
      "prediction": "suitor",
      "ground_truths": [
        "the pregnancy."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b835ba9d0b7463589f3a5a140fd372c",
      "question": "Where is Natalia Vodianova from?",
      "prediction": "Nizhny Novgorod",
      "ground_truths": [
        "I was born in Nizhny Novgorod"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9b4b8e59f2ae42309c527a39ee3fb62e",
      "question": "What is Natalia Vodianova's occupation?",
      "prediction": "Russian supermodel",
      "ground_truths": [
        "supermodel"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8b02e7e972a048cdb2835c247225be55",
      "question": "Where were authorities were called to?",
      "prediction": "near Fort Bragg",
      "ground_truths": [
        "Fort Bragg in North Carolina."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ed8e566b72f247d5a7f9838b76acd5a7",
      "question": "Who was still missing after the fire?",
      "prediction": "Holley Wimunc.",
      "ground_truths": [
        "female soldier,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "103394fd775f4bec8e17686dfbd6c16e",
      "question": "What did she say in the court filing?",
      "prediction": "in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,",
      "ground_truths": [
        "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,"
      ],
      "em": 0,
      "f1": 0.9473684210526316
    },
    {
      "id": "d29d882518d246d5a12a3f4e4543a54c",
      "question": "Who did she say was threatening her in court?",
      "prediction": "John Wimunc",
      "ground_truths": [
        "husband"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2b76b4f9a9b74046ae103cc87f19639c",
      "question": "when were the authorities called",
      "prediction": "Thursday",
      "ground_truths": [
        "Thursday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02faadc509b54a07a40ecb311a448c8e",
      "question": "What statement did a missing soldier's family make?",
      "prediction": "their are \"still trying to absorb the impact of this week's stunning events.\"",
      "ground_truths": [
        "they are \"still trying to absorb the impact of this week's stunning events.\""
      ],
      "em": 0,
      "f1": 0.9166666666666666
    },
    {
      "id": "9078e22fc4a74f44b0d692850bc4fe82",
      "question": "what did the family say",
      "prediction": "\"still trying to absorb the impact of this week's stunning events.\"",
      "ground_truths": [
        "\"still trying to absorb the impact of this week's stunning events.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "10dc12a62b1048f286bfc224f79ec612",
      "question": "What happened to the fire?",
      "prediction": "Holley Wimunc, 24.",
      "ground_truths": [
        "No one was inside the apartment at the time of the fire, police said."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c46db8111bf4306ad1afe8eec0e8d4d",
      "question": "Who is missing?",
      "prediction": "Holley Wimunc.",
      "ground_truths": [
        "Lt. Holley Wimunc."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "67b6f116f59b45d7b224fad351757029",
      "question": "Where were authorities called to fire at on Thursday?",
      "prediction": "Fayetteville, North Carolina.",
      "ground_truths": [
        "Fort Bragg in North Carolina."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1905d03db9fb4f42a537ba45c34f1301",
      "question": "Who says two soldiers likely not alive?",
      "prediction": "Israeli Prime Minister Ehud Olmert",
      "ground_truths": [
        "Israeli Prime Minister Ehud Olmert"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95d3252b70fc4d5aa8082d3a85433fa1",
      "question": "Who agreed to swap prisoners?",
      "prediction": "Israel",
      "ground_truths": [
        "Israel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebbc94f432924b7c9934aec7ca367caf",
      "question": "What will Israel swap for captured soldiers?",
      "prediction": "two Israeli",
      "ground_truths": [
        "five Lebanese prisoners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f8cc47e7023d4379a08aa2f18ba94cb2",
      "question": "Who are the two soldiers thought to be dead?",
      "prediction": "Ehud \"Udi\" Goldwasser and Eldad Regev.",
      "ground_truths": [
        "Ehud \"Udi\" Goldwasser and Eldad Regev."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "530b0b3b0802450d9e7ebda6283e6b6c",
      "question": "What is the number of Lebanese prisoners that will be released?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e08c7860a95e47bb97a9dfa2fd2a6713",
      "question": "Who will release five Lebanese prisoners?",
      "prediction": "Israel",
      "ground_truths": [
        "Israel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77b194a548d2400a96ecd5709b8b1d64",
      "question": "How many prisoners will be released?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4888bc9c3cf5402e8ccd794fbac2588e",
      "question": "What is the Israeli Cabinet willing to swap prisoners for?",
      "prediction": "two Israeli soldiers,",
      "ground_truths": [
        "soldiers,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "96a893350b4a47a2b0d56d5b007cf6e9",
      "question": "What nationality are the prisoners about to be released?",
      "prediction": "Lebanese",
      "ground_truths": [
        "Lebanese"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e33f9ac0e542439281cdf842b17cb53e",
      "question": "What does the prime minister say?",
      "prediction": "the two soldiers are not alive, \"as far as we know.\"",
      "ground_truths": [
        "told his Cabinet that the two soldiers are not alive, \"as far as we know.\""
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "782be7eadeb040b6ae37b1cfc7918c5c",
      "question": "What does the Israeli Cabinet agree to do?",
      "prediction": "release a notorious killer",
      "ground_truths": [
        "release a notorious killer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "689aed65128147c2948d8fc665e6ef0d",
      "question": "What background is her family from?",
      "prediction": "many different",
      "ground_truths": [
        "many different"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "df8fc2dc6ff34ade9ae6b4242632e8b1",
      "question": "What was a part of radio station promotion?",
      "prediction": "three years ago",
      "ground_truths": [
        "\"learn how to dance and feel sexy,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fdc90f6593464af6b65c2f38b2c814c6",
      "question": "Who is Miss USA?",
      "prediction": "Rima Fakih",
      "ground_truths": [
        "Rima Fakih"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc44917bc70242a7b01f7b024c5fa3ea",
      "question": "who said that pole-dancing pictures were part of radio station promotion?",
      "prediction": "Rima Fakih",
      "ground_truths": [
        "Rima Fakih"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c411868fbe054e80b90a12463b2beddd",
      "question": "who downplays downplays significance of photographs that emerged online this week?",
      "prediction": "Rima Fakih",
      "ground_truths": [
        "Miss USA Rima Fakih"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9a83f32bddac46a19097e899dd1e1d65",
      "question": "Which background does she say her family is from?",
      "prediction": "many different backgrounds",
      "ground_truths": [
        "many different"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "90f5064a214f4e05b9a65eb206d773ea",
      "question": "Who downplayed the significance of photographs?",
      "prediction": "Rima Fakih",
      "ground_truths": [
        "Fakih"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b74bd4ddf00e4e59bd280742fa82144d",
      "question": "What were the pictures of?",
      "prediction": "dancing against a stripper's pole.",
      "ground_truths": [
        "dancing against a stripper's pole."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "46e09c1bc9304e71a144d33bf875c619",
      "question": "is she from a muslim background?",
      "prediction": "family comes from many different backgrounds and religions.\"",
      "ground_truths": [
        "a"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf138cf693f0444e960d28a85e72f955",
      "question": "Who can not afford to pay for cable?",
      "prediction": "More than 6.5 million U.S. households,",
      "ground_truths": [
        "James Richter"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "734ca4956e434fc8b6712a684fcb7bdc",
      "question": "who delayed the switch?",
      "prediction": "Congress",
      "ground_truths": [
        "Congress"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8fc9240c389d407db9a7410d11f3352e",
      "question": "Who delayed a nationwide switch?",
      "prediction": "Congress",
      "ground_truths": [
        "Congress"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e5d9591150248d8b366c339f50b3aad",
      "question": "What was delayed?",
      "prediction": "The 36-year-old from Atlanta, Georgia,",
      "ground_truths": [
        "the end of TV's rabbit-ears era."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44aa6bf4f6a74816940787187cfe1bac",
      "question": "what is the switch?",
      "prediction": "to digital",
      "ground_truths": [
        "to digital."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0e4899d00d6a44a69ed4715342726352",
      "question": "The criticism over what act caused the move?",
      "prediction": "Defense of Marriage",
      "ground_truths": [
        "Defense of Marriage"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf25dbd17a734546aa9207995fedf187",
      "question": "what is the criticism?",
      "prediction": "gay rights activists",
      "ground_truths": [
        "over a Justice Department motion filed last week in support of the Defense of Marriage Act"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d699f52ba254a96b8622bb286e28072",
      "question": "Who spoke in favor of gay and lesbian rights?",
      "prediction": "President Obama",
      "ground_truths": [
        "Carisa Cunningham,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbf2b2f61b4d40ad91f086f4dc927b97",
      "question": "what does the coverage have to do with gay marriage?",
      "prediction": "\"We've got more work to do to ensure that government treats all its citizens equally, to fight injustice and intolerance in all its forms and to bring about that more perfect union,\"",
      "ground_truths": [
        "granting some benefits"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1354074638a34aac988882a24671fb15",
      "question": "Does executive order grant full health coverage?",
      "prediction": "it does not",
      "ground_truths": [
        "health-care"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37b3b28e375e4216888e4da3c16a3f4c",
      "question": "Who told fire victims they won't be forgotten in Washington?",
      "prediction": "President Bush",
      "ground_truths": [
        "the president"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f9bd82e4e8474f5aa64203fb5ebb742f",
      "question": "What caused the bodies to be charred?",
      "prediction": "The number of deaths attributed directly",
      "ground_truths": [
        "the fires"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe3dc6c9564e490f8e5b857613093186",
      "question": "Who spoke to fire victims?",
      "prediction": "President Bush",
      "ground_truths": [
        "President Bush"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0e199868b2fa47cea32382c96ab8a91e",
      "question": "Where is the Qualcomm Stadium?",
      "prediction": "San Diego,",
      "ground_truths": [
        "San Diego,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f19f066e81ec40d7aae87630df542d27",
      "question": "What has been found near San Diego?",
      "prediction": "four bodies",
      "ground_truths": [
        "Four bodies"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4a02dd17a70249c79c782040d1ff267a",
      "question": "where were the bodies found?",
      "prediction": "in a canyon in the path of the blaze",
      "ground_truths": [
        "in a canyon in the path of the blaze Thursday."
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "891aa475e0f64276bbcec79f79a8d50e",
      "question": "what was found near san diego?",
      "prediction": "Four bodies",
      "ground_truths": [
        "Four bodies were"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "cfd40ade7d554772833d0880f051d043",
      "question": "What stadium is closing at noon on Friday?",
      "prediction": "San Diego,",
      "ground_truths": [
        "Qualcomm"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7693a75ee18f4901a997aebeb4b22fcb",
      "question": "what did Bush say to victims?",
      "prediction": "\"We're not going to forget you in Washington, D.C.,\"",
      "ground_truths": [
        "\"We're not going"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0ec6bb1684c543b09f6617fe46af7f85",
      "question": "Who says border has never been more protected?",
      "prediction": "patrol agents",
      "ground_truths": [
        "Arizona Gov. Jan Brewer"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "22f4ef32a6a84b5681374e3a2aa13b64",
      "question": "What did President Obama send the National Guard for?",
      "prediction": "reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.",
      "ground_truths": [
        "reallocate reconnaissance helicopters and robotic surveillance craft"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "5f54b72f33ee4bc885764e1c0674eb2b",
      "question": "What has Arizona asked?",
      "prediction": "for air support.",
      "ground_truths": [
        "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f40fa2cb594a4b1abf731b8c04c9ed95",
      "question": "What did Arizona ask for?",
      "prediction": "air support.",
      "ground_truths": [
        "air support."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2d500e47e4d4a18bd068db4ff12063c",
      "question": "What causes people to hire illegals?",
      "prediction": "poor economy",
      "ground_truths": [
        "to do jobs that Arizonans wouldn't do."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "459a9cdc3e864194b24734d08893c10a",
      "question": "What did Arizona asked for to beef up border security?",
      "prediction": "air support.",
      "ground_truths": [
        "helicopters and unmanned aerial vehicles"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "708b1af75c064fbbbb9101748af472db",
      "question": "What is his sport?",
      "prediction": "sailing",
      "ground_truths": [
        "sailing"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "90fe0d6a432c429b8015817c244c5a1e",
      "question": "Who is Alexandre Caizergues?",
      "prediction": "The new holder of the record,",
      "ground_truths": [
        "kite surfers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8be75204345642ec87b18f1f7e8d3513",
      "question": "who holds the record",
      "prediction": "Alexandre Caizergues,",
      "ground_truths": [
        "Alexandre Caizergues,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dae18363ce314d5e9dfe606d1c1a8f9e",
      "question": "how many formal applications",
      "prediction": "about 50",
      "ground_truths": [
        "50"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "70c751e856e1426091722028754d12be",
      "question": "what year had 5o applications",
      "prediction": "2008.",
      "ground_truths": [
        "2008."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "be4a1b1eb4d44d628dbaefa4ede886a3",
      "question": "Who holds the world sailing speed record?",
      "prediction": "Alexandre Caizergues,",
      "ground_truths": [
        "Alexandre Caizergues,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2a4384bdb549450fb8a8a634bb71db5b",
      "question": "What happened in 2008?",
      "prediction": "50 formal applications for speed attempts",
      "ground_truths": [
        "WSSRC had received about 50 formal applications for speed attempts"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "d150b928a5c9471399cc5faec90e8b16",
      "question": "What does the Council oversees?",
      "prediction": "attempts",
      "ground_truths": [
        "speed attempts"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "11701192341248d787b000a454c9792b",
      "question": "Hoe many were left behind in Japan?",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "83da903b59c94d51b07757675a1cd160",
      "question": "How many Oka brothers served in the military?",
      "prediction": "seven",
      "ground_truths": [
        "seven"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e1c1e7a07b394af3975a151f1ff02f54",
      "question": "How many brothers were too young to fight?",
      "prediction": "four",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b82eb60ec4c476e9025892dcfb76873",
      "question": "When did the two youngest serve on the U.S. side?",
      "prediction": "1941",
      "ground_truths": [
        "the Korean War"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "17734567580e41fda19424773eb5ad5a",
      "question": "How many were in the U.S. army?",
      "prediction": "seven",
      "ground_truths": [
        "\"Five of us for the United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73b5f1567c354aa7bc37e3664f61d0e6",
      "question": "What did all seven brothers do?",
      "prediction": "served in the military,",
      "ground_truths": [
        "served in the military,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ef90843df6b46609bb78fe1748c188e",
      "question": "What is the club about?",
      "prediction": "The Valley Swim",
      "ground_truths": [
        "Swim"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "99e20c89ab1848fab5ad31ee18d07ab5",
      "question": "What claims to be \"very diverse\"?",
      "prediction": "The Valley Swim Club",
      "ground_truths": [
        "The Valley Swim Club"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc4b8826a9a041699442a8e89f061aa3",
      "question": "What did the president say?",
      "prediction": "\"It was never our intention to offend anyone,\"",
      "ground_truths": [
        "\"It was never our intention to offend anyone,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "937c40c196d0426591642dd8137cf223",
      "question": "What did John Duesler say?",
      "prediction": "he underestimated the number of swimmers who would come to swim at the club.",
      "ground_truths": [
        "underestimated the number of swimmers who would come to swim at the club."
      ],
      "em": 0,
      "f1": 0.9565217391304348
    },
    {
      "id": "6cf873665ae247898bae6330c367c09f",
      "question": "What is the club",
      "prediction": "The Valley Swim",
      "ground_truths": [
        "The Valley Swim"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "74ff309b62674179b7c7ee1d6f202157",
      "question": "Who has to decide to reinstate the center's contract?",
      "prediction": "the club president",
      "ground_truths": [
        "the club's board"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1dbd79c72724f15bb814757570cfaeb",
      "question": "What is theboard deciding",
      "prediction": "the contract",
      "ground_truths": [
        "how it will proceed."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1e1eb993a1ca4265978c2db209a671b7",
      "question": "Who owned the helicopter that crashed?",
      "prediction": "Vertikal-T,",
      "ground_truths": [
        "Russian air company Vertikal-T,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "5f24d26d866b410da298cec18f321605",
      "question": "Who was the chopper owned by?",
      "prediction": "the Russian air company Vertikal-T,",
      "ground_truths": [
        "the Russian air company Vertikal-T,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a07201b5d4514f918eb547a2825af689",
      "question": "where did the helicopter crash",
      "prediction": "Kandahar airfield",
      "ground_truths": [
        "Kandahar airfield in southern Afghanistan"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "f58b2227f5a54648811bf0959527bf7d",
      "question": "who owned the helicopter",
      "prediction": "Vertikal-T,",
      "ground_truths": [
        "by the Russian air company Vertikal-T,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "f8d21ecd07db4b0888ea7c2c91f9135c",
      "question": "What were all of the dead?",
      "prediction": "civilians,",
      "ground_truths": [
        "civilians,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bae286302f3748f9bc00ffc4b45bfab7",
      "question": "Where did the helicopter crash take place?",
      "prediction": "Kandahar airfield",
      "ground_truths": [
        "Kandahar airfield in southern Afghanistan"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8907b0e8c2e142a395a35b0c5560fe6a",
      "question": "What also makes a hard landing?",
      "prediction": "a U.S. military helicopter",
      "ground_truths": [
        "U.S. military helicopter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6d1b542705c74dfcb9ca889be8fc5c82",
      "question": "what is the condition of the five casualties",
      "prediction": "condition was not known,",
      "ground_truths": [
        "was not known,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "3b6c10cafc3944a098e4ee170fbfeb9e",
      "question": "What happened in southeastern U.S?",
      "prediction": "severe flooding",
      "ground_truths": [
        "severe flooding"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ba16cbd6e7b4c3193e234e5c68ae150",
      "question": "Where is the war being fought?",
      "prediction": "Afghanistan.",
      "ground_truths": [
        "Afghanistan."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "76f80eb5f49e41c7b8c76f093e985b88",
      "question": "Which international airports were badly affected?",
      "prediction": "Stansted and Gatwick,",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c0d30ddd6da4ba9a174843ca4669dc8",
      "question": "Who said that the snow was worst in southeastern England in 18 years?",
      "prediction": "(CNN)",
      "ground_truths": [
        "(CNN)"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91fb0d81f0f94b89a6a60783d5742723",
      "question": "What was one of the airports affected by the snow?",
      "prediction": "London's Heathrow",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5f0e807c9422490e8fed93d5bbd37826",
      "question": "Which airports are affected?",
      "prediction": "London City",
      "ground_truths": [
        "London's Heathrow"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "663600c74e02454896eb1494fb334bc1",
      "question": "What kind of weather was England experiencing?",
      "prediction": "The worst snowstorm",
      "ground_truths": [
        "snowstorm"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "64a570a28516471bbb028a25beec8028",
      "question": "What is the worst in 18 years?",
      "prediction": "The",
      "ground_truths": [
        "snowstorm"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bec5942b28c94983a86a4b16adc0b4e7",
      "question": "Where is Gatwick?",
      "prediction": "British",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11682b8601734e57bd4ba82eb9f20e77",
      "question": "what has been a local favorite for 68 years?",
      "prediction": "The Ski Train",
      "ground_truths": [
        "The Ski Train"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e8c205f129ea4cff9a5204db9b89ddf2",
      "question": "where does the train travel from?",
      "prediction": "Denver",
      "ground_truths": [
        "Denver, Colorado."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "cc143e8060be487db153d91159e40e82",
      "question": "where does the train pass through",
      "prediction": "almost 30 tunnels,",
      "ground_truths": [
        "the Rockies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8adf6db963574dad935d7158c4c5d364",
      "question": "where does the train take skiers to",
      "prediction": "Winter Park.",
      "ground_truths": [
        "between Denver and Winter Park."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "d67b0638b8c943c596db2bb2795e4edb",
      "question": "what do residents say about the train ride",
      "prediction": "it's worth the cost to avoid the traffic hassles of the oft-congested I-70.",
      "ground_truths": [
        "riders love the trip route, which winds through the Rockies and climbs to 9,000 feet."
      ],
      "em": 0,
      "f1": 0.08695652173913043
    },
    {
      "id": "d72a1897a39b4a289c85a26e68a970b2",
      "question": "what is the name of the tunnel underneath the Continental Divide?",
      "prediction": "6.2-mile Moffat",
      "ground_truths": [
        "Moffat"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d406698c75a144f88c496b4e90f26ca7",
      "question": "Where has the Saudi militant been hiding?",
      "prediction": "Yemen,",
      "ground_truths": [
        "in Yemen,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f06f7d84a78f412282d3bc847fcca401",
      "question": "Who is wanted?",
      "prediction": "Abdullah Hassan Talea' Asiri, a",
      "ground_truths": [
        "Abdullah Hassan Talea' Asiri,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b3a6721fd03c4610b1485c6b394d885a",
      "question": "W\\Who did Asiri meet with?",
      "prediction": "Prince Mohammed bin Naif,",
      "ground_truths": [
        "Saudi Prince Mohammed bin Naif, the assistant minister of Interior for Security Affairs."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2192ae48715343f7b729bfebfe062bed",
      "question": "Where had Asiri been hiding?",
      "prediction": "Yemen,",
      "ground_truths": [
        "Yemen,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5075af622c6440fe9b693341c37a0b73",
      "question": "Who fell at her California home?",
      "prediction": "Nancy Reagan,",
      "ground_truths": [
        "Reagan,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5b20d1dccf164c7f9e35acc6d37599b1",
      "question": "From her hospital bed, How is Nancy Regan reportedly feeling?",
      "prediction": "good",
      "ground_truths": [
        "Reagan is in good spirits, especially comforted to be receiving care"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "e3d3f69a45554550ad9a8f8efb0752a9",
      "question": "what was the fracture",
      "prediction": "a broken pelvis,",
      "ground_truths": [
        "fractured pelvis and sacrum"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "0641421ccb1349ff8a9131e411e0a878",
      "question": "Where is nancy reagan being treated?",
      "prediction": "Ronald Reagan UCLA Medical Center,",
      "ground_truths": [
        "Ronald"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "06bff66d3af04edd808a81c4b0a106a7",
      "question": "What other organization is TAK affiliated with?",
      "prediction": "the Kurdistan Workers' Party,",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "64fee0d345024f42864a361022475420",
      "question": "What does PKK stand for?",
      "prediction": "Kurdistan Workers' Party,",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc7d87cd43814b8db5d9bcc43cba1da3",
      "question": "Who is TAK affiliated with?",
      "prediction": "the Kurdistan Workers' Party,",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4c20cfad09754954a40daa931403ff60",
      "question": "Who is the Turkish President?",
      "prediction": "Abdullah Gul,",
      "ground_truths": [
        "Abdullah Gul,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba68aac128784f47a20eafdb4fa84627",
      "question": "Where did PKK attack Turkey from?",
      "prediction": "inside northern Iraq.",
      "ground_truths": [
        "inside northern Iraq."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb776aaae451464692b974373b39e4c5",
      "question": "What country has PKK been attacking?",
      "prediction": "Turkey",
      "ground_truths": [
        "Turkey"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7b190c41b59249bcab29c887d579bb26",
      "question": "What nation has the PKK been attacking?",
      "prediction": "Turkey",
      "ground_truths": [
        "Turkey"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "79d983f421ab4c3e92364b83b7186e99",
      "question": "Who met with President Bush?",
      "prediction": "Turkish President Abdullah Gul,",
      "ground_truths": [
        "Turkish President Abdullah Gul,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af41b3be86a24a4ca131ec6768beab58",
      "question": "Who is on the list of terrorist groups?",
      "prediction": "Kurdistan Freedom Falcons,",
      "ground_truths": [
        "Kurdistan Freedom Falcons, known as TAK,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8bff718944c84becbf657952138356fd",
      "question": "Who has been attacking Turkey?",
      "prediction": "Kurdistan Workers' Party, or PKK",
      "ground_truths": [
        "Kurdistan Workers' Party,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "e772ff7304f54b78a3280763cf427221",
      "question": "What group did the US recently put on the list of terror groups?",
      "prediction": "Kurdistan Freedom Falcons, known as TAK,",
      "ground_truths": [
        "Kurdistan Freedom Falcons,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "62a5565f76544eaf979a8fb42db7a3df",
      "question": "What group did US put on list of terrorist groups?",
      "prediction": "Kurdistan Freedom Falcons, known as TAK,",
      "ground_truths": [
        "Kurdistan Freedom Falcons, known as TAK,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f621718fc1bf4d0685f06afc881ef447",
      "question": "What was found at an LA construction site?",
      "prediction": "Columbian mammoth fossil \"Zed.\"",
      "ground_truths": [
        "Zed"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "bd682a62e0f0404aa2c058c8dd6f2a1a",
      "question": "What animal was found?",
      "prediction": "mammoth",
      "ground_truths": [
        "Columbian mammoth"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7bb016eec4214226bfb492489fca9cfe",
      "question": "What was found at a construction site?",
      "prediction": "\"Zed.\"",
      "ground_truths": [
        "\"Zed,\" a Columbian mammoth"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "48f4f88751c04d0c9809e86c21902c91",
      "question": "What part of the animal was especially interesting?",
      "prediction": "bone",
      "ground_truths": [
        "two complete, beautifully preserved tusks.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "866d127d05bf4f6981065c7d4a8142e2",
      "question": "Where can you find the La Brea tar pits?",
      "prediction": "Los Angeles.",
      "ground_truths": [
        "Los Angeles"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a57629ec5b2439594ebb8e1f89ac3ae",
      "question": "What was found in Los Angeles?",
      "prediction": "The mammoth's fossil",
      "ground_truths": [
        "Columbian mammoth fossil \"Zed.\""
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "6a117ca4fb504f389b8888df250f21eb",
      "question": "Where was the skeleton unearthed?",
      "prediction": "Los Angeles.",
      "ground_truths": [
        "a construction site in the heart of Los Angeles."
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "44f0cff0d54445d9888cefb0752b4d3f",
      "question": "Where was it found?",
      "prediction": "construction site in the heart of Los Angeles.",
      "ground_truths": [
        "Los Angeles."
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "b636f32bd23e46cd840fe8725a8e32c0",
      "question": "What was a rare occurrence?",
      "prediction": "to have these two complete, beautifully preserved tusks.\"",
      "ground_truths": [
        "Zed's tusks were found intact,"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "09397d660e8d46119fac2f6ad902a6e6",
      "question": "What did they call the most important?",
      "prediction": "discovery\"",
      "ground_truths": [
        "\"a whole new treasure trove of fossils\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2d1cd230a1d9409a9231d27d442b34c2",
      "question": "What is the disconvery called?",
      "prediction": "\"Zed,\"",
      "ground_truths": [
        "\"Zed.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e33b52e8485c41518aaf76f1048c35e4",
      "question": "What is the name of the movie that Crowe starred in?",
      "prediction": "\"State of Play\"",
      "ground_truths": [
        "\"State of Play\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "198f769edd5c4bd7b829ffeaa6d595c1",
      "question": "What was the box office debut for \"17 Again\"?",
      "prediction": "The high school time-warp comedy,",
      "ground_truths": [
        "$24.1 million,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f82125c096845fc8b5fc65418c486aa",
      "question": "What were the box office earnings?",
      "prediction": "$24.1 million,",
      "ground_truths": [
        "$12.3 million"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2cda8c267a624be184e87b1d2cd0b05c",
      "question": "Who stars in the movie?",
      "prediction": "Zac Efron",
      "ground_truths": [
        "Zac Efron"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fe0d714e8be546d994f3469fbc3bd068",
      "question": "What movie debuted at No. 1?",
      "prediction": "\"17 Again,\"",
      "ground_truths": [
        "\"17 Again,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "74518b730d2941859c8fc206f74de46a",
      "question": "Which film was at number two?",
      "prediction": "\"State of Play\"",
      "ground_truths": [
        "\"State of Play\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce5d0f11b9664510a27bf3c483792f13",
      "question": "What film debuted at number one at the box office?",
      "prediction": "\"17 Again\"",
      "ground_truths": [
        "\"17 Again,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "33313f986ec740f3b824bc3accb9111e",
      "question": "How much money did \"17 Again\"pull in?",
      "prediction": "$24.1 million,",
      "ground_truths": [
        "$24.1 million,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "29544317d3ab466d8b13e1eac4463c10",
      "question": "What were the box office earnings of the film at the top of the charts?",
      "prediction": "$24.1 million,",
      "ground_truths": [
        "$24.1"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a4c2d78a44cb4c65b98f86adebf8e4b5",
      "question": "Where did the bombing occur?",
      "prediction": "Peshawar, capital of North West Frontier Province.",
      "ground_truths": [
        "Pakistani city of Peshawar, capital of North West Frontier Province."
      ],
      "em": 0,
      "f1": 0.8235294117647058
    },
    {
      "id": "4d010f61af224f21bc9f870d5935aaf6",
      "question": "who is responsible for the bombing",
      "prediction": "Taliban militants.",
      "ground_truths": [
        "militants"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3d31f21644c548788fef6f077ae10d1f",
      "question": "What was the number of men involved?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc591d2c425c47d2914bcf642d6e412b",
      "question": "According to the Interior Minister, how many men plotted to carry out the attacks?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a4dc0db408144d23a01a2b5e6d6f1b44",
      "question": "What happened to the suspect?",
      "prediction": "arrested",
      "ground_truths": [
        "blew himself up."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64a62c4af43e49438c5f0c1c8a993ac1",
      "question": "How many people did the suicide bomber kill?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa8d8e1df27b48e187d9a8c95f42bf36",
      "question": "how many were involved",
      "prediction": "three men",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "61c0ad1bdfa54b6ca64936e0b1162737",
      "question": "Had the suspect blown himself up after running out of bullets?",
      "prediction": "ran out of",
      "ground_truths": [
        "of"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4973d06104ff42a48ec95b6d8266cffa",
      "question": "How many military officers were sentenced?",
      "prediction": "Sixteen",
      "ground_truths": [
        "Sixteen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9206f18617d749b09fc00c5f949e4a54",
      "question": "Who was known as the blonde angel of death?",
      "prediction": "Alfredo Astiz,",
      "ground_truths": [
        "Alfredo Astiz,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9a80717a88e74b44856439a7cc7e8eac",
      "question": "Where did the victims wait to hear the judge read the sentences?",
      "prediction": "inside a packed courtroom in downtown Buenos Aires,",
      "ground_truths": [
        "courtroom in downtown Buenos Aires,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "727106ef76a0404cbf006524c2e6529f",
      "question": "What number of military officers was sentenced for abuses?",
      "prediction": "Sixteen",
      "ground_truths": [
        "Sixteen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dde94541f9794aa4a55a19e0b20cf54f",
      "question": "Who is known as the Blonde Angel of Death?",
      "prediction": "Alfredo Astiz,",
      "ground_truths": [
        "Alfredo Astiz,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9d0acdc75fd74611860f42cbd35c0e91",
      "question": "Who waited outside the courthouse?",
      "prediction": "Thousands perished",
      "ground_truths": [
        "victims' family members and friends,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c01e2730fd0544988a6e4c5dc11e0734",
      "question": "Which party opposed the surtax?",
      "prediction": "Republicans",
      "ground_truths": [
        "Republicans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c4eb8e9e88a64fa49128ad4e99776c91",
      "question": "what would the bill provide",
      "prediction": "tax incentives for businesses",
      "ground_truths": [
        "a remedy to unemployment among veterans."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0c9774b9297641e39224f703c4e13ec6",
      "question": "who removed the surtax",
      "prediction": "Republicans",
      "ground_truths": [
        "Republicans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "144e38b6eea047c99e7ec1d8dbb6cbbc",
      "question": "What is controversial?",
      "prediction": "millionaire's surtax,",
      "ground_truths": [
        "millionaire's surtax,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8b256cf4c08f4507b8b2f96d7289b585",
      "question": "What criteria would be used to give tax incentives?",
      "prediction": "bringing together Republican and Democrat, House and Senate, congressional and administration ideas in putting this package forward.",
      "ground_truths": [
        "businesses hiring veterans as"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b33e0d818b364b87878b747b09987f28",
      "question": "What incentives are offered to hire veterans?",
      "prediction": "tax",
      "ground_truths": [
        "tax"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e6a17a43379a40d1a0388edd679e1fc3",
      "question": "Who opposes the surtax?",
      "prediction": "Republicans",
      "ground_truths": [
        "Republicans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "13c7ec87c0324be5bee05d51772027c5",
      "question": "What would be provided for those leaving the military?",
      "prediction": "job training",
      "ground_truths": [
        "job training"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "937709a34e1b44b487be29fdbd96c709",
      "question": "What are they waking up to?",
      "prediction": "it",
      "ground_truths": [
        "black is beautiful,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccdaabb0544c42e4b61b3efc59a1ca47",
      "question": "What brand did the girls wear?",
      "prediction": "J.Crew",
      "ground_truths": [
        "J.Crew"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ce8d7d648dd46d3ac228d1002fc64ac",
      "question": "Which website crashed?",
      "prediction": "J.Crew",
      "ground_truths": [
        "J.Crew"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3164fc40406b46299e4ae8f1c7d9bcf8",
      "question": "Like whose daughter does she look?",
      "prediction": "Sasha Obama.",
      "ground_truths": [
        "Obama."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7c41bc4cbdea419fb9fcebfbb4615ab8",
      "question": "Who does one young model look like?",
      "prediction": "Sasha Obama.",
      "ground_truths": [
        "Sasha"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b9d1e57a0992485f93d0d153df752959",
      "question": "What are marketers waking up to?",
      "prediction": "African-American models",
      "ground_truths": [
        "black is beautiful,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67580c657c3e42ae928091f7fdb6940a",
      "question": "What is beautiful?",
      "prediction": "black family",
      "ground_truths": [
        "black family"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "31e73b41abf14323a275438fa89ec36e",
      "question": "What wil the federal government do?",
      "prediction": "would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.",
      "ground_truths": [
        "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks."
      ],
      "em": 0,
      "f1": 0.9696969696969697
    },
    {
      "id": "08655f61d8a04962866a2cfd76710d6c",
      "question": "A judge ruled that how many Chinese Muslims must be released?",
      "prediction": "17",
      "ground_truths": [
        "17"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0168fac7c43d42b59e77d2c1f6a0c1e9",
      "question": "What is the federal government going to do?",
      "prediction": "17 detainees to appear in his Washington courtroom at 10 a.m. Friday and said he would hold a hearing next week",
      "ground_truths": [
        "immediate release"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da24c5186346416db01377a9fb9fdb96",
      "question": "What part of China are the detainees mostly from?",
      "prediction": "Muslim autonomous region in western",
      "ground_truths": [
        "western"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "41771f0c93994375956cde79ea9c4136",
      "question": "Who ruled they must be released?",
      "prediction": "A federal judge",
      "ground_truths": [
        "A federal judge"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "08b1f43797d5403baa51de8c8b298d6a",
      "question": "What action did the Federal government take?",
      "prediction": "A",
      "ground_truths": [
        "immediate release into the United States of 17 Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73dc231d896a4e13b9d6acaa98dd5e8f",
      "question": "What is the ethnicity of the 17 detainees?",
      "prediction": "ethnic Uighurs,",
      "ground_truths": [
        "Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "40700ca8ac7f425bbd23ab9943f843eb",
      "question": "what is your latest novel?",
      "prediction": "\"Rin Tin Tin: The Life and the Legend\"",
      "ground_truths": [
        "\"Rin Tin Tin: The Life and the Legend\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d19cfaa002124609a6dbed47adea985b",
      "question": "What is she drawn to?",
      "prediction": "obsession.",
      "ground_truths": [
        "to the idea of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "835a3a2903524ba1a9d9eea52f339d73",
      "question": "where is Her latest novel is a biography?",
      "prediction": "Rin Tin Tin,",
      "ground_truths": [
        "Rin Tin Tin,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7ae02f7a3e84e70b7522d91392dc247",
      "question": "What is Susan Orlean known for?",
      "prediction": "the masterful 1998 best-seller \"The Orchid Thief\"",
      "ground_truths": [
        "1998 best-seller \"The Orchid Thief\""
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "ad9c3f85a2a546998c8e27ac3395eca8",
      "question": "What is her latest novel?",
      "prediction": "\"The Orchid Thief\"",
      "ground_truths": [
        "\"Rin Tin Tin: The Life and the Legend\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "205530b4bd49448ab55e7cce2fcf6b00",
      "question": "What was Olson a member of?",
      "prediction": "Symbionese Liberation Army",
      "ground_truths": [
        "the self-styled revolutionary Symbionese Liberation Army"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "ff9e7d0ab1b046a99f617df0ad8c16b5",
      "question": "What did Olson belong to?",
      "prediction": "Symbionese Liberation Army",
      "ground_truths": [
        "Symbionese Liberation Army"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "22ea755508484f639d641a28f1bc01af",
      "question": "What did Olson do during the bank robbery?",
      "prediction": "\"entered the bank with a firearm and kicked a nonresisting pregnant teller in the stomach.",
      "ground_truths": [
        "with a firearm and kicked a nonresisting pregnant teller in the stomach."
      ],
      "em": 0,
      "f1": 0.9
    },
    {
      "id": "b344f9c87e2d4e5498338ea56023892c",
      "question": "Which King's palace?",
      "prediction": "Narayanhiti Royal",
      "ground_truths": [
        "Narayanthi Royal"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d1f6974f4b8f4a189e64cdf36c986df7",
      "question": "What is being converted into museum?",
      "prediction": "Narayanthi Royal Palace",
      "ground_truths": [
        "deposed king's opulent palace"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "68ac44d7d9e24aee8fcaa91e0f364062",
      "question": "When did the monarchy begin?",
      "prediction": "239 years of rule",
      "ground_truths": [
        "239 years of rule"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bb334d8819054ded95d25bd214397832",
      "question": "What kingdom was declared a republic?",
      "prediction": "Himalayan",
      "ground_truths": [
        "Nepal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "53600e1f83ab4c559e064fcac5f821bb",
      "question": "What was being ended?",
      "prediction": "monarchy.",
      "ground_truths": [
        "The monarchy's end after 239 years of rule"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6dd10f33c4f8423080bdcf84c8a01ef1",
      "question": "what happened in nepal",
      "prediction": "Nepal's new government",
      "ground_truths": [
        "new government has converted its deposed king's opulent palace into"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "9508601ca2724c259442daa574ec6e2f",
      "question": "Who will she play in the final?",
      "prediction": "Joan Rivers'",
      "ground_truths": [
        "Annie Duke"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "947f12b8aa8c4209a1472f10f2424a90",
      "question": "Who has won this season?",
      "prediction": "Joan Rivers'",
      "ground_truths": [
        "Joan Rivers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "682c7b2e730244809d631539f9bf69da",
      "question": "Who won this season's Celebrity Apprentice?",
      "prediction": "Joan Rivers'",
      "ground_truths": [
        "Joan Rivers'"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "be16a44a8c3f4e229442e38c9701ef74",
      "question": "What does the victory mean for the charity?",
      "prediction": "$250,000",
      "ground_truths": [
        "$250,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "455c1d1b4207431f8ff8db6ca4989d2a",
      "question": "How much does the Charity get?",
      "prediction": "$250,000",
      "ground_truths": [
        "$250,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eec978f73e724203b8b4974255dcfa4e",
      "question": "What does the victory mean?",
      "prediction": "$250,000 for Rivers' charity: God's Love We Deliver.",
      "ground_truths": [
        "$250,000 for Rivers' charity: God's Love We Deliver."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8efa4e3802454905a846335425e82876",
      "question": "Who wins this season's \"Celebrity Apprentice\"?",
      "prediction": "Joan Rivers'",
      "ground_truths": [
        "Joan Rivers'"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5190259794744c2bb57eb2ec83a7044b",
      "question": "Who goes up against poker champion Annie Duke in finale?",
      "prediction": "Donald Trump and Joan Rivers",
      "ground_truths": [
        "Joan Rivers"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "059ba875878347a6a578f79b78f09c28",
      "question": "Who attracts more celebrities?",
      "prediction": "Rivers",
      "ground_truths": [
        "Rivers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e45eb88379924f609da7299935497b64",
      "question": "What do they expect to raise?",
      "prediction": "up",
      "ground_truths": [
        "$50,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dbdd82848efd4be2b7afbc34e09b5576",
      "question": "Letters from whom are being auctioned?",
      "prediction": "Barbara Dainton-West,",
      "ground_truths": [
        "Barbara Dainton-West,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6505e3709d6a4b3eb880d1ea9247377c",
      "question": "What age was the 97 year old when the Titanic sank?",
      "prediction": "9-week-old",
      "ground_truths": [
        "9-week-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef6478513dd74b0a91e11ba14a4952ab",
      "question": "What year did the Titanic sink?",
      "prediction": "1912.",
      "ground_truths": [
        "1912."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c9b5f70b74384cdb89a8f83b5d82a275",
      "question": "Whom are the letters from?",
      "prediction": "the estate of Titanic survivor Barbara Dainton-West,",
      "ground_truths": [
        "Barbara Dainton-West,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "14702b4909444cf990223aebd601eed3",
      "question": "what is her survival story?",
      "prediction": "Titanic,",
      "ground_truths": [
        "lifted from the lifeboat onto Carpathia, the rescue ship, in a mail sack,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b904c9a67823413d910dc3245aa38164",
      "question": "What is the auction supposed to raise?",
      "prediction": "up to $50,000",
      "ground_truths": [
        "$50,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8244060627d4466ebb975089f7b231b5",
      "question": "What day was the American aid worker killed?",
      "prediction": "Wednesday.",
      "ground_truths": [
        "Wednesday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f7db4258609847718815614377635a9c",
      "question": "What nationality was the aid worker?",
      "prediction": "American",
      "ground_truths": [
        "American"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f961611cfd2d4098a1d13b8bc086b9bb",
      "question": "Where was the diplomat kidnapped?",
      "prediction": "northwest Pakistan",
      "ground_truths": [
        "near his home in Peshawar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f727854da0824a49a0125f8ce6a39de5",
      "question": "Who was kidnapped?",
      "prediction": "Heshmatollah Attarzadeh",
      "ground_truths": [
        "Heshmatollah Attarzadeh"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ec33ded837448e79f160a1287b7b41b",
      "question": "He was traveling from his home to what destination?",
      "prediction": "work at the Iranian consulate,",
      "ground_truths": [
        "Iranian consulate,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bfb29a9e3ea3412fb08f16040f6d37ce",
      "question": "Where was the American aid worker slain?",
      "prediction": "outside the Iranian consulate in Peshawar",
      "ground_truths": [
        "outside the Iranian consulate"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "c3b3caf88510477499d31a1e2081f9e3",
      "question": "Where is Peshawar?",
      "prediction": "Pakistan",
      "ground_truths": [
        "is the capital of Pakistan's North West Frontier Province,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25a75f71b62f4c9cab280dda8ac44613",
      "question": "Where was the bodyguard killed?",
      "prediction": "near his home in Peshawar",
      "ground_truths": [
        "home in Peshawar"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "b95ec30fdf864e6e9d204977c3426aef",
      "question": "Who is the kidnapped diplomat?",
      "prediction": "Heshmatollah Attarzadeh",
      "ground_truths": [
        "Heshmatollah Attarzadeh"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9100743ff9db4bbd8a4a415210d2b163",
      "question": "Where was the diplomat and his bodyguard killed at?",
      "prediction": "Pakistani policemen inspect a bullet-riddled car of a kidnapped Iranian",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5b28c7d93a7f4127b220b587537a53d6",
      "question": "where was this shooting?",
      "prediction": "Najaf's Adala neighborhood",
      "ground_truths": [
        "Najaf."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4100b5dc885d40898d7cf76134188ca0",
      "question": "Where was the attack?",
      "prediction": "Najaf.",
      "ground_truths": [
        "City"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5d8e875e536471eb6011003ce161c39",
      "question": "Did the Iraqi patrol come under fire?",
      "prediction": "attacked by small-arms, machine-gun and RPG",
      "ground_truths": [
        "Iraqi and U.S. soldiers were attacked by small-arms, machine-gun and RPG fire"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4b8b1d7070fa42268d219fe74316bfa8",
      "question": "what local indian official blames?",
      "prediction": "\"unnamed international terror group\"",
      "ground_truths": [
        "\"unnamed international terror group\" for the attack,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e1b81a04e32e4647b7fb66a70536fdc7",
      "question": "What do police impose?",
      "prediction": "a curfew",
      "ground_truths": [
        "curfew"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec480559c4764db5a0ca181e89bfaaa4",
      "question": "What did police suspect were used to carry the bombs?",
      "prediction": "newly-purchased bicycles",
      "ground_truths": [
        "bicycles"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7c758cd8261847cc80c73b534f761843",
      "question": "Was anyone wounded by the bombs?",
      "prediction": "more than 200.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da9083ce238648d09164693834e78137",
      "question": "What number of people are wounded?",
      "prediction": "more than 200.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c4f372d821e44cdd85dda6cb06f07d3d",
      "question": "what's the amount of peopole that wounded?",
      "prediction": "more than 200.",
      "ground_truths": [
        "more than 200."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8a224f7e8f194d13b127bbd05de5e273",
      "question": "Who is responsible for the bombing that killed 63 people?",
      "prediction": "\"unnamed international terror group\"",
      "ground_truths": [
        "\"unnamed international terror group\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c423ed161c034b12b0dc03d335f8e995",
      "question": "what police impose?",
      "prediction": "imposed a curfew in Jaipur",
      "ground_truths": [
        "curfew"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "54ccee8119e04360811e2aed351e6729",
      "question": "Where did police impose a curfew?",
      "prediction": "Jaipur",
      "ground_truths": [
        "Jaipur"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c7e6e2409cea4bd486b3c7821a188bd4",
      "question": "Whom does a local Indian official blame for the blasts?",
      "prediction": "an \"unnamed international terror group\"",
      "ground_truths": [
        "\"unnamed international terror group\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c2e66b663b45441ea9a9d8b3865639f7",
      "question": "What does the Bangladeshi army vow to do?",
      "prediction": "punish participants in this week's bloody mutiny,",
      "ground_truths": [
        "punish participants in this week's bloody mutiny,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8e1e606409b437eac9e6e88c9ca3470",
      "question": "How many bodies were recovered outside Dhaka?",
      "prediction": "More than 160 army officers",
      "ground_truths": [
        "88"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "096419c95ada496db118b0f52228fcd8",
      "question": "What country did this take place in?",
      "prediction": "Bangladesh",
      "ground_truths": [
        "Bangladesh"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fdb2b92b9cd74fb4a2c82bf4987657d8",
      "question": "Where were the people when the mutiny erupted?",
      "prediction": "inside the headquarters of the Bangladesh Rifles (BDR)",
      "ground_truths": [
        "inside the headquarters of the Bangladesh Rifles"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "1130ea0f1e4840d889ea305956bd0c5d",
      "question": "How many were in the Rifles headquarters when the mutiny started?",
      "prediction": "More than 160 army officers",
      "ground_truths": [
        "More than 160 army officers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "05f1edf5739f4b4ca3e0e533c1022fc3",
      "question": "How many bodies have been discovered?",
      "prediction": "More than 160 army officers",
      "ground_truths": [
        "88"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d30021dabd0a415b9ea1e2cbe57725f4",
      "question": "When did the mutiny occur?",
      "prediction": "Wednesday morning,",
      "ground_truths": [
        "Wednesday morning,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c930dbb4f888467398ea923ccbae92d7",
      "question": "what river is rising",
      "prediction": "Red",
      "ground_truths": [
        "Red"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "81b540cb16e540b3a95faba2f7d2857f",
      "question": "What is the river expected to crest at?",
      "prediction": "20 feet",
      "ground_truths": [
        "about 20 feet above flood stage."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d5d038576cc64dd6bdd1b5085f907a13",
      "question": "What river is quickly rising?",
      "prediction": "Red",
      "ground_truths": [
        "Red"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "97f11d0f20674a169eec6dbd832aacfb",
      "question": "What is Fargo preparing for?",
      "prediction": "floodwaters,",
      "ground_truths": [
        "near-record floodwaters,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "53c827a1b93d409c8b3f023472dc7b87",
      "question": "Who do forecasters predict?",
      "prediction": "the river will crest Saturday about 20 feet above flood stage.",
      "ground_truths": [
        "the river will crest Saturday about 20 feet above flood stage."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bb4218e344aa4716958a782ecf3d0015",
      "question": "when will it crest",
      "prediction": "Saturday",
      "ground_truths": [
        "Saturday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8a62183488d94fad91b27f15d35939fe",
      "question": "how many are without power",
      "prediction": "Thousands of customers",
      "ground_truths": [
        "Thousands of customers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "84adc9beb827473db0a3de0fde246902",
      "question": "When will the river crest?",
      "prediction": "Saturday",
      "ground_truths": [
        "Saturday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e614e0c6001c425f9eecd8d01c5da057",
      "question": "What were the deaths due to?",
      "prediction": "storm,",
      "ground_truths": [
        "the storm,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "87b53cb7f58349ef9de6f8a1033699c4",
      "question": "What may do what?",
      "prediction": "melt as soon as 2050,",
      "ground_truths": [
        "Glaciers in the European Alps"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d3d3a6d45102452993e2ad22b6e261d3",
      "question": "will the glaciers melt",
      "prediction": "some scientists say.",
      "ground_truths": [
        "may"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a24064f15c0c4365a7358f2816906914",
      "question": "What bugs are ruining the forest?",
      "prediction": "Herds of tiny pine beetles",
      "ground_truths": [
        "pine beetles"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "05c9c5f852f14a30ad866b3d517b6091",
      "question": "What is happening in Colorado?",
      "prediction": "Herds of tiny pine beetles are munching away at",
      "ground_truths": [
        "Herds of tiny pine beetles are munching away at"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5e12a545101d40f58842b623e28df786",
      "question": "When will the glaciers melt?",
      "prediction": "2050,",
      "ground_truths": [
        "as soon as 2050,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "ec7b492ce5d54f9f9b7dbf41e149313c",
      "question": "What punishment does she want her attacker to receive?",
      "prediction": "blind Majid Movahedi,",
      "ground_truths": [
        "an eye for an eye,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73897fac4ef14479b8efb8bcaad56eab",
      "question": "What happened to the woman?",
      "prediction": "acid attack",
      "ground_truths": [
        "acid attack"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "451089bab6df4914a385a9ceacb60f6d",
      "question": "Who wants \"eve for an eye\" punishment?",
      "prediction": "Ameneh Bahrami",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95c846a744a244d09a9e6440113637a8",
      "question": "Who disfigured by acid wins?",
      "prediction": "Ameneh Bahrami",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b209472f93cb447286295096a3f0ed6f",
      "question": "Who ordered blinding of Majid Movahedi?",
      "prediction": "Ameneh Bahrami",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f692b91ee3dc4bc0bbcc27b0814c1919",
      "question": "What was this woman blinded by?",
      "prediction": "acid",
      "ground_truths": [
        "acid attack"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1adadbe9d04843a483266128f2af67cb",
      "question": "Who was blinded?",
      "prediction": "Ameneh Bahrami",
      "ground_truths": [
        "Ameneh Bahrami"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60fb4646b8694ce49cf02c1eda1fb242",
      "question": "What is the name of her attacker?",
      "prediction": "Majid Movahedi,",
      "ground_truths": [
        "Majid Movahedi,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dadbf5853ac746239f484a0c7ff02245",
      "question": "What did Cannavaro test positive for?",
      "prediction": "the banned substance cortisone.",
      "ground_truths": [
        "cortisone."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a4c31c16dd1a4cf493b8c20500a88539",
      "question": "Who is the match against?",
      "prediction": "Ireland",
      "ground_truths": [
        "Cyprus"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1329f988ad3845129c304b54501c934c",
      "question": "Who will join the Italian Squad?",
      "prediction": "Cannavaro",
      "ground_truths": [
        "Fabio"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "427c5359e1bd4e2ca065c379a9c5d227",
      "question": "Cannavaro will join what squad?",
      "prediction": "Italian national",
      "ground_truths": [
        "Italian national"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "359f8256bb2b4d5589fa81885ef48257",
      "question": "Who tested positive for cortisone?",
      "prediction": "Cannavaro",
      "ground_truths": [
        "Cannavaro"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "574737b3f4284419a8e2bcf43167e253",
      "question": "Who has joined the italian squad?",
      "prediction": "Fabioo Cannavaro",
      "ground_truths": [
        "Cannavaro"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "73d58decb36a470e91d5491834972a95",
      "question": "What did he test positive for?",
      "prediction": "the banned substance cortisone.",
      "ground_truths": [
        "cortisone."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "bd70e61aec6e44b78f99a40ec3fc5e80",
      "question": "who has no plans to shoot rocket down?",
      "prediction": "U.S.",
      "ground_truths": [
        "The United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0f20e36d52924359a2d55b4d89ff5b40",
      "question": "What do North Koreans intend to launch?",
      "prediction": "a satellite.",
      "ground_truths": [
        "long-range missile"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8474a7dd84b47e29cac3fff91ee02e2",
      "question": "what are officials doing to respond?",
      "prediction": "raise the issue with the U.N. Security Council",
      "ground_truths": [
        "dissuade the North Koreans from going forward,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47996e3b9a744a8a8466cc117b02b270",
      "question": "who intend to launch a communications satellite?",
      "prediction": "North Korea",
      "ground_truths": [
        "North Korea"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c91eab99addd46a1bf4bf424d84e2afd",
      "question": "Who is the U.S. Secretary of State?",
      "prediction": "Hillary Clinton",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1ed16d36f0014fc09e8b76ea26ec7fe9",
      "question": "what is making us not shoot the rockets?",
      "prediction": "\"As the U.N. resolutions prohibit (North Korea) from engaging in ballistic missile activities,",
      "ground_truths": [
        "U.N. Security Council"
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "3fe6d86f0533409599c19272067c65f2",
      "question": "What does N. Korea put into position?",
      "prediction": "long-range missile",
      "ground_truths": [
        "missile"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7932c4f7532f4436a69d592a90f8dc78",
      "question": "what could launch either a warhead or a satellite?",
      "prediction": "North Korean missile",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "361aa33d63984db4a873b0616085fe57",
      "question": "What made him choose Cameroon?",
      "prediction": "Cameroonian President Paul Biya,",
      "ground_truths": [
        "AIDS and HIV"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d92f93c967248f2b391b8769cdc72e1",
      "question": "Who is visiting Cameroon?",
      "prediction": "Pope Benedict XVI",
      "ground_truths": [
        "Pope"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "4917dd8c60ac45e7a7ed07f84af0032b",
      "question": "What did he do in Africa?",
      "prediction": "arrived",
      "ground_truths": [
        "reiterated the Vatican's policy on condom use"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6499d7053acd45a7a8ca79b44e0c0bec",
      "question": "What is he visiting?",
      "prediction": "Africa",
      "ground_truths": [
        "Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c0ea03c08ec4af9a3cb897c11f0e0e4",
      "question": "Where did AIDS and HIV hit the hardest?",
      "prediction": "Sub-Saharan Africa",
      "ground_truths": [
        "Sub-Saharan Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bee6bd27c2e644d9b8c491d08fccbb3a",
      "question": "What is the current AIDS and HIV rate?",
      "prediction": "Sub-Saharan Africa has been hit harder by",
      "ground_truths": [
        "Nine out of 10 children"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dc7bb4c7ce2b4e07a85ed077fea5f775",
      "question": "What is the name of the Pope?",
      "prediction": "XVI",
      "ground_truths": [
        "Benedict XVI"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "cd7d9d4801f34e1495ceb2575429fd7d",
      "question": "Where did he arrive?",
      "prediction": "Africa",
      "ground_truths": [
        "Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a5502926815147f38b7ceb7dd0ca039a",
      "question": "How many extra people has Florida asked for Federal funding to house",
      "prediction": "10,000",
      "ground_truths": [
        "10,000 refugees,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0327e130d14548a08ba2764687c1743a",
      "question": "Are Haitians trying to cross into the U.S.?",
      "prediction": "have not intercepted any",
      "ground_truths": [
        "attempting illegal crossings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81cc91fcae584458a6c7e62c4261a9a6",
      "question": "What is the maritine traffic around Haiti like according to the USCG",
      "prediction": "normal",
      "ground_truths": [
        "normal maritime"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0a3512558e3a4fcc9cb10e2e76802eee",
      "question": "Where are Coast Guard?",
      "prediction": "Haiti,",
      "ground_truths": [
        "Haiti,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb4f693849d444c999342dfa7bc25764",
      "question": "Has the coast guard intercepted Haitians trying to cross illegally into US waters?",
      "prediction": "not intercepted any",
      "ground_truths": [
        "not"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5017529885c2433ba38ad514834d5adc",
      "question": "Since when did she have had clearance?",
      "prediction": "2004.",
      "ground_truths": [
        "2004."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f4bec4c857534e4a8a100e3c52e6e040",
      "question": "who led face transplant",
      "prediction": "Cleveland Clinic doctors",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29e2b15ab5c243208dcf0cacd969290f",
      "question": "Who led the face transplant team?",
      "prediction": "Doctors Frank A. Papay,",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d2944a1f990b4c85ac818037f926742b",
      "question": "Who has had clearance to perform full facial transplant since 2004?",
      "prediction": "Siemionow",
      "ground_truths": [
        "Dr. Maria Siemionow,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a93ae6ded2e4413baabb2502e0218a35",
      "question": "What kind of team did she lead?",
      "prediction": "eight surgeons",
      "ground_truths": [
        "plastic surgery"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cd81079cfdd843c2b3bec6ee865747f9",
      "question": "What was reportedly transplanted?",
      "prediction": "nose, cheeks, upper jaw",
      "ground_truths": [
        "nose, cheeks, upper jaw and facial tissue from a female"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "1cdabac233b94436b51ba5e1cee7affd",
      "question": "How many percent of trauma patient's face was reportedly transplanted?",
      "prediction": "80",
      "ground_truths": [
        "80"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b27ebf1694c348eea8d7f2b9cb5dfbf1",
      "question": "how much was transplanted",
      "prediction": "80 percent of a woman's face",
      "ground_truths": [
        "80 percent of a woman's face"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0e0e203984c14621a53fabe63c820df8",
      "question": "who got clearance",
      "prediction": "Siemionow",
      "ground_truths": [
        "Siemionow"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "73134003873744b3a6e20f3620eed515",
      "question": "What was Miami's reputation in the 80's?",
      "prediction": "glamorous and hedonism",
      "ground_truths": [
        "for glamour and hedonism came with a side order of carjackings and gangland shootouts."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "2fb53cfd5f7d4d2c8fec50ffecd35a06",
      "question": "What reputation did Miami shed?",
      "prediction": "the 80s",
      "ground_truths": [
        "much of its unwanted baggage from the 80s"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "a01a2fedcb1b4e23889682462a6780d3",
      "question": "What reputation has Miami shed?",
      "prediction": "much of its unwanted baggage",
      "ground_truths": [
        "glamour and hedonism"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d113fa2d3c2345558c1c57e8956bf36e",
      "question": "What is a part of modern Miami?",
      "prediction": "Art Basel",
      "ground_truths": [
        "food, music, culture and language of Latin America"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c3707a187594231945d865cbffb4fa8",
      "question": "How many of the victims were prostitutes?",
      "prediction": "five",
      "ground_truths": [
        "six"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962be7c101cc435eb0f1851dd0e73f07",
      "question": "How long has the suspect has avoided arrest ?",
      "prediction": "23 years.",
      "ground_truths": [
        "23 years."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3b9a8e9ebfeb4a76931188f5fd249cc2",
      "question": "What police chief said,Investigation will require tips from the community ?",
      "prediction": "Edward Flynn,",
      "ground_truths": [
        "Edward Flynn,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "55268af5e30a4c1e87552207e7005b73",
      "question": "Is the suspect in the DNA database?",
      "prediction": "has never been arrested for a felony as he does not appear in any",
      "ground_truths": [
        "database,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e0eb1f767c8a4ed094393acc6901f990",
      "question": "how many people did he kill?",
      "prediction": "at least seven",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "bd70bd2ed231441798e1118384f67db2",
      "question": "Who matched  unknown man's DNA with seven slain women ?",
      "prediction": "Law enforcement officials in the Wisconsin city",
      "ground_truths": [
        "Law enforcement officials in the Wisconsin city"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3cb379cc531f4a468e254cadcb629ba5",
      "question": "what is the victims name?",
      "prediction": "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior.",
      "ground_truths": [
        "Debora Harris, Joyce Mims, Tonya Miller, Quithreaun Stokes, Sheila Farrior."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45769852c71b4d0498f41be9a1d7eb62",
      "question": "How many women were slain?",
      "prediction": "five victims",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55bc45f2e63149869e4d0e52da6cb421",
      "question": "who designed the vessel",
      "prediction": "Wally",
      "ground_truths": [
        "super-yacht designers Wally"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "f61206d61cf94401b8fbb566b0b67ec7",
      "question": "Who designed the 99-meter vessel?",
      "prediction": "Wally",
      "ground_truths": [
        "super-yacht designers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb0bceec8bca491db5c90c3962960eea",
      "question": "what is still in the design stage",
      "prediction": "'Wally Island'",
      "ground_truths": [
        "Wally Island:"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e23472157c0d4ebe81a0fa5c027b3359",
      "question": "What can the deck be converted into?",
      "prediction": "such features as a full garden and pool, a tennis court, or several heli-pads.",
      "ground_truths": [
        "a full garden and pool, a tennis court, or several heli-pads."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "0af0869096df4dff8325c3f2ff584087",
      "question": "What does the deck contain?",
      "prediction": "full garden and pool, a tennis court, or several heli-pads.",
      "ground_truths": [
        "full garden and pool, a tennis court,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "c754c326486641409687edae575f81ee",
      "question": "How long is the yacht?",
      "prediction": "100 meter",
      "ground_truths": [
        "100 meter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a4e339480574584b91ba856c053fced",
      "question": "what told german chancellor",
      "prediction": "of Germany's \"Holocaust shame,\"",
      "ground_truths": [
        "Israel of Germany's \"Holocaust shame,\""
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "5ca535efd2994e87a2a3d6e63d00b1f1",
      "question": "who is angela merkel",
      "prediction": "German Chancellor",
      "ground_truths": [
        "German Chancellor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3cc3e6da95314c41b64f84c32602f347",
      "question": "Merkel said she supports what solution to the Mideast conflict?",
      "prediction": "two-state",
      "ground_truths": [
        "further sanctions on Iran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3349542eaf3543508ecc4b365b866099",
      "question": "Who told the Knesset of Germany's \"Holocaust shame\"?",
      "prediction": "Merkel",
      "ground_truths": [
        "German Chancellor Angela Merkel"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "4066ef1e0dba4abd92bf1b2a98754d63",
      "question": "Angela called on Iran to prove what?",
      "prediction": "to the world that it does not want the nuclear bomb.\"",
      "ground_truths": [
        "that it does not want the nuclear bomb.\""
      ],
      "em": 0,
      "f1": 0.8750000000000001
    },
    {
      "id": "aaf15d0aa8364c8ebbce91a2d699ef08",
      "question": "Who became the first German chancellor to address Israeli parliament?",
      "prediction": "Angela Merkel",
      "ground_truths": [
        "Angela Merkel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1af22045cbd249e99d4961fe2ec40c6b",
      "question": "Merkel said she supports what in the Middle East conflict?",
      "prediction": "support for the Jewish state",
      "ground_truths": [
        "the two-state solution to the Mideast"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21e3cc9a483347919e816f0bacf6aec1",
      "question": "what merkel said",
      "prediction": "Germany's \"Holocaust shame,\"",
      "ground_truths": [
        "\"The Holocaust fills us with shame,\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "ff093c58236e4045aadf6741032cdbf7",
      "question": "For what reason did police visit the actor's home?",
      "prediction": "civil disturbance",
      "ground_truths": [
        "civil disturbance call,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "c3464ea46cfa4e228dbc3acecd3c6fb2",
      "question": "Where was Gary Coleman arrested?",
      "prediction": "Utah jail",
      "ground_truths": [
        "Santaquin City, Utah, home"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "4e4c75ad37f14cdfb607fb967fdafd7e",
      "question": "What did he do to get in jail if no charges were filed?",
      "prediction": "posted a $1,725 bail,",
      "ground_truths": [
        "outstanding"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c5b4234bb04478e8ffc466b63c6c641",
      "question": "Is it a felony or misdemeanour?",
      "prediction": "failure to appear in court warrant",
      "ground_truths": [
        "misdemeanor"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f29a05ad0c5444b8a6a88ff73dfe84aa",
      "question": "Warrant from what incident?",
      "prediction": "domestic violence",
      "ground_truths": [
        "a domestic violence case,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "4d31b962e2aa4ff194eb5234b796dff0",
      "question": "What amount bail must Coleman post?",
      "prediction": "$1,725",
      "ground_truths": [
        "$1,725"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b189954307e7414b9d81ad99c7b730bf",
      "question": "What did India estimate?",
      "prediction": "70,000 or so are estimated to be there now.",
      "ground_truths": [
        "\"Estimates on the number of civilians trapped vary, but 70,000 or so are estimated to be there now."
      ],
      "em": 0,
      "f1": 0.6923076923076924
    },
    {
      "id": "64215d84427e4d709e2914e76df055c5",
      "question": "What do the Tamil Tigers want?",
      "prediction": "\"release\" civilians,",
      "ground_truths": [
        "an independent homeland for the country's ethnic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8d3d4915ed104234bca2e7e8eb591d6e",
      "question": "What is India urging Tamil Tiger rebels to do?",
      "prediction": "\"release\" civilians,",
      "ground_truths": [
        "\"release\" civilians,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f20bdc9681624087af1dda063b3a6945",
      "question": "Number India estimates are trapped in the region by the conflict?",
      "prediction": "70,000",
      "ground_truths": [
        "70,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f22c94617c404f388ea469654e9a580f",
      "question": "How many are estimated to be trapped?",
      "prediction": "70,000",
      "ground_truths": [
        "vary, but 70,000 or so are"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "65e2cb72034947f0ad2e4fbae41a103e",
      "question": "What does India urge?",
      "prediction": "Sri Lanka's Tamil rebels to \"release\" civilians,",
      "ground_truths": [
        "urged Sri Lanka's Tamil rebels to \"release\" civilians,"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "14c07142ff80440483ee9fdc0f2d8b06",
      "question": "What is happening to 70,000 people?",
      "prediction": "leftover civilians trapped",
      "ground_truths": [
        "trapped"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "b4aa0f16d9604c36980b576421ac9df5",
      "question": "Who did India urge to allow civilians to leave?",
      "prediction": "Sri Lanka's Tamil rebels",
      "ground_truths": [
        "Sri Lanka's Tamil rebels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc27edf1253345dabb1ae19ef7d8a829",
      "question": "What other country is communicating with the Tamil Tigers?",
      "prediction": "India",
      "ground_truths": [
        "India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a77bad2720ec43aba758a5c99624b6c3",
      "question": "Who locked in battle?",
      "prediction": "Government troops and the Tamil Tigers",
      "ground_truths": [
        "Government troops and the"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "ac318d10625648128b8eb68051609f14",
      "question": "When did the crash happen",
      "prediction": "Monday.",
      "ground_truths": [
        "Monday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4054abd26fcb4958b130fe94dda895ee",
      "question": "Who severely bruised her arm?",
      "prediction": "Lindsey Vonn",
      "ground_truths": [
        "Lindsey Vonn"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eaeb8739f0bc4e37af026660abd61cd1",
      "question": "What is the point lead",
      "prediction": "581",
      "ground_truths": [
        "581"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eb2fd89fd658434d97ecf961b12aed59",
      "question": "Who retains the lead?",
      "prediction": "Lindsey Vonn",
      "ground_truths": [
        "Vonn"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9ed3661a6d2945fbb92425ece8bc177e",
      "question": "where this world cup happened",
      "prediction": "Lienz",
      "ground_truths": [
        "Lienz"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc947534038744198b74eab0dc89a6fe",
      "question": "Who is the overall world cup leader?",
      "prediction": "Lindsey Vonn",
      "ground_truths": [
        "Vonn"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "90df72b6413b403f9be47b068768d92d",
      "question": "When was the crash",
      "prediction": "Friday",
      "ground_truths": [
        "in a freak accident in qualifying for Saturday's Hungarian Grand Prix."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dba999234e0a45f5b4c1cf9f5a0d8711",
      "question": "what is massa's condition",
      "prediction": "injured",
      "ground_truths": [
        "has improved in the past 24 hours with doctors taking him out of a medically-induced coma"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa3c208ecd8b4ae9bddfdae7ac243278",
      "question": "Name the short-term replacement for Felipe Massa",
      "prediction": "Michael Schumacher",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5734b464d1a1441bb4fa7071bd511c3f",
      "question": "What champion may return to Formula One",
      "prediction": "Michael Schumacher",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "52055b058df944c1b0d75a699cc5aa20",
      "question": "what is the name of the injured driver?",
      "prediction": "Felipe Massa.",
      "ground_truths": [
        "Felipe Massa."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f64d4bcaf1b4c17a25ee8a58fbd3e34",
      "question": "who could replace massa",
      "prediction": "Schumacher",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "068ff9a7ec8f43959e260f6986ad7059",
      "question": "what league is Shumacher returning to?",
      "prediction": "Formula One",
      "ground_truths": [
        "Formula"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e305beef690942e68bef0f6d6cca79cd",
      "question": "What did Williams do?",
      "prediction": "killed a man, the latter cheated on his wife.",
      "ground_truths": [
        "fatally shooting a limo driver"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca7e95d6dba546ff8507c9b08363d65e",
      "question": "Williams has been sentenced for how many years?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4202629286034d6ea50274ca3efdb911",
      "question": "How many years was Williams sentenced for?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0f713617b12d4ad3a9a5478775900cbf",
      "question": "How many were legally wrong?",
      "prediction": "one",
      "ground_truths": [
        "one"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03a537e991514cb98dee6d3aeaf4ba04",
      "question": "What was Williams' crime?",
      "prediction": "fatally shooting a limo driver",
      "ground_truths": [
        "fatally shooting a limo driver"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "848a7c4fbc474aeca0cfe9b00b765f0c",
      "question": "How many men are  legally wrong?",
      "prediction": "two",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e77e1fe488745f8bc371cb83fa07bc4",
      "question": "How many men are morally wrong?",
      "prediction": "two",
      "ground_truths": [
        "Both"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "48b0344f035e40e793cb0fdf88ae94e1",
      "question": "Who did the U.N. accuse of actions amounting to war crimes?",
      "prediction": "both sides",
      "ground_truths": [
        "Jewish state and the Palestinian Hamas movement"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ae2e3779eb6406da8b25d8e62037b5c",
      "question": "Which body has written a report?",
      "prediction": "United Nations",
      "ground_truths": [
        "commission chaired by South African Judge Richard Goldstone"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "987810b275d34e78a80f1bf73e0619c9",
      "question": "What justified military actions in the 2009 Gaza offensive?",
      "prediction": "report",
      "ground_truths": [
        "stopping militant rocket fire into Israel."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e003871051e146b68946ad760e6987c3",
      "question": "When was the Gaza offenswive?",
      "prediction": "just over a year ago.",
      "ground_truths": [
        "just over a year ago."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "69edaef722274f1ea6eb6885e57ffd20",
      "question": "What did UN call for both sides?",
      "prediction": "conduct independent inquiries to examine charges that both sides committed war crimes during the course of the Gaza war fought just over a year ago.",
      "ground_truths": [
        "to conduct independent inquiries to examine charges that"
      ],
      "em": 0,
      "f1": 0.4666666666666667
    },
    {
      "id": "0c64ef2bbeb543bf87efbb0b2617f2b1",
      "question": "How many lashes will the Nigeria footballer receive?",
      "prediction": "40",
      "ground_truths": [
        "40"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e3fe3ef5daa6485a8a3f91dfccc9d797",
      "question": "What could Stephen Worgu receive?",
      "prediction": "40 lashes",
      "ground_truths": [
        "40 lashes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60438978506949e79645c14afd3a21dd",
      "question": "What is illegal in the Muslim north of Sudan?",
      "prediction": "Alcohol",
      "ground_truths": [
        "drinking alcohol"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "161fd2e6343d49b7a3611066d7a8a805",
      "question": "Where is alcohol illegal?",
      "prediction": "Muslim north of Sudan",
      "ground_truths": [
        "Muslim north of Sudan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6f8f3d49dd8845de8307ad2d9fabf269",
      "question": "Who was appealed against punishing handed out to Worgu?",
      "prediction": "The forward's lawyer",
      "ground_truths": [
        "The forward's lawyer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "99429dc4d1044cb49e70ceb6f3914e07",
      "question": "What has the lawyer for Al Merreikh appelaed?",
      "prediction": "against the punishment",
      "ground_truths": [
        "the player"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f86d2b3d690a47bea80091f88a1758b3",
      "question": "Nigeria footballer Stephen Worgu could receive what?",
      "prediction": "40 lashes",
      "ground_truths": [
        "40 lashes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "99798459730c4bc988c9f665fa0fd7df",
      "question": "What has the lawyer appealed against?",
      "prediction": "the punishment for the player",
      "ground_truths": [
        "the punishment for the player"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e46e0220a18483fb0fd448bcc36815e",
      "question": "What liquid is illegal North of Sudan",
      "prediction": "Alcohol",
      "ground_truths": [
        "Alcohol"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d37cc022c8344772b7d7261033a37c29",
      "question": "what did the doodle show",
      "prediction": "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.",
      "ground_truths": [
        "some of the most gigantic pumpkins in the world,"
      ],
      "em": 0,
      "f1": 0.5384615384615384
    },
    {
      "id": "b83b50ae80c3463aa27b05bb0bf4b493",
      "question": "how much did they weigh",
      "prediction": "well over 1,000 pounds).",
      "ground_truths": [
        "well over 1,000 pounds)."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "76f360f25ece4e449c4682de553cee3f",
      "question": "What does the video show the emplees doing?",
      "prediction": "caring about Halloween",
      "ground_truths": [
        "carving a pumpkin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d531ea0078248ebb3dd503bb338071f",
      "question": "What's the doodle of?",
      "prediction": "the most gigantic pumpkins in the world,",
      "ground_truths": [
        "fascinating transformation that takes place when carving a pumpkin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "319e4521a8454f6a908b91d874ce2e08",
      "question": "What does the video show?",
      "prediction": "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.",
      "ground_truths": [
        "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95a04d9fc49441cd901191b2390f05cd",
      "question": "What weight are the pumpkins?",
      "prediction": "well over 1,000 pounds).",
      "ground_truths": [
        "(some weighing well over 1,000 pounds)."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "c2bf5fa95d404050b8f86692d9100be0",
      "question": "What is the weight of some of the pumpkins?",
      "prediction": "well over 1,000 pounds).",
      "ground_truths": [
        "over 1,000 pounds)."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "581d677951c7404d922d56e144504b82",
      "question": "What is the theme on Google?",
      "prediction": "Halloween",
      "ground_truths": [
        "Halloween"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "89971aa59e4746938e2ceb7b1ee84c0a",
      "question": "Where did the flight originate?",
      "prediction": "Tehran,",
      "ground_truths": [
        "Tehran, IRNA"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d2b2c9929c364bf6b0ed1567c4a73534",
      "question": "in which city fire happened?",
      "prediction": "Mashhad",
      "ground_truths": [
        "Mashhad"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "96cd08874f4d48c09e4ea6c90e16349d",
      "question": "where was the flight coming from",
      "prediction": "Tehran,",
      "ground_truths": [
        "Tehran,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0846ed359a9f4778ad82790f63e8de93",
      "question": "how many people killed?",
      "prediction": "17",
      "ground_truths": [
        "at least 17"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "453a4ebc18724109b77caf26eb14465b",
      "question": "What skid off the runway?",
      "prediction": "A passenger plane",
      "ground_truths": [
        "Aryan Airlines Flight 1625"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2e576cca2f10440cadcf63d44340d580",
      "question": "how many passengers on board in flight?",
      "prediction": "150",
      "ground_truths": [
        "150"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ea341dc6868e4322b279688f0eff7ac7",
      "question": "Where did the plane catch on fire?",
      "prediction": "airport",
      "ground_truths": [
        "Iranian city of Mashhad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8acc6ebd692d495eb4ad37b3045f257f",
      "question": "How many passengers were on board?",
      "prediction": "150",
      "ground_truths": [
        "150"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8f0cfcb0d46d43eeb6c7ee04cd29c027",
      "question": "Number of Blackhawk helicopters that crashed?",
      "prediction": "Two",
      "ground_truths": [
        "Two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f1cd28d437e44ac586e3494371c55ff1",
      "question": "What did the Interior Ministry official say?",
      "prediction": "U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.",
      "ground_truths": [
        "a U.S. helicopter crashed in northeastern Baghdad as"
      ],
      "em": 0,
      "f1": 0.6086956521739131
    },
    {
      "id": "9d3dd78635084be1bb29a74d6ae963f6",
      "question": "In what city were the helicopters landing?",
      "prediction": "Baghdad,",
      "ground_truths": [
        "Baghdad,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "beefda6210c047899b54a61290a9837c",
      "question": "What kind of helicopters crashed?",
      "prediction": "UH-60 Blackhawk",
      "ground_truths": [
        "UH-60 Blackhawk"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e8ac0b2d2c448099dea09b0b88032b6",
      "question": "What crashed while landing in Baghdad?",
      "prediction": "Two UH-60 Blackhawk helicopters",
      "ground_truths": [
        "Two UH-60 Blackhawk helicopters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2d006592676144faac44e16423bfb178",
      "question": "What type of helicopters crashed?",
      "prediction": "UH-60 Blackhawk",
      "ground_truths": [
        "UH-60 Blackhawk"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1028f19198147868af85aa60943519d",
      "question": "Who was among the injured?",
      "prediction": "Two U.S. troops and two Iraqi soldiers",
      "ground_truths": [
        "Two U.S. troops and two Iraqi soldiers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "787d235e4c2b4d89b4d9aebff5779cf6",
      "question": "Which country's Interior Ministry was quoted?",
      "prediction": "Iraqi",
      "ground_truths": [
        "Iraqi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "831bd2a4a96f4496947a5656ac12135d",
      "question": "What did the Interior Ministry say?",
      "prediction": "U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen.",
      "ground_truths": [
        "that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.S.-backed Iraqi forces and gunmen."
      ],
      "em": 0,
      "f1": 0.9696969696969697
    },
    {
      "id": "99a7d09d04634cc08435d1e4bde155b5",
      "question": "Where were the helicopters trying to land in Iraq?",
      "prediction": "northern Baghdad,",
      "ground_truths": [
        "Baghdad,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d809c82de0cc41a391a8476fbedc4638",
      "question": "How many Iraqi soldiers were killed?",
      "prediction": "Two",
      "ground_truths": [
        "one"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b7ccf71e49054da2a836cbd7dcca51ae",
      "question": "How many people were killed in this incident?",
      "prediction": "one",
      "ground_truths": [
        "one"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91e7485f2caf4a2fb0099f4c75b5095e",
      "question": "1907 ball was covered with how many bulbs?",
      "prediction": "100 light bulbs.",
      "ground_truths": [
        "100"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c1397fb85c744106923a646a76576d60",
      "question": "who many people were expected to attend?",
      "prediction": "a million",
      "ground_truths": [
        "about a million"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "125351aec7474e38b01e13323598d810",
      "question": "what kind of power usage is it?",
      "prediction": "LEDs will use the same amount of electricity as about 10 toasters,",
      "ground_truths": [
        "energy-efficient light-emitting diodes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c16cdeb37272460f86fbd392245ce63f",
      "question": "What killed the woman?",
      "prediction": "insurgents small arms fire.",
      "ground_truths": [
        "crossfire by insurgent small arms fire,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "14d8b36c4c854cee90760516d799d7c3",
      "question": "Where did NATO say 12 militants died?",
      "prediction": "Afghanistan,",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "430fcb6dfaaf44b48f79b64f77fa8a09",
      "question": "by whom was woman killed according to nato?",
      "prediction": "U.S. Marines",
      "ground_truths": [
        "militants,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f231490345764c8ca9cf74cf86fdfbe5",
      "question": "Who was investigating the woman's death?",
      "prediction": "Afghan National Security Forces",
      "ground_truths": [
        "Afghan National Security Forces"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68f96b0c4d124a7e87e12b96e02bed15",
      "question": "Where was the battle?",
      "prediction": "Hulmand province",
      "ground_truths": [
        "Now Zad in Helmand province, Afghanistan."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "eac0e28fcd9146678e656bd4a88eda04",
      "question": "What is stretched to the limit?",
      "prediction": "resources",
      "ground_truths": [
        "resources"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e3f4fb0782c04e9bb5f1cb09103e9b3c",
      "question": "Which shelter has been killing the horses?",
      "prediction": "Lifesavers Wild Horse Rescue,",
      "ground_truths": [
        "Lifesavers Wild"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "09400b686045458e93e51f71357024d3",
      "question": "For what reason are the horses being abandoned?",
      "prediction": "due to the economy",
      "ground_truths": [
        "\"People have lost their homes, their jobs, their hope,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7f9035673c548bb8854b64481d758c6",
      "question": "What organization ordered warning to be put on certain antibiotics?",
      "prediction": "The FDA",
      "ground_truths": [
        "U.S. Food and Drug Administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d977e5ba1134167a59c03b6adc78b16",
      "question": "What box will alert patients of possible tendon problems?",
      "prediction": "antibiotic",
      "ground_truths": [
        "\"black box\" label warning"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c961ca099aa4c18a45b492025dc071d",
      "question": "Which drugs are included in the black box labeling?",
      "prediction": "Cipro",
      "ground_truths": [
        "include Cipro, Levaquin, Avelox, Noroxin and Floxin."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "8639290204ee4b349154594e72774990",
      "question": "What has the FDA ordered?",
      "prediction": "makers of certain antibiotics to add a \"black box\" label warning -- the",
      "ground_truths": [
        "add a \"black box\" label warning"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5138cd8c69c74702a6a6bd9f88ab354e",
      "question": "What other drug is relevent besides  Cipro, Levaquin, Avelox and Noroxin?",
      "prediction": "fluoroquinolone",
      "ground_truths": [
        "Floxin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7c7410c8df1f4ca7a61a17d4a9bcda59",
      "question": "What does the \"black box\" label do?",
      "prediction": "warning",
      "ground_truths": [
        "alert patients of possible tendon ruptures and tendonitis."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9fac8c8a718346b6b4f1fc4d10a6f2e1",
      "question": "What order is the consumer group happy about?",
      "prediction": "FDA's",
      "ground_truths": [
        "put the \"black box\" warning on Cipro and other fluoroquinolones,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "922195399a674a88909c8d7d4bb22654",
      "question": "What drugs were included in the FDA warning to put the strongest warning on their labels?",
      "prediction": "\"black box\"",
      "ground_truths": [
        "Cipro, Levaquin, Avelox, Noroxin and Floxin."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eac66451df5e4241bb6b7f195d5a6cb5",
      "question": "What does the black box label mean?",
      "prediction": "the FDA will require a \"black",
      "ground_truths": [
        "to alert patients of possible tendon ruptures and tendonitis."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8364ad576679429cae578898bcaa31be",
      "question": "Do groups believe that the FDA has done enough?",
      "prediction": "the FDA must do.\"",
      "ground_truths": [
        "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cef8c20feb547078f9f6aa19ef33d42",
      "question": "What is the cause of the tendon problems?",
      "prediction": "the antibiotic",
      "ground_truths": [
        "Cipro"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "843521ec385d4ca797c3a37842424dae",
      "question": "What is the prosecutor accused of?",
      "prediction": "corruption",
      "ground_truths": [
        "buckling under pressure from the ruling party."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c00bac5c39534761b85d1adeea7bdd23",
      "question": "Name the president of the ANC?",
      "prediction": "Jacob Zuma",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2c278dfb89b444389ac7fa574c14697",
      "question": "What is the name of  the most powerful party in South Africa?",
      "prediction": "African National Congress",
      "ground_truths": [
        "African National Congress"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba52569b388b4b339129a0277c7eb2e8",
      "question": "Who is ANC president?",
      "prediction": "Jacob Zuma",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e53753e6f784486091c8dce069d90994",
      "question": "Zuma is president of what party?",
      "prediction": "African National Congress",
      "ground_truths": [
        "African National Congress"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ff383ab32c9942908db5946a8fe654d3",
      "question": "Who denied claims of accepting bribes, money laundering, among others?",
      "prediction": "Jacob Zuma",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc27c4c56b014020bd9e8a799860b06e",
      "question": "What is the name of the president of the ANC?",
      "prediction": "Jacob Zuma,",
      "ground_truths": [
        "Jacob Zuma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebe6da22787d42bab5690387505c2674",
      "question": "When is the vote expected?",
      "prediction": "April 22.",
      "ground_truths": [
        "April 22."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a36c55cdd449476f90afb817a588273a",
      "question": "Were there any fatalties reported?",
      "prediction": "had been",
      "ground_truths": [
        "no fatalities had been"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0b677213a397416480d12c20369f8da6",
      "question": "Who issued the original recall?",
      "prediction": "the U.S. Department of Agriculture",
      "ground_truths": [
        "JBS Swift Beef Company, of Greeley, Colorado,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "a5870a48cfa84c7e9cfa52794214ffd1",
      "question": "Where is the company located?",
      "prediction": "Colorado,",
      "ground_truths": [
        "Greeley, Colorado,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "cd259ac2833f49f39de5b1904e515d11",
      "question": "What was the beef affected by?",
      "prediction": "E. coli",
      "ground_truths": [
        "E. coli"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "04d4f617361a4901a70eda7bc93936b5",
      "question": "Who is the USDA spokesman?",
      "prediction": "Brian Mabry",
      "ground_truths": [
        "Brian Mabry"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c045bb738b404d33bae09e79d127ac17",
      "question": "What company is involved in this recall?",
      "prediction": "JBS Swift Beef",
      "ground_truths": [
        "JBS Swift Beef"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d66ab87ca38642a8ad7e469f0974d04c",
      "question": "How many pounds of meat were recalled?",
      "prediction": "41,280",
      "ground_truths": [
        "41,280"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "586aa6e73def460c94ba612b73b94990",
      "question": "What did the spoke's person say about the recall?",
      "prediction": "\"Each of our customers will be personally informed of this",
      "ground_truths": [
        "his company's products may have had nothing to do with the outbreak."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ecd82d14cb3e4e28b8307e44f4784ff5",
      "question": "is Child actor best known for Diff'rent Strokes?",
      "prediction": "Coleman is",
      "ground_truths": [
        "Gary Coleman"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "bfb2c3311c5d45789500d104b3965147",
      "question": "What are the names of the cast members of the TV show \"Diff'rent Strokes\"?",
      "prediction": "Gary Coleman",
      "ground_truths": [
        "Gary Coleman"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a1b8855de6ff4bf2b8afc5942486e88c",
      "question": "Will Hospital give details on his illness?",
      "prediction": "would not release any other information.",
      "ground_truths": [
        "would not release any other information."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7f9ec97994d42c3bf9dfc7e853a3d66",
      "question": "What did hospital say about Gary Coleman's condition?",
      "prediction": "is in critical",
      "ground_truths": [
        "in critical"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "1854615c2d334e5aa3a608b90f0547b5",
      "question": "What is the name of the hospital where Gary Coleman was admitted?",
      "prediction": "Provo, Utah,",
      "ground_truths": [
        "Utah Valley Regional Medical Center,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "f726d61075424d38b751748553a88ced",
      "question": "who is giving a speech?",
      "prediction": "No. 2 man",
      "ground_truths": [
        "Democratic VP candidate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aee05b8aa4e046ff8de171c4f0baed09",
      "question": "What did Barack Obama's campaign do?",
      "prediction": "sent an e-mail",
      "ground_truths": [
        "sent an e-mail to reporters Wednesday with the subject line \"Vice presidential"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "bfd989110c54493bbb9a628ec703df9a",
      "question": "Who is considered top tier contenders?",
      "prediction": "Sen. Joe Biden",
      "ground_truths": [
        "Biden, Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "725a2fdd55e44ad28dda4216e620f070",
      "question": "What day does Obama have a big campaign event?",
      "prediction": "Saturday.",
      "ground_truths": [
        "Saturday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "11caee047c7c458da0dc2cc880574171",
      "question": "who has a big event?",
      "prediction": "Barack Obama",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "538a1322b4e84194b82f2614c9a2ae02",
      "question": "Who is giving a big speech at the convention?",
      "prediction": "No. 2 man",
      "ground_truths": [
        "Democratic VP candidate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "07f89034983c43de89406bab63985c58",
      "question": "what is the subject of the email?",
      "prediction": "\"Vice presidential ...\"",
      "ground_truths": [
        "\"Vice presidential"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3f0a9fcbfeb422ca0996a6fc94fc5e4",
      "question": "What will the Democratic VP candidate do next Wednesday?",
      "prediction": "delivers a big speech",
      "ground_truths": [
        "delivers a big speech"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dda0f54af5084a31a2746f72210b0bd4",
      "question": "what was Al-Douri",
      "prediction": "highest ranking former member of Saddam Hussein's regime",
      "ground_truths": [
        "highest ranking former"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "775eadd02718492c8d6a6c09bf69f073",
      "question": "who was vice chairman?",
      "prediction": "Al-Douri,",
      "ground_truths": [
        "Al-Douri,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0ccce61a18fa45aebbd6583a6c92052d",
      "question": "how long was the broadcast?",
      "prediction": "30-minute",
      "ground_truths": [
        "30-minute"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "de7fae20ac624dc9af5949d97f30a79a",
      "question": "What was Al-Douri vice chairman of?",
      "prediction": "Hussein's Revolutionary Command Council.",
      "ground_truths": [
        "Hussein's Revolutionary Command Council."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0eed9d8178c4928b016bdf106f59e9e",
      "question": "what did the US say",
      "prediction": "al-Douri played key roles in the chemical attack on the Kurdish town of Halabja in 1988 and in putting down Kurdish and Shiite revolts after the 1991 Persian Gulf War.",
      "ground_truths": [
        "he has helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002."
      ],
      "em": 0,
      "f1": 0.07843137254901962
    },
    {
      "id": "3e8a38a0b3e64f809e791704ac768b95",
      "question": "what length was the message",
      "prediction": "30-minute",
      "ground_truths": [
        "30-minute recorded"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "64ca162a7daf4ca0b747b8b3d75eca51",
      "question": "What is CNN trying to do?",
      "prediction": "not been able to independently verify the authenticity of the voice on the tape.",
      "ground_truths": [
        "verify the authenticity of the voice on the tape."
      ],
      "em": 0,
      "f1": 0.7058823529411764
    },
    {
      "id": "031cc546d2904282a49ea01dcfa886c8",
      "question": "who does the voice belong to?",
      "prediction": "Izzat Ibrahim al-Douri,",
      "ground_truths": [
        "Izzat Ibrahim al-Douri,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c69f8eda4633488393fff0fe8e8deb1f",
      "question": "What does the U.S. claim Al-Douri has done?",
      "prediction": "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria",
      "ground_truths": [
        "helped finance the insurgency against"
      ],
      "em": 0,
      "f1": 0.4210526315789474
    },
    {
      "id": "e2f7ce0d6e2a4a048f97fb86e8836009",
      "question": "How long has Jacob been the top boy name?",
      "prediction": "11th year",
      "ground_truths": [
        "11th year in a row."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7d7537366071415381b140fc2d6bd90b",
      "question": "What replaced Emma as the most popular name?",
      "prediction": "Jacob,",
      "ground_truths": [
        "Isabella"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a9c56b5a5ae43b78f7f7d9dccdca491",
      "question": "For how long has Jacob been the most popular boy name?",
      "prediction": "the 11th year",
      "ground_truths": [
        "11th year in a row."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "93447ee40a954604bc8e1c926079b785",
      "question": "Who replaced Emma?",
      "prediction": "Jacob,",
      "ground_truths": [
        "Isabella"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "671b2208301e460ba1fa588e350b82d6",
      "question": "Which series inspired the popularity of the name Cullen?",
      "prediction": "\"Twilight\"",
      "ground_truths": [
        "\"Twilight\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "80a9457351e14111aaf3fb84d3528092",
      "question": "What is the most popular boy name?",
      "prediction": "Jacob,",
      "ground_truths": [
        "Jacob,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "67a58f5e2ff7455f8807463f414f9b99",
      "question": "What names are on the way out?",
      "prediction": "Lindsay and Jonas (think actress Lindsay Lohan",
      "ground_truths": [
        "Jonas"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "2c076d8175484c409ffb8ff3dadd914d",
      "question": "Who is sending the marines?",
      "prediction": "Obama's",
      "ground_truths": [
        "President Obama's"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4443abaa1aaf48d1b44d99d10e0968ec",
      "question": "Where are marines being sent?",
      "prediction": "Afghanistan.",
      "ground_truths": [
        "hostile war zones,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "331ff9acbc094ef7b4ecdd751637c564",
      "question": "Where do the marines head to this week?",
      "prediction": "Afghanistan's restive provinces",
      "ground_truths": [
        "Afghanistan's restive provinces"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "89bec77a5e54425a8bfe2f014139705e",
      "question": "What branch of the military is being sent to Afghanistan?",
      "prediction": "Marines",
      "ground_truths": [
        "1,500 Marines will be part of the initial wave of President Obama's surge plan"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "ef04d080cbcb459b8803c4c04a7ab199",
      "question": "Who is ecstatic about the sitaution?",
      "prediction": "Sgt. Jason Bendett",
      "ground_truths": [
        "Sgt. Jason Bendett"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b9ca82e2c48540c0b4c004b86f316109",
      "question": "Who was the part of the initial surge plan?",
      "prediction": "first of 1,500 Marines",
      "ground_truths": [
        "1,500 Marines"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c3648af18d4b400aaa4924a0eed0f5b4",
      "question": "Who says he is ecstatic about the situation?",
      "prediction": "Sgt. Jason Bendett",
      "ground_truths": [
        "Sgt. Jason Bendett"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1740d78b92a4377bcd6b4846b5b42c2",
      "question": "Who is part of initial wave?",
      "prediction": "1,500 Marines",
      "ground_truths": [
        "1,500 Marines"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a58047b20f864cdebdef070c674d4cae",
      "question": "What is the largest age group that gets injured?",
      "prediction": "14 to 17.",
      "ground_truths": [
        "14 to 17."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af6b6e66c61443b9a54c8008d9c2500f",
      "question": "What age group are often the most injured?",
      "prediction": "between the",
      "ground_truths": [
        "14 to 17."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4819fd5cd1a64222a14e089b391f2f5e",
      "question": "In what months do the most accidents occur?",
      "prediction": "June 20 and July 20,\"",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "97aa5f73225c4f1782a0240cd5fbbb3e",
      "question": "When does the most number of accidents occur?",
      "prediction": "between June 20 and July 20,\"",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62b2d8702439498197acee357c30a8bd",
      "question": "What needed a warning?",
      "prediction": "Fireworks",
      "ground_truths": [
        "urging families to put safety first when celebrating with fireworks."
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "402f5f93e4a04d998cb41298d57b8115",
      "question": "Who put the warning out on fireworks?",
      "prediction": "Consumer Product Safety Commission",
      "ground_truths": [
        "the Consumer Product Safety Commission"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2bae9bfec5c847b3baa6807ce263570d",
      "question": "When do 70% of fireworks accidents occur?",
      "prediction": "between June 20 and July 20,\"",
      "ground_truths": [
        "between June 20 and July 20,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "09553563f3b94bfd88d2887043744b77",
      "question": "What percentage of fireworks accidents occur in June-July?",
      "prediction": "70 percent",
      "ground_truths": [
        "70 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d12c3c9ff7254b76a83a3c702e0f1411",
      "question": "What percentage of fireworks accidents happen between June 20 and July 20?",
      "prediction": "70 percent",
      "ground_truths": [
        "70 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "15b1f7554b6b4c8b9c7d341211e79299",
      "question": "Which country is home to the $15 billion Bollywood industry?",
      "prediction": "India",
      "ground_truths": [
        "India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc829fbf47fa4579ab8bd284a198f698",
      "question": "What is the number of Bollywood movies that Kumar has appeared in?",
      "prediction": "more than 80",
      "ground_truths": [
        "more than 80 features to his name,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "28b0b2de8b1942a0b148be400c797bfc",
      "question": "Who finds himself at the forefront of where Bollywood meets Hollywood?",
      "prediction": "Akshay Kumar",
      "ground_truths": [
        "Akshay Kumar"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cf95ec8d543e4fbb858d992b7b40e850",
      "question": "what did kumar teach",
      "prediction": "martial arts,",
      "ground_truths": [
        "martial arts,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b9baaeedf23f47088fe44a56c08433ca",
      "question": "what is the second largest film industry",
      "prediction": "Bollywood",
      "ground_truths": [
        "Bollywood"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ff13ef6297264061a1e92721b068d241",
      "question": "What is the value of India's film industry",
      "prediction": "$15 billion",
      "ground_truths": [
        "$15 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a8707b5b977a4a75b4052142553f008e",
      "question": "Were were zimbabweans fleeing from?",
      "prediction": "repression and dire economic circumstances.",
      "ground_truths": [
        "Johannesburg's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a46f17684194695b99b053c0a8ab490",
      "question": "At least how many were killed?",
      "prediction": "22 people",
      "ground_truths": [
        "22 people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0d214f6804e4725b7de25e93c53c411",
      "question": "At leasst how many were arrestd?",
      "prediction": "200 people",
      "ground_truths": [
        "more than 200 people"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3db1657272374d31897a6e338e408644",
      "question": "Who is South Africa's Archbishop?",
      "prediction": "Desmond Tutu",
      "ground_truths": [
        "Desmond Tutu"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95b699a446f346298c2fae56ed9e0fc9",
      "question": "how many people were arrested",
      "prediction": "more than 200",
      "ground_truths": [
        "more than 200"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3fef6306e73548568dc9ea2747801b4b",
      "question": "How many did police arrest?",
      "prediction": "200",
      "ground_truths": [
        "more than 200 people"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "0fee8c420cf040f08425629d25214152",
      "question": "who condemned the attack",
      "prediction": "Nelson Mandela Foundation",
      "ground_truths": [
        "The Nelson Mandela Foundation issued a statement condemning the \"senseless violence\""
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "baace490a2214705a98eb69dfe65893c",
      "question": "how many people died",
      "prediction": "22",
      "ground_truths": [
        "22"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2624dd60e3e94efca3172a09af392342",
      "question": "What is the senator doing about it?",
      "prediction": "to improve the military's suicide-prevention programs.",
      "ground_truths": [
        "introduce legislation Thursday to improve the military's suicide-prevention programs."
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "cb87b0e309514c2a9999aa0b3884248e",
      "question": "What is the Psychiatrist name?",
      "prediction": "Col. Elspeth Cameron-Ritchie,",
      "ground_truths": [
        "Col. Elspeth Cameron-Ritchie,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d092853597ce419a8c8b0374b3b0aa38",
      "question": "What type of care is currently offered?",
      "prediction": "mental health treatment,",
      "ground_truths": [
        "an 800 number to call for help."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "330b66b7ec924ca2b07fe88a5b775ca5",
      "question": "What treatment does soldiers need?",
      "prediction": "mental health",
      "ground_truths": [
        "mental health"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "67a6c59fe0504df68a7509a1c1d0c4cc",
      "question": "What was Michael Phelps seen doing in NY?",
      "prediction": "partying your face off in public",
      "ground_truths": [
        "partying"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "7f63ffe42abf4714a5fcd629bc263216",
      "question": "What did Debbie Phelps say?",
      "prediction": "doesn't get caught up in gossip, Mr. King.",
      "ground_truths": [
        "tells Larry King her son has strong values."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "bfa3a133f4fc48eda18b7464dddc5e70",
      "question": "What did commuters express?",
      "prediction": "anti-strike",
      "ground_truths": [
        "\"They should be grateful for good jobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18fa422e1baf476fbce6d230e0858000",
      "question": "What can't the transit authority and union agree on?",
      "prediction": "disagreements",
      "ground_truths": [
        "disagreements"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7374f521a2854922b8f6f8890ecda23d",
      "question": "What does the strike affect?",
      "prediction": "buses, subways and trolleys",
      "ground_truths": [
        "buses, subways and trolleys"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "055162a66045404b8d06e59773ef2e49",
      "question": "who walked out",
      "prediction": "union representing thousands of transit workers",
      "ground_truths": [
        "Transport Workers Union leaders"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "8c5e2893db634635891ae6afd30261c4",
      "question": "How many people could they affect with this protest?",
      "prediction": "almost a million",
      "ground_truths": [
        "almost a million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "82f2a270b951424e82df7540c14072d3",
      "question": "What forced the workers to walk out and protest?",
      "prediction": "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.",
      "ground_truths": [
        "a shortfall in their pension fund"
      ],
      "em": 0,
      "f1": 0.3571428571428571
    },
    {
      "id": "e91faab29ff8471fa9aa9b64c8165fad",
      "question": "The dad asked what of the judge?",
      "prediction": "if she would try to take the children and flee to Japan.",
      "ground_truths": [
        "to stop Noriko Savoie from being able to travel to Japan for summer vacation."
      ],
      "em": 0,
      "f1": 0.23999999999999996
    },
    {
      "id": "1aa6667afb814849b2341323adacad27",
      "question": "who are now in jail in japan?",
      "prediction": "Christopher Savoie",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21fc57f2c1204f899cdc9311c7f65211",
      "question": "For what reason is Christopher Savoie now in a Japan jail?",
      "prediction": "he had feared would come.",
      "ground_truths": [
        "Christopher Savoie is in jail in Japan after trying to get back his son, Isaac, and daughter, Rebecca."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19174950ad6b4759929f41cb2aa6b979",
      "question": "What has been under British rule since 1833?",
      "prediction": "Falklands,",
      "ground_truths": [
        "The Falklands,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c5d1601dece84ce895260d42931118c3",
      "question": "What are the tensions over?",
      "prediction": "territory",
      "ground_truths": [
        "territory"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "72a4722fda5f436e81437aea9042aaa8",
      "question": "What did Argentina claim?",
      "prediction": "sovereignty over them.",
      "ground_truths": [
        "sovereignty over them."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ca61917ae999408c84f855c26dc48c68",
      "question": "Which country requires ships to carry a permit?",
      "prediction": "Argentina",
      "ground_truths": [
        "Argentina"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7ac7d29accd74afcbb72758df68371ab",
      "question": "Mourning of what?",
      "prediction": "Carnival",
      "ground_truths": [
        "Charles Jubert,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f387e74cf245420383e2a30d4fc287e4",
      "question": "Who have celebrated Carnival through dictatorships, military coups and bloodshed?",
      "prediction": "Haitians",
      "ground_truths": [
        "Haitians"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "74893a21959544df8340dd43d3803ed5",
      "question": "Less than a week ahead of Haiti's Carnival celebration, what was replaced with mourning?",
      "prediction": "revelry",
      "ground_truths": [
        "revelry"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "16d9f9f1862c42aeb427231e3a5ba863",
      "question": "How many days is the upcoming festivals?",
      "prediction": "three-day",
      "ground_truths": [
        "three-day"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ac07aac0de34bf88eec492a2a3fd199",
      "question": "Who scored twice in serie a?",
      "prediction": "Kaka",
      "ground_truths": [
        "Kaka"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc65525f5d3e4e24a5e44c7043a0f580",
      "question": "Who scored twice?",
      "prediction": "Kaka",
      "ground_truths": [
        "Kaka"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3423cd2fdc574e95beaedc8b92873a08",
      "question": "Who hits first goal?",
      "prediction": "David Beckham",
      "ground_truths": [
        "David Beckham"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "22e9fd9280544f50a52d79f5662e016c",
      "question": "What did beckham do this week?",
      "prediction": "made his mark",
      "ground_truths": [
        "claimed his first goal in Italian football."
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "c33dc18ec5144e5987f9bbff8ccb49e2",
      "question": "When did the goals come?",
      "prediction": "Sunday",
      "ground_truths": [
        "Sunday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5746b6864dc24560aff89223011a242b",
      "question": "Who is the Brazil star?",
      "prediction": "Kaka,",
      "ground_truths": [
        "Kaka,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "acb6c90d9a64483e889a7752bd9f219c",
      "question": "Where are wooden clogs from?",
      "prediction": "Toffelmakaren.",
      "ground_truths": [
        "Toffelmakaren."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "307baee708c242ba8ef6e62fbb84c35a",
      "question": "Who is a designer?",
      "prediction": "Robert & Blad",
      "ground_truths": [
        "Marc Jacobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c15786561763476fbe723ff394d12d0b",
      "question": "What is a local Swedish designer?",
      "prediction": "Robert & Blad",
      "ground_truths": [
        "Marc Jacobs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3b46daafe80c43b0a2a3c953a3a7cd5a",
      "question": "What are traditional Swedish shoes called?",
      "prediction": "clogs",
      "ground_truths": [
        "clogs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "916043a8be5f47ff886d8a93e751a204",
      "question": "Where should you go for fashion forward apparel?",
      "prediction": "the shop at the Form Design Center.",
      "ground_truths": [
        "Kit of Elsinore"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3facff8b0e6e4f36a362e784462b4595",
      "question": "What make great souvenirs?",
      "prediction": "Hokeriet,",
      "ground_truths": [
        "hand-painted Swedish wooden clogs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3d23205e1ff4203820bddb1cef6c47d",
      "question": "Where should you shop?",
      "prediction": "Malmo",
      "ground_truths": [
        "Form Design Center."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "34103331d9c148c080d63622ec1662d9",
      "question": "What's a good place for eco friendly shopping?",
      "prediction": "Drottningtorget",
      "ground_truths": [
        "Drottningtorget"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "48ff8745a7404de7a2a37e7a75dbf6ae",
      "question": "who is without power",
      "prediction": "About 125,000 customers in New York, New Jersey and Connecticut",
      "ground_truths": [
        "About 125,000 customers in New York, New Jersey and Connecticut"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "291f806867f243fda01f7b182dbea6fc",
      "question": "what river is rising",
      "prediction": "Red",
      "ground_truths": [
        "Red"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "86cd984d3e61489e96d4c82e8469c5b5",
      "question": "What work are making about 300 workers?",
      "prediction": "sandbag lines",
      "ground_truths": [
        "helping on the sandbag lines"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e902ca034f7c443bb88cad1251b45b91",
      "question": "what River rises 12 feet above flood stage at Fargo?",
      "prediction": "Red",
      "ground_truths": [
        "Red"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2d9aed3487ca4525a3ce8631ef89a750",
      "question": "Who won Bahrain's first-ever medal?",
      "prediction": "Yusuf Saad Kamel",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9824c871bc92468d964780dd27c6a665",
      "question": "who claimed gold in men's 1,500m?",
      "prediction": "Yusuf Saad Kamel",
      "ground_truths": [
        "Yusuf Saad Kamel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d28acffe4fbd4653a8fcdf35eafb77ba",
      "question": "Who did the IAAF ask to undergo a gender test?",
      "prediction": "Caster Semenya",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8090fcada6904680acfba927c67cd2e0",
      "question": "What event did Caster Semenya win gold in?",
      "prediction": "women's 800 meters",
      "ground_truths": [
        "women's 800 meters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4f358d3d4714b30b80d895b0251a95f",
      "question": "What country got the first ever medal?",
      "prediction": "Bahrain's",
      "ground_truths": [
        "Bahrain's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9f51e0704d694c92b06f362be43f38d3",
      "question": "who asked for gender test?",
      "prediction": "IAAF.",
      "ground_truths": [
        "IAAF"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ccf0a0ada422491d9e6b0daaa45b54d0",
      "question": "Who won a women's gold at World Athletics Championships?",
      "prediction": "Caster Semenya",
      "ground_truths": [
        "Caster Semenya"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce6b747cc3a140f8b3ca1bc9df4eafdc",
      "question": "Where did armed militants attack a mosque during midday prayers?",
      "prediction": "Rawalpindi",
      "ground_truths": [
        "Rawalpindi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a7be659d2b7f470c8ed212a4160145bf",
      "question": "Who is the mosque frequented by?",
      "prediction": "military personnel,",
      "ground_truths": [
        "military personnel,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0864b8b2a9fc4087b6c3c2cd0ed5c938",
      "question": "What did a suicide bomber target on November 2?",
      "prediction": "bank",
      "ground_truths": [
        "a bank"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a23ee41a56174a2eb02865e5ecaa13d3",
      "question": "How many were injured?",
      "prediction": "75.",
      "ground_truths": [
        "75."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7d8839e3e6a14b33ae41ec27e47ed765",
      "question": "Which birthday would Anne Frank be celebrating this week?",
      "prediction": "80th",
      "ground_truths": [
        "80th"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7a23f7acba534695acb96f8d150e0f67",
      "question": "What did eva schloss flee from?",
      "prediction": "Nazis",
      "ground_truths": [
        "Nazis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9f3b062728ba48a78f1f56afeea3fd54",
      "question": "what is one of the worlds most read books?",
      "prediction": "'I'm not going to talk so much,'",
      "ground_truths": [
        "Frank's diary."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cbffa1d44044b6885bc0566f5ebfd0f",
      "question": "Where did anne frank die?",
      "prediction": "in a Nazi concentration camp,",
      "ground_truths": [
        "a Nazi concentration camp,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "f2deb08987064f74bb987878d41a3346",
      "question": "Which birthday would Anne Frank have celebrated?",
      "prediction": "80th",
      "ground_truths": [
        "80th"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8557402c6bda4ed2859de806c39689e9",
      "question": "What was her age when she died at Auschwitz concentration camp in Poland?",
      "prediction": "15.",
      "ground_truths": [
        "15."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "188ddb201c1e42eb91dab05b6afab94a",
      "question": "Where did Frank die?",
      "prediction": "concentration camp,",
      "ground_truths": [
        "concentration camps,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a0316e8dbe4c496a8dd5d3addc96fbae",
      "question": "what fears swirl around American Airlines?",
      "prediction": "bankruptcy",
      "ground_truths": [
        "bankruptcies"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f5ec1c574c842cfa194293fecf7ab43",
      "question": "what is american airlines",
      "prediction": "An airline",
      "ground_truths": [
        "carrier based in Texas."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "44eeb9811ef7469aa6de979324bc43e8",
      "question": "What had Jackson said about his father?",
      "prediction": "criticized",
      "ground_truths": [
        "criticized"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c258d5feec51488082d163009bd23e15",
      "question": "What has Michael Jackson criticized?",
      "prediction": "his father's parenting skills.",
      "ground_truths": [
        "his father's parenting skills."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "51162157d5a74f8a95994cbcb39d64a3",
      "question": "Who was left out of will?",
      "prediction": "Brian Oxman,",
      "ground_truths": [
        "Joe Jackson,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8e8fac9eb654ea786830a96010c6b5c",
      "question": "Who is getting the bulk of Jackson's estate?",
      "prediction": "his mother, Katherine",
      "ground_truths": [
        "mother and his three children receive a court-ordered allowance from the"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "2d8b7ed4836443c0a4227146a31abda2",
      "question": "What does Joe Jackson want?",
      "prediction": "a judge to order the pop star's estate to pay him a monthly allowance,",
      "ground_truths": [
        "a judge to order the pop star's estate"
      ],
      "em": 0,
      "f1": 0.7058823529411764
    },
    {
      "id": "958824a5f24e4f9c8845cb4615bd2b88",
      "question": "Who gets the bulk of Jackson's estate?",
      "prediction": "his mother, Katherine",
      "ground_truths": [
        "Katherine"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "dc627af063d44b94a5bbcf9c0e0970f9",
      "question": "What does Joe Jackson want the judge to give him?",
      "prediction": "a monthly allowance,",
      "ground_truths": [
        "a monthly allowance,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6420f1540eeb4dc890dc682714ddb755",
      "question": "Michael Jackson had publicly criticized whos parenting?",
      "prediction": "his father's",
      "ground_truths": [
        "his father's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc702af4678a4967bd9d8200f35b5df3",
      "question": "who committed suicide at 27, 15 years ago",
      "prediction": "Kurt Cobain's",
      "ground_truths": [
        "Kurt Cobain"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "3dd8316956464ff9bbe2ba917cbd02ad",
      "question": "Who did Kurt Cobain play for?",
      "prediction": "diehard fans,\"",
      "ground_truths": [
        "Nirvana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff99affcb32442d08543e10db96a1157",
      "question": "At what age did Kurt Cobain die?",
      "prediction": "27,",
      "ground_truths": [
        "27,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "342bc95b32974fda9e6737cb3f524f59",
      "question": "Which band was Cobain a part of?",
      "prediction": "Nirvana",
      "ground_truths": [
        "Nirvana"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6118310e56f44b68bd3507c69aab4605",
      "question": "Who was Cobain's widow?",
      "prediction": "Courtney Love,",
      "ground_truths": [
        "Courtney Love,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "adfc122496824596b75ef249b137e164",
      "question": "who is Cobain's window",
      "prediction": "Courtney Love,",
      "ground_truths": [
        "Courtney Love,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14399ecd55f84f16ad4928db308d6e93",
      "question": "When did Kurt Cobain die?",
      "prediction": "Wednesday,",
      "ground_truths": [
        "Fifteen years ago Wednesday,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "7629a3dc263e48209e9b18f6c9e16a63",
      "question": "who was the frontman for Nirvana",
      "prediction": "Kurt Cobain",
      "ground_truths": [
        "Kurt Cobain's"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "93e125a935e64bd2b88901d36b50a516",
      "question": "what was the verdict",
      "prediction": "of not guilty",
      "ground_truths": [
        "of not guilty"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8ccdb4a0320940ea982529d8cb9c97a4",
      "question": "who says \"decent man and devoted husband\"?",
      "prediction": "High Court Judge Justice Davis",
      "ground_truths": [
        "High Court Judge Justice Davis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "388fb8074ef94516862a0ffc9e46f530",
      "question": "where was the trial",
      "prediction": "Swansea Crown Court,",
      "ground_truths": [
        "Swansea Crown Court,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1585635901ef4403a84ed3252832a1ae",
      "question": "what was he accused of",
      "prediction": "strangling his wife",
      "ground_truths": [
        "murder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b60020073ada49a09f687e0788f24fa5",
      "question": "what do prosecutors say",
      "prediction": "The UK's Crown Prosecution Service requested that the case against Brian Thomas, who killed wife Christine while they were on vacation in 2008, be dropped due to a \"unique set of circumstances.\"",
      "ground_truths": [
        "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\""
      ],
      "em": 0,
      "f1": 0.126984126984127
    },
    {
      "id": "6e35ee6ee0a64726b3ba3d493c5add87",
      "question": "when were they on vacation",
      "prediction": "2008,",
      "ground_truths": [
        "2008,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "61441c5df37b447a982cabc26fea8eaa",
      "question": "what  Obama offers?",
      "prediction": "$60 billion on America's infrastructure.",
      "ground_truths": [
        "education, infrastructure, energy"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c6a9da345c8049d1b45204b5d5a958f7",
      "question": "What is being spent on renewable energy?",
      "prediction": "$60 billion",
      "ground_truths": [
        "$150 billion"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "969ebd74ec8745dcaa074576e171b43a",
      "question": "Who offers plan to spend $10B on schools?",
      "prediction": "Sen. Barack Obama",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "db26836b6cf8475e97616c10654e52f0",
      "question": "what McCain's campaign says?",
      "prediction": "the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"",
      "ground_truths": [
        "\"To help create jobs in America, we need to lower taxes and open up foreign markets to American goods,\""
      ],
      "em": 0,
      "f1": 0.380952380952381
    },
    {
      "id": "24398c1a07234d538f257ff0be6ed411",
      "question": "Who rejects protectionist trade polices?",
      "prediction": "Barack Obama",
      "ground_truths": [
        "Obama"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "db9616cbdd2844cf9598a38c4c6fd8f5",
      "question": "What does Obama's plan to spend?",
      "prediction": "$60 billion on America's infrastructure.",
      "ground_truths": [
        "$60 billion"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "866d6db39ec6461790477b8dcef42c1e",
      "question": "Who said it could hurt economy?",
      "prediction": "Sen. Barack Obama",
      "ground_truths": [
        "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ca5014ef14df4b26bac5ef092615dcb4",
      "question": "where presidential candidate would spend?",
      "prediction": "$60 billion on America's infrastructure.",
      "ground_truths": [
        "America's infrastructure."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "391928c43e8948d489166553b139bb4e",
      "question": "Who would spend $150B on renewable energy?",
      "prediction": "Sen. Barack Obama",
      "ground_truths": [
        "Sen. Barack Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7d3f87c46ec74f8da310cf30f72df338",
      "question": "Where is Mr. Sohail from?",
      "prediction": "Pakistani",
      "ground_truths": [
        "Long Island"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe1a537935c349ff91672468348c55f1",
      "question": "what was wielded?",
      "prediction": "baseball bat",
      "ground_truths": [
        "baseball bat"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0435c9a64724c84a091c7d2fac8aa23",
      "question": "what was offered?",
      "prediction": "a loaf of bread.",
      "ground_truths": [
        "$40 and a loaf of bread."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "0727e0bae0224061be5c486adf21e088",
      "question": "who has the gun?",
      "prediction": "Mohammad Sohail",
      "ground_truths": [
        "Mohammad Sohail"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0adf2b210210490ba1ee417a6fa58133",
      "question": "What did Sohail give to the theif?",
      "prediction": "$40 and a loaf of bread.",
      "ground_truths": [
        "$40 and a loaf of bread."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "db2c0c695c8b42b8b252793a037be57d",
      "question": "What is the religion of the man?",
      "prediction": "Pakistani",
      "ground_truths": [
        "Muslim"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "783ec86fa5ca450ebf038faae46cb32d",
      "question": "What did Sohail do when the man started crying?",
      "prediction": "\"He says, 'Don't call police, don't shoot me, I have no money, I have no food in my house.' \"",
      "ground_truths": [
        "gave him $40 and a loaf of bread."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "81f733a833b14d018b0b4d4492e92d20",
      "question": "what did vivanco say",
      "prediction": "conviction of Peru's ex-president is a warning to those who deny human rights.",
      "ground_truths": [
        "conviction of Peru's ex-president is a warning to those who deny human rights."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "55f63d74a744491aae187434ac8c4809",
      "question": "did vivanco say anything",
      "prediction": "conviction of Peru's ex-president is a warning to those who deny human rights.",
      "ground_truths": [
        "conviction of Peru's ex-president is a warning to those who deny human rights."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "79b62004336a4ee8bc64e1b588407038",
      "question": "what was fujimori convicted for",
      "prediction": "of egregious human rights abuses, including the massacre of innocent civilians, and sentenced him to 25 years in prison",
      "ground_truths": [
        "death squad killings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b099694c8a4144c28e496f8b27303276",
      "question": "What was Fuijimori conviced of ?",
      "prediction": "for death squad killings carried out during his rule in the 1990s.",
      "ground_truths": [
        "authorizing killings and kidnappings by paramilitary death squads."
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "31f8a4e5f2d542c9904b80d29a0e9dab",
      "question": "What was Fujimori convicted of?",
      "prediction": "of egregious human rights abuses,",
      "ground_truths": [
        "for death squad killings carried out during his rule in the 1990s."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f31f62bee784274b1f0f25497d50707",
      "question": "Who is José Miguel Vivanco?",
      "prediction": "executive director of the Americas Division of Human Rights Watch,",
      "ground_truths": [
        "executive director of the Americas Division of Human Rights Watch,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4931a582bfb048e29b40c732b2e994ce",
      "question": "Where was this case tried?",
      "prediction": "Peru's",
      "ground_truths": [
        "The Peruvian Supreme Court"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "80b33fe3a59c4d25a5b6f62ae79f93cb",
      "question": "What is improving?",
      "prediction": "quality of teaching and learning in American schools",
      "ground_truths": [
        "Test scores and graduation rates"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "3c34d01e48c64f5081002fd67a272117",
      "question": "Whose achievement has been improving?",
      "prediction": "the quality of teaching and learning in American schools",
      "ground_truths": [
        "Blacks and Hispanics"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "9c7364bc94374e31a097ba17c443cd18",
      "question": "What will the vitality of the U.S. depend on?",
      "prediction": "closing these racial gaps.",
      "ground_truths": [
        "closing these racial gaps."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2dae96a089cc424fb9886a641eb50949",
      "question": "Where is the man from?",
      "prediction": "Florida",
      "ground_truths": [
        "Bailey, Colorado,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b9f35bf604d432db2dd139fe20302ff",
      "question": "Where do they camp?",
      "prediction": "in the driveway,",
      "ground_truths": [
        "\"out in the woods\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f3d4871c16764417a6f31fee26aaee78",
      "question": "How many were found dead in the immigration centre?",
      "prediction": "14",
      "ground_truths": [
        "14"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4686a8f258b4bd4942f2304ee16c736",
      "question": "what are the suspects name?",
      "prediction": "Jiverly Wong,",
      "ground_truths": [
        "Jiverly Wong,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "650f37e79de742ccb8e372671e453720",
      "question": "Who survived the attack?",
      "prediction": "four more people",
      "ground_truths": [
        "Zhanar Tokhtabayeba,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3359a6b06b204873935ee40341d4e42f",
      "question": "How many people survived?",
      "prediction": "37",
      "ground_truths": [
        "37"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0d0500984e084bf8b7f7aad64ba75019",
      "question": "Who was the suspect?",
      "prediction": "Jiverly Wong,",
      "ground_truths": [
        "Jiverly Wong,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf43702e012a4da2b2328e06961d7f06",
      "question": "Where did the shooting occur?",
      "prediction": "Binghamton, New York, immigration center.",
      "ground_truths": [
        "Binghamton, New York,"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "59c97e698fe0492890ba2dec20b8ff97",
      "question": "What date is the concert now?",
      "prediction": "June 17 and 18,",
      "ground_truths": [
        "June 17"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "74953752801446a2abe6b330ce63c179",
      "question": "What show dates have been cancled?",
      "prediction": "Tuesday's",
      "ground_truths": [
        "first four"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9723fcbfda3b41f1a2928a72b5614b05",
      "question": "What caused the postponement of the shows?",
      "prediction": "Illness",
      "ground_truths": [
        "Illness"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c60d0e46b4a49068acd95570c520c6f",
      "question": "What's she suffering from?",
      "prediction": "an upper respiratory infection,\"",
      "ground_truths": [
        "upper respiratory infection,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed981c4aeb414002ad995530646965da",
      "question": "Which cities are affected?",
      "prediction": "Glasgow, Scotland",
      "ground_truths": [
        "Manchester,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4b8fa26b7e741fd8b4b8e4120d39d41",
      "question": "What is Houston's tour named?",
      "prediction": "\"Nothing But Love\"",
      "ground_truths": [
        "\"Nothing But Love\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e6de19d338a44b6bd4bbaec2c3c0635",
      "question": "What is Houston suffering from?",
      "prediction": "upper respiratory infection,\"",
      "ground_truths": [
        "upper respiratory infection,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8de2ee8d56464a9d971fdae65ae0f72b",
      "question": "What concert will now be held on May 1?",
      "prediction": "Glasgow, Scotland",
      "ground_truths": [
        "Whitney Houston"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d674d0b82684e87bdc226d9d614e6e4",
      "question": "What star is ill?",
      "prediction": "Whitney Houston",
      "ground_truths": [
        "Whitney Houston"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa048de742634fc294e93e2471a6e0bf",
      "question": "What is the name of the exhibit?",
      "prediction": "\"The Cycle of Life,\"",
      "ground_truths": [
        "\"Body Works\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac59acc0c3b84767bf3118ef7a3c8799",
      "question": "how many human bodies were featured?",
      "prediction": "200",
      "ground_truths": [
        "200"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cbe01529cf464019bd71c4bdf75683ad",
      "question": "When was the first UK public autopsy in 170 years filmed?",
      "prediction": "2002",
      "ground_truths": [
        "2002"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd845ef39f0944719eb67e6b8f95e18a",
      "question": "How many bodies are there?",
      "prediction": "200",
      "ground_truths": [
        "200"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f9e3c2ce88304e44bceda6f6556de1e2",
      "question": "Who performed the first UK public autopsy in 170 years?",
      "prediction": "Gunther von Hagens",
      "ground_truths": [
        "Gunther von Hagens'"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e8a99c1df04495f823dbdfb764ffe9a",
      "question": "What does Cycle of Life depict?",
      "prediction": "\"The Cycle of Life,\" is showing at Berlin's Postbahnhof",
      "ground_truths": [
        "200 human bodies at various"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "4b545e66deaf45ff9296dd65db911414",
      "question": "Whats the exhibition called?",
      "prediction": "\"Body Works\"",
      "ground_truths": [
        "\"Body Works\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fa50aa7aa1e34f5e95a89f5182159659",
      "question": "When was the first UK public autopsy?",
      "prediction": "2002",
      "ground_truths": [
        "2002 for British broadcaster Channel 4"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "0f374544a6744955b8a0caeae293626f",
      "question": "Who was tortured for days?",
      "prediction": "Jaime Andrade",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb6294881cc7497cba33ad6b2ef3683f",
      "question": "What do they do to get money",
      "prediction": "would later tell police, was a mechanic and freelance human smuggler,",
      "ground_truths": [
        "abducting each other for ransoms or retribution."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3cee4a6b50334a829428d9b657afa7cc",
      "question": "who was tortured",
      "prediction": "Jaime Andrade",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dbf2fd52c52b4d22b9e952a72ba2c8fe",
      "question": "Who investigated the kidnappings",
      "prediction": "Phoenix, Arizona, police",
      "ground_truths": [
        "police"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "499adaa2b2934b7c83e3e56900f430a2",
      "question": "who was kidnapped because of her neighbour?",
      "prediction": "Jaime Andrade",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ced6f2f26e2843dc9824aed928ddd1dc",
      "question": "Who did men kidnap?",
      "prediction": "Jaime Andrade",
      "ground_truths": [
        "Jaime Andrade"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f542a3f921174d47b54d203adea547da",
      "question": "How long has Barnett played Republicans in practice debates?",
      "prediction": "eight",
      "ground_truths": [
        "eight national presidential"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "fec7fec9315345d08442effe788cb11f",
      "question": "Who will make the case aggressively?",
      "prediction": "Robert Barnett,",
      "ground_truths": [
        "Robert Barnett"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eab1831fc59f4491acc17e1f38edeff2",
      "question": "Who said he'll make sure to make the cases aggressively?",
      "prediction": "Robert Barnett",
      "ground_truths": [
        "Barnett,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3a381589e16c4e249160820a41252097",
      "question": "Who has played Republicans?",
      "prediction": "Robert Barnett,",
      "ground_truths": [
        "Robert"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0f883fc267164b07a2b3ae3225a53376",
      "question": "Who said candidates must make sure they answer town hall questions?",
      "prediction": "CNN:",
      "ground_truths": [
        "Robert Barnett"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6588a9418fd841a68831126aa265cd81",
      "question": "What party is Barnett associated with?",
      "prediction": "Republican",
      "ground_truths": [
        "Democrats"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "643ed515d4554bc59480448e97351311",
      "question": "What has Barnett played?",
      "prediction": "George H.W. Bush",
      "ground_truths": [
        "the role of George H.W. Bush in practice debates with Geraldine Ferraro in 1984 and with Michael Dukakis in 1988, and practice debated Bill Clinton more than 20 times during the 1992 campaign. He also"
      ],
      "em": 0,
      "f1": 0.16666666666666669
    },
    {
      "id": "173c195371c44efa8b41ab43de4cf834",
      "question": "What must candidates answer?",
      "prediction": "The individuals will not necessarily express the question as a journalist would.",
      "ground_truths": [
        "the question,"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "64937a18ace249a0aed6fa80d0b779a8",
      "question": "What does Barnett try to prepare candidates for?",
      "prediction": "prepped,",
      "ground_truths": [
        "debate preparation."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47c2581d5c1e449e830ef3754d9e4473",
      "question": "how many finals are in American Idol?",
      "prediction": "final two contestants.",
      "ground_truths": [
        "two contestants."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "18ffd76b9b954bfea21323c0568cac34",
      "question": "Where is Adam Lambert from?",
      "prediction": "Southern California",
      "ground_truths": [
        "Southern California"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "189225ed625e43a6b8542f237e74aafd",
      "question": "Who developed a glam-rock persona?",
      "prediction": "Adam Lambert",
      "ground_truths": [
        "Adam Lambert"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb30a08750a744b2ad37125b735959c1",
      "question": "Who has guy-next-door appeal?",
      "prediction": "Kris Allen,",
      "ground_truths": [
        "Kris Allen,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "13961f0e464a435bb693eb52253911c7",
      "question": "When is the American Idol results show?",
      "prediction": "Tuesday night",
      "ground_truths": [
        "Wednesday night"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "b889b1ad2a4144ff80a036267c53b416",
      "question": "Who are the final two contestants?",
      "prediction": "Adam Lambert and Kris Allen,",
      "ground_truths": [
        "Adam Lambert and Kris Allen,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "93af1a999ad0480592200969e79ad8bf",
      "question": "who resident in Arkansas?",
      "prediction": "Kris Allen,",
      "ground_truths": [
        "Kris Allen,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "947a70e77e3947a5b677feb0c381d0cc",
      "question": "What human rights groups say Ethiopia failed to provde justice",
      "prediction": "\"The purpose of this verdict is to scare away all journalists from reporting in the Ogaden. But as journalists we have to continue reporting from closed areas.",
      "ground_truths": [
        "Reporters Without Borders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6be9520acc2e409594c65672107c545a",
      "question": "who is very disappointed by the verdict?",
      "prediction": "Tomas Olsson,",
      "ground_truths": [
        "Tomas Olsson,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5436e24c1e06445da6163e646a165b25",
      "question": "Who says the two failed to prove their innocence?",
      "prediction": "Tomas Olsson,",
      "ground_truths": [
        "judge Shemsu Sirgaga"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d8b81a14273744e48c1737ac742f6a65",
      "question": "What prison term could be a death sentence",
      "prediction": "11 years",
      "ground_truths": [
        "11-year"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f48f89ef9f2f444b8438b4ecef84359a",
      "question": "what do human rights groups say?",
      "prediction": "Journalists and aid workers are prohibited from entering the Ogaden, where",
      "ground_truths": [
        "abuses against ethnic Somalis by rebels and Ethiopian troops are rampant."
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "cb271a55b4e14b5fb0606944599515cc",
      "question": "Who  says they are \"very disappointed\" by the verdict?",
      "prediction": "Thomas Olsson,",
      "ground_truths": [
        "Tomas Olsson, the journalists' Swedish attorney."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "46414d4b1b7a4838987e3ee887a448bb",
      "question": "What are the names of the other top albums this week?",
      "prediction": "Michael Jackson's \"Number Ones\"",
      "ground_truths": [
        "(\"Number Ones,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c5c6a6a15e594958ba61f8231ad1c9d0",
      "question": "How many copies sold Daughtry's?",
      "prediction": "269,000",
      "ground_truths": [
        "269,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3555cf721db54ce6a35be172f4f0213c",
      "question": "What is the name of Michael Jackson's biggest selling album in the past weeks?",
      "prediction": "\"Number Ones\"",
      "ground_truths": [
        "\"Number Ones\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2d6a236fd6104736bfb0ae4404ee6ea9",
      "question": "Who has been biggest-selling album?",
      "prediction": "Michael Jackson's \"Number Ones\"",
      "ground_truths": [
        "Michael Jackson's"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "283c5867606b46258127fa5fed7cf62d",
      "question": "How many copies of Doughtry's first album sell?",
      "prediction": "269,000",
      "ground_truths": [
        "304,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f2894e10d02a42c2b91b26caefadfbc9",
      "question": "What was the biggest selling album in the last two weeks?",
      "prediction": "\"Number Ones\"",
      "ground_truths": [
        "\"Number Ones\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7904eba267c04097b63fb228937b62bb",
      "question": "Which year was released first Daughtry's album?",
      "prediction": "2006,",
      "ground_truths": [
        "2006,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "66d0b1463d394d90a300a5302c278a3d",
      "question": "How many copies did Daughtry's chart sell?",
      "prediction": "269,000",
      "ground_truths": [
        "269,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0134a374dcf1451a820c7fd997862851",
      "question": "How many decades was his career?",
      "prediction": "40th",
      "ground_truths": [
        "20 years.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "12859cbba89f494b98e6b7be8b4c88ff",
      "question": "Has he won any awards?",
      "prediction": "two Emmys",
      "ground_truths": [
        "Emmy-winning"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2737e76f68ed4c889e75300d5d707fbb",
      "question": "Who is Patrick McGoohan?",
      "prediction": "actor",
      "ground_truths": [
        "actor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dab26af5192549f1a07bed7fa63bf47e",
      "question": "What was he best known for?",
      "prediction": "the 1960s show 'The Danger Man,'",
      "ground_truths": [
        "thrillers,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08473edda228492e9cfe9b6583977e50",
      "question": "Who is the actor who passed away?",
      "prediction": "Patrick McGoohan,",
      "ground_truths": [
        "Cleve Landsberg,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27c9d5c1e5d149a9824924cd86f3d48c",
      "question": "Did he win any Emmy awards?",
      "prediction": "two Emmys",
      "ground_truths": [
        "Patrick McGoohan,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "689c990ac1bc4eb38b2ebd26349a37f2",
      "question": "Who is the Emmy winning actor?",
      "prediction": "Patrick McGoohan,",
      "ground_truths": [
        "Patrick McGoohan,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc20c452e45644e4beb92b0f35859105",
      "question": "What is the age of Patrick McGoohan?",
      "prediction": "80,",
      "ground_truths": [
        "80,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "35a6135bbde747f390617f9bb5cdd139",
      "question": "what happened to the stock market",
      "prediction": "opened considerably higher",
      "ground_truths": [
        "opened considerably higher"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "493a0407ac6747ee89cbd1944cd53d29",
      "question": "who elects the members of parliament of Egypt",
      "prediction": "Egypt's election committee",
      "ground_truths": [
        "Citizens"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ef9db99eba14a2db167a1a13f553a95",
      "question": "whats Voters pick members of the lower house of Egypt's parliament?",
      "prediction": "a second day of voting,",
      "ground_truths": [
        "Citizens"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1239f593603846238d67b686d7fd6192",
      "question": "how many were wounded",
      "prediction": "88",
      "ground_truths": [
        "88"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "efe8d4d886a543a0b888a86a02c89940",
      "question": "what did the lower house do",
      "prediction": "drafting a new constitution",
      "ground_truths": [
        "will be tasked with drafting a new constitution"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "7315c30824814fa4b9c02aa012a4ed93",
      "question": "who will have the task of drafting the new constitution of Egypt",
      "prediction": "lower house of parliament,",
      "ground_truths": [
        "lower house of parliament,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d878c3d7ac484b759acbee38ce21b091",
      "question": "What role will EU forces take now?",
      "prediction": "naval",
      "ground_truths": [
        "escorting United Nations World Food Program vessels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b38edc1380ff473eb0e9126d03b25445",
      "question": "Who will take over the role of escorting U.N. World Food Program vessels?",
      "prediction": "EU naval force",
      "ground_truths": [
        "EU naval force"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "49aa2a1e0fb443169edf8af22288973b",
      "question": "What will take over the role of escorting?",
      "prediction": "EU naval force",
      "ground_truths": [
        "The EU naval force"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "19d93283469843c5a343f26a38a4b76e",
      "question": "What country are the pirates from?",
      "prediction": "Somalia",
      "ground_truths": [
        "Somali-based"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac20eebd924947158dc6de1c9cb9039d",
      "question": "Warships from what areas also patrol the region?",
      "prediction": "United Nations World Food Program",
      "ground_truths": [
        "United States, NATO member states, Russia and India"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "2f627367a8e64574b1a3fc00c386aac8",
      "question": "Where are the warships from?",
      "prediction": "the United States, NATO member states, Russia and India",
      "ground_truths": [
        "United States, NATO member states, Russia and India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "700122011f4e4d9494fa8b4a9e9bd808",
      "question": "Who has attacked almost 100 vessels off Somalia's coast this year?",
      "prediction": "Piracy",
      "ground_truths": [
        "pirates"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89863b3cc756459497568b3bdc70588d",
      "question": "What countries have warships patrol the region?",
      "prediction": "United States, NATO member states, Russia and India",
      "ground_truths": [
        "United States, NATO member states, Russia"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "23406aeb357e4344bc1c22a24bf8b757",
      "question": "Who wil take over the role of escorting U.N. vessels?",
      "prediction": "EU naval force",
      "ground_truths": [
        "EU naval force"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e2b7b6be2784668bb3d2e749a2fe9e1",
      "question": "How many ships have pirates attacked?",
      "prediction": "almost 100 vessels",
      "ground_truths": [
        "almost 100 vessels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f96fb8c46a3f42caab3e23b301606942",
      "question": "What North American country sent warships to this region?",
      "prediction": "United States,",
      "ground_truths": [
        "United States,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b52c72e60b6d49a5a650724e57141268",
      "question": "Pirates have attacked how many vessels?",
      "prediction": "almost 100",
      "ground_truths": [
        "almost 100"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf72c01037194351b8d906e8e8c4c0cc",
      "question": "What is the captain's name?",
      "prediction": "Chesley \"Sully\" Sullenberger",
      "ground_truths": [
        "Chesley \"Sully\" Sullenberger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8067ee0ec3df4208924631047ae0c537",
      "question": "What have been separated from the body of the aircraft?",
      "prediction": "wings,",
      "ground_truths": [
        "The wings,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d89807b1fdda40c2bebb55edf64c3938",
      "question": "Where is the Airbus plane listed for sale?",
      "prediction": "\"AS IS/WHERE IS\" at a salvage yard in Kearny, New Jersey.",
      "ground_truths": [
        "at a salvage yard in Kearny, New Jersey."
      ],
      "em": 0,
      "f1": 0.8235294117647058
    },
    {
      "id": "fadd2528784f4a9fa3b391626e69ffc0",
      "question": "What is listed for sale?",
      "prediction": "The plane, an Airbus A320-214,",
      "ground_truths": [
        "an Airbus A320-214,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "644d4774db194bb0b491e58deff0fe6c",
      "question": "When is the auction set to end?",
      "prediction": "March 27",
      "ground_truths": [
        "March 27 at 4:30 p.m. ET"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "b71eed014a8c4e51ba0bb48aeb1939ae",
      "question": "Where is the salvage yard?",
      "prediction": "Kearny, New Jersey.",
      "ground_truths": [
        "Kearny, New Jersey."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5d66710233f441d8b72e50abfae9b25e",
      "question": "What safely landed in the Hudson?",
      "prediction": "US Airways Flight 1549",
      "ground_truths": [
        "US Airways Flight 1549"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3750487af26c40af96cb49550876a52a",
      "question": "What is the name of the plane's pilot?",
      "prediction": "Chesley \"Sully\" Sullenberger",
      "ground_truths": [
        "Chesley \"Sully\" Sullenberger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "01649970ebdc4958a48c133492620c31",
      "question": "What have the wings been separated from?",
      "prediction": "the body of the aircraft and the body of the aircraft and the",
      "ground_truths": [
        "the body of the aircraft"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "80f8912075b14d1a91b62f0c2d06ca84",
      "question": "Name the music label that owns studios",
      "prediction": "EMI,",
      "ground_truths": [
        "EMI,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bdc7cf3241594e52a3164b489f50370d",
      "question": "Beatles made studios famous with what album?",
      "prediction": "\"Abbey Road\"",
      "ground_truths": [
        "\"Abbey Road.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "98bbd1deff4143b780e5d123925bdbee",
      "question": "What is Andrew Lloyd Webber's occupation?",
      "prediction": "composer of \"Phantom of the Opera\" and \"Cats\"",
      "ground_truths": [
        "composer"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "b5ada864c0934f8b98d919dae0bcec35",
      "question": "Andrew Lloyd Webber expressed interest in buying what?",
      "prediction": "Abbey Road music studios",
      "ground_truths": [
        "Abbey Road music studios"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f299e2d5b4264decb2fd2ab4d6a53c2d",
      "question": "does anyone own them",
      "prediction": "The iconic Abbey Road music studios made famous by the Beatles are not for sale,",
      "ground_truths": [
        "EMI,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "94b8ba1c855e4789a90cd5692ee3e710",
      "question": "Who made studios famous with album \"Abbey Road\"?",
      "prediction": "Beatles",
      "ground_truths": [
        "the Beatles"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "08183378ccb3472d903fd95b3b82e132",
      "question": "Where was the Swedish man poised to fly from?",
      "prediction": "Amsterdam, in the Netherlands,",
      "ground_truths": [
        "Amsterdam,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "81a06cc7f0ed45fba03d40062246bf0f",
      "question": "What had he been charged with several years ago by Swedish investigators?",
      "prediction": "flying with a fake license,",
      "ground_truths": [
        "flying"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b26cac2e25c243dd93e2ac30801d55c9",
      "question": "What did investigators charge him with several years ago?",
      "prediction": "flying with a fake license,",
      "ground_truths": [
        "flying"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b8427699228e4350957f89c0fa5a3ac7",
      "question": "What did authorties charge the man with?",
      "prediction": "forgery and flying without a valid license,",
      "ground_truths": [
        "forgery and flying without a valid license,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c175bdf51494ae1a97be4ed1a365825",
      "question": "What is he charged with?",
      "prediction": "forgery and flying without a valid license,",
      "ground_truths": [
        "forgery and flying without a valid license,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f6ea20f22a394788814a7b4374246cf9",
      "question": "Where is the man flying?",
      "prediction": "Turkey,",
      "ground_truths": [
        "from Amsterdam, in the Netherlands, to Ankara, Turkey,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c6e9bca537a64d47a1f219d04aa484cf",
      "question": "What reasons do a man traveling with false documents?",
      "prediction": "flight without a valid license,",
      "ground_truths": [
        "never renewed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "807640dab41142919b0c0452a24b2e79",
      "question": "Who hired this man?",
      "prediction": "a Turkish company",
      "ground_truths": [
        "a Turkish company"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c4c1471da58b4cf087e179b9b71adef4",
      "question": "What did Reid win in 2004?",
      "prediction": "a Daytime Emmy Lifetime Achievement Award.",
      "ground_truths": [
        "Daytime Emmy Lifetime Achievement Award."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4d2ab04164304690b5bbc05618e82a59",
      "question": "What few years old actress Frances Reid died?",
      "prediction": "95.",
      "ground_truths": [
        "95."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a73cab123714432c9731e0568fd9d43c",
      "question": "When did Reid die?",
      "prediction": "Wednesday",
      "ground_truths": [
        "Wednesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef15b705b7c54e77b276f22c804ce15d",
      "question": "Who died Wednesday at the age of 95?",
      "prediction": "Frances",
      "ground_truths": [
        "Frances"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b5736ba3b8574e3388b2a5ebccb15b09",
      "question": "What prize won in 2004?",
      "prediction": "a Daytime Emmy Lifetime Achievement Award.",
      "ground_truths": [
        "Daytime Emmy Lifetime Achievement Award."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "420222ccc3794c10a9750e897a4532e0",
      "question": "What roles were carried fame?",
      "prediction": "Martriarch",
      "ground_truths": [
        "Alice Horton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33ad75a283674ce5a8178438d8739810",
      "question": "What was she best known for?",
      "prediction": "her decades-long portrayal of Alice Horton",
      "ground_truths": [
        "her decades-long portrayal of Alice Horton on"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "5ecdabf59c2343dbb91c9e4974e7512b",
      "question": "what has Demjanjuk been doing?",
      "prediction": "underwent pre-leukemia, kidney problems, spinal problems",
      "ground_truths": [
        "fighting charges of Nazi war crimes"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "277b88f5630044ec9dd5acfe543735e3",
      "question": "What is Demjanjuk accused of?",
      "prediction": "of war crimes and crimes against humanity.",
      "ground_truths": [
        "Nazi war crimes"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "2ea18cf48103402e90df928bd342898a",
      "question": "what does the judge reopen?",
      "prediction": "derogade proceedings.",
      "ground_truths": [
        "deportation proceedings."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0ad390a8383b449d86576e0ce552596b",
      "question": "how long has Demjanjuk been fighting charges",
      "prediction": "over two decades.",
      "ground_truths": [
        "over two decades."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d226036b088f4ed68249977782b8abd1",
      "question": "Who is John Demjanjuk?",
      "prediction": "immigrant judge with the U.S. Justice Department",
      "ground_truths": [
        "Nazi war crimes suspect"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "013ee76e68e646f3b80e544ff576f1db",
      "question": "Who accused him of accesory to murder at Sobibor?",
      "prediction": "Demjanjuk",
      "ground_truths": [
        "German authorities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cca9e488f0104c7fb0c2e2d384b34e5c",
      "question": "what does the attorney say about his client's health",
      "prediction": "has seriously deteriorated,\"",
      "ground_truths": [
        "has seriously deteriorated,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "11807b5f5a844c9086e91a8de993695d",
      "question": "what does John Demjanjuk's attorney say?",
      "prediction": "the stay was ordered after Judge Wayne Iskra in Arlington, Virginia, decided to reopen deportation proceedings.",
      "ground_truths": [
        "since his deportation was ordered, his health has seriously deteriorated,\""
      ],
      "em": 0,
      "f1": 0.24
    },
    {
      "id": "eb3457e3628748db84d3fa4ea3228461",
      "question": "What does Paul McCartney think about his music?",
      "prediction": "\"We were just kids from Liverpool,\"",
      "ground_truths": [
        "we have left really good"
      ],
      "em": 0,
      "f1": 0.1818181818181818
    },
    {
      "id": "3dcf5ed5800a4b2ebd859d1d84051742",
      "question": "Did Larry King talk with former Beatles?",
      "prediction": "interviewed",
      "ground_truths": [
        "Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "32268b9dbb924482bba1be56dbaf21ef",
      "question": "When did Paul say his music is better?",
      "prediction": "\"We were just kids from Liverpool,\"",
      "ground_truths": [
        "in 5.1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f0394084598847c8838f40352bca705f",
      "question": "Who also appeared on Larry King?",
      "prediction": "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr",
      "ground_truths": [
        "Paul McCartney, Yoko Ono Lennon, Olivia Harrison and Ringo Starr"
      ],
      "em": 0,
      "f1": 0.9523809523809523
    },
    {
      "id": "beda7e02aef94d1dbaf2f4cc9f0ae353",
      "question": "Whose widows also appeared",
      "prediction": "(l-r) Paul McCartney, Yoko Ono Lennon, Olivia Harrison",
      "ground_truths": [
        "of John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "33f9b0041e204c29b91e907624f5e7d3",
      "question": "What CNN host interviewed the former Beattle",
      "prediction": "Larry King",
      "ground_truths": [
        "Larry King,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14d7e3999cbc40c4b39857a1c235f81d",
      "question": "Who's widows also appeared?",
      "prediction": "Yoko Ono Lennon",
      "ground_truths": [
        "John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "ce59f23189e04609b555d7ec7ce8b0c3",
      "question": "Who is paul mcCartney?",
      "prediction": "Former Beatles",
      "ground_truths": [
        "Former Beatles"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c13d55e314a449d0995d6a4a5a7d2d36",
      "question": "Which former Beatles spoke with Larry King?",
      "prediction": "Ringo Starr",
      "ground_truths": [
        "Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "dd49e8874cba45c0888db2a51756b3a0",
      "question": "Who did Larry King interview",
      "prediction": "Paul McCartney",
      "ground_truths": [
        "Former Beatles Paul McCartney and Ringo Starr"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "40edae0a62794df38c7beb0ea0b0a12e",
      "question": "Whose widows appeared?",
      "prediction": "Yoko Ono Lennon",
      "ground_truths": [
        "of John Lennon and George Harrison,"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "1e56a4a2b1644ea4873578c3206ba949",
      "question": "Where is Paul from?",
      "prediction": "Liverpool,\"",
      "ground_truths": [
        "Liverpool,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ed1bc3066a44b759964f0abe310e725",
      "question": "what company does larry king work for?",
      "prediction": "CNN",
      "ground_truths": [
        "CNN's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ab608be8990f436382e64ca5cb3c10a8",
      "question": "What do the wives of the Beatles think about the band?",
      "prediction": "is very powerful and very strong. But the incredible thing",
      "ground_truths": [
        "everything they left the world and left us is uplifting and joyful.\""
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "0ec8e595494d4dd98aebe09545224171",
      "question": "Who do the former beatles talk to?",
      "prediction": "Larry King Live.\"",
      "ground_truths": [
        "CNN's \"Larry King Live.\""
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "dff8610ffbea4d1985f6746b99933f8f",
      "question": "Where did the  Beatles appear?",
      "prediction": "Las Vegas.",
      "ground_truths": [
        "\"Larry King Live.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1eb5c9be5fa41f589d2e9d56624312a",
      "question": "What did Harrison and Ono say?",
      "prediction": "her husband was friends with Cirque du Soleil founder Guy Laliberte",
      "ground_truths": [
        "\"We feel so strongly about our husbands that sometimes it's hard for us, isn't it?\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce0ab333567e4a6ea61eff034e0bf5a2",
      "question": "What was the name of the talk show host?",
      "prediction": "Larry King",
      "ground_truths": [
        "Larry King,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a4557a69506546079558b3c1cf41eb17",
      "question": "Who said their music sounds better",
      "prediction": "John Lennon",
      "ground_truths": [
        "Starr"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b0d97074e304423bab66769456c4890d",
      "question": "Who is hot on Bayern's heels?",
      "prediction": "Schalke",
      "ground_truths": [
        "Schalke"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "311ea5e6c28d46ecb378b855eb0ffe59",
      "question": "What does Marcell Jansen score?",
      "prediction": "scored the only goal of the game",
      "ground_truths": [
        "scored the only goal of the game"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa9da743626a453780aaaf56e7816508",
      "question": "Where was a 4-1 win obtained?",
      "prediction": "Schalke",
      "ground_truths": [
        "Eintracht Frankfurt"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "04aa223c25b44b43ba9c2cc94fb5ccc1",
      "question": "Who scored the only goal of the game?",
      "prediction": "Schalke",
      "ground_truths": [
        "Marcell Jansen"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76ed5aaaa5bb47d7923d45360b3332ec",
      "question": "where did they cruise for a win?",
      "prediction": "Eintracht Frankfurt",
      "ground_truths": [
        "Eintracht Frankfurt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8447a26148c9454da18385e3d9fa4a3c",
      "question": "What does Bastian Schweinsteiger rescue?",
      "prediction": "a point for Bayern Munich",
      "ground_truths": [
        "a point"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "374bacece7a6435484e32852ba73f4a8",
      "question": "Who rescued a point?",
      "prediction": "Schweinsteiger",
      "ground_truths": [
        "Schweinsteiger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc1fb48848df419fba9692dffef22844",
      "question": "where was the game?",
      "prediction": "RheinEnergieStadion.",
      "ground_truths": [
        "RheinEnergieStadion."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0c1be19d610b4fa59930cc5cc930c9d2",
      "question": "In what year was the 1st ruling",
      "prediction": "2006",
      "ground_truths": [
        "2006,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "22f46485365f4209a992b2f67feb0440",
      "question": "who is fisher suing",
      "prediction": "Gary Brooker",
      "ground_truths": [
        "former Procol Harum bandmate Gary Brooker"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6082452e7aeb4e828079e15ab4e7a9a9",
      "question": "What is the name of the person who is suing?",
      "prediction": "Matthew Fisher",
      "ground_truths": [
        "Matthew Fisher,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "930d23fe662b49d4bb01a291e8a6ad98",
      "question": "what is fisher suing for",
      "prediction": "rights in the royalties",
      "ground_truths": [
        "a share in the royalties"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ec552a3b81734f36ae2a031fc34b7519",
      "question": "For what was Fisher seeking share of royalties",
      "prediction": "in the",
      "ground_truths": [
        "1960s song \"A Whiter Shade of Pale\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b1de8025e01448dcabb96c7e5fab0186",
      "question": "What is the name of the song?",
      "prediction": "\"A Whiter Shade of Pale\"",
      "ground_truths": [
        "\"A Whiter Shade of Pale\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8c513011d82543469328cb90f190d67c",
      "question": "What is the name of the person being sued?",
      "prediction": "Matthew Fisher",
      "ground_truths": [
        "Gary Brooker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5cac964bb57446e39b0f58d6043c12d7",
      "question": "Gary Brooker was sued by who",
      "prediction": "Matthew Fisher",
      "ground_truths": [
        "Matthew Fisher"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d001d1eb3aed47ecbc488bbd04cde7ec",
      "question": "what is his defense",
      "prediction": "lawyers",
      "ground_truths": [
        "acute stress disorder"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c236a6472cee467c9f07223ae7733fee",
      "question": "Are they trying to pursue the death penalty?",
      "prediction": "Defense lawyers",
      "ground_truths": [
        "save their client from the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "722eae176eb34f668fc3e2dcc49d2112",
      "question": "when disorder did they fail to diagnose?",
      "prediction": "last week",
      "ground_truths": [
        "acute stress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "de09dd75884c4188a5d1190511a0a5a6",
      "question": "who is convicted of rape?",
      "prediction": "Steven Green",
      "ground_truths": [
        "Steven Green"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c2708a4ecb1f46bcb3ab479f79b044cd",
      "question": "what was steven green convicted of",
      "prediction": "the death penalty",
      "ground_truths": [
        "raping and killing a 14-year-old Iraqi girl."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7801d3c5fd846ef9062800d44834ac5",
      "question": "Who was convicted of rape?",
      "prediction": "Steven Green",
      "ground_truths": [
        "Steven Green"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5f53831743434c24b29ed9b858d87e4b",
      "question": "Who believes Schumacher will come out of retirement?",
      "prediction": "Luca di Montezemolo",
      "ground_truths": [
        "president Luca di Montezemolo"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "ffc6152628c540ecae6e6bab7f420f28",
      "question": "Who says the seven-time champion will join Mercedes?",
      "prediction": "Ferrari president Luca di Montezemolo",
      "ground_truths": [
        "president Luca di Montezemolo"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "980522a80742433e96b144a86fd2ddce",
      "question": "Who is believed to possibly come out of retirement?",
      "prediction": "Michael Schumacher",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "143a0df7e66a473ba5804b1c75f56c86",
      "question": "What is his age?",
      "prediction": "40-year-old",
      "ground_truths": [
        "40-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45a6d0b6364d49129a59f0a062a2f9a8",
      "question": "Who was the 40-year-old unable to make a comeback with?",
      "prediction": "Ferrari",
      "ground_truths": [
        "Michael Schumacher"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e4038716e3948b4b94243cad4cbfd1e",
      "question": "Where was he injured?",
      "prediction": "neck",
      "ground_truths": [
        "neck"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a8dc8f89ecde440e84e3efe2189030f6",
      "question": "Where did the criminals flee to?",
      "prediction": "college campus.",
      "ground_truths": [
        "onto the college campus."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "dfc221a3796f460ca42274a48ae054db",
      "question": "What were soldiers chasing?",
      "prediction": "criminals",
      "ground_truths": [
        "criminals who had fired on an army patrol"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "fa3a3ada93644452869b454ef6524da7",
      "question": "Where did this take place?",
      "prediction": "Monterrey,",
      "ground_truths": [
        "Technological Institute of Higher Learning of Monterrey,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "c8b13771a57b4743b822b8914ae68666",
      "question": "Where did they flee?",
      "prediction": "onto the college campus.",
      "ground_truths": [
        "Some of the criminals had fled onto the college campus."
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "7cd7549b7d9041d9b9f4d730df07876c",
      "question": "What are the findings of the study?",
      "prediction": "Afghanistan's economy.",
      "ground_truths": [
        "export value of this year's poppy harvest stood at around $4 billion,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d21618a59053475dbfae6173d388ac04",
      "question": "What is the agency pushing NATO forces for?",
      "prediction": "Afghanistan's economy.",
      "ground_truths": [
        "stop the Afghan opium trade"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70f76d8aab60460f8933a443a2209d67",
      "question": "What narcotic makes up half of Afghanistan's GDP?",
      "prediction": "opium",
      "ground_truths": [
        "opium"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1776154b9a104d19991ffd5b8d6e9c6c",
      "question": "what is the skyrocketing problem?",
      "prediction": "the export value of this year's poppy harvest stood at around $4 billion, a 29 per cent increase over 2006.",
      "ground_truths": [
        "the narcotics trade."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09dc411bbb1f48eaa2263fd2feedfd7e",
      "question": "What drug makes up half of Afghanistan's gross domestic product?",
      "prediction": "opium",
      "ground_truths": [
        "opium"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7074419bbc394a039c03413007094b94",
      "question": "what NATO forces will attack?",
      "prediction": "terrorists",
      "ground_truths": [
        "Afghan opium trade"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d46d8974b9744a3bafb6d69a29735554",
      "question": "Which military force does the agency push to attack the problem?",
      "prediction": "NATO",
      "ground_truths": [
        "NATO"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "943da1a3b8b14a7f97e82a17737557a4",
      "question": "Who advocates providing income alternative for poppy farmers",
      "prediction": "Antonio Maria Costa,",
      "ground_truths": [
        "Appathurai"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25954c5bd38744d7872a71f0b604576c",
      "question": "What flower do the Taliban trade in?",
      "prediction": "opium",
      "ground_truths": [
        "opium poppies"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e77d740409e349ecbebe4fc81e84d520",
      "question": "What do drugs fund?",
      "prediction": "insurgency,",
      "ground_truths": [
        "the insurgency,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f1a99ce0121f4c7892a68a47221ff8ba",
      "question": "Which country did the study find opium made up half of it's GDP?",
      "prediction": "Afghanistan's",
      "ground_truths": [
        "Afghanistan's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62c23c6b9c3b4d9896873ec4a545e70a",
      "question": "What is the U.N. drug chief advocating?",
      "prediction": "NATO to do more to stop the Afghan opium trade",
      "ground_truths": [
        "urged NATO to take a more active role in countering the spread of the"
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "b6652e6bad6f4d90b962716fecbe5cf1",
      "question": "what is unchecked in Taliban strongholds?",
      "prediction": "crop,",
      "ground_truths": [
        "poppy production"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b460e2ad790b419e84c17ee5ae051a26",
      "question": "What makes up half of Afghanistans GDP?",
      "prediction": "Opium",
      "ground_truths": [
        "opium"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6f63891debcb465aaee5fcbd3ede0a86",
      "question": "What makes up half of Afghanistan's gross domestic product",
      "prediction": "Opium",
      "ground_truths": [
        "opium"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6fc267ea05794476a82917156e3e519e",
      "question": "What does the U.N drug chief advocate?",
      "prediction": "NATO to do more to stop the Afghan opium trade",
      "ground_truths": [
        "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "861b642c508e487798f314f2c3ade27a",
      "question": "What makes half of Afghanistan's gross domestic product?",
      "prediction": "Opium",
      "ground_truths": [
        "opium"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "718236817764436eaf999352c380876e",
      "question": "Where is poppy trade unchecked?",
      "prediction": "Taliban-controlled Helmand province in April 2007.",
      "ground_truths": [
        "Taliban stronghold of Helmand,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "951d6908625f412daef5fc65020c52f0",
      "question": "What do the drugs fund",
      "prediction": "insurgency,",
      "ground_truths": [
        "the insurgency,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a9a18aead40e4b4191e1269e1f24bd8c",
      "question": "Where is the trade unchecked?",
      "prediction": "southern provinces",
      "ground_truths": [
        "in the lawless southern provinces and especially in the Taliban stronghold of Helmand,"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "fb2604a388d048308734395577e3d885",
      "question": "What is funding the insurgency?",
      "prediction": "Opium",
      "ground_truths": [
        "drugs"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b95f7ffb86c34a2e82ac7ea8ac2a843f",
      "question": "What does UN Drug chief advocate?",
      "prediction": "NATO to do more",
      "ground_truths": [
        "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d360891ed1b84beea625c88b3d894e9b",
      "question": "What does the U.N. drug chief advocate?",
      "prediction": "NATO to do more to stop the Afghan opium trade",
      "ground_truths": [
        "urged NATO to take a more active role in countering the spread of the"
      ],
      "em": 0,
      "f1": 0.3
    },
    {
      "id": "bb9cd87d40ec484583f1d04e7cb6f0fc",
      "question": "Whose home was the actress found dead at?",
      "prediction": "Phil Spector",
      "ground_truths": [
        "Phil Spector"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7c7150625e3f48dc92f4240336c4795f",
      "question": "Who was found dead at Phil Spector's home?",
      "prediction": "B-movie queen Lana Clarkson",
      "ground_truths": [
        "Lana Clarkson"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "874bbc325247476e9a6d8cbb373aa18b",
      "question": "What did the defense say Clarkson did?",
      "prediction": "\"I think I killed somebody,\"",
      "ground_truths": [
        "grabbed the gun and"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "af764b251ad54f0aada935fe01d05a7a",
      "question": "Where did the defense say Clarkson shoot herself?",
      "prediction": "\"I think I killed somebody,\"",
      "ground_truths": [
        "mouth."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ed9bbda118d04a9c8eab9ed97ade146e",
      "question": "Where did the defense say she shot herself?",
      "prediction": "House of Blues in Hollywood.",
      "ground_truths": [
        "in the mouth."
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "d11eb1148eef497488293a76f1d2be07",
      "question": "Who was found dead?",
      "prediction": "Lana Clarkson",
      "ground_truths": [
        "Lana Clarkson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ce14ece239c49158eb1780e0fa5ed6c",
      "question": "when Rick Steves born?",
      "prediction": "1979",
      "ground_truths": [
        "1979"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fbfd988c52a043089ff4af3593d77131",
      "question": "What series is Steves working on?",
      "prediction": "(Tribune Media Services)",
      "ground_truths": [
        "my recent 12-day trip to Iran to film a public-television show."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "046f31421adb4c58959d44292095a970",
      "question": "When did the rig sink?",
      "prediction": "April 22,",
      "ground_truths": [
        "April 22,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "061a2eadb2db4bc6a8dfb22cc441e562",
      "question": "What did Landry say?",
      "prediction": "the first sign of trouble was when drilling \"mud\"",
      "ground_truths": [
        "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top of the derrick,\""
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "ce27cf9031894392a2ada8c5c5933d8a",
      "question": "who radioed rig's bridge?",
      "prediction": "Alwin Landry's supply vessel Damon Bankston",
      "ground_truths": [
        "Landry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "169e048fb1f84de0b9dff5bbe3ddfa9e",
      "question": "what was alongside Deepwater Horizon at time of blast?",
      "prediction": "Alwin Landry's supply vessel Damon Bankston",
      "ground_truths": [
        "Alwin Landry's supply vessel Damon Bankston"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd80a000288946039bdf2cf26930cff1",
      "question": "Whose supply vessel was alongside Deepwater Horizon?",
      "prediction": "Alwin Landry's",
      "ground_truths": [
        "Alwin Landry's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a4c865a666034aefa02347e9d9c5d9be",
      "question": "Where was the Deepwater Horizon at the time of the blast?",
      "prediction": "Kenner, Louisiana",
      "ground_truths": [
        "Kenner, Louisiana"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0ba6e9a2c0ee4d4487d63a05d1570e50",
      "question": "Who is Alwin Landry?",
      "prediction": "cabinets of the drilling",
      "ground_truths": [
        "the captain of a nearby ship"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "2fbb04100c444abda7f972b3c7ca55b9",
      "question": "What does Bloomberg blame the delay on?",
      "prediction": "\"disagreements\" with the Port Authority of New York and New Jersey,",
      "ground_truths": [
        "\"disagreements\" with the Port Authority of New York and New Jersey,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5464eecea5074ee49a1f281e3855f2d1",
      "question": "Bloomberg blames delayed on where?",
      "prediction": "\"disagreements\" with the Port Authority of New York and New Jersey,",
      "ground_truths": [
        "\"disagreements\" with the Port Authority of New York and New Jersey,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7a9a5bd73444bfd8f0d54e0ee743304",
      "question": "Who said \"There is no chance of it being open on time\"?",
      "prediction": "Bloomberg",
      "ground_truths": [
        "Bloomberg"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "386263cd9738415982f8a63c4d30c2c5",
      "question": "Will it be open on time?",
      "prediction": "no chance of",
      "ground_truths": [
        "no chance"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "ef60d03b67d34a1190d04693fee8767f",
      "question": "where did it took place",
      "prediction": "New York City",
      "ground_truths": [
        "Located underground in the former World Trade Center's \"archaeological heart,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b2726529c0374da2bcb670a3c89f58a4",
      "question": "what was scheduled to open?",
      "prediction": "National September 11 Memorial Museum",
      "ground_truths": [
        "the National September 11 Memorial Museum"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "241f5227a1124be591341cf33b2ecb2f",
      "question": "When was the museum scheduled to open?",
      "prediction": "11th anniversary of the September 11, 2001, terror attacks.",
      "ground_truths": [
        "11th anniversary of the September 11, 2001,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "cff5c9d1f6fd4c6887578a64c02eb0b2",
      "question": "Who is the antichrist?",
      "prediction": "Charlotte Gainsbourg and Willem Dafoe",
      "ground_truths": [
        "Lars von Trier."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eb833d4f9eb4430689b0e718be45a714",
      "question": "Which film was met with a vitriolic reaction?",
      "prediction": "\"Antichrist.\"",
      "ground_truths": [
        "\"Antichrist.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b777ca5768e048c792d5c44d0c2b198c",
      "question": "Who does the film star?",
      "prediction": "Charlotte Gainsbourg and Willem Dafoe",
      "ground_truths": [
        "Charlotte Gainsbourg and Willem Dafoe"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "46e965e7eede423cab40e895aa8c755b",
      "question": "Willem Dafoe and which other actor star in the film?",
      "prediction": "Charlotte Gainsbourg",
      "ground_truths": [
        "Charlotte Gainsbourg"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0940d4e2f8b4c23817a178bf3f2ee85",
      "question": "how many nations meet?",
      "prediction": "more than 30",
      "ground_truths": [
        "30"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6eb7f187d8aa4f319e63ad269404dab2",
      "question": "from how many nations did the Leaders took place at the regional summit in Cancun?",
      "prediction": "more than 30",
      "ground_truths": [
        "more than 30"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6778d6c630a84294a6dd81fbfabf9177",
      "question": "what will be discuss?",
      "prediction": "that the United States and Canada",
      "ground_truths": [
        "whether to recognize Porfirio Lobo as the legitimate president of Honduras."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b612d7f272cd452dab8379571e41f3dd",
      "question": "What is the objective of summit?",
      "prediction": "is intended to take the Rio Group to a new level by creating the organization.",
      "ground_truths": [
        "to take the Rio Group to a new level by creating the organization."
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "50b4c8f6f61a48c289fff1e90d3c1c68",
      "question": "Who meet at regional summit in cancun?",
      "prediction": "Leaders of more than 30 Latin American and Caribbean nations",
      "ground_truths": [
        "Brazil, Argentina, Mexico, Colombia and Venezuela."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "987b4118a9304f2f8193ab27a36b0489",
      "question": "what will the Summit discuss about?",
      "prediction": "whether to recognize Porfirio Lobo as the legitimate president of Honduras.",
      "ground_truths": [
        "whether to recognize Porfirio Lobo as the legitimate president of Honduras."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a8dca68f6c5497e9a6bbe453f46fb7c",
      "question": "What was important in his success?",
      "prediction": "unparalleled fundraising",
      "ground_truths": [
        "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to"
      ],
      "em": 0,
      "f1": 0.12903225806451613
    },
    {
      "id": "e30767b1bf7e4bc89234ad7a4f69713b",
      "question": "who campaign helped fortunes soar?",
      "prediction": "Barack Obama,",
      "ground_truths": [
        "Obama's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6858be8329164598910400cf9a7d4287",
      "question": "Who is Al-Shabaab linked to?",
      "prediction": "al Qaeda,",
      "ground_truths": [
        "al Qaeda,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f52c8019fef94e7d88918d8444d0fdf3",
      "question": "Which governments pledged cooperation",
      "prediction": "Kenya",
      "ground_truths": [
        "Kenyan and Somali"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f542a53e2f7453fbeb43c7da3154697",
      "question": "Which countries troops were moved",
      "prediction": "Somali",
      "ground_truths": [
        "Islamic militants"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20e05974b36d4960976e61bce8c630a2",
      "question": "What 2 governements are involved?",
      "prediction": "Kenyan and Somali",
      "ground_truths": [
        "Kenyan and Somali"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3c814241efdb4f528a806c5090744d06",
      "question": "Against who is the fight",
      "prediction": "Kenya",
      "ground_truths": [
        "Al-Shabaab"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6cdb70e3b8ee43c08d280a50de57daf1",
      "question": "what are the kenyan and somali governments pledging",
      "prediction": "joint communique declaring Al-Shabaab \"a common enemy to both countries.\"",
      "ground_truths": [
        "military"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ebbe2ff724e489f9707b61473c31ee5",
      "question": "what does al-shabaab deny",
      "prediction": "responsibility for the abductions.",
      "ground_truths": [
        "responsibility for the abductions."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2f912f57b1ee4376a561c03370a092a3",
      "question": "What university does a student leader attend?",
      "prediction": "Stanford",
      "ground_truths": [
        "Stanford"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab125bf5e094423eb2f4d733160ad567",
      "question": "Where were the soldiers and taliban fighting?",
      "prediction": "Swat Valley,",
      "ground_truths": [
        "Swat Valley."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e99da60aff5a43b2892b05062baec6d5",
      "question": "Who is helping to lead them?",
      "prediction": "a group of college students of Pakistani background",
      "ground_truths": [
        "Shiza Shahid,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec9157de0a6b4e9b9624dd1903625528",
      "question": "Who are the victims?",
      "prediction": "girls",
      "ground_truths": [
        "School-age girls"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "942bf4f0c7314363b7e4c3ec2b28128f",
      "question": "Where was the fight?",
      "prediction": "Swat Valley,",
      "ground_truths": [
        "Swat Valley."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88ddb1ac0c1a4575aa6e022f3b851ede",
      "question": "What is the retreat giving girls?",
      "prediction": "safe surroundings.",
      "ground_truths": [
        "safer surroundings."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6e82a50369b147508672ea8bf7f81e33",
      "question": "In which city is the retreat?",
      "prediction": "Islamabad",
      "ground_truths": [
        "Islamabad"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "568e71d3966d49cd9d3ae1c99ac195d7",
      "question": "Where is the safe haven?",
      "prediction": "Swat Valley,",
      "ground_truths": [
        "Islamabad"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef2637a7f4cd4b1cb41e48fba82dfc56",
      "question": "When did the Delta Queen first serve?",
      "prediction": "1800s and the era of Mark Twain,",
      "ground_truths": [
        "1927"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fdec7d459d40463c89b0fe6c78d175ea",
      "question": "what has been in service",
      "prediction": "The Delta Queen",
      "ground_truths": [
        "The Delta Queen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "626b8540033d498daef0671e87f1206d",
      "question": "What does the boat provide?",
      "prediction": "economic opportunities.",
      "ground_truths": [
        "economic opportunities."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9fea05ecc2f44c2bbad533f62b3c9ddc",
      "question": "What will be an integrated part of the new OS?",
      "prediction": "iCloud",
      "ground_truths": [
        "iCloud service"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d4215b15e07845fb921dca91acb0ff2f",
      "question": "What has been overshadowd",
      "prediction": "MacBook",
      "ground_truths": [
        "iPods"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d1c7a748e934b4dbc7ee928eae541b9",
      "question": "What was overshadowed",
      "prediction": "iPod",
      "ground_truths": [
        "iPods"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2a9ab320d8a94bfdb157ed5e57c7eb0a",
      "question": "How many new clock faces were unvieled",
      "prediction": "16",
      "ground_truths": [
        "16"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d6f655d36c854ff98ea63df9d1e883cf",
      "question": "What will allow storage on remote servers",
      "prediction": "iCloud service",
      "ground_truths": [
        "The iCloud service"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d50e709ce450451797883945fe60b25f",
      "question": "How many new clock faces did Apple unveil?",
      "prediction": "16",
      "ground_truths": [
        "16"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60ad8cecb37143498bfe42a3af10fed4",
      "question": "What ws unveiled",
      "prediction": "new Touch,",
      "ground_truths": [
        "new Touch,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9c75b8a679d24dd192577a07dfed38a9",
      "question": "What devices got freshened  up by Apple?",
      "prediction": "iPod",
      "ground_truths": [
        "iPod Touch?"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "af5369c41cba4379978174fe353ddff6",
      "question": "What is the name of the shop?",
      "prediction": "Haeftling,",
      "ground_truths": [
        "Haeftling,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cff18af0aed84c39ab0d156de7f507f8",
      "question": "What prisoner rights organizations benefit?",
      "prediction": "Amnesty International.",
      "ground_truths": [
        "Amnesty International."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "afa333ac95d848f0a60cde4f10d6e261",
      "question": "where do the profits go",
      "prediction": "to organizations that support prisoners' rights and better conditions for inmates,",
      "ground_truths": [
        "organizations that support prisoners' rights and better conditions for inmates, like Amnesty International."
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "56a527cc368e4b10ac88117b33ab6370",
      "question": "Where does a portion of the profit go?",
      "prediction": "support prisoners' rights and better conditions for inmates,",
      "ground_truths": [
        "to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International."
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "57f26f2947864f429083b603d80000aa",
      "question": "who designs clothes",
      "prediction": "prison inmates.",
      "ground_truths": [
        "Inmates"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "79e87ee729dd4f40b1492c9d24d26541",
      "question": "What are the prison inmates designing?",
      "prediction": "stylish clothes",
      "ground_truths": [
        "stylish clothes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0a7880cce6f4bce95aaaf322c41355f",
      "question": "What portion of profits go to the prisoner rights organizations?",
      "prediction": "part of the proceeds from sales",
      "ground_truths": [
        "part of the proceeds"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "de51e8fbf7aa47c08b17fd80a80b4a38",
      "question": "When should you take special care?",
      "prediction": "someone needs medical attention after a head injury.",
      "ground_truths": [
        "after a head injury,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "026a2a679a324a94ab9066e17933cc7d",
      "question": "What was the cause of death for Richardson?",
      "prediction": "head injury.",
      "ground_truths": [
        "head injury."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8542e729373d4f66a5a663b56e6616f8",
      "question": "Whose death is raising questions?",
      "prediction": "Natasha Richardson",
      "ground_truths": [
        "Natasha Richardson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "983656ab5e7a4ae9b635769d195b0d2b",
      "question": "Where do visitors take up Jewish  tradition of leaving stones as  grave markers?",
      "prediction": "at Capt. Marissa Alexander's",
      "ground_truths": [
        "Arlington National Cemetery's Section 60,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2aa6b5ff469342ad9ff08b7cc57d664c",
      "question": "Which culture leaves stones on grave markers?",
      "prediction": "Jewish",
      "ground_truths": [
        "Jewish"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d44d14a1f37e4cd7b716c2820f5ae814",
      "question": "The national cementary's Section 60 contains dead people from which wars?",
      "prediction": "Iraq and Afghanistan.",
      "ground_truths": [
        "in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "9d60ac112d634c7fbb6ea52d2c6f3776",
      "question": "What are some mourners doing?",
      "prediction": "put down flowers.",
      "ground_truths": [
        "walk slowly between the rows and rows of headstones, looking for a familiar name."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "506e226a5b1f417ba085b01d2cb18913",
      "question": "What is Section 60 holding?",
      "prediction": "a small stone on the headstones to show that a visitor had been to the grave.",
      "ground_truths": [
        "many casualties of the wars in Iraq and Afghanistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b7377028d0274399b8189b9562f5bb1d",
      "question": "What kind of publication was placed behind store counters?",
      "prediction": "Lifeway Christian",
      "ground_truths": [
        "magazine"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "64e4eebd844f4487b8aef254b3a7bf0c",
      "question": "Who owns Lifeway stores?",
      "prediction": "Roland S. Martin",
      "ground_truths": [
        "Southern Baptist Convention,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47d30bda511f43808d50a044925ef722",
      "question": "Who was featured on the magazine's cover?",
      "prediction": "five female pastors",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6f3ed56417de4830bb62a1de86cee38f",
      "question": "What did Martin say about Lifeway?",
      "prediction": "should be given hell",
      "ground_truths": [
        "Christian Stores should be given hell for its actions against GospelToday."
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "89b7bbc7406a47cfbb086010e72d1331",
      "question": "What was on the cover of the magazine?",
      "prediction": "five female pastors",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "736112d0d87748f0943c9088c28a57a2",
      "question": "Who was featured on the cover?",
      "prediction": "five female pastors",
      "ground_truths": [
        "five"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "af104d8adfb148d68ee36715cbbd0d7c",
      "question": "Where did Lifeway stores put Christian magazine?",
      "prediction": "behind the counter.",
      "ground_truths": [
        "placed behind the counter."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2158e5af296446b2b3cef69beb058717",
      "question": "What was featured on the cover of the magazine?",
      "prediction": "five female pastors",
      "ground_truths": [
        "celebrities"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3dfb0ef71f1e4c4b88695ca0bcb4add2",
      "question": "Who put a Christian magazine behind the counter?",
      "prediction": "Lifeway Christian Stores",
      "ground_truths": [
        "Lifeway's 100-plus stores nationwide"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "95e677b8475e40e3a6abedec6fe17104",
      "question": "What was the score?",
      "prediction": "3-2",
      "ground_truths": [
        "3-2 victory."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "383db81acf1f476d8aa21be94cc9daa7",
      "question": "Who did Fulham beat in Switzerland",
      "prediction": "Basel",
      "ground_truths": [
        "Basel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60b894ff9c6c43c58bc90fcc2aa78e43",
      "question": "What countries were the games played?",
      "prediction": "Switzerland",
      "ground_truths": [
        "Switzerland"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b5b3c4c6f37c4de3907df3b82c13a2d7",
      "question": "Who beat Lisbon?",
      "prediction": "Galatasaray",
      "ground_truths": [
        "Bundesliga Hertha Berlin"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5c6d814143a4f1eb7cc7546d9654173",
      "question": "Who did Fulham beat 3-2?",
      "prediction": "Basel",
      "ground_truths": [
        "Basel"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d58a27153bc74f08bfa857c4a80b1c90",
      "question": "What was the score in the Werder Bremen Athletic Bilbao game",
      "prediction": "3-0",
      "ground_truths": [
        "3-0"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3dd9a7133b594635ac914a6abccf0e46",
      "question": "Who beat Bucharest?",
      "prediction": "Panathinaikos",
      "ground_truths": [
        "Panathinaikos"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b583c0af110a4ab99bd001f7c487baae",
      "question": "Bishop was charged with what?",
      "prediction": "capital murder and three counts of attempted murder",
      "ground_truths": [
        "capital murder and three counts of attempted"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "527ce6c8c0fb4e34a273e792ef21446d",
      "question": "When was the faculty meeting?",
      "prediction": "Friday's",
      "ground_truths": [
        "last week."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccc7685833724221ac2c3f779dbbfadd",
      "question": "What is Miller's first name?",
      "prediction": "Roy",
      "ground_truths": [
        "Roy"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9662731a19dd4d7eb774368c797ff071",
      "question": "what has roy miller said",
      "prediction": "\"something's wrong with this lady.\"",
      "ground_truths": [
        "\"something's wrong with this lady.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "16a9d0ec55594c81a528e59ddb908fee",
      "question": "What is Bishop charged with?",
      "prediction": "capital murder and three counts of attempted murder",
      "ground_truths": [
        "capital murder and three counts of attempted murder"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "04a914d3e5484f15bce5c235722323ff",
      "question": "Lawyer told reporters he regrets calling client Amy Bishop what?",
      "prediction": "described her as \"wacko.\"",
      "ground_truths": [
        "\"wacko.\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "35110c01e79f454299244040123f6ef2",
      "question": "what did the lawyer say",
      "prediction": "\"something's wrong with this lady.\"",
      "ground_truths": [
        "he regrets describing her as \"wacko.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a3b5274b19141bf9e88083177746248",
      "question": "Who called Bishop \"wacko?\"",
      "prediction": "Roy Miller",
      "ground_truths": [
        "Roy Miller"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60df89b8d9d34129982e9438443b7d96",
      "question": "who were two victims",
      "prediction": "Dan Parris,",
      "ground_truths": [
        "Rob Lehr,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c5baaa251894e0688ca552be083606f",
      "question": "Was there any witnesses?",
      "prediction": "said it was flying unusually low,\"",
      "ground_truths": [
        "flying unusually low,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "493624c8263b40ec97c127e45b45d5c1",
      "question": "Who was killed in the plane crash?",
      "prediction": "A flight engineer,",
      "ground_truths": [
        "The pilot,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e446b260a3db4e45b9948514d5d49865",
      "question": "Where did the plane crash?",
      "prediction": "central Nairobi.",
      "ground_truths": [
        "a three-story residential building in downtown Nairobi."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "0ae51cad51b340cb9ce156a667e11a5e",
      "question": "Were there any taken to hospital?",
      "prediction": "Both men were hospitalized and expected",
      "ground_truths": [
        "hospitalized"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "6536963fab4940258cbdea45da005467",
      "question": "who was killed",
      "prediction": "pilot,",
      "ground_truths": [
        "The pilot,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ede5f452230413ca47938c5b9144011",
      "question": "What were the two victims shooting?",
      "prediction": "an independent documentary on poverty in Africa.",
      "ground_truths": [
        "an independent documentary on poverty in Africa."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "051e0cb33cb64409bbf8e8e79165bacf",
      "question": "What made landfall near Cocodrie, Louisiana?",
      "prediction": "Hurricane Gustav",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4bf907acc0204e0785974060d84db533",
      "question": "What will it do to Texas?",
      "prediction": "stalled over Louisiana and northeast",
      "ground_truths": [
        "\"exacerbate the threat of heavy rains and inland flooding.\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "bb0d3b5086f845df8acfbf503fa4c2ff",
      "question": "What category storm was it?",
      "prediction": "2",
      "ground_truths": [
        "2"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fa3b764edd7a4c44be5b8921112e8bf6",
      "question": "what time did this occur?",
      "prediction": "8 a.m. CT.",
      "ground_truths": [
        "8 a.m. CT."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "916d57f6183b4c52999933fe394cae6e",
      "question": "What is stalling over northeast Texas?",
      "prediction": "Hurricane Gustav",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cf18870732dc40f89d6dd59b7c67e691",
      "question": "What made landfall?",
      "prediction": "Hurricane Gustav",
      "ground_truths": [
        "Hurricane Gustav"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8e376a7767224f8381c8ee661c43ad16",
      "question": "What downgrades it to a Category 2 storm?",
      "prediction": "gust of at least 131 mph,",
      "ground_truths": [
        "110 mph,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "29cd58ae62e94dc4b7315ce4b50010ef",
      "question": "what were the hits",
      "prediction": "\"Walk -- Don't Run\"",
      "ground_truths": [
        "\"Hawaii Five-O\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1a4a586dd10143f3a68247ae4c535713",
      "question": "What age was Bob Bogle when he died?",
      "prediction": "75.",
      "ground_truths": [
        "75."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8657886cea34216b10630e96b7a2932",
      "question": "Who influenced countless bands with guitar sound?",
      "prediction": "Bob Bogle,",
      "ground_truths": [
        "Bob Bogle,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bc61a95b6c25463db3459cd626a11ad0",
      "question": "What hits did they have?",
      "prediction": "\"Walk",
      "ground_truths": [
        "Don't Run\" and \"Hawaii Five-O\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "73df4341944b4832a7110492fec95939",
      "question": "Who was Ventures' lead guitarist?",
      "prediction": "Bob Bogle,",
      "ground_truths": [
        "Bob Bogle,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "786e56a75654431f8335c0a57f0e3d3e",
      "question": "Which band was he lead guitarist of?",
      "prediction": "Venture",
      "ground_truths": [
        "Ventures"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7538664bfbdf4f4686e914d02414532e",
      "question": "How long will the examination take?",
      "prediction": "several weeks,",
      "ground_truths": [
        "several weeks,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5c5aa44f8e2f4f8d9f649441002cf806",
      "question": "Where did the deputy shoot himself at?",
      "prediction": "Crandon, Wisconsin,",
      "ground_truths": [
        "in the head"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb4519ed1e374168848b6c5286447f32",
      "question": "Forensic examination of the deputy could take how many weeks?",
      "prediction": "several",
      "ground_truths": [
        "several"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e8a0ee8622a34c72b67364ba7b87d7cc",
      "question": "What did Peterson confess to friends?",
      "prediction": "about the shootings,",
      "ground_truths": [
        "about the shootings,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e45b34a0a6c46e98d2e25f54b59c906",
      "question": "What type of wounds were the shots consistent with?",
      "prediction": "self-inflicted wounds,",
      "ground_truths": [
        "self-inflicted"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2fb6d6705bd14c00912a97b755696ad6",
      "question": "How many people were killed at the party?",
      "prediction": "six",
      "ground_truths": [
        "six young"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d962251db8ec4ea6ac1f7458725f09cd",
      "question": "What did the Attorney General say happened?",
      "prediction": "sheriff's deputy who killed six young people at a house party",
      "ground_truths": [
        "himself three times in the head with a .40-caliber pistol,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4b3003e7d6b492e876c74ed3fcc7a20",
      "question": "How many timesdid the deputy shoot himself?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "423c074089a34a9087516b566aec95ac",
      "question": "How many people did Peterson kill at a party?",
      "prediction": "six",
      "ground_truths": [
        "six"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28a94431b64446adbfcca8f7030b8374",
      "question": "Who shot six people at the party?",
      "prediction": "Tyler Peterson",
      "ground_truths": [
        "Tyler Peterson, a sheriff's deputy,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2586234cb1754cfdab82bd4d52a73115",
      "question": "Where did the deputy shoot himself?",
      "prediction": "House",
      "ground_truths": [
        "head"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "129ac221f74d4da6a9d78059c18caf86",
      "question": "What does Jindal hope?",
      "prediction": "people look at the content of the speech, not just the delivery.",
      "ground_truths": [
        "people look at the content of the speech, not just the delivery."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b117989cc724148a5e4b3d94b8e0b9d",
      "question": "What did Jindal talk about?",
      "prediction": "the stimulus package,",
      "ground_truths": [
        "Barack Obama:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f9679f4837d4c1b809870b506382a30",
      "question": "Who is Jindal interviewing with?",
      "prediction": "Larry King",
      "ground_truths": [
        "Larry King"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "11a8100cb9134bc88de9c849856ce25d",
      "question": "Who had the prime time exclusive?",
      "prediction": "Larry King",
      "ground_truths": [
        "Larry King"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e021412ffee94b2b8a062f71b1298fd4",
      "question": "Who talked to Larry King?",
      "prediction": "Gov. Bobby Jindal",
      "ground_truths": [
        "Republican Gov. Bobby Jindal"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "66f90ce76e994be098c155d5873bba30",
      "question": "What does Jindal hope people judge?",
      "prediction": "content of the speech,",
      "ground_truths": [
        "look at the content of the speech, not just the delivery."
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "fa043db827ff44d48d9ec1814c1c7223",
      "question": "What state does Bobby Jindal lead?",
      "prediction": "Louisiana",
      "ground_truths": [
        "Lousiana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d35e612afe4d4fc694cf30e00b988d54",
      "question": "What did Jindal say about the content of his speech?",
      "prediction": "I was outlining a philosophical disagreement that says we need to get businesses hiring again. We need to put more money in the private sector.",
      "ground_truths": [
        "\"I'm certainly not nearly as good of a speaker as he is.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afff7321cf184e6b97fd59a959a10bc0",
      "question": "Who is the Louisiana governer?",
      "prediction": "Bobby Jindal",
      "ground_truths": [
        "Bobby Jindal"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f895a8e048ca4a8a860ede9678d9a572",
      "question": "How many people were killed this month by lightning strikes?",
      "prediction": "11",
      "ground_truths": [
        "11"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fdf5459888074c2689115afbbdf1d86b",
      "question": "What season is the deadliest for lightning?",
      "prediction": "Summer",
      "ground_truths": [
        "Summer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91aa94133dfd4f64904d1c79b36d8f10",
      "question": "How many lightning deaths have occurred in the US this year?",
      "prediction": "six",
      "ground_truths": [
        "15"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3500fc318e674c8698b5f0859c22339b",
      "question": "This week is the first official what of summer?",
      "prediction": "\"Lightning Safety Awareness Week\"",
      "ground_truths": [
        "\"Lightning Safety Awareness Week\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "573580d045a1403d9082b4ee81aec116",
      "question": "Where did the 15 deaths occur?",
      "prediction": "California, Texas",
      "ground_truths": [
        "in the U.S."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "583d02cebe5241639c457f4026bfb108",
      "question": "How many people have been killed already this month by lightning strikes in the U.S.?",
      "prediction": "11",
      "ground_truths": [
        "11"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cab4c82707424a6a81b9710b01c2c4fc",
      "question": "What is summer deadliest season for?",
      "prediction": "lightning strikes",
      "ground_truths": [
        "Lightning strikes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d5ba53a46f9b4a1bb0fbe9dcfd80ddf0",
      "question": "Where were the country's lawmakers stranded?",
      "prediction": "Djibouti,",
      "ground_truths": [
        "Djibouti,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7554be436374465c81b24a7a2e0bd4c1",
      "question": "who seized control?",
      "prediction": "Radical",
      "ground_truths": [
        "Islamist fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5e4452ea2976418eb7099bc248dbed10",
      "question": "What did the home Depot ceo say?",
      "prediction": "\"This is how a civilization disappears,\"",
      "ground_truths": [
        "If a retailer has not gotten involved with fight over this bill, he \"should be shot,\""
      ],
      "em": 0,
      "f1": 0.1
    },
    {
      "id": "f107128551c149079e2da4669ceb0b8f",
      "question": "What was the rally in support of?",
      "prediction": "the Employee Free Choice Act",
      "ground_truths": [
        "Employee Free Choice act"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d1fcfcaf63b94a048bcebf88ad55d818",
      "question": "In support of what is the rally held?",
      "prediction": "Employee Free Choice act",
      "ground_truths": [
        "Employee Free Choice act"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd25e49e70ae419e9a6b5e76355f0f86",
      "question": "Who says \"this is how a civilization disappears\"?",
      "prediction": "Bernie Marcus,",
      "ground_truths": [
        "Marcus"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a229d468794241bbbc6ea1ed0993099d",
      "question": "How many days would the company have to negotiate?",
      "prediction": "90",
      "ground_truths": [
        "90"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "159cd77edb104c1ebf8e3dbef2bab476",
      "question": "What did the \"reset\" button actually say?",
      "prediction": "Russian word.",
      "ground_truths": [
        "\"peregruzka\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc5c610055d74dec9317c671ae0532ee",
      "question": "What country wants to reset relations with Russia?",
      "prediction": "The Obama administration",
      "ground_truths": [
        "the United States,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2160f240eb1847239f1438b3d839f4b0",
      "question": "What countries want to reset relations?",
      "prediction": "Russia",
      "ground_truths": [
        "Russia"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "441bc101ad874330a800cd51ac465226",
      "question": "Who is the US trying to \"Reset\" relations with?",
      "prediction": "Russia",
      "ground_truths": [
        "Russia"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7c332d3bebac4a458ad358a37ffcbb13",
      "question": "What does the text on the button actually translate to?",
      "prediction": "'peregruzka,' which means 'overcharged.'",
      "ground_truths": [
        "'overcharged.'\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "4269197f565b46028a9227548797dbd1",
      "question": "What did the text on the button translate to?",
      "prediction": "'We want to reset our relationship",
      "ground_truths": [
        "'overcharged.'\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c1a605ea40554db7b369db840ed6d9a4",
      "question": "Which Secretary of State gave their Russian Counterpart a Reset Button?",
      "prediction": "Hillary Clinton",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce4eaf2d23b14736a9a30d4e272b930f",
      "question": "What treaty did both countries want to renegotiate?",
      "prediction": "Strategic Arms Reduction",
      "ground_truths": [
        "Strategic Arms Reduction"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d6141c837f8445bc8f171995f15b8647",
      "question": "Who gave a reset button to her Russian counterpart?",
      "prediction": "Secretary of State Hillary Clinton",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "1a82de30a1e546bbaf099a05d79a6230",
      "question": "What could the states refuse?",
      "prediction": "renew registration until the manufacturer's fix has been made.",
      "ground_truths": [
        "renew registration"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "07ebc87534e64dfca8d40500fe2ecd3b",
      "question": "Do they ensure auto owners comply?",
      "prediction": "there is not a process to",
      "ground_truths": [
        "that"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2662231ec6934df58d84e851aaff80fc",
      "question": "What do most recalls involve?",
      "prediction": "free fixes for the consumer.",
      "ground_truths": [
        "free fixes for the consumer."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a10d412bc874427b9f8df9a729c686a8",
      "question": "What kind of process does not exist?",
      "prediction": "a way to ensure compliance with auto recalls?",
      "ground_truths": [
        "to ensure that auto owners comply with recalls."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "61727044bdac4760964296c822ea1ffa",
      "question": "What percentage of consumers don't comply?",
      "prediction": "25 percent",
      "ground_truths": [
        "25"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "614e09625ec04274b0a1981dc910ce79",
      "question": "What can insurers use VINs for?",
      "prediction": "contact the insured drivers who have failed to comply,\"",
      "ground_truths": [
        "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\""
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "f74b173f48654bc9832934834956c10f",
      "question": "When will employees celebrate Steve Jobs' life?",
      "prediction": "October 19,",
      "ground_truths": [
        "October 19,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d54a2fd059b54e2d80b503109524db0a",
      "question": "What has Jobs battles for years?",
      "prediction": "cancer",
      "ground_truths": [
        "cancer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f9bf09905dc249e69f69c4db9b323d0f",
      "question": "Who will celebrate Steve Jobs' life?",
      "prediction": "Apple employees",
      "ground_truths": [
        "Apple employees"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "daa68770d88b4dbabfc2955ecc647750",
      "question": "What did Jobs battle?",
      "prediction": "cancer",
      "ground_truths": [
        "cancer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "42c9c8fe7ee843dbbe578fdc4dbba39a",
      "question": "when did Apple employees will celebrate Steve Jobs' life ?",
      "prediction": "October 19,",
      "ground_truths": [
        "October 19,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bcdd509239274509afc8ae77cb5df604",
      "question": "Who co-founded Apple?",
      "prediction": "Steve Wozniak.",
      "ground_truths": [
        "Steve Jobs"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "703c1f0b768d4732afa9e6a5a2d05091",
      "question": "when did Jobs die?",
      "prediction": "Tuesday",
      "ground_truths": [
        "3 p.m. Wednesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "20bdba32de3c4cd3829be8b6b99a004e",
      "question": "What are the slim risks?",
      "prediction": "devices carry few security",
      "ground_truths": [
        "\"brain hacking\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9630268d37cf48ef8fe8d155ad2298b8",
      "question": "What says expert security?",
      "prediction": "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"",
      "ground_truths": [
        "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\" said computer"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "bd3b71b3e5014081a2bd8383ab23111c",
      "question": "Who can use thoughts to operate computers?",
      "prediction": "Scientists",
      "ground_truths": [
        "Scientists"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d28c55c539de46f9922e84f3c87321fc",
      "question": "Who is game's creator?",
      "prediction": "Alexey Pajitnov",
      "ground_truths": [
        "Alexey Pajitnov"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dcfec78b61864784bec20d769fa6c923",
      "question": "How many years old is Tetris turning?",
      "prediction": "25",
      "ground_truths": [
        "25"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62d1568d094842c390de7d3319f03c28",
      "question": "What percent of mobile games are Tetris?",
      "prediction": "10",
      "ground_truths": [
        "10"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9df988aed472449b894ff1f198dd4e3d",
      "question": "Who did CNN talk to?",
      "prediction": "Pajitnov",
      "ground_truths": [
        "Alexey Pajitnov,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a8834ebda8454adcadf347f0eeb73d54",
      "question": "What kind of game is Tetris?",
      "prediction": "puzzle video",
      "ground_truths": [
        "the simple puzzle video"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "8a9c6dc3980740d18d876d0e75fe2f27",
      "question": "What percentage of all games was Tetris?",
      "prediction": "10 percent",
      "ground_truths": [
        "10 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2616151407964974abead40e1f46a464",
      "question": "What is Tetris?",
      "prediction": "a computer version?",
      "ground_truths": [
        "simple puzzle video game,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "293d474acb924ae2af1cf26af60c1cd1",
      "question": "What game turned 35?",
      "prediction": "Tetris",
      "ground_truths": [
        "Tetris,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e78a6a43ee564f9f9c1718636a38c1b2",
      "question": "Who is supporting calls for an investigation into Gadhafi's death?",
      "prediction": "United Nations and international human rights groups",
      "ground_truths": [
        "United Nations and international human rights groups"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "22f9b367696d43efb409a9f06a662261",
      "question": "Gadhafi's son Saadi is \"outraged\" over circumstances surrounding what?",
      "prediction": "the murder of his father and brother.\"",
      "ground_truths": [
        "the vicious brutality which accompanied the murders of his father and brother.\""
      ],
      "em": 0,
      "f1": 0.625
    },
    {
      "id": "2b116d81534646e585d2c57d2f7915ec",
      "question": "What is the name of Gadhafi's son?",
      "prediction": "Mutassim,",
      "ground_truths": [
        "Mutassim,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3d05c6f2b8364971bb5f58d40f42a5a7",
      "question": "On which day of the week did Gadhafi die?",
      "prediction": "Thursday",
      "ground_truths": [
        "Thursday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6011e253fa9a4402bf031d32ad510952",
      "question": "Doctors have not revealed additional details about their what?",
      "prediction": "death of the Libyan leader",
      "ground_truths": [
        "findings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7ad336fd9164278b9f5af9c66bc17ff",
      "question": "Gadhafi was killed on which day?",
      "prediction": "Sunday,",
      "ground_truths": [
        "Thursday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a8b3bfa1d284b16a9bf7a36cf5c92f1",
      "question": "Who was Hulk Hogan married to?",
      "prediction": "Linda Ballone",
      "ground_truths": [
        "Linda"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "973faaebcd914fb4847b7e6a3d89a8d6",
      "question": "Which publication did the interview appear in?",
      "prediction": "Rolling Stone",
      "ground_truths": [
        "Rolling Stone"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "49732a926aa84e26ada40a2ae4ed10ee",
      "question": "What was Hogans age?",
      "prediction": "55-year-old",
      "ground_truths": [
        "49,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "327578d5238c4a138fa53efc5155622e",
      "question": "What magazine did Hogan talk to?",
      "prediction": "Rolling Stone",
      "ground_truths": [
        "Rolling Stone"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d544bfc71bbd45648168ab5b4c40a407",
      "question": "Who is in a bitter divorce?",
      "prediction": "Linda Hogan",
      "ground_truths": [
        "Hulk Hogan"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cd6f21280a094412800c4f004572d539",
      "question": "What statement amounts to a death threat?",
      "prediction": "\"I could have turned everything into a crime scene like O.J., cutting everybody's throat,\"",
      "ground_truths": [
        "The pro wrestling legend said Wednesday that he \"took the high road\" and \"didn't do the O.J. Simpson thing\" despite the pain of his bitter divorce fight with wife Linda."
      ],
      "em": 0,
      "f1": 0.05128205128205129
    },
    {
      "id": "a518c99b699b46fd8ea244c26187de18",
      "question": "Was the Hogans' divorce amicable?",
      "prediction": "bitter",
      "ground_truths": [
        "bitter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "90abd77cf12642bf8e438280c5706070",
      "question": "Who had a divorce?",
      "prediction": "Linda Hogan",
      "ground_truths": [
        "Linda and Hulk Hogan"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "df99b6074900447d958fc5c3648ba2b9",
      "question": "What may be telling people about you?",
      "prediction": "a Mercedes-Benz R-Class:",
      "ground_truths": [
        "cars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e15d3c388ed140f4ae8a2a90098fd9d4",
      "question": "Who would drive a Prius?",
      "prediction": "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"",
      "ground_truths": [
        "buyers are more often women, have fewer kids and more often have college educations.\""
      ],
      "em": 0,
      "f1": 0.07692307692307691
    },
    {
      "id": "cd727caa638247bb9753aeda19092113",
      "question": "What does a Prius tell about its driver?",
      "prediction": "you love the environment and hate using fuel,\"",
      "ground_truths": [
        "shows the world that you love the environment and hate using fuel,\""
      ],
      "em": 0,
      "f1": 0.8235294117647058
    },
    {
      "id": "855125b07a3844ccadbc58809afbec07",
      "question": "What may be telling people all about you?",
      "prediction": "your car.\"",
      "ground_truths": [
        "their cars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f37d6e92ba9b4d0caa75d393f6b4ff09",
      "question": "Does the model of car you drive say something about your personality?",
      "prediction": "about",
      "ground_truths": [
        "people have chosen their rides based on what their"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "768c1a7348494f9d8a6501fd8bc2107f",
      "question": "What is a typical Corvette driver?",
      "prediction": "older than the industry average,",
      "ground_truths": [
        "older than the industry average,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d3fb745f01b74425ac0fb8568b472006",
      "question": "What does a Prius say about it's driver?",
      "prediction": "the world that you love the environment and hate using fuel,\"",
      "ground_truths": [
        "shows the world that you love the environment and hate using fuel,\""
      ],
      "em": 0,
      "f1": 0.9473684210526316
    },
    {
      "id": "c16ade387482433dac37b23b76d5eefc",
      "question": "Who did the Foreign Ministry describe as \"mentally deranged\"?",
      "prediction": "Dick Cheney",
      "ground_truths": [
        "U.S. Vice President Dick Cheney"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "e486c5209904438a99a52fa1e7b0b1cc",
      "question": "What did the paper describe Bush as?",
      "prediction": "\"by no means intelligent\" and a \"funny lady.\"",
      "ground_truths": [
        "\"A chicken soaked in the rain,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65366b90dc6f45ceaafab48e4ee736d1",
      "question": "what paper called the president incompetant",
      "prediction": "The spokesman blasted Clinton for",
      "ground_truths": [
        "Rodong Sinmun"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5165d2b3476341ab8e2205cafdce0110",
      "question": "Does the Foreign Ministry consider Dick Cheney to be mentally competent?",
      "prediction": "deranged person",
      "ground_truths": [
        "\"mentally deranged person"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f330e981d0f24684b5ebf162b114097c",
      "question": "who launches personal attack",
      "prediction": "North",
      "ground_truths": [
        "Korea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "320c4a080c3d4d3aabcdb8a9e5a161bc",
      "question": "What is the name of Hillary Clinton's job in the government?",
      "prediction": "Secretary of State",
      "ground_truths": [
        "U.S. Secretary of State"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "b7695dac18f9424988da7bcfc565aaa1",
      "question": "Whom did N. Korea launch a personal attack on?",
      "prediction": "U.S. Secretary of State Hillary Clinton,",
      "ground_truths": [
        "Clinton,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "7d3a612ae4ba46b4be6b981fa3999a12",
      "question": "What types of troops will be deployed?",
      "prediction": "U.S.",
      "ground_truths": [
        "30,000 additional"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91fa1c21d7e741eaba1f5dcb5ca323f9",
      "question": "What did the president say the goals were",
      "prediction": "• Deny al Qaeda a safe haven.",
      "ground_truths": [
        "part of a strategy to reverse the Taliban's momentum and stabilize the country's government."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6204fa51927744d486eaf1f3486ecec3",
      "question": "What did Obama say about troops?",
      "prediction": "is part of a strategy to reverse the Taliban's momentum and stabilize the country's government.",
      "ground_truths": [
        "to Afghanistan is part of a strategy to reverse the Taliban's momentum and stabilize the country's government."
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "0bd15b16540c4322bfbde43fe61c0126",
      "question": "How many troops will be deployed?",
      "prediction": "30,000",
      "ground_truths": [
        "30,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0f9bb1d77bcd4bddb36982ac4a71ac63",
      "question": "What did the president say about goals?",
      "prediction": "We are in Afghanistan to prevent a cancer from once again spreading through that country. But this same cancer has also taken root in the border region of Pakistan. That is why we need a strategy that works on both sides of the border,\"",
      "ground_truths": [
        "withdrawing most U.S. forces by the end of his current term,"
      ],
      "em": 0,
      "f1": 0.04000000000000001
    },
    {
      "id": "700d26f635674641bd9de55bad1ac975",
      "question": "Who will the troops fight while out on deployment?",
      "prediction": "Taliban's",
      "ground_truths": [
        "Taliban"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "abfb70e6902b46239b124dba7b1f52a7",
      "question": "Take a drive back where?",
      "prediction": "A Look Back",
      "ground_truths": [
        "in time"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fe42a6cc7a4a4cc1a489093e09b26540",
      "question": "Who is concerned with security?",
      "prediction": "President Obama",
      "ground_truths": [
        "President Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "65bfa2010969404a92b8a132ddee9c02",
      "question": "Who has cybersecurity concerns?",
      "prediction": "President Obama",
      "ground_truths": [
        "President Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6af5094254834df995e52a0779f96097",
      "question": "What do they call the campus of the school?",
      "prediction": "model of sustainability.",
      "ground_truths": [
        "a model of sustainability."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f2f51019a2c490b9fa0950fe702d3ae",
      "question": "Who's wheels came off?",
      "prediction": "President Obama",
      "ground_truths": [
        "General Motors'"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1c8f7e13689440aa1a4408dd5ea3641",
      "question": "Where was the photo donated?",
      "prediction": "the U.S. Holocaust Memorial Museum",
      "ground_truths": [
        "U.S. Holocaust Memorial Museum"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "268aaf83d92e42b490cc7f29dacd4167",
      "question": "Where was the slave camp located?",
      "prediction": "Berga an der Elster",
      "ground_truths": [
        "Berga an der Elster"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45207b6ffb714233925df562354b1781",
      "question": "What is the new photo showing?",
      "prediction": "Nazi Party members digging up graves of American soldiers",
      "ground_truths": [
        "\"Nazi Party members digging up American bodies at Berga.\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4b930ffbea3e483eb80725e988a583a7",
      "question": "Who was held in the camp?",
      "prediction": "350 U.S. soldiers",
      "ground_truths": [
        "American soldiers"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d010c18153064c8bb5bf99903ef5d63b",
      "question": "Who is holding Captain Phillips hostage?",
      "prediction": "pirates",
      "ground_truths": [
        "pirates"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd0437e47bee4d22972d1de07917139d",
      "question": "What did Gen. David Petraeus say?",
      "prediction": "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"",
      "ground_truths": [
        "that the Bainbridge would be getting backup shortly."
      ],
      "em": 0,
      "f1": 0.10526315789473685
    },
    {
      "id": "7b08901fb3d74143840d29a608e4deac",
      "question": "Where did Maersk Alabama leave?",
      "prediction": "Horn of Africa.",
      "ground_truths": [
        "Horn of Africa."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "56cd3c16bce94b719b287ac4575ae645",
      "question": "Who left the scene with armed detail?",
      "prediction": "Maersk Alabama",
      "ground_truths": [
        "the Maersk Alabama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8ee9c1b4ff684e4c9a38415448a74952",
      "question": "Which agency is involved in negotiations with pirates holding Capt.Phillips?",
      "prediction": "MAREK Line Ltd.",
      "ground_truths": [
        "FBI"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d938320190741ec88254807ab655077",
      "question": "Who assisted in negotiations?",
      "prediction": "FBI negotiators",
      "ground_truths": [
        "FBI."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2cf2f676009a42e18873dd7470397945",
      "question": "Were the protests violent?",
      "prediction": "clashes broke out",
      "ground_truths": [
        "killed and 90 wounded"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2cb7794cd52940ddb5359060714f07bc",
      "question": "What was the cause of the deaths?",
      "prediction": "clashes",
      "ground_truths": [
        "clashes between Coptic Christians"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "8c4728a5bb874d90b7d5dccf8500dc29",
      "question": "Who is launching an investigation?",
      "prediction": "The Egyptian military",
      "ground_truths": [
        "Egyptian military"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a82c8d581d8048049d215d9fb873f5b0",
      "question": "Copts protest what?",
      "prediction": "burning of a church.",
      "ground_truths": [
        "last week's burning of a church."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "751d7a50e0b94fd89ba99fc6210f2a4b",
      "question": "How many were wounded?",
      "prediction": "90",
      "ground_truths": [
        "90"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fff2f61569ca4862b0a408c0515a441b",
      "question": "Who burnt the church?",
      "prediction": "Coptic Christians",
      "ground_truths": [
        "Palestinian Islamic Army,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4d67ed2995b40b58c85257f233d1f87",
      "question": "Who denies it spy agence helped plan bombing?",
      "prediction": "Pakistan",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "44fa4d16c1084a748db47393708efe30",
      "question": "Who do India say is involved?",
      "prediction": "Pakistan",
      "ground_truths": [
        "Pakistan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02dab2bb8ea24f2d8796223f83f28014",
      "question": "Which country's agents where involved in the attack?",
      "prediction": "some unspecified aid came from Pakistan's Directorate of Inter-Services Intelligence, or ISI.",
      "ground_truths": [
        "Pakistan's"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "9a5df0f6413c4e8e91fed29d8279c438",
      "question": "What was the number killed in the bombing?",
      "prediction": "58",
      "ground_truths": [
        "58"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3d4ffb819b5545cf8e728c62d91ba641",
      "question": "Who said this was an 'effort to malign the ISI'?",
      "prediction": "Afghanistan and India",
      "ground_truths": [
        "Pakistani Maj. Gen. Athar Abbas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d838b4a128d648ec8beda32fd8a4e19d",
      "question": "Who does U.S. intelligence point to in attack?",
      "prediction": "members of Pakistan's spy service",
      "ground_truths": [
        "some members of Pakistan's spy service"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "0a3b187e277c41cca2bd6c9592f86150",
      "question": "Who accused Pakistan in a role in the attack?",
      "prediction": "India",
      "ground_truths": [
        "Afghanistan and India"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "29405d332ddb4385830e20d84df5f50b",
      "question": "What did Maj. Gen Athar Abbas say about the report?",
      "prediction": "\"unfounded and malicious\" and an \"effort to malign the ISI,\"",
      "ground_truths": [
        "\"unfounded and malicious\" and an \"effort to malign the ISI,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "155ca8b5e3a94befbeea9a3dd8a1da2e",
      "question": "How much is being spent on the clean up?",
      "prediction": "$2 billion",
      "ground_truths": [
        "nearly $2 billion"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "27f0b55a6463487c9e33e0473e7741a4",
      "question": "what is the cost of the clean up",
      "prediction": "$2 billion",
      "ground_truths": [
        "$2 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e05be86a52234d69a86ae37a4fd9a600",
      "question": "Where is the nuclear site ?",
      "prediction": "Hanford",
      "ground_truths": [
        "Washington"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3b4678773dc4e6c89054b908d400a1a",
      "question": "What does stimulus fund?",
      "prediction": "clean up Washington State's decommissioned Hanford nuclear site,",
      "ground_truths": [
        "clean up Washington State's decommissioned Hanford nuclear site,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "46a118e8fbeb4526b7959d52324ffc5f",
      "question": "what is being cleaned up",
      "prediction": "Hanford nuclear site,",
      "ground_truths": [
        "Washington State's decommissioned Hanford nuclear site,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7876bfdb1fac411cb32f3b0837a064cf",
      "question": "What said about Hanford?",
      "prediction": "is getting more money than many states in stimulus funds and you would expect to see real progress for clean-up [to] happen with those dollars.\"",
      "ground_truths": [
        "\"Hanford is getting more money than many states in stimulus funds"
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "4bffec9430e947afa5d74a1cff1e9509",
      "question": "Who says that the company has been restructuring over the last three years?",
      "prediction": "Mark Fields:",
      "ground_truths": [
        "Mark Fields,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d62d29c3780648ef890e64b2b625606d",
      "question": "Who says the retail market share has increased?",
      "prediction": "Fields:",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d4efa88c9db4467a961182bb950f2d66",
      "question": "Who says they are in a different position to their competitors?",
      "prediction": "Mark Fields,",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "98e977789cfb48d5a4e69d50f1e9e353",
      "question": "How many years has Ford been restructuring ?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1dbac370639f49c4abf926c2ba542713",
      "question": "What is the biggest issue in the marketplace?",
      "prediction": "consumer confidence",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e8be52fe629c4dfdab5b8dbb9796b7c1",
      "question": "What does Field say is the biggest issue in the marketplace?W",
      "prediction": "consumer confidence",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5cba966283084dc3834906561150aeaa",
      "question": "What has retail market shares done in last few months?",
      "prediction": "up three of the",
      "ground_truths": [
        "up"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6db33a2cb76b41afaf584d3af4868201",
      "question": "Who says they're in a different position to competitors?",
      "prediction": "Mark Fields",
      "ground_truths": [
        "Mark Fields:"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2b055c5420c4ac9ad825973907d8c1e",
      "question": "What is the biggest issue in marketplace?",
      "prediction": "consumer confidence",
      "ground_truths": [
        "consumer confidence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3cb0174eaaf54d35878f92077740136c",
      "question": "Who is now the official defender of America's cup?",
      "prediction": "Golden Gate Yacht Club of San Francisco",
      "ground_truths": [
        "Golden Gate Yacht Club of San Francisco"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "054d98807bed447f896aaa343005bb89",
      "question": "Where is the America's Cup taking place?",
      "prediction": "Valencia",
      "ground_truths": [
        "off Valencia"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9827d6f7626d46c188fc893ff0e12fd9",
      "question": "Who is Larry Ellison?",
      "prediction": "Syndicate, founded by software magnate",
      "ground_truths": [
        "software magnate"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "1f73b3ea836548d496ef68c08f4369f4",
      "question": "who founded BMW Oracle over 10 years ago?",
      "prediction": "Larry Ellison,",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2242d2a664df4bf1967d3689e8b069d0",
      "question": "BMW Oracle take a winning lead of what?",
      "prediction": "223ft",
      "ground_truths": [
        "America's Cup"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7889488ae4d3460bafe900d6917b650e",
      "question": "Who owns the BMW Oracle?",
      "prediction": "Larry Ellison,",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b455ff336ebe403e896b9be3465811f7",
      "question": "Who founded BMW?",
      "prediction": "Larry Ellison,",
      "ground_truths": [
        "Larry Ellison,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d09f358431284dea95dec8e61590dbf4",
      "question": "Who beat the Swiss holders?",
      "prediction": "BMW Oracle",
      "ground_truths": [
        "BMW"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b43ea874d5ef47208ee0d3b5f0f30717",
      "question": "Who retains fourth place as Demba Ba scores hat-trick against Blackburn?",
      "prediction": "Newcastle",
      "ground_truths": [
        "Newcastle"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28a0dae2562544dd8a4bcab4d422b79e",
      "question": "Manchester United drop first points of English season with 1-1 draw where?",
      "prediction": "Stoke City.",
      "ground_truths": [
        "Stoke City."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ad3511f19a5845b7be81022faec9f4cf",
      "question": "What place retained fourth place",
      "prediction": "Newcastle",
      "ground_truths": [
        "Newcastle"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fa92fc83863d46e9a66c60b76b2efd5d",
      "question": "What person beat swansea",
      "prediction": "Chelsea",
      "ground_truths": [
        "Fernando Torres"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b5a018ab1b044c29c0cc5999d50c8b4",
      "question": "Who heads table on goal difference from rivals Manchester City, who beat Everton?",
      "prediction": "Tottenham",
      "ground_truths": [
        "United's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2297ef49d5a54ad8a415426c7760398a",
      "question": "what is the final score?",
      "prediction": "1-1 draw",
      "ground_truths": [
        "1-1 draw"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a78189a634e347cd97e9f6f3d554bd47",
      "question": "Did Manchester United lose?",
      "prediction": "1-1",
      "ground_truths": [
        "1-1 draw"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "df9dc201bee94070a6c46e8370e25824",
      "question": "What was the Barron's nickname?",
      "prediction": "\"the Strawberry,\"",
      "ground_truths": [
        "\"the Strawberry,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "25eed9d207a44fd8b3f5b05f35e6f8ae",
      "question": "What does he head?",
      "prediction": "\"Michoacan Family,\"",
      "ground_truths": [
        "\"Michoacan Family,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed5d7f29f1104c7097ee8adf54dd3e62",
      "question": "Who did Mexico arrest?",
      "prediction": "Alberto Espinoza Barron,",
      "ground_truths": [
        "Alberto Espinoza Barron,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5737be0b4281430dbcc27d3f2f98bf95",
      "question": "What is the number of dead killed by a grenade?",
      "prediction": "eight",
      "ground_truths": [
        "eight"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5cf15a557be045bf86125a1ff29cf4c1",
      "question": "for what crime was he arrested",
      "prediction": "narcotrafficking.",
      "ground_truths": [
        "set off two grenades during a public celebration in September,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1c8d70a0e1fb4f6ab0d90a651c331467",
      "question": "What is he the head of?",
      "prediction": "a drug cartel",
      "ground_truths": [
        "\"Michoacan Family,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "47c39c5a4353463dae51fb43d3c1c7ff",
      "question": "When did the last bombing happen?",
      "prediction": "Sunday,",
      "ground_truths": [
        "The blast follows another huge attack on Sunday,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "66ef940edd3045d3a03ea5dd7e476bac",
      "question": "What did IRAQI Pm urge nation's sheikhs to do?",
      "prediction": "participate in Iraq's government.",
      "ground_truths": [
        "participate in Iraq's government."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fea7a36c699a4cd2a68f4049e19f9ee6",
      "question": "Who were attending a conference during the attack?",
      "prediction": "Sunni Arab and Shiite tribal leaders",
      "ground_truths": [
        "Sunni Arab and Shiite tribal leaders"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e30d3bec60e42f9a781558824495985",
      "question": "What is the number of dead?",
      "prediction": "At least 33",
      "ground_truths": [
        "33"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "972169de02914a8b8afb656a20579a99",
      "question": "How many were killed in the suicide bombing?",
      "prediction": "33 people",
      "ground_truths": [
        "At least 33 people"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3c8e1d09154b4b74873b9888b20d6e6f",
      "question": "When did the attack occur?",
      "prediction": "outside the municipal building of Abu Ghraib in western Baghdad",
      "ground_truths": [
        "Sunday's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b43f21b5f3ff4cb39f781974666b0aaf",
      "question": "In what manner did they die?",
      "prediction": "suicide car bombing",
      "ground_truths": [
        "suicide car bombing"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6f1309e8046b49e1836ea7217aff694d",
      "question": "How many were killed in Baghdad suicide bombing?",
      "prediction": "At least 33 people",
      "ground_truths": [
        "At least 33 people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7992ef8a5e8f4d9da5a3f75f157136a0",
      "question": "where will obama hold meeting?",
      "prediction": "Shanghai",
      "ground_truths": [
        "Shanghai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5544aab55fe744fab13706c13fcb4f96",
      "question": "What is the name of President Obama's half brother?",
      "prediction": "Mark",
      "ground_truths": [
        "Mark"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fe94d55d37024a71afeebcce19c9779c",
      "question": "how long is obamas trip?",
      "prediction": "eight-day",
      "ground_truths": [
        "eight-day"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "490da89fb4c743858167b5b75af14ddd",
      "question": "What countries is Obama visiting?",
      "prediction": "China",
      "ground_truths": [
        "Japan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fe12d7bff7a46d79f76ccc655caf66c",
      "question": "Who lives in China",
      "prediction": "Ndesandjo",
      "ground_truths": [
        "Hu Jintao."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0338d7711b1646d6be2b815c279cff54",
      "question": "Where will President Obama hold town hall-style meeting?",
      "prediction": "Shanghai",
      "ground_truths": [
        "Shanghai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cb08cc75e99a4ec9807321f51dd40566",
      "question": "How many days long is President Obama's trip?",
      "prediction": "eight-day journey",
      "ground_truths": [
        "eight-day"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "811785fb9d684ccd9577541cd4c94555",
      "question": "where does mark obama live?",
      "prediction": "Shenzhen in southern China.",
      "ground_truths": [
        "Shenzhen in southern China."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dcfcb5a3789141ab83b5589c4af05105",
      "question": "Who are being paied tribute to ?",
      "prediction": "military veterans",
      "ground_truths": [
        "military veterans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c410a3904061469597449e8e38019436",
      "question": "Which world leaders attended D-Day tribute?",
      "prediction": "Obama joined Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper at a ceremony at the American Cemetary in Normandy, close where many died in the World War II offensive.",
      "ground_truths": [
        "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "d1b2349ba351429bb9601c27e5afb4eb",
      "question": "Who paid tribute to D-Day fallen?",
      "prediction": "150,000 allied troops,",
      "ground_truths": [
        "World leaders"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65f9fc6a0c8442eab8509181524c2b37",
      "question": "When did D-Day operations occur?",
      "prediction": "June 6, 1944,",
      "ground_truths": [
        "June 6, 1944,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "07e2bccb50e64fe5b550ffaa6a34733b",
      "question": "How many allied troops took part in D-Day ?",
      "prediction": "More than 150,000",
      "ground_truths": [
        "More than 150,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "31df9cfaf26e42e988ae21e6ccad7ddc",
      "question": "How many allied troops took part in D-Day operations?",
      "prediction": "150,000",
      "ground_truths": [
        "More than 150,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d1e55b947f6241c18e818b671aa800a4",
      "question": "When did the operation take part?",
      "prediction": "June 6, 1944,",
      "ground_truths": [
        "June 6, 1944,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc6b46e7ec704ef283cb759cc4aef9c1",
      "question": "Who attended the tribute?",
      "prediction": "President Obama and Britain's Prince Charles",
      "ground_truths": [
        "President Obama and Britain's Prince Charles"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "822b186317c04e1db6315150718e194e",
      "question": "Who teamed up?",
      "prediction": "Business\"",
      "ground_truths": [
        "ClimateCare,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4189184928d9434294d6708f3ef5bae2",
      "question": "Which industry was unfairly criticized?",
      "prediction": "aviation",
      "ground_truths": [
        "The aviation"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8821fe2537b8468c9e90ab7b6dde661f",
      "question": "Who teamed up with Climate Care?",
      "prediction": "Jet Republic,",
      "ground_truths": [
        "Jet Republic"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "417e56a4601b4d81aaaef763b97c99e5",
      "question": "What is aviation industry responsible for?",
      "prediction": "around 3.5 percent of global greenhouse emissions.",
      "ground_truths": [
        "around 3.5 percent of global greenhouse emissions."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c51a684309aa401cb036c5730dab58af",
      "question": "What percent of global carbon emissions is the airline industry responsible for?",
      "prediction": "3.5",
      "ground_truths": [
        "3.5"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4acfdfd9af1f459f9a0e34526b3bd68c",
      "question": "whom did they team up with",
      "prediction": "ClimateCare,",
      "ground_truths": [
        "ClimateCare, one of Europe's most experienced providers of carbon offsets,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "328dfae8a432499e91c71bad92e2bd79",
      "question": "How many times did Francesco Totti score",
      "prediction": "two",
      "ground_truths": [
        "twice."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b5e7dc56ca2e4d68bc9b59696bca7f75",
      "question": "What did Inzaghi do?",
      "prediction": "scored a hat-trick as AC Milan went second in Serie A",
      "ground_truths": [
        "scored a hat-trick"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "ead58bae18d04b9c89be6f0bbe3f155f",
      "question": "Who scored twice?",
      "prediction": "Filippo Inzaghi",
      "ground_truths": [
        "captain Francesco Totti"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c452c1a025c44bdab1694894c2799e64",
      "question": "Did AS Roma beat Lecce?",
      "prediction": "beat",
      "ground_truths": [
        "AS"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8be65f22a2a0454eaea4506b06382027",
      "question": "Who continues a impressive scoring streak?",
      "prediction": "IInzaghi",
      "ground_truths": [
        "Filippo Inzaghi"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "996e6a4534b049b080d457235def719d",
      "question": "Where was the victim found?",
      "prediction": "in a motel,",
      "ground_truths": [
        "in a motel,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8a79bf4bbfee412d943b4c1c240dac5b",
      "question": "What are authorities in North Carolina investigating?",
      "prediction": "the death of a pregnant soldier whose body was found",
      "ground_truths": [
        "the death of a pregnant soldier"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "677c4e29d5d84e6c983c87988ba82274",
      "question": "Who was the pregnant service member?",
      "prediction": "Spc. Megan Lynn Touma,",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b849fa3ccdd941a2a81fab71b34644a2",
      "question": "Who was the victim?",
      "prediction": "Spc. Megan Lynn Touma,",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dbab094264f6482bb5dda2381fff9847",
      "question": "Who is investigating the death?",
      "prediction": "Authorities",
      "ground_truths": [
        "in Fayetteville, North Carolina,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5dd21e68a59d43b7b88e561d92bd7f3f",
      "question": "Was it natural causes or murder?",
      "prediction": "death.",
      "ground_truths": [
        "Laurean killed Lauterbach"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d58e98347651493d99ad62e97fbe7902",
      "question": "Who was found dead?",
      "prediction": "Spc. Megan Lynn Touma,",
      "ground_truths": [
        "pregnant soldier"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "34856d67c1e14740a8629203fdae72e7",
      "question": "What are authorities investigating?",
      "prediction": "death of a pregnant soldier",
      "ground_truths": [
        "the death of a pregnant soldier"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4b718b75f3834c16bc4c74ff15d7e341",
      "question": "What happened to the pregnant soldier?",
      "prediction": "died",
      "ground_truths": [
        "death"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d301d0914b9847e892d259e8428b1fb4",
      "question": "Who was second N.C.-based pregnant solider found dead?",
      "prediction": "Spc. Megan Lynn Touma,",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53c64df9090c45cb80df1707e3276239",
      "question": "Who was found dead in motel room?",
      "prediction": "Spc. Megan Lynn Touma,",
      "ground_truths": [
        "Spc. Megan Lynn Touma,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bd551fb2f84a4f4eac4416b396eeae9a",
      "question": "where did she grow up",
      "prediction": "Bronx.",
      "ground_truths": [
        "Bronx."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d575bcc01562419b8b038b257da7fad0",
      "question": "what High school classmate said she saw Sotomayor?",
      "prediction": "Mary Procidano,",
      "ground_truths": [
        "Mary Procidano,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba86dc3a6b50472e8daa169f2eef0c51",
      "question": "what did the classmate say",
      "prediction": "\"And she wasn't just talking out of the top of her head",
      "ground_truths": [
        "\"She was focused so much on learning that she didn't notice,\""
      ],
      "em": 0,
      "f1": 0.09090909090909091
    },
    {
      "id": "166f4f7cb93048bc8d6af0661c0aba69",
      "question": "who died when she was 9",
      "prediction": "Juan Sotomayor,",
      "ground_truths": [
        "Juan Sotomayor, Sonia's father,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "98bc1ca26fc045d3a1267a0c6051d16a",
      "question": "Who is holding her confirmation hearing?",
      "prediction": "full Senate",
      "ground_truths": [
        "Sonia Sotomayor,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d6dc2b279594850a8ffce1f1b7892aa",
      "question": "Where is the painting displayed?",
      "prediction": "Harlem, New York.",
      "ground_truths": [
        "Dancy-Power Automotive in Harlem, New York."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8b0fb3b8cfee4daba5b93311dbe5c5b5",
      "question": "The painting shows jackson holding what?",
      "prediction": "a book.",
      "ground_truths": [
        "a book."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f6b7de0428ac4bf18fe4707e7f59d047",
      "question": "which is close to Apollo Theater?",
      "prediction": "\"The Book\"",
      "ground_truths": [
        "the showroom"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "733d2fd24b0b40f7ba5f0edaed20cca9",
      "question": "What does the painting show show?",
      "prediction": "Jackson sitting in Renaissance-era clothes and holding a book.",
      "ground_truths": [
        "Jackson sitting in Renaissance-era clothes and holding a book."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a697c765c3e74e0cb48088ea3ecdcdb1",
      "question": "What type of painting did michael jackson sit for?",
      "prediction": "oil",
      "ground_truths": [
        "oil"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5f87d6a314604c62aa83466f74fdb86a",
      "question": "What is the only painting Jackson sat for?",
      "prediction": "\"The Book\"",
      "ground_truths": [
        "\"The Book\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "82eb7d4504834ceabad83742c2d0ab5c",
      "question": "What kind of craft is Earthrace?",
      "prediction": "a powerboat",
      "ground_truths": [
        "green powerboat"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8809bd64552f4f7aa8f53f493602ceae",
      "question": "How much is Bethune selling Earthrace for?",
      "prediction": "$1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7e11924f8a34063ba5445261471e552",
      "question": "Where is trimaran currently touring?",
      "prediction": "Australia and New Zealand",
      "ground_truths": [
        "Australia and New Zealand"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9724aa5ceb194973a23f99d088987015",
      "question": "What is the asking price for Earthrace?",
      "prediction": "$1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7876cd87c6e845f484f841d243a021a0",
      "question": "What does Earthrace hold the world record for?",
      "prediction": "fastest time in circling the globe in a powerboat.",
      "ground_truths": [
        "fastest"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "68d323c49f9740abbda05876aebfd5d7",
      "question": "How much is Earthrace selling for?",
      "prediction": "$1.5 million.",
      "ground_truths": [
        "$1.5 million."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8be91aa0873e4a4bb89703628089432a",
      "question": "Who owns Earthrace?",
      "prediction": "New Zealander Pete Bethune,",
      "ground_truths": [
        "New Zealander Pete Bethune,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "361d3843c29e48cb89a94b00c8ff9017",
      "question": "which party the president belongs",
      "prediction": "Workers'",
      "ground_truths": [
        "Workers'"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2ff8a0d87c24abeae1d7612663751d0",
      "question": "who says \"the greatest gift that a president could have\"?",
      "prediction": "Lula da Silva",
      "ground_truths": [
        "to host the Olympic Games in Rio de Janeiro."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "157c103f7f3942c8be2ee84825ff7f32",
      "question": "What kind of heart issues does Elizabeth Taylor have?",
      "prediction": "leaky valve",
      "ground_truths": [
        "leaky valve"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebee9fdad5b64bab83267a364ea4d10b",
      "question": "what did taylor announce",
      "prediction": "\"procedure on her heart,\"",
      "ground_truths": [
        "is having a \"procedure on her heart,\""
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "3e1bed5b98f248738744f4c55f0e6d06",
      "question": "did she go into details",
      "prediction": "not",
      "ground_truths": [
        "not"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec9dedf313ba4f90883a2b01909553b3",
      "question": "for what did she use twitter",
      "prediction": "to share personal information.",
      "ground_truths": [
        "to share personal information."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0984831a98b8496a96e880939f6f77b9",
      "question": "On what organ in the body was Ms Taylor having issues with",
      "prediction": "heart",
      "ground_truths": [
        "heart,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "edb3d5b64911443798e8e83b07ae5b99",
      "question": "For what reason did Elizabeth Taylor take to twitter",
      "prediction": "to share personal information.",
      "ground_truths": [
        "\"procedure on her heart,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d51096337128469da3308f102dbbb0b6",
      "question": "Who is Elizabeth Taylor?",
      "prediction": "Oscar winner",
      "ground_truths": [
        "actress"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d16db94dc08840cb930ce9cf59151bef",
      "question": "Elizabth Taylor used twitter as to talk about how she cope after the death of whom",
      "prediction": "Michael Jackson",
      "ground_truths": [
        "Michael Jackson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6bb51d8500504ea3bec2a62487e4ceb1",
      "question": "Where was the ship heading?",
      "prediction": "Jeddah, Saudi Arabia,",
      "ground_truths": [
        "Jeddah, Saudi Arabia,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f476af0b22074b3f9458d73e17415502",
      "question": "Who was headed to Saudi Arabia?",
      "prediction": "The Al Nisr Al Saudi",
      "ground_truths": [
        "The Al Nisr Al"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "1c9487f1dd6a464584d6e3aeeb5476cd",
      "question": "What was hijacked?",
      "prediction": "a tanker",
      "ground_truths": [
        "a tanker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a255f0d0a964a13914f508a9a99f50c",
      "question": "Where is the ship being held?",
      "prediction": "near Garacad, Somalia,",
      "ground_truths": [
        "is near Garacad, Somalia,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "1f851796112743eb9929ce58de7b2b38",
      "question": "Where is it being held?",
      "prediction": "Saudi Arabia,",
      "ground_truths": [
        "near Garacad, Somalia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "691ed5c57db64cdbb7b5ec4d1f71e14c",
      "question": "Where was the vessel hijacked?",
      "prediction": "Saudi",
      "ground_truths": [
        "Gulf of Aden,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "254f6546961e4eddb55cf96fa4810f9d",
      "question": "What is the name of the vessel?",
      "prediction": "Al Nisr Al Saudi",
      "ground_truths": [
        "The Al Nisr Al Saudi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "34b9e47ee9754759ab352055e5ec6a0b",
      "question": "Who says Muslims worldwide have shared concern about key issues?",
      "prediction": "President Obama",
      "ground_truths": [
        "Nafees Syed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11f7550099824a199a4b8396d0516ced",
      "question": "What did Obama's interview with Al-Arabiya talk of?",
      "prediction": "America is \"ready to initiate a new partnership based on mutual respect and mutual interest,\"",
      "ground_truths": [
        "Palestinian-Israeli issue"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ed9fee4ae4648c08bfd6771e299334c",
      "question": "Who said anything about Bush?",
      "prediction": "Nafees Syed",
      "ground_truths": [
        "Muslim countries,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b72b19ddb79a4c829f3bcfcb0348f52f",
      "question": "What is Obama doing?",
      "prediction": "extended a hand of friendship to the Muslim world.",
      "ground_truths": [
        "has given the Muslim community around the world the message we have been waiting for."
      ],
      "em": 0,
      "f1": 0.2105263157894737
    },
    {
      "id": "4d88557eb7e7407d890bcaa9f3f3a2d7",
      "question": "What Nafees Syed said about Obama`s reaching to Muslim world?",
      "prediction": "says President Obama's early words and actions send a powerful, positive signal to Muslims.",
      "ground_truths": [
        "positive signal"
      ],
      "em": 0,
      "f1": 0.2666666666666667
    },
    {
      "id": "222248c60f334686964c020608621580",
      "question": "Who elevated them to their current seat?",
      "prediction": "President Clinton.",
      "ground_truths": [
        "President Clinton."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b191b0318ac5487a930d6523e1e44bd8",
      "question": "What did Sotomayor say?",
      "prediction": "the nomination is the \"most humbling honor \" of her life.",
      "ground_truths": [
        "the nomination is the \"most humbling honor"
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "413e30cf485e4d24882f2e19dce26ebf",
      "question": "What is Sotomayor's age?",
      "prediction": "54-year-old",
      "ground_truths": [
        "54-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8d73509690a14ba2b2bb462c23e84c2f",
      "question": "Where was Sotomayor born?",
      "prediction": "the Bronx",
      "ground_truths": [
        "the Bronx"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e729c9b6571b45c2bda8a3d5d1be79bf",
      "question": "Where was she born?",
      "prediction": "Bronx",
      "ground_truths": [
        "in the Bronx"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a5fb0382999c4519b92a6a68c4cb0a4a",
      "question": "Where was Sonia Sotomayor born?",
      "prediction": "the Bronx",
      "ground_truths": [
        "in the Bronx"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c019feb626ea4fd9b8ac736b2ffab022",
      "question": "Where did he grow up?",
      "prediction": "in a public housing project,",
      "ground_truths": [
        "in a public housing project,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "13bfebca830044c383f83fa8444e9aa5",
      "question": "Where did the break occur?",
      "prediction": "Ohio River",
      "ground_truths": [
        "Ohio River"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6d4b273cba0d4db4a320c92747c2378c",
      "question": "used to relieve blocked river traffic",
      "prediction": "lock",
      "ground_truths": [
        "auxiliary lock"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1f309b8814d048f28caf8c256a49f914",
      "question": "On what day did the lock break?",
      "prediction": "Sunday,",
      "ground_truths": [
        "Sunday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "684a53de3b9143edbe480a9fa3e05e52",
      "question": "What can outdoorsy types catch up withq",
      "prediction": "\"Hillbilly Handfishin'\" on Animal Planet (9 a.m.-5 p.m.),",
      "ground_truths": [
        "\"Hillbilly Handfishin'\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "aa0c393490734dffa0a70fb6cbb33a74",
      "question": "Reed Between the Lines is on what network?",
      "prediction": "BET",
      "ground_truths": [
        "BET"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1aad0544e7814463b5f532d2e844c63e",
      "question": "What time does the Bones marathon start?",
      "prediction": "10 a.m.-1 a.m. Friday.",
      "ground_truths": [
        "10 a.m.,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9e6ad1c38b6e4d0684675bedea267a87",
      "question": "Hillbilly Handfishin is on what network?",
      "prediction": "Animal Planet",
      "ground_truths": [
        "Animal Planet"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b75cf2ceb1c41b78b4273119abf85f3",
      "question": "When is Bones airing",
      "prediction": "all day starting at 10 a.m.,",
      "ground_truths": [
        "all day starting at 10 a.m.,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e0a8f8db62043568324ed921d1202ce",
      "question": "What is the original offering of",
      "prediction": "BET",
      "ground_truths": [
        "\"Reed Between the Lines\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d416fa56e7c74be78686712a6be384e8",
      "question": "who is she speaking out for?",
      "prediction": "in similar situations,\"",
      "ground_truths": [
        "the other women who couldn't or wouldn't.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1496858ec1dd447d974b8d466b0a68ec",
      "question": "Who appeared on CNN's \"Piers Morgan Tonight\"?",
      "prediction": "Sharon Bialek",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "43ee55e2f5b6431ba69170c42b35de1f",
      "question": "Who appeared on CNN's Piers Morgan Tonight?",
      "prediction": "Sharon Bialek",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "47c10c1b7bd840139c5af3e82f478a7e",
      "question": "Who has denied the allegation?",
      "prediction": "Herman Cain",
      "ground_truths": [
        "Cain"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "66a767acd3a044378348a030fef5e233",
      "question": "What did Bialek say she was doing?",
      "prediction": "she came forward Monday \"for the other women who couldn't or wouldn't.\"",
      "ground_truths": [
        "came forward Monday \"for the other women who couldn't or wouldn't.\""
      ],
      "em": 0,
      "f1": 0.9523809523809523
    },
    {
      "id": "639ff711ef014a44b92f76df744b0eed",
      "question": "What network is this show on?",
      "prediction": "CNN's",
      "ground_truths": [
        "CNN's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "613e8614ab454406950db15bf973c1e9",
      "question": "What television show did Sharon Bialek appear on?",
      "prediction": "\"Piers Morgan Tonight\"",
      "ground_truths": [
        "\"Piers Morgan Tonight\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9d0f923d56ae49cd9b28741a94aed6bc",
      "question": "Who has denied any allegation of sexual harassment?",
      "prediction": "Cain",
      "ground_truths": [
        "Herman Cain"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "676f69de7aec48e69edc1e4dc0299a6b",
      "question": "Who works for gore's current tv",
      "prediction": "Bill Clinton",
      "ground_truths": [
        "Euna Lee,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9fe87fea995f4463b50822a33ee3e8df",
      "question": "What does the US not have with North Korea?",
      "prediction": "diplomatic relations",
      "ground_truths": [
        "no diplomatic relations"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "619ae1269dda477eb2232aad82ba1d5e",
      "question": "Which country does the US not have diplomatic relations with?",
      "prediction": "North Korea,",
      "ground_truths": [
        "North Korea,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0cb2c91e1de243f988fb1da96906374f",
      "question": "What are the names of the reporters?",
      "prediction": "Laura Ling",
      "ground_truths": [
        "Laura Ling and Euna Lee,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "ca8b2d0d0afc4128a3accb37ab6f442b",
      "question": "Who has no relation to north korea",
      "prediction": "U.S.",
      "ground_truths": [
        "United States"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0779694d75bc4c859ea6dd084ca62ae8",
      "question": "Where do Ling and Lee work?",
      "prediction": "California-based Current TV",
      "ground_truths": [
        "Current TV"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2d0fdf9383ec439ab15a116a63b3421c",
      "question": "What does Chinoy say?",
      "prediction": "\"The bigger, broader and more important question is",
      "ground_truths": [
        "\"I suspect that it was made pretty clear in advance that Bill Clinton would be able to return with these two women otherwise it would be a terrible loss of face for him,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "43ee49e31b614f168aa199dc923b09a3",
      "question": "How many protesters gathered at the Sudanese Embassy?",
      "prediction": "3,000",
      "ground_truths": [
        "3,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e5d337300d344dc5878baeb061f7a4d7",
      "question": "how many killed in conflict?",
      "prediction": "more than 200,000 people,",
      "ground_truths": [
        "more than 200,000 people,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0152a392343845f093ed902e84665b09",
      "question": "What did protesters shout?",
      "prediction": "\"Hey Bush, you can't hide! Help us end this genocide!\"",
      "ground_truths": [
        "\"Hey Bush, you can't hide! Help us end this genocide!\" and \"President Bush! No more excuses!\""
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "065ce886fe8c41088ba6b71c82ab6229",
      "question": "Who is Ross Perot?",
      "prediction": "businessman",
      "ground_truths": [
        "businessman"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "480a34c34b094ac6b071bc658c0da810",
      "question": "how much is it expected to go to",
      "prediction": "at least $20 million to $30 million,",
      "ground_truths": [
        "$30 million,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "625274d8050f451ebd131104306237c1",
      "question": "what is magna carta",
      "prediction": "was the first document to acknowledge the legal right to freedom from tyranny,",
      "ground_truths": [
        "The charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law."
      ],
      "em": 0,
      "f1": 0.13333333333333333
    },
    {
      "id": "531b64eef1bd44568360b3ab40da40da",
      "question": "what is expected price",
      "prediction": "at least $20 million to $30 million,",
      "ground_truths": [
        "$20 million to $30 million,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "b33f0ec8e82f44baa01e6b09f9e69104",
      "question": "who bought the version",
      "prediction": "Ross Perot.",
      "ground_truths": [
        "Ross Perot."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f724593e7f4842569187dae70a2560d9",
      "question": "Where will the auction be held?",
      "prediction": "National Archives in Washington.",
      "ground_truths": [
        "Sotheby's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "afc8672a21f441369ab4b0ffb8532c21",
      "question": "what will sotheby do",
      "prediction": "auction off one of the earliest versions of the Magna Carta",
      "ground_truths": [
        "auction off one of the earliest versions of the Magna Carta"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c89d314ad5224fe9896b825054544eed",
      "question": "Christie denied show tax credit because",
      "prediction": "it was unjustifiable",
      "ground_truths": [
        "it was unjustifiable"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "81ac809606274fe3b7fa4191fda48454",
      "question": "what did Christie denied show tax credit ?",
      "prediction": "that makes New Jersey look bad,",
      "ground_truths": [
        "20% tax credit"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "82498b7e764c4a77ae54de9e49a211ce",
      "question": "Which museum is the world biggest repositories of ancient Egypt?",
      "prediction": "The Louvre",
      "ground_truths": [
        "France's famous Louvre"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cac7bd69b3e546a8aeccaeabc394f172",
      "question": "Which country accuses museum?",
      "prediction": "Egypt",
      "ground_truths": [
        "Egypt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7df3df70d1c74ae9a1255fbee295ef62",
      "question": "Who accuse the museum of failing to return stolen goods?",
      "prediction": "Egypt",
      "ground_truths": [
        "Egypt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e89b8741115d4a248603dde70032bccd",
      "question": "Who suspends ties with France?",
      "prediction": "Egypt",
      "ground_truths": [
        "Egypt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77581c9e1c504eb6a88f1cc8693a1f7b",
      "question": "Who did Egypt suspend ties with?",
      "prediction": "France's",
      "ground_truths": [
        "France's famous Louvre museum"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "af91062c61d0431fa90edb3cffc48da0",
      "question": "What is the one of worlds biggest repositories?",
      "prediction": "The Louvre",
      "ground_truths": [
        "The Louvre"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ca31a2be70584f73bd02f10dfb274f1b",
      "question": "Who pioneered inventions?",
      "prediction": "Steve Jobs",
      "ground_truths": [
        "Jobs"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "112b26c0fb2a42f8ad17dedcfa162a14",
      "question": "Who made the first fully computer generated animated film?",
      "prediction": "Steve Jobs",
      "ground_truths": [
        "Jobs"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6fd4830b5450487bbc05567bc69af124",
      "question": "What did Steve Jobs pioneer?",
      "prediction": "computer-generated animated film",
      "ground_truths": [
        "iTunes Music Store,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "374759f65388474086782e711a4bdb75",
      "question": "What did Pixar make?",
      "prediction": "\"Toy Story\"",
      "ground_truths": [
        "\"Toy Story\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "179a0bc5fd2d43e886af75064e440362",
      "question": "What made buying music easy?",
      "prediction": "iTunes",
      "ground_truths": [
        "iTunes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "067d7ffc28744dc280160df2299cb954",
      "question": "What did Jobs inventions do?",
      "prediction": "revolutionized the world of music downloads.",
      "ground_truths": [
        "changed the business of music,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "8aa7295647af426cb083aedce76baaf9",
      "question": "Who made the  first fully computer-generated animated film?",
      "prediction": "Apple",
      "ground_truths": [
        "Pixar's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a92a2147d2454159bfb10fb49300c7c6",
      "question": "What was the result of Jobs invention?",
      "prediction": "made it exciting and simple and effortless and fun.\"",
      "ground_truths": [
        "left his indelible fingerprints on the entertainment industry."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "091ce4b419e4483888c050e9b7e4d26d",
      "question": "Who were arrested Tuesday?",
      "prediction": "Mohler Sr. and his sons Burrell Edward Mohler Jr., 53; David A. Mohler, 52; Jared Leroy Mohler, 48; and Roland Neil Mohler, 47.",
      "ground_truths": [
        "Mohler Sr. and his sons Burrell Edward Mohler Jr., 53; David A. Mohler, 52; Jared Leroy Mohler, 48; and Roland Neil Mohler, 47."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bccbcc543c334277bf59683a13353d43",
      "question": "Who was charged with two counts of rape Friday?",
      "prediction": "Darrel Mohler",
      "ground_truths": [
        "Darrel Mohler"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce76340da112438eb21e2fb526bb777a",
      "question": "What was Darrel Mohler charged with?",
      "prediction": "two counts of rape,",
      "ground_truths": [
        "two counts of rape,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9140e5ce275a4c9489694b8624c7d0f1",
      "question": "What years do the allegations come from?",
      "prediction": "1980s,",
      "ground_truths": [
        "mid-1980s until 1995"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f121c2d17b442f08c3eed64fcc67102",
      "question": "How many people in total were arrested?",
      "prediction": "six",
      "ground_truths": [
        "sixth"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2c97e4992b2b44ab89e61a7f71effca3",
      "question": "Who were the accusers?",
      "prediction": "five suspects,",
      "ground_truths": [
        "relatives of the five suspects,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "463a29cc0b9a400c9fa0836baa81c58d",
      "question": "acres overlooking the Pacific",
      "prediction": "127",
      "ground_truths": [
        "127"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "20d07b81a2454a0f8e835bf5075aec92",
      "question": "Where is the estate?",
      "prediction": "Hearst Castle.",
      "ground_truths": [
        "San Simeon, California,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "16585bf289824fe78653e08246fd2ac6",
      "question": "What did Hearst fill the rooms with?",
      "prediction": "religious icons, art and other worldly treasures.",
      "ground_truths": [
        "Museum-worthy pieces"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad74e88eda9444b4a4255b4a8d4c7d33",
      "question": "Where is the estate located?",
      "prediction": "Hearst Castle",
      "ground_truths": [
        "San Simeon, California,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "880a5931cab045d5ba624102c4b4a3af",
      "question": "Who entertained Hollywood stars at his estate?",
      "prediction": "William Randolph Hearst.",
      "ground_truths": [
        "William Randolph Hearst."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9032f2b4a10c49a58bfcaab938baba2b",
      "question": "How many rooms does it have?",
      "prediction": "165-room",
      "ground_truths": [
        "165-room"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6dfdbc3e53da4319b04870079060d1bf",
      "question": "What did he fill the rooms with?",
      "prediction": "art and antiquities",
      "ground_truths": [
        "Museum-worthy pieces"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49d9ebe7135b4a2890ced8b689c6dbd8",
      "question": "How many people did the Camorra clan kill?",
      "prediction": "53-year-old Italian owner,",
      "ground_truths": [
        "an Italian and six Africans"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "cae54358803c4415988d062256f38b21",
      "question": "What larger crime organization is mentioned?",
      "prediction": "Camorra clan",
      "ground_truths": [
        "Mafia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5223d20b95664375ad1b2133f36ba1ca",
      "question": "How many times has the army been sent in?",
      "prediction": "the second",
      "ground_truths": [
        "This will be the second"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "88c911ecac0e4af284c5eb43fe8d9eb5",
      "question": "Where would a majority of the troops be sent to?",
      "prediction": "the southern city of Naples",
      "ground_truths": [
        "southern city of Naples"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4feceb16c4441d8b4ce29dcae36016f",
      "question": "Number of people that were murdered by the local Camorra clan?",
      "prediction": "(five)",
      "ground_truths": [
        "an Italian and six Africans"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aacdcc8d1b8345ce9f2a4199aac5f721",
      "question": "Six people were murdered by what clan last week?",
      "prediction": "Camorra",
      "ground_truths": [
        "The Casalesi Camorra"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "40a50aaeb28e4892956a7667321f0fd5",
      "question": "How many times since the 1990s has the army be sent to combat mafia crime?",
      "prediction": "second",
      "ground_truths": [
        "second"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "390ff5dbc5774ef6b94e51d2e2910123",
      "question": "Who is the Defense Minister that is quoted?",
      "prediction": "Ignazio La Russa",
      "ground_truths": [
        "Ignazio La Russa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c289df329d5411989370e36f4d4fc97",
      "question": "Where would the majority of the tropps be sent?",
      "prediction": "southern city of Naples",
      "ground_truths": [
        "Naples"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c53f2dd5200e4beb9e81348f91a8e824",
      "question": "Who murdered six people?",
      "prediction": "Casalesi Camorra clan",
      "ground_truths": [
        "The Casalesi Camorra clan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4f667caccc9e49a8b5a6dede296fbd1e",
      "question": "The majority of troops will be sent where?",
      "prediction": "southern city of Naples",
      "ground_truths": [
        "southern city of Naples"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cff10ff0cb9f4753af9add4c23a5a6a7",
      "question": "Who was the army sent to combat?",
      "prediction": "Mafia crime",
      "ground_truths": [
        "Mafia"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9c0af32b6fce49cc84b8e5a98d5619e4",
      "question": "Who fled the wreckage?",
      "prediction": "\"Average\"",
      "ground_truths": [
        "Mawise Gumba"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "65095a05ce414524b7799e1fa87e7f11",
      "question": "What is the percentage of inflation?",
      "prediction": "5,000",
      "ground_truths": [
        "close to 5,000 percent,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "a348607263324459aa73a678e65c2cac",
      "question": "Where do they flee to?",
      "prediction": "South Africa,",
      "ground_truths": [
        "JOHANNESBURG, South Africa"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2d419111063f489d8eefde205de5fa68",
      "question": "Where is Zimbabwe?",
      "prediction": "South Africa.",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2911e86c54494f89abe1c854bb25830e",
      "question": "When did Mugabe become President?",
      "prediction": "1980,",
      "ground_truths": [
        "1980,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3e39336ef5994bc9a78d2557d64331f6",
      "question": "what did he do",
      "prediction": "lost his job as the supermarket chain",
      "ground_truths": [
        "fled Zimbabwe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09efa84b0e684c3f8c4d631b68f372ce",
      "question": "What happened to once prosperous nation?",
      "prediction": "is an increasingly common one.",
      "ground_truths": [
        "spiral into economic disaster."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "db571ac79d34404b9a59b62c7fd38c56",
      "question": "What did the congressman say?",
      "prediction": "\"it should stay that way.\"",
      "ground_truths": [
        "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\" and"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a018ca4c1e8e4a44b48188655383c8a5",
      "question": "who is Congressman ?",
      "prediction": "Paul Ryan",
      "ground_truths": [
        "Paul Ryan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "44bf7ea0266142f6997e0596077986b7",
      "question": "What is Paul Ryan's position?",
      "prediction": "on the bill",
      "ground_truths": [
        "will not support the Stop Online Piracy Act,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0cecc7cafa884126923093cb39466c96",
      "question": "Ryan says Internet is an expression of freedom and should",
      "prediction": "\"it should stay that way.\"",
      "ground_truths": [
        "stay that way.\""
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "e88a0c1244b5484b830b44a6139bec80",
      "question": "who did  says Internet is an expression?",
      "prediction": "Paul Ryan",
      "ground_truths": [
        "Congressman Paul Ryan"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "928e2cc022da44a4893b47a7fa37e1c6",
      "question": "what did Congressman say?",
      "prediction": "SOPA is an attempted solution to the \"legitimate problem\" of digital piracy, but the bill \"creates the precedent and possibility for undue regulation, censorship and legal abuse.\"",
      "ground_truths": [
        "\"one of the most magnificent expressions of freedom and free enterprise in history\""
      ],
      "em": 0,
      "f1": 0.11428571428571427
    },
    {
      "id": "7ca689bb9bd94572af15fb45f2257266",
      "question": "What did Ryan say is an expression of freedom?",
      "prediction": "Internet",
      "ground_truths": [
        "the Internet"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5a3111eeeaff40f8876c1d5ebc1f7855",
      "question": "What does the West suspect?",
      "prediction": "Iran of trying to build nuclear bombs,",
      "ground_truths": [
        "Iran of trying to build nuclear bombs,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3094f8c287a241f6a310c9518a88ca39",
      "question": "who is suspected to build bombs?",
      "prediction": "Iran",
      "ground_truths": [
        "Iran"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "285b1eeffd2143f98100690ea1c00b92",
      "question": "what was considered a provocative act?",
      "prediction": "Iran test-launched a rocket capable of carrying a satellite,",
      "ground_truths": [
        "Iran test-launched a rocket capable of carrying a satellite,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77a760bc179749ddb620a86729122ce1",
      "question": "Who has been fired?",
      "prediction": "Elizabeth Birnbaum",
      "ground_truths": [
        "Elizabeth Birnbaum"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dd60a5cbf3f74a689e9996a315253cee",
      "question": "What has been introduced in part to help crack down on the MMS?",
      "prediction": "legislation",
      "ground_truths": [
        "requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "34ba486115794ddea6fe7f05181250b5",
      "question": "who report inappropriate relationship?",
      "prediction": "Interior Department's inspector general's",
      "ground_truths": [
        "Interior Department's inspector general,"
      ],
      "em": 0,
      "f1": 0.75
    },
    {
      "id": "b812244cbb374be1906ecd90aa03cac8",
      "question": "What does Interior Secretary Ken Salazar insist?",
      "prediction": "\"including taking any and all appropriate personnel actions including termination, discipline and referrals of any wrongdoing for criminal prosecution.\"",
      "ground_truths": [
        "that Birnbaum had resigned \"on her own terms and own volition.\""
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "d897192ab88743b8bf6ea3c00bc845a8",
      "question": "What is the name of the PM?",
      "prediction": "Abhisit Vejjajiva",
      "ground_truths": [
        "Abhisit Vejjajiva"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1fe3dce165394bee88bb3829fc87ba0c",
      "question": "What does the opposition  red shirt demand?",
      "prediction": "that the prime minister dissolve the parliament within 15 days.",
      "ground_truths": [
        "that the prime minister dissolve the parliament within 15 days."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d72847aa89bd4373b5f03086a4cbdd0b",
      "question": "How many protesters occupy commercial heart?",
      "prediction": "Thousands",
      "ground_truths": [
        "50,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "293fe307f8554032bfd957d206dd780f",
      "question": "Who say they will continue until their demands have been met?",
      "prediction": "Thousands of anti-government protesters",
      "ground_truths": [
        "anti-government protesters"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "27b67e1a4f4d40b5b5d242ef0b789b9a",
      "question": "What is Shinawatra's first name?",
      "prediction": "Thaksin",
      "ground_truths": [
        "Thaksin"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af4cc2d2ba214356adcef50279172202",
      "question": "Who demmand that PM Abhisit dissolver?",
      "prediction": "\"red shirts,\"",
      "ground_truths": [
        "The opposition group, also known as the \"red shirts,\""
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "0372808323d540e3859a93d9ae4843f1",
      "question": "Who died in Afghanistan?",
      "prediction": "Phillip A. Myers.",
      "ground_truths": [
        "Phillip A. Myers."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bae7071c3f5d47edac66d6ecf05633b7",
      "question": "Who died in Afghanistan on Saturday?",
      "prediction": "Phillip Myers,",
      "ground_truths": [
        "Phillip A. Myers."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8860bc24541401bb994c72c67e5c7af",
      "question": "Who lifted the media ban?",
      "prediction": "President Obama",
      "ground_truths": [
        "President Bill Clinton"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "8bc486ca6580487586a39a78cc2ce5d2",
      "question": "What rank was Phillip Myers?",
      "prediction": "staff sergeant",
      "ground_truths": [
        "staff sergeant"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "80726cf8c538458ea11fd99b21b45cf8",
      "question": "What can be covered by media for the first time in 20 years?",
      "prediction": "the return of a fallen U.S. service member was able to",
      "ground_truths": [
        "return of a fallen U.S. service member"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f07750ea1251487eaf95f98457f8e5bf",
      "question": "Who lifted the ban on media coverage?",
      "prediction": "President Obama",
      "ground_truths": [
        "Bill Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "63df6e921a2d41c3a6c8c2ed0b8de7c2",
      "question": "What was the pilot trying to do?",
      "prediction": "fake his own death",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a89369b7e712438388b7ef5b61ae505d",
      "question": "What were the investigators investigating?",
      "prediction": "possible securities violations",
      "ground_truths": [
        "his business dealings"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c52b52625cd74d95a45e60169d9826dd",
      "question": "Who are officials searching for?",
      "prediction": "Marcus Schrenker,",
      "ground_truths": [
        "Marcus Schrenker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "65ac4247c8504132a16e920b53ac63d1",
      "question": "What did the manager in Alabama say about Shrenker?",
      "prediction": "\"an accomplished pilot\" who owns \"a couple of airplanes\"",
      "ground_truths": [
        "Schrenker was the only guest overnight. He signed in as Jason Galouzs of Bolingbrook, Illinois,"
      ],
      "em": 0,
      "f1": 0.09523809523809523
    },
    {
      "id": "2c36ce6243e44160a69bead31c2ac6e8",
      "question": "What did the manager say about Schrenker?",
      "prediction": "was the only guest overnight.",
      "ground_truths": [
        "is \"an accomplished pilot\" who owns \"a couple of airplanes\" and flies regularly."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a011e7f0db714fbcb0c05b9cd00b01e7",
      "question": "What did authorities say the pilot tried to do?",
      "prediction": "fake his own death",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "aa3df64ad91a43e48011ccc75494e27d",
      "question": "Who did the manager of the hotel said stayed at the hotel overnight?",
      "prediction": "Jason Galouzs",
      "ground_truths": [
        "Schrenker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "33de3e9db2a54cbea1114b4744212328",
      "question": "What type of violations are they looking into?",
      "prediction": "securities",
      "ground_truths": [
        "securities"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "646a975b50a442eabb7ec808f829bb09",
      "question": "What did the person try to do?",
      "prediction": "fake his own death by crashing his private plane into a Florida swamp.",
      "ground_truths": [
        "fake his own death by crashing his private plane into a Florida swamp."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2825e144990a449cb613d652b4188c2e",
      "question": "Who was evicted?",
      "prediction": "Lisa Brown",
      "ground_truths": [
        "Brown and her family"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "be34ac10abbf40838b850950f70bdcd5",
      "question": "Who is working on a law to help renters?",
      "prediction": "New York State Sen. Jeff Klein",
      "ground_truths": [
        "Sen. Jeff Klein"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ff9f6bcd4a914ee4b5d2a993bd5d9ea8",
      "question": "What happened to Lisa Brown?",
      "prediction": "moved into her rental house because it is facing foreclosure",
      "ground_truths": [
        "being evicted"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5baea57b39364a358c852ba09273d51d",
      "question": "Who is working on a law to warn renters of foreclosures?",
      "prediction": "New York State Sen. Jeff Klein",
      "ground_truths": [
        "New York State Sen. Jeff Klein"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "128b2d769fe045ca8e1bff19e037b7da",
      "question": "Who is working on the law?",
      "prediction": "New York State Sen. Jeff Klein",
      "ground_truths": [
        "Sen. Jeff Klein"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e08060ab217a47f6b0fd8f4ca9d96c11",
      "question": "What did Brown want for her children?",
      "prediction": "a better environment.\"",
      "ground_truths": [
        "\"to give my kids a better environment.\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c9e7a34ffb9949a7a4cdc4fd89436a80",
      "question": "Whose landlord was foreclosed upon?",
      "prediction": "Lisa Brown",
      "ground_truths": [
        "Lisa Brown"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0f7e7b9139d849eda2591b83348e037e",
      "question": "What show features Wozniak this season?",
      "prediction": "\"Dancing With the Stars\"",
      "ground_truths": [
        "\"Dancing With the Stars.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c989b740d7774626a6b659c806935559",
      "question": "What TV show is Steve Wozniak going to be on?",
      "prediction": "\"Dancing With the Stars.\"",
      "ground_truths": [
        "\"Dancing With the Stars.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5d27fdf94cc74998bd78ccda5998134a",
      "question": "Who co-founded Apple?",
      "prediction": "Steve Wozniak",
      "ground_truths": [
        "Wozniak"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e2e347c7201d45779244494690d6d481",
      "question": "What company did Wozniak help create?",
      "prediction": "Apple Inc.",
      "ground_truths": [
        "Apple Inc."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f07e6d91823b4cb8856d3672a83a75f3",
      "question": "Wozniak being supported via voting and what else?",
      "prediction": "social networking",
      "ground_truths": [
        "social networking"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "005040ca6e354afba3d171349bdde420",
      "question": "Who is supporting Wozniak?",
      "prediction": "Aaron Petrey",
      "ground_truths": [
        "fans of Woz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8f6c0a2a8ad24bed90083ee1fade53da",
      "question": "Who is the Apple co-founder?",
      "prediction": "Steve Wozniak",
      "ground_truths": [
        "Wozniak"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "022385a9defe4d71b208f1b405b61b1b",
      "question": "Who spoke in favor of the judge's confirmation?",
      "prediction": "Frank Ricci,",
      "ground_truths": [
        "New York City Mayor Michael Bloomberg"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf0c2ff686124703a81f6e2e492e57df",
      "question": "Who spoke in favor of judge's confirmation?",
      "prediction": "Frank Ricci,",
      "ground_truths": [
        "Arkansas Attorney General Dustin McDaniel testified on Sotomayor's behalf,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3121eac762014f83a9df1a81ede1386b",
      "question": "What did the firefighters claim?",
      "prediction": "the city of New Haven, she noted, was at risk of being sued by employees who could show they were \"disparately impacted\" by the test.",
      "ground_truths": [
        "reverse discrimination"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "793f5116a5c44b67b76f7e63340cb867",
      "question": "Who was one of 20 firefighters who claimed reverse discrimination in promotions?",
      "prediction": "New Haven firefighter Frank Ricci,",
      "ground_truths": [
        "Frank Ricci"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "124127b3d7994f27a8763e3bd21f369b",
      "question": "Who was the plaintiff in this case?",
      "prediction": "New Haven firefighter Frank Ricci",
      "ground_truths": [
        "Frank Ricci,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8546907869db4cf9bb91335962dad764",
      "question": "what was the ruling based on?",
      "prediction": "the law required it to do.",
      "ground_truths": [
        "\"a very thorough, 78-page decision by the district court\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c7aace68925a4affbb0e4cbb611d9207",
      "question": "who is Frank Ricci?",
      "prediction": "New Haven firefighter",
      "ground_truths": [
        "New Haven, Connecticut, firefighter"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "4988ebcead954a1a9d712f4e3db68031",
      "question": "What undermined the concept of a merit-based civil service system?",
      "prediction": "rejection of his reverse discrimination claim",
      "ground_truths": [
        "that Sotomayor's rejection of his reverse discrimination claim"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "1bfe21f2aa3b42e68d31695509ba53d9",
      "question": "What comments were a new low?",
      "prediction": "Biden",
      "ground_truths": [
        "a debate over who cares more for special needs children."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9cdb6245f11c40f1b5e26abd657677d9",
      "question": "what has the attorney denounced",
      "prediction": "the fact that the teens were charged as adults.",
      "ground_truths": [
        "the fact that the teens were charged as adults."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "90238bd9190f43b9a104cc5d0a6f0583",
      "question": "what does the attorney denounce?",
      "prediction": "the fact that the teens were charged as adults.",
      "ground_truths": [
        "the fact that the teens were charged as adults."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "37ce5358517f4fce998cca3ed31292aa",
      "question": "what percentage burns did Brewer suffer?",
      "prediction": "65 percent",
      "ground_truths": [
        "over 65 percent of his body."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "36db4e1d502a4aae9f49373c4ccc53a7",
      "question": "How much of his body burned?",
      "prediction": "65 percent",
      "ground_truths": [
        "over 65 percent of"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "52cba38cc68d493bb73a3a751e0f849d",
      "question": "how many teens were charged?",
      "prediction": "Three",
      "ground_truths": [
        "Three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e75b5fe1656c408a8091bae8561f4215",
      "question": "what have the boys been charged with",
      "prediction": "one count of attempted murder in the second degree",
      "ground_truths": [
        "attempted murder,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "0c3b1ed51cc14bbea2ccc8f4ec29c94e",
      "question": "How many people where charged?",
      "prediction": "Three",
      "ground_truths": [
        "Three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a90d7f4e49ed4041882d877e60323398",
      "question": "what does he have on his body",
      "prediction": "65 percent of",
      "ground_truths": [
        "burns"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97409d2d3afe44e48f287814cdb36dfb",
      "question": "Who leads the Genocide Prevention Task Force?",
      "prediction": "Madeleine K. Albright",
      "ground_truths": [
        "Madeleine K. Albright"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "09c79cbef9b6454cac1fe2e0aeb0cbf8",
      "question": "When will the panel's final report be released?",
      "prediction": "next week.",
      "ground_truths": [
        "next week.)"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cd041a253c746be9389d119b5de2d14",
      "question": "What are the thoughts of Albright and Cohen?",
      "prediction": "are increasingly confronted in their living rooms",
      "ground_truths": [
        "we believe that leadership is an indispensable ingredient."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ecd8bbe295e4e25be8acdc72d307e86",
      "question": "Who is Madeleine Albright?",
      "prediction": "former U.S. secretary of state.",
      "ground_truths": [
        "former U.S. secretary of state."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7a21c1eb21aa49af8a0ba5450b7da9dc",
      "question": "What do Madeleine Albright and William Cohen co-chair?",
      "prediction": "Genocide Prevention Task Force.",
      "ground_truths": [
        "Genocide Prevention Task Force."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7173278ae5a5424d8b5b49177dae0264",
      "question": "When did the shooting take place?",
      "prediction": "May 11,",
      "ground_truths": [
        "May 11,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62cf86565d62481d8db7e902fa795eed",
      "question": "What did the police spokesperson say?",
      "prediction": "\"Indeed, it is ugly to see people behaving as if that was not a dead body and going on their daily routine,\"",
      "ground_truths": [
        "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county."
      ],
      "em": 0,
      "f1": 0.05405405405405405
    },
    {
      "id": "bc670fde5854495bb9cddeaaec024ce1",
      "question": "Who appeared unfazed by the neighborhood shooting?",
      "prediction": "Passers-by",
      "ground_truths": [
        "Passers-by"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "65eac78c98514b18b0d5462462cc29a8",
      "question": "What did the video show?",
      "prediction": "man wearing a baseball cap, dark jersey, blue jeans and running shoes entering a store, walking to the back and looking around, then walking out. As he exits, he pulls a pistol from his right front pants pocket and shoots a man standing outside the store.",
      "ground_truths": [
        "the execution."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2fd937838f0c4d2d84ca2688fc5e0ebc",
      "question": "What is the amount of the reward being offered by Italy's Green Party for informaation about the whereabouts of the shooter and accomplice?",
      "prediction": "2,000 euros",
      "ground_truths": [
        "2,000 euros ($2,963)"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "3842d04ed8de486f910aadd2233a5e2a",
      "question": "What mosque was raided?",
      "prediction": "a mosque in the southern Gaza city of Rafah,",
      "ground_truths": [
        "in the southern Gaza city of Rafah,"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "e65ae069b67b4bf69b25fc669ee363b0",
      "question": "What group blew up home?",
      "prediction": "Jund Ansar Allah",
      "ground_truths": [
        "Hamas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8fa77691446e44ad8f9cba8904cf69da",
      "question": "Who claims sheikh's followers are \"outlaws?\"",
      "prediction": "Hamas ministry spokesman Taher Nunu",
      "ground_truths": [
        "Taher Nunu"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "9b3ab470f12a42239f0ab1a1f27acff5",
      "question": "Where did Hamas raid the mosque?",
      "prediction": "Rafah",
      "ground_truths": [
        "in the southern Gaza city of Rafah,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "288f88ddfaf34bdabc2ff3748757ac21",
      "question": "What sparked clashes with Hamas?",
      "prediction": "A radical Muslim sheikh called Friday for the creation of an Islamic emirate in Gaza,",
      "ground_truths": [
        "Sheikh Abu al-Nour al-Maqdessi,"
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "9c8375d231b04c74b817c1ef9db3ec33",
      "question": "What did Hamas spokesman call sheikh's followers?",
      "prediction": "\"outlaws\"",
      "ground_truths": [
        "\"outlaws\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a8eff0d1b134de888817e44d2e889b1",
      "question": "What did Jero do to the Japanese songs?",
      "prediction": "singing",
      "ground_truths": [
        "is making old, new again in Japan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "599f7e94db9a48019f887451a66c4e1d",
      "question": "When did he sing enka?",
      "prediction": "1940's",
      "ground_truths": [
        "in early 2008,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b3d58c3cb86044e1ac53ac99a0246d3b",
      "question": "Where is Jero came from?",
      "prediction": "African-American",
      "ground_truths": [
        "Pittsburgh"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "62179d55c8ab4178bb0450eafda198cf",
      "question": "What ranking did his first single make?",
      "prediction": "No 4,",
      "ground_truths": [
        "No 4,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "17ca78d8ef664047ad404f91d1f62c4b",
      "question": "when was he in talent show?",
      "prediction": "early 2008,",
      "ground_truths": [
        "2008,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "daccc4268b874f63a33873b7171ba8ff",
      "question": "what is an enka song?",
      "prediction": "traditional form of lounge music that flourished in 1940's Japan.",
      "ground_truths": [
        "traditional form of lounge music"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e62bc50eddf84d3a9d77bae7b77fc196",
      "question": "What did Bush say the U.S. will do?",
      "prediction": "do its part to improve the environment by taking on greenhouse gas emissions.",
      "ground_truths": [
        "its part to improve the environment by taking on greenhouse gas emissions."
      ],
      "em": 0,
      "f1": 0.9565217391304348
    },
    {
      "id": "53259c05362845bda84bd8a1783f10e7",
      "question": "What did the president propose?",
      "prediction": "a global climate change conference",
      "ground_truths": [
        "to come together and \"set a long-term goal for reducing\" greenhouse emissions."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3ff75cbd8cf48ff834d6ef8803ff63f",
      "question": "did bush say something",
      "prediction": "\"We take this issue seriously.\"",
      "ground_truths": [
        "\"we take this issue seriously.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68021cc757c14f9199e2d4da38f53356",
      "question": "What did Bush ask nations to reduce?",
      "prediction": "greenhouse gas emissions.",
      "ground_truths": [
        "greenhouse emissions."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "362eea3f213c4d51a9994cf1d52a17bc",
      "question": "what did bush call for",
      "prediction": "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\"",
      "ground_truths": [
        "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal"
      ],
      "em": 0,
      "f1": 0.9523809523809523
    },
    {
      "id": "430156c52a22415980c710c67d9a19fe",
      "question": "What is the president's proposal?",
      "prediction": "taking on greenhouse gas emissions.",
      "ground_truths": [
        "initiative to develop a common approach to combat global warming"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4c961dd89b054526b342c921486477a6",
      "question": "How many years did he win the title?",
      "prediction": "fifth successive season",
      "ground_truths": [
        "18th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e5fb043f8b0245f2acc78136af1bb5dd",
      "question": "Who won the Italian Serie A title?",
      "prediction": "Milan",
      "ground_truths": [
        "Milan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62e9113dcca84b61af8304429b1570da",
      "question": "Who scored the only goal?",
      "prediction": "Diego Milito's",
      "ground_truths": [
        "Diego Milito's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "09a4f669c3ce4432954d3b5348ffde56",
      "question": "What did Inter Milan win?",
      "prediction": "Italian Serie A title",
      "ground_truths": [
        "Italian Serie A title"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "26c180c03a734a1aaee4138be308a078",
      "question": "who won the Italian Serie A title for a fifth successive year ?",
      "prediction": "Milan",
      "ground_truths": [
        "Milan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba1325c4040040a0b7809a32536da5fe",
      "question": "in what positon do Rome finish?",
      "prediction": "pole",
      "ground_truths": [
        "second-placed"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9966c7735a6649d88366224ff6552c68",
      "question": "who scores the only goal ?",
      "prediction": "Diego Milito's",
      "ground_truths": [
        "Diego Milito's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8bf5d3ca87e449fda5afd9410581c273",
      "question": "Where are US forces standing by?",
      "prediction": "to provide security",
      "ground_truths": [
        "Port-au-Prince"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5d5a5752b1ea4b928cb778bc543ee042",
      "question": "What does the Haitian police force represent?",
      "prediction": "is responsible for security on the streets, with backing from U.N. peacekeepers.",
      "ground_truths": [
        "\"The first line of law and order"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac3359aa637e49f2b14c9cb0391999b4",
      "question": "What do the UN forces do?",
      "prediction": "security",
      "ground_truths": [
        "standing by to provide security"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "8f50caa41d034d3fa83ed2e4c4907830",
      "question": "What are US forces standing by to do if needed?",
      "prediction": "provide security",
      "ground_truths": [
        "to provide security as needed.\""
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "76cefaf0a5ec4b3db7ddb400b9d6efce",
      "question": "What does the National police chief say?",
      "prediction": "The Port-au-Prince force of 4,000 has dropped to about 1,500, he said.",
      "ground_truths": [
        "the department has been severely affected by the earthquake,"
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "9014fb3e6aec4cf0bfbdd77ddf364ec1",
      "question": "How many bodies are police looking to recover?",
      "prediction": "Two people",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "9cdd0335d31d4fdca006adeea4058692",
      "question": "How many people are in critical condition?",
      "prediction": "Four",
      "ground_truths": [
        "Four"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "caeeaa8b6a4b422981f799e26dfb4eba",
      "question": "How many were taken to hospital?",
      "prediction": "At least 38 people",
      "ground_truths": [
        "At least 38 people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc31691c8a774a739d53ecc5a4f24279",
      "question": "What caused the roof collapse at ConAgra Foods in NC?",
      "prediction": "ammonium leaks",
      "ground_truths": [
        "cause of the blast was unknown,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cce903a7338748cdae3a5e8fdc9d97dd",
      "question": "What are the police looking for?",
      "prediction": "two bodies out of the plant,",
      "ground_truths": [
        "two bodies out of the plant,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "273287c070454cbc85b8affade5d1354",
      "question": "What is the cause of the explosion?",
      "prediction": "Unknown,",
      "ground_truths": [
        "unknown,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9aceab5d60c648b89e292afe7c551041",
      "question": "What is the number of people in the hospital?",
      "prediction": "More than 300",
      "ground_truths": [
        "38"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a1f9c266d93643f6aa8d1e452094db07",
      "question": "What collapsed?",
      "prediction": "ConAgra Foods plant",
      "ground_truths": [
        "ConAgra Foods plant"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28f7a52945994183bf9e7aa9a78088ce",
      "question": "What are teams doing!",
      "prediction": "continue trying to get the two bodies out of the plant,",
      "ground_truths": [
        "trying to get the two bodies out of the plant,"
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "b3ac5354aeb34281aad0d65e39aee055",
      "question": "Who issued military orders before his father's death?",
      "prediction": "Kim Jong Un",
      "ground_truths": [
        "Kim Jong Un"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4cc2ec62d53a4f6ab37f777a1c8f1884",
      "question": "When did Kim Jon Un issue military orders?",
      "prediction": "just before the death of his father was announced,",
      "ground_truths": [
        "before the death of his father"
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "fc87a9e0038f41a7856ce71bc10a51ac",
      "question": "What does a defector tell CNN?",
      "prediction": "\"North Koreans don't speak openly,\"",
      "ground_truths": [
        "\"If anyone knows I'm talking, I would be sent to prison and there's no mercy there. I would be shot dead.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ff81c4136e94b4791b5df78165c0613",
      "question": "What did a defector tell CNN about?",
      "prediction": "the death of their \"dear leader,\" as he was called in his country.",
      "ground_truths": [
        "people are starving, aid is scarce, and the only operating factories serve the military."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3a09e4e4c1e4595ba0ce01bf501c5e1",
      "question": "What did the state-run news agency say?",
      "prediction": "KCN",
      "ground_truths": [
        "Kim Jong Un issued his first military orders as leader of North Korea just"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3d1f0f420458417fb7f17d81786eb726",
      "question": "Who controls the military, according to the state-run news agency?",
      "prediction": "Kim Jong Un",
      "ground_truths": [
        "Kim Jong Un"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "46921e1f3e304da5b75f8ca3afff20ff",
      "question": "What did the defector tell CNN?",
      "prediction": "He painted a grim picture of life in North Korea,",
      "ground_truths": [
        "\"North Koreans don't speak openly,\""
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "c41854c556a044ccbf0031953cbf72cf",
      "question": "What were 2 top South Korean officials under fire for?",
      "prediction": "admitting they learned of the death from TV news coverage,",
      "ground_truths": [
        "from TV news coverage,"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "24826e64b8864860ad6daffddd3e2ade",
      "question": "What did Kim Jong Un do before his father's death?",
      "prediction": "issued his first military orders",
      "ground_truths": [
        "issued"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "0f3330082ab946a7b182367e4acf9923",
      "question": "What is Jonathan Pryce honored for?",
      "prediction": "Notable achievement in any field.",
      "ground_truths": [
        "a Golden Globe-nominated actor who has had roles in the \"Pirates of the Caribbean\" movies, \"Ronin,\" and the James Bond film \"Tomorrow Never Dies.\" He won a best actor award in 1995 at Cannes"
      ],
      "em": 0,
      "f1": 0.0588235294117647
    },
    {
      "id": "d8a24a9b52424e14bc0d9973651d0254",
      "question": "When is the honor day?",
      "prediction": "2023",
      "ground_truths": [
        "once on New Year's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11a57027e3ff46fb8465d96bd0fbb6fe",
      "question": "What is the weather like in the ITCZ?",
      "prediction": "Can hurricanes form in the ITCZ?",
      "ground_truths": [
        "volatile"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1b9f87a19463462dac00774f0c77eb01",
      "question": "Where did the plane crash?",
      "prediction": "Intertropical Convergence Zone, or ITCZ,",
      "ground_truths": [
        "volatile zone along the equator between South America and Africa."
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "7c74487fc1c943879ac66dda1e03d85f",
      "question": "Which airlines plane was involved?",
      "prediction": "Air France",
      "ground_truths": [
        "Air France"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ac3aadc635c40e1a687524905e7cef0",
      "question": "Where is the ITCZ?",
      "prediction": "around the Earth near the equator,",
      "ground_truths": [
        "near the equator,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5b4a540c8adf40a181b5fd70e898fad2",
      "question": "Where did the Air France plane go down?",
      "prediction": "Intertropical Convergence Zone,",
      "ground_truths": [
        "along the equator between South America and Africa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a73d6910ac6346e2a8251edac9754aca",
      "question": "What is the name of the airline?",
      "prediction": "France",
      "ground_truths": [
        "Air France"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5a0112def1ba44d2b4b535e772a43041",
      "question": "Where is the zone located?",
      "prediction": "Intertropical Convergence",
      "ground_truths": [
        "The Intertropical Convergence"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ff26483d64741f898660d55d3139a45",
      "question": "what is the pandemic",
      "prediction": "Spanish influenza",
      "ground_truths": [
        "Spanish flu"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "65bf7e1dc06d4affa15e9f99b800d94a",
      "question": "when did it ravage military camps",
      "prediction": "World War I rages in Europe,",
      "ground_truths": [
        "1918-1919."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "991079f7ad3d4e86b206d44758bee040",
      "question": "When was the outbreak?",
      "prediction": "1918-1919.",
      "ground_truths": [
        "1918-1919."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ded3279534804212b9f775a74529df74",
      "question": "Who survived flu",
      "prediction": "Stephen Foster's classic song",
      "ground_truths": [
        "Martin \"Al\" Culhane,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4897f36620f8405e8d1b6178fbd8fc1a",
      "question": "is the zetas cartel from the veracruz state",
      "prediction": "a",
      "ground_truths": [
        "the"
      ],
      "em": 1,
      "f1": 1
    },
    {
      "id": "465185c4d005477785c716fc985c1d64",
      "question": "will authoroties investigate these groups",
      "prediction": "Mexico's attorney general's office responded",
      "ground_truths": [
        "it would"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac6b3390afcf49538fc8c6e4f4d90b92",
      "question": "Which state does the Zetas cartel operate in?",
      "prediction": "Veracruz, Mexico,",
      "ground_truths": [
        "Veracruz, Mexico,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9a38ee24afda42c78816baccc7c10da8",
      "question": "What is Zeta Killers' goal?",
      "prediction": "the eradication of the Zetas cartel from the state of Veracruz,",
      "ground_truths": [
        "to kill members of the"
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "441a8fbb81d34b0cb294a47cb163900c",
      "question": "which is the goal of the group",
      "prediction": "is the eradication of the Zetas cartel from the state of Veracruz,",
      "ground_truths": [
        "cartel from the state of Veracruz, Mexico,"
      ],
      "em": 0,
      "f1": 0.6666666666666667
    },
    {
      "id": "8ffd7b74fa0b4ffc9aa2efd87a33b330",
      "question": "Where was the video released?",
      "prediction": "YouTube",
      "ground_truths": [
        "YouTube"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eed13bd468bf433ca4c03cc75f8dce9d",
      "question": "What law might the deal break?",
      "prediction": "European labor",
      "ground_truths": [
        "European labor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b57d7f7dff44384a3b3603662f82149",
      "question": "what was the reason  for the  striking",
      "prediction": "the way Britain implements European Union employment directives.",
      "ground_truths": [
        "opposed to plans to employ non-British workers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27cb6b4286584508b80a414b95f48853",
      "question": "What caused the protests?",
      "prediction": "the hiring of hundreds of foreign workers for a construction project",
      "ground_truths": [
        "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England."
      ],
      "em": 0,
      "f1": 0.72
    },
    {
      "id": "4dfec6615c4b490986e2c90a5587269b",
      "question": "where did this occur",
      "prediction": "Lindsford oil refinery",
      "ground_truths": [
        "Lindsay Oil Refinery"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bed2b391202744bd9619d3ecd0e410f8",
      "question": "What sparked the protests?",
      "prediction": "hiring of hundreds of foreign workers for a construction project",
      "ground_truths": [
        "hiring of hundreds of foreign workers"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e4c238fe056f476cbdc11d6c081f5638",
      "question": "What is number of new jobs offered?",
      "prediction": "101",
      "ground_truths": [
        "101"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3485ce6a752a4017bac723554c115dec",
      "question": "Who tries to snatch children from his ex-wife?",
      "prediction": "Christopher Savoie",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c614fa519d3c4da3a71fec5baec6fd60",
      "question": "What was the father charged with?",
      "prediction": "the abduction of minors.",
      "ground_truths": [
        "the abduction of minors."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2afec4767b340b1aa3a2504820aca8c",
      "question": "Who has legal custody in the U.S?",
      "prediction": "Christopher Savoie",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ac0b5576a66945a299d5ff45ce412c7a",
      "question": "Ex-wife of who?",
      "prediction": "Savoie",
      "ground_truths": [
        "Christopher Savoie"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "45b58998fc8d495d8f0bc8b2755c1b8a",
      "question": "Where is the wife living?",
      "prediction": "Franklin,",
      "ground_truths": [
        "Fukuoka,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5189071c6f2841a390270f836ae93532",
      "question": "Who did Woods play?",
      "prediction": "Tiger",
      "ground_truths": [
        "Adam Scott,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5682e5cacae448f7aeb4d656fa1169aa",
      "question": "Who said he was sorry?",
      "prediction": "Steve Williams",
      "ground_truths": [
        "Steve Williams"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2ef4a73b4534189a675c9106a6fcc91",
      "question": "What started the incedent?",
      "prediction": "sex scandal in which he was embroiled",
      "ground_truths": [
        "racially-tinged remark"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "78bc788e403442bb8ab8024f85ae1619",
      "question": "What did Woods say?",
      "prediction": "\"I apologize for comments I made last night at the Annual Caddy Awards dinner in Shanghai,\"",
      "ground_truths": [
        "\"It was a wrong thing to"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "defe2a4ffe5743fda12c7112beabd0c9",
      "question": "What is Tiger Woods famous for?",
      "prediction": "Golfer",
      "ground_truths": [
        "Golfer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2aeacc63714a45baa4d6e99d076562c7",
      "question": "Where did they meet?",
      "prediction": "at the Lakes Golf Club in Sydney,",
      "ground_truths": [
        "Lakes Golf Club in Sydney,"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "bc43174d52dc40a28f477aa1fdd10893",
      "question": "What was Williams's comment?",
      "prediction": "\"I wanted to shove it up that black a--.\"",
      "ground_truths": [
        "\"I wanted to shove it up that black a--.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c30db74e7ad04470b02788614ef84a19",
      "question": "What does the Governor call for?",
      "prediction": "abdicated his brother to surrender.",
      "ground_truths": [
        "his brother to surrender."
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "39931ad8113f4243bad10af56e0a23f7",
      "question": "How many were killed?",
      "prediction": "at least 18",
      "ground_truths": [
        "at least 18 federal agents and two soldiers have been"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "6246b3d00d3a42af98aa3d31b06e6759",
      "question": "Who was arrested Saturday?",
      "prediction": "10 municipal police officers",
      "ground_truths": [
        "10 municipal police officers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6376ffdc2d5a43059b06993284c0a527",
      "question": "Who were found Tuesday?",
      "prediction": "The officers arrested Saturday are on the police force in the city of Arteaga.",
      "ground_truths": [
        "federal officers' bodies"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "0851a285aa9e4cd190820a5d7bc66eaf",
      "question": "Who was arrested/",
      "prediction": "10 municipal police officers",
      "ground_truths": [
        "10 municipal police officers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9927fb78653545cf81d796c766606c28",
      "question": "Where  did it occur?",
      "prediction": "Mexico",
      "ground_truths": [
        "in southwestern Mexico,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "dc506c99b8ad43b4b8b0ee48037a3dc6",
      "question": "Who shows CNN around her \"Golden City,\" Prague?",
      "prediction": "Petra Nemcova",
      "ground_truths": [
        "Petra Nemcova"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ab0e867881248999050368b8154ddb9",
      "question": "What did the supermodel show CNN around?",
      "prediction": "Prague",
      "ground_truths": [
        "Prague and tells of her love for the \"Golden City,\""
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "7ee54cdbbd32446d838b0e66fbca3303",
      "question": "Who survived the 2004 tsunami?",
      "prediction": "Petra Nemcova",
      "ground_truths": [
        "Petra Nemcova"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03c1dba3b8b24280a68bf48f65005770",
      "question": "What did Nemcova say?",
      "prediction": "her most important work is her charity, the Happy Hearts Fund.",
      "ground_truths": [
        "her most important work is her charity, the Happy Hearts Fund."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2516b55124254e5188c40f9ca4803822",
      "question": "What does Nemcova say about Prague?",
      "prediction": "it's a city of romance, of incredible architecture and history. Some people call it the \"golden city,\" some people say it's the heart of Europe -- although maybe the French don't like that very much! Paris is bigger obviously and a bit more hectic and Prague is smaller and has more of a village-y feeling compared to Paris. Prague has more of a calming vibe. It's not rush-rush-rush.",
      "ground_truths": [
        "is a city of romance, of incredible architecture and history."
      ],
      "em": 0,
      "f1": 0.2608695652173913
    },
    {
      "id": "79169de1c595423a8fe3e1554a5e9339",
      "question": "What is Prague like?",
      "prediction": "is a city of romance, of incredible architecture and history. Some people call it the \"golden city,\" some people say it's the heart of Europe",
      "ground_truths": [
        "city of romance, of incredible architecture and history."
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "a6dbdda734bb43baa65aa91d4323d5a9",
      "question": "Which president spoke out during the campaign?",
      "prediction": "Obama",
      "ground_truths": [
        "Clinton"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "39b27b09528b44b6b53c10b42e0ba36b",
      "question": "who accuses the Obama camp of distorting her remarks on civil rights?",
      "prediction": "Clinton",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e403867966a540de9f7437bf6e78baa2",
      "question": "who speaks out in latest back-and-forth between campaigns?",
      "prediction": "Clinton",
      "ground_truths": [
        "President Clinton"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "19d6ff42a894452a88032c752d5faf90",
      "question": "Whats the BET founder called?",
      "prediction": "Bob Johnson",
      "ground_truths": [
        "Bob Johnson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1fd8126d933f4bbeba9ce4310a7d0573",
      "question": "who founded BET?",
      "prediction": "Bob Johnson",
      "ground_truths": [
        "Bob Johnson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3ed50e9d4fab41f1b18c6c91425fc9bf",
      "question": "Who accused the Obaam camp of distorting her remark?",
      "prediction": "Hillary Clinton",
      "ground_truths": [
        "Hillary Clinton"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d1381a9741714a0ea37173cfffae4471",
      "question": "What was the charge?",
      "prediction": "attempted burglary",
      "ground_truths": [
        "attempted burglary"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "253b5699390142a6a05d5c8fa01ea03e",
      "question": "What was Brancato cleared of?",
      "prediction": "attempted burglary",
      "ground_truths": [
        "Enchautegui's death,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "892800797761464b8fb078e1582e470d",
      "question": "Who played a wannabe mobster?",
      "prediction": "Lillo Brancato Jr.",
      "ground_truths": [
        "Lillo Brancato Jr."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2f3f9d62ed945aea125ef7fcd1b2382",
      "question": "Who sought 15 years?",
      "prediction": "The Bronx County District Attorneys Office",
      "ground_truths": [
        "The Bronx County District Attorneys Office"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a080a3af08b47bf9adda601a84ac0fc",
      "question": "Who was cleared in officer's death?",
      "prediction": "Brancato",
      "ground_truths": [
        "Brancato"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2071b336b9de4a18a892086a553afd50",
      "question": "What was the allegation?",
      "prediction": "shooting and sentenced to life in prison",
      "ground_truths": [
        "attempted burglary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a5b5d4f85c274be49ad11c9d258a3a99",
      "question": "Who sought 15 years for Lillo Brancato Jr. ?",
      "prediction": "The Bronx County District Attorneys Office",
      "ground_truths": [
        "Bronx County District Attorneys Office"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cbf8dbab5324461592f31283f4beb63c",
      "question": "Did the court hear the cases?",
      "prediction": "refuse to",
      "ground_truths": [
        "would refuse to even consider them."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c432211d434a4182823641e78e5cbadf",
      "question": "who were the women accusing?",
      "prediction": "Herman Cain,",
      "ground_truths": [
        "Herman Cain,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d248197351dc49c69e5a91d0820211bb",
      "question": "what claims are often settled out of court?",
      "prediction": "sexual harassment",
      "ground_truths": [
        "sexual harassment"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0a60524d20f34373bc8dd3389afaab6d",
      "question": "who is accusing Cain?",
      "prediction": "Gloria Allred,",
      "ground_truths": [
        "Sharon Bialek"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1dc2fde787694953a64b5c5b60b4fd7c",
      "question": "What did Harrison die of?",
      "prediction": "cancer",
      "ground_truths": [
        "cancer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0eebb69418524a59813c284930197bc9",
      "question": "What did Harrison die from?",
      "prediction": "cancer",
      "ground_truths": [
        "cancer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4bbfb9a271d84fe688e97aeae21286c0",
      "question": "What was Harrison's star next to?",
      "prediction": "the iconic Hollywood headquarters of Capitol Records,",
      "ground_truths": [
        "the iconic Hollywood headquarters of Capitol Records,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a1ad33c195e24a21bb21cd62dd90083e",
      "question": "Where is Harrison's star?",
      "prediction": "on the Walk of Fame.",
      "ground_truths": [
        "Walk of Fame."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "e53e39ed53ca4ffb9cc6159bb9771a8b",
      "question": "Where is Capitol Records?",
      "prediction": "Hollywood",
      "ground_truths": [
        "Hollywood"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "abd3dc653b6c4e1184e70f270296d79f",
      "question": "What did Harrison pen?",
      "prediction": "his first solo greatest hits collection",
      "ground_truths": [
        "\"Let it Roll:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da45498817ed4a53b4a18ee852c43ba5",
      "question": "what country was the quake",
      "prediction": "Haiti",
      "ground_truths": [
        "Haiti"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e03180d675aa4e1eb05fb44121ffa00f",
      "question": "Where was the earthquake?",
      "prediction": "Port-au-Prince",
      "ground_truths": [
        "Port-au-Prince harbor"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "434191151c744073952ffe063744b8c5",
      "question": "what were the damages",
      "prediction": "7.0-magnitude earthquake",
      "ground_truths": [
        "a quarter-mile pier crumbling into the sea along with two of his trucks."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03292748a1724185affa6b5fb0e7624c",
      "question": "What happens without the port?",
      "prediction": "become unloaded quickly.",
      "ground_truths": [
        "serious consequences for Haiti,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d84a1972d944779becf26d14afdcce2",
      "question": "what city was damaged",
      "prediction": "Port-au-Prince",
      "ground_truths": [
        "Port-au-Prince"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cfc8297fe16243e68fd7df070d7bd587",
      "question": "what does bill haas claimed?",
      "prediction": "one-shot victory in the Bob Hope Classic",
      "ground_truths": [
        "a one-shot victory in the Bob Hope Classic"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e326f32613a417fb55b23337862c62b",
      "question": "who completed victory?",
      "prediction": "Bill Haas",
      "ground_truths": [
        "Bill Haas"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8c780963994f44c7835580cb22bc2fdc",
      "question": "American Bill Haas has claimed what?",
      "prediction": "Bob Hope Classic",
      "ground_truths": [
        "a one-shot victory in the Bob Hope Classic on the final hole"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "16d87b6d44b74c9d9c705b0471dcb4b2",
      "question": "who finished tie for second place?",
      "prediction": "Tim Clark, Matt Kuchar and Bubba Watson",
      "ground_truths": [
        "Tim Clark, Matt Kuchar and Bubba Watson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c24d3c06799444db84d703cd715d2979",
      "question": "Haas completed his victory with what?",
      "prediction": "a birdie four at the last hole",
      "ground_truths": [
        "one-shot"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38ec897108f245b19aeba0e93e304360",
      "question": "Who all finished tied for second?",
      "prediction": "Tim Clark, Matt Kuchar and Bubba Watson",
      "ground_truths": [
        "Tim Clark, Matt Kuchar and Bubba Watson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95ba4e0a74d24753bcb222ddfbfc9c32",
      "question": "What does the directive pertain to?",
      "prediction": "raids of Afghan homes and compounds,",
      "ground_truths": [
        "coalition forces in Afghanistan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ff1d1c601834b59be0b1500e3cf8887",
      "question": "Where were the raids?",
      "prediction": "Afghan homes",
      "ground_truths": [
        "Afghan homes and compounds,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2955002485c14b53a2044b2d8638b2fb",
      "question": "What did the directive pertain to?",
      "prediction": "restrictions on nighttime raids of Afghan homes and compounds,",
      "ground_truths": [
        "nighttime raids of Afghan homes and compounds,"
      ],
      "em": 0,
      "f1": 0.8750000000000001
    },
    {
      "id": "28a25e17eb8149e3bbd7f11b62f978b2",
      "question": "Who was trying to lower tension?",
      "prediction": "the coalition",
      "ground_truths": [
        "the coalition"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "918682dda07b40c2b5435339f4948ab9",
      "question": "What did the new rules call for?",
      "prediction": "restrictions on nighttime raids of Afghan homes and compounds,",
      "ground_truths": [
        "restrictions"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "7bb51f59808149d2ba80ba54073183dd",
      "question": "Who apologized to his teammates and fans?",
      "prediction": "Ben Roethlisberger",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b8f339350064021b229ab45ee46f007",
      "question": "Who won't appeal the six-game suspension?",
      "prediction": "Ben Roethlisberger",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8cb684a8be9a41dc9fcb35b383078039",
      "question": "Who is not pursuing charges, saying \"we do not prosecute morals.\"",
      "prediction": "Fred Bright,",
      "ground_truths": [
        "Fred Bright,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "76d462d4bd4748cc9eb14ebfeabe71e9",
      "question": "What is the Pittsburgh Steelers quarterback being accused of?",
      "prediction": "rape",
      "ground_truths": [
        "raping"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a17b9869bbd14bf2975af4343af6cdaa",
      "question": "Who is the Pittsburgh Steelers quarterback?",
      "prediction": "Ben Roethlisberger",
      "ground_truths": [
        "Ben Roethlisberger"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5042953c54d4fd4b792e423cb3f8a96",
      "question": "What does the former shortstop have lined up?",
      "prediction": "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"",
      "ground_truths": [
        "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\""
      ],
      "em": 0,
      "f1": 0.8750000000000001
    },
    {
      "id": "83641374d7ea41caaf1eee54213989b9",
      "question": "What does Ripken do now?",
      "prediction": "businessman, team owner, radio-show host and author.",
      "ground_truths": [
        "is a businessman, team owner, radio-show host and author."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "cc2e232f0d624c46a0c1af0788838811",
      "question": "What is Ripkin's brother's name?",
      "prediction": "Bill,",
      "ground_truths": [
        "Bill,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c580289b1a2d49aa86268e2b1aa1090f",
      "question": "When do President obama sign the Bill?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eb9d02063e8a42ca8b34b7d82d4c4189",
      "question": "who signed the law?",
      "prediction": "Members of the military",
      "ground_truths": [
        "President Obama."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d68b266ae434abb9d73cd2def6f8201",
      "question": "what is going to study the TSA?",
      "prediction": "ways to speed up screening of service members",
      "ground_truths": [
        "ways to speed up screening of service members and, to the extent possible, their families,"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "f2512611b47d4f24bf38225a97ff0bc6",
      "question": "What the new law required?",
      "prediction": "the Transportation Security Administration to study ways to speed up screening of service members and, to the extent possible, their families, when the service members are in uniform and traveling on orders.",
      "ground_truths": [
        "the Transportation Security Administration to study ways to speed up screening of service members and,"
      ],
      "em": 0,
      "f1": 0.6511627906976745
    },
    {
      "id": "6d5eb886bc37490aa1854938f9b36903",
      "question": "From where did the miliitants cross?",
      "prediction": "Afghanistan,",
      "ground_truths": [
        "Afghanistan,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce3ef968a0704a3fa36f7f605ed0031a",
      "question": "Who many soldiers were killed?",
      "prediction": "six",
      "ground_truths": [
        "40 militants and six Pakistan"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "a2643b6d7fe94e54bb74da276f0a4646",
      "question": "how many militants are?",
      "prediction": "Hundreds",
      "ground_truths": [
        "Hundreds"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "755056d3d07940929387e2e7e76f4acf",
      "question": "What happened?",
      "prediction": "Hundreds of militants, believed to be foreign fighters, launched attacks",
      "ground_truths": [
        "Hundreds of militants, believed to be foreign fighters, launched attacks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "96478949a8214ae4bd74130d64466203",
      "question": "who crossed into Pakistan From Afghanistan?",
      "prediction": "militants,",
      "ground_truths": [
        "the fighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "006c10a191a846239c8314068e065ec3",
      "question": "How many killed into fighting?",
      "prediction": "40 militants",
      "ground_truths": [
        "40 militants and six Pakistan soldiers"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "121d9ead9e87477baedf4605be92bbf7",
      "question": "Where did the attacks occur?",
      "prediction": "various military check posts in Pakistan's border with Afghanistan",
      "ground_truths": [
        "Pakistan's border with Afghanistan"
      ],
      "em": 0,
      "f1": 0.6153846153846153
    },
    {
      "id": "4cbd0481ba0f49cba4e5d1d6ef2d082f",
      "question": "What is to blame for the suicides?",
      "prediction": "a \"stressed and tired force\"",
      "ground_truths": [
        "Long troop deployments"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "68efecfb6ac145c08a2be17c94889324",
      "question": "What does the Admiral say about suicides?",
      "prediction": "are the third leading cause of death in the Navy.",
      "ground_truths": [
        "\"stressed and tired force\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50ddaaafdf0249f7afea9f2690b5a905",
      "question": "What are a third leading cause of death?",
      "prediction": "suicides are the",
      "ground_truths": [
        "suicides"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b6096cea90274f34908139e77db8caac",
      "question": "What is the third leading cause of death in the Navy?",
      "prediction": "suicides",
      "ground_truths": [
        "suicides"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "52c9cb22b1f84a37b49234eb7862ce03",
      "question": "Who said we must find ways to relieve stress?",
      "prediction": "Gen. Peter W. Chiarelli,",
      "ground_truths": [
        "Gen. Peter W. Chiarelli,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dac3c581c100493a93baccc6a59a7959",
      "question": "What is cited as part of the problem?",
      "prediction": "multiple deployments,",
      "ground_truths": [
        "Long troop deployments in Iraq,"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "f5f68ca83de14fa8a5663c76fa27eeb1",
      "question": "What was said to contribute to the problem?",
      "prediction": "multiple deployments,",
      "ground_truths": [
        "seeking help"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ddc318071f01453ab87ceebe18062971",
      "question": "Who is Harbhajan Singh?",
      "prediction": "Off-spinner",
      "ground_truths": [
        "Off-spinner"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "78112e7810214d80b8fab5fa51c61200",
      "question": "What team is in the lead?",
      "prediction": "Sri Lanka",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebf5e6a8dd1742b68312c2c6ada5aad0",
      "question": "Against which team did he play?",
      "prediction": "India",
      "ground_truths": [
        "India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cae1c72a196e41c0919e17bc003a3a5f",
      "question": "Who scored the best?",
      "prediction": "Tillakaratne Dilshan",
      "ground_truths": [
        "Tillakaratne Dilshan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d62824da543e474da60ae07308009055",
      "question": "What countries where playing?",
      "prediction": "India",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "455324ce37fa42c8af627f9f832cfb24",
      "question": "What is happened in Sri Lanka?",
      "prediction": "a fine start to the third match of their series against India",
      "ground_truths": [
        "Tillakaratne Dilshan scored his sixth Test century"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c6d27c29d4774501b4f046af3bb83dfd",
      "question": "Where did they play?",
      "prediction": "Mumbai",
      "ground_truths": [
        "Mumbai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "69e712f057644faf86a9688a85a2deee",
      "question": "For which country did Dilshan scored ?",
      "prediction": "Sri Lanka",
      "ground_truths": [
        "Sri Lanka"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d156cb80d58d45778ef7cf9fa0522bee",
      "question": "Who is expected to make a full recovery?",
      "prediction": "Kerstin",
      "ground_truths": [
        "Kerstin"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f711b14dc8b341deaa8c3bfd6302219a",
      "question": "Who spent her entire life in a cellar?",
      "prediction": "Kerstin Fritzl,",
      "ground_truths": [
        "Kerstin Fritzl,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b5a9875a58c46deb2bf7ef981366e70",
      "question": "Where was she taken?",
      "prediction": "a hospital in Amstetten,",
      "ground_truths": [
        "a hospital"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a7c65f086477430ea2d5692f28290f18",
      "question": "who is kerstin fritzl",
      "prediction": "the oldest daughter of an incestuous relationship",
      "ground_truths": [
        "19-year-old woman whose hospitalization exposed a shocking Austrian incest case"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb946aafe087450ea5405c800ddd069d",
      "question": "who is josef fritzl",
      "prediction": "Elisabeth's father,",
      "ground_truths": [
        "Elisabeth's father,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dadebe93fbd14ac3bbb95f4dcd0a5427",
      "question": "Who has confessed?",
      "prediction": "Josef Fritzl,",
      "ground_truths": [
        "Josef Fritzl,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e35535336794e239e8fd7be704a930f",
      "question": "where was the dungeon",
      "prediction": "Beaverville,",
      "ground_truths": [
        "in the cellar"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c2d72fa301114b31b83223143318edf2",
      "question": "Who is reunited with her family?",
      "prediction": "Kerstin Fritzl,",
      "ground_truths": [
        "Kerstin Fritzl,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f892ceb5a8ff484b87750d97f6a28809",
      "question": "What is her age?",
      "prediction": "19-year-old",
      "ground_truths": [
        "19-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "92149fe0d2d841c99595358e6fb2d914",
      "question": "What is Olivia Newton-John's profession?",
      "prediction": "actress,",
      "ground_truths": [
        "actress,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f198fe791ff44a0f8ec3ba8e26f7404a",
      "question": "What musical did she star in?",
      "prediction": "\"Grease\".",
      "ground_truths": [
        "\"Grease\"."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7df0dd3409f4db0bd2d1ec8bdae1165",
      "question": "Who is the hit star of \"Grease\"?",
      "prediction": "Olivia Newton-John",
      "ground_truths": [
        "Olivia Newton-John"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bb8d260acb134ba1b5d63ec07b2d6029",
      "question": "What campaign does this actress promote?",
      "prediction": "early detection and helping other women cope with the disease.",
      "ground_truths": [
        "education about rainforests."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5f6b232704774af9bc55a6c9740fa718",
      "question": "What is she passionate about?",
      "prediction": "early detection and helping other women cope with the disease.",
      "ground_truths": [
        "early detection and helping other women cope with the disease."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0e941c4d2c7a425dba59fbd1dfc48813",
      "question": "What does Olivia Newton-John advocate?",
      "prediction": "early detection and helping other women cope with the disease.",
      "ground_truths": [
        "early detection"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "fd89ef9e375742d8bd0ef102e999b8f8",
      "question": "Who did Newton-John speak to?",
      "prediction": "CNN's Max Foster,",
      "ground_truths": [
        "CNN's Max Foster,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b17e011932344d83a7ac20554d83877a",
      "question": "Who interviewed Olivia Newton-John?",
      "prediction": "Max Foster,",
      "ground_truths": [
        "CNN's Max Foster,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "af446346d828479ca02c375ec2973773",
      "question": "Who were aboard?",
      "prediction": "A witness said one of the trains rammed into the back of a stationary train about 2 a.m. at the resort's Ticket and Transport Center.",
      "ground_truths": [
        "eight people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef7ff14fda8347479995319185ade05f",
      "question": "Who says one monorail train rammed?",
      "prediction": "A witness",
      "ground_truths": [
        "A witness"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0d5bc4fc6f0c425b98565b8f80bb6e43",
      "question": "What is the name of the driver?",
      "prediction": "Austin Wuennenberg,",
      "ground_truths": [
        "Austin Wuennenberg,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "986fec610e0e4bf9bb3de6be1fa0356e",
      "question": "How many people were on board?",
      "prediction": "eight",
      "ground_truths": [
        "eight"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d18ef7feb7c345b589599ab84b02d3df",
      "question": "Where did the crash occur?",
      "prediction": "at Walt Disney World",
      "ground_truths": [
        "Disney World"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1b9decbc39c949e8bf28c759dfe4baa2",
      "question": "Who was identified as Austin Wuennenberg?",
      "prediction": "a senior at Stetson University studying computer science.",
      "ground_truths": [
        "the driver"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "08b635995bc64aefba8d3fdb405e1b4b",
      "question": "What age was the  driver?",
      "prediction": "21-year-old",
      "ground_truths": [
        "21-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7dd1e96f50b44d3fbe98d02cb6856adc",
      "question": "Who is called a cavewoman?",
      "prediction": "The Tinkler.",
      "ground_truths": [
        "The Tinkler"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f889e3e683014dd686734d48bbbb1235",
      "question": "Where was eclectic assortment of alternative-energy vehicles seen this week?",
      "prediction": "Capitol Hill.",
      "ground_truths": [
        "Capitol Hill,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "896fd94d638040e488decb12656d6a66",
      "question": "Did the display attract anyone?",
      "prediction": "attracted some U.S. senators",
      "ground_truths": [
        "some U.S. senators"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "1fbe4ecf8907445c896a9af395235274",
      "question": "who was the event organized by",
      "prediction": "Bright Automotive,",
      "ground_truths": [
        "Bright Automotive,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6dd0ca13b6304a9c9bc6db4983f8b06b",
      "question": "who took the cars for a spin",
      "prediction": "Sen. Thomas Carper,",
      "ground_truths": [
        "U.S. senators"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b96caff10a14ab8863236af774fa135",
      "question": "What is happening on Capitol Hill this week?",
      "prediction": "alternative-energy vehicles",
      "ground_truths": [
        "\"The Energy and Environmental Showcase,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cb8d2b66f60041de88554c497fc8f67f",
      "question": "Who organized the event?",
      "prediction": "Bright Automotive,",
      "ground_truths": [
        "Bright Automotive,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "06657adf0df043b8a5e47a0d276ff377",
      "question": "Who  couldn't resist taking the cars for a spin?",
      "prediction": "Sen. Thomas Carper,",
      "ground_truths": [
        "U.S. senators"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "954351e660e347e29070d3f06766e253",
      "question": "What brand did the owners rave about?",
      "prediction": "Saturn",
      "ground_truths": [
        "Saturn"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c87043201d84e269c6f2019f9a5794f",
      "question": "What parade is mentioned?",
      "prediction": "Christmas",
      "ground_truths": [
        "Sunday's Christmas"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c9ccb7f8f9784e9f8205e5510c340b2e",
      "question": "How many owners of Sky roadsters drive in the Christmas parade?",
      "prediction": "Koonce",
      "ground_truths": [
        "2,500"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2666396df440428abe681afcaf00c173",
      "question": "Who makes Sky roadsters?",
      "prediction": "Saturn",
      "ground_truths": [
        "Saturn"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "874e8e95e0b6405bb51e97f09a2551bc",
      "question": "What brand is vulnerable?",
      "prediction": "Saturn,",
      "ground_truths": [
        "Saturn"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7115423510b54a3ab131a64a9b78d8e1",
      "question": "Where did Stephen Johns work?",
      "prediction": "Washington, D.C.,",
      "ground_truths": [
        "U.S. Holocaust Memorial Museum,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac5ab6db315d4483a9c584fe0b1d6c15",
      "question": "Who is accused of killing him?",
      "prediction": "88-year-old white supremacist",
      "ground_truths": [
        "88-year-old white supremacist"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0bc085b22b8f46a7aa0e0117495a60cd",
      "question": "What did the Gentle Giant do?",
      "prediction": "opened the door for the man police say was his killer.",
      "ground_truths": [
        "opening the door for the man who shot him,"
      ],
      "em": 0,
      "f1": 0.375
    },
    {
      "id": "06d07db2a1554ed0923e70dbb93c9880",
      "question": "What enabled  his killer to get into the museum?",
      "prediction": "security officer Stephen Johns reportedly opened the door for the man police say was his",
      "ground_truths": [
        "Security officer Stephen Johns reportedly opened the door for the man police say was"
      ],
      "em": 0,
      "f1": 0.9600000000000001
    },
    {
      "id": "49d1aeccefc748c09d6350910ec681cc",
      "question": "What were his interests?",
      "prediction": "travel,",
      "ground_truths": [
        "Washington Redskins fan and loved to travel,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "698a7fabf429487f95c64776a7da5bd1",
      "question": "What did the security guard love?",
      "prediction": "Washington Redskins fan",
      "ground_truths": [
        "loved to travel,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0353ec5692504be7803b7868ad8878a4",
      "question": "Who is the \"gentle giant\"?",
      "prediction": "Johns,",
      "ground_truths": [
        "Stephen Tyrone Johns"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "380e41515ca8462f8f87a2721989f282",
      "question": "What was the security guard's age?",
      "prediction": "39,",
      "ground_truths": [
        "39,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f80e02b065344e0b8b45656370f35395",
      "question": "how many countrys affect the virus?",
      "prediction": "11",
      "ground_truths": [
        "11"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f4e61efd586744d9941da1aea777b7d7",
      "question": "How many counties encountered the virus?",
      "prediction": "11",
      "ground_truths": [
        "11 countries,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e3602ad5a2864891955a696e59594e76",
      "question": "Where was the largest outbreak?",
      "prediction": "Mexico",
      "ground_truths": [
        "Mexico"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c9a9b912b07b4c91a98f4134fbc3c6f5",
      "question": "in what country was largest outbreak?",
      "prediction": "Mexico",
      "ground_truths": [
        "Mexico"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e20dba2bc3be4f5e9829da374bbd0164",
      "question": "What is mcDonald's going to do?",
      "prediction": "open a restaurant in the prestigious museum",
      "ground_truths": [
        "serving its fast burgers in the Carrousel du Louvre,"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "68a598230ecc4f168727f19bde806e07",
      "question": "What has been attracting people in the museum?",
      "prediction": "McDonald's",
      "ground_truths": [
        "restaurants and boutiques,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "02650d3813a44872ad30447adccc3f0a",
      "question": "Where is McDonald's going to open a new store at?",
      "prediction": "Loul.",
      "ground_truths": [
        "the Louvre."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9952bc66f13406c9d502751699d4aef",
      "question": "Where is the restuarant not the only fast food joint?",
      "prediction": "in the Louvre.",
      "ground_truths": [
        "Carrousel du Louvre,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "66b1143618d34663a5151d1dba850890",
      "question": "Who is opening up in the mall?",
      "prediction": "McDonald's",
      "ground_truths": [
        "McDonald's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41411cf3a7db452ea9f4059bf01969c0",
      "question": "Where is McDonald's to open a restaurant?",
      "prediction": "Carrousel du Louvre,",
      "ground_truths": [
        "--the Louvre."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5f92118f88a84ea1805aa655a77e0757",
      "question": "Who did the crash kill?",
      "prediction": "Princess Diana",
      "ground_truths": [
        "Princess Diana"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "29790bf111e64372aaccd31af02cba74",
      "question": "Who employed Rees, the bodyguard?",
      "prediction": "Dodi Fayed's father, Mohamed al Fayed,",
      "ground_truths": [
        "Princess Diana"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "667d1d15e8634cfaa77c52ab1b9f5377",
      "question": "Who is Dodi Fayed?",
      "prediction": "the boyfriend,",
      "ground_truths": [
        "her boyfriend,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2d46318c6e274504bbccc0a69725807e",
      "question": "Who died in the accident with princess Diana",
      "prediction": "Treasure Rees",
      "ground_truths": [
        "her boyfriend, Dodi Fayed, and their driver, Henri Paul."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "27cbadd4e283464483af698e10bb3d6c",
      "question": "After the crash who received threatening calls and letters?",
      "prediction": "Rees",
      "ground_truths": [
        "Trevor Rees,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "bb25fe6199404b3ab0e45da51e453da3",
      "question": "Who died in the accident?",
      "prediction": "Princess Diana",
      "ground_truths": [
        "Diana,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c629896db95e4b6a8fd3bc475603b485",
      "question": "Who was Rees employed by?",
      "prediction": "al Fayed's security team",
      "ground_truths": [
        "working for al Fayed's security team"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "328f361c3c2445f7bf38a8c3c6adf490",
      "question": "Who released a list of best and worst used cars?",
      "prediction": "Consumer Reports magazine",
      "ground_truths": [
        "Consumer Reports magazine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2de95c207ed54376a49d51d5d65f5489",
      "question": "Where do some buyers go to get a luxury car?",
      "prediction": "used-luxury market",
      "ground_truths": [
        "used-luxury market"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "82e6628da7fb4bf6acb8ff7b4faa635f",
      "question": "What is a reason some buyers go to used-car markets?",
      "prediction": "need,",
      "ground_truths": [
        "a new model is simply out of their reach."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "452c6099ee4a4cea83295935ac292139",
      "question": "Where were best and worst used cars listed?",
      "prediction": "Consumer Reports magazine",
      "ground_truths": [
        "Consumer Reports magazine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f2dd91bd67c247dcbb01198427f8878a",
      "question": "What magazine is discussed?",
      "prediction": "Consumer Reports",
      "ground_truths": [
        "Consumer Reports"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bc4a407717f431aafc52dd9191baf1a",
      "question": "What car made the list of cars under 30k?",
      "prediction": "Acura MDX",
      "ground_truths": [
        "2005 & 2006 Acura MDX"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "eaaea7421086462889eb8afd1484b01b",
      "question": "What are the names of Jackson's sisters?",
      "prediction": "Janet and La Toya,",
      "ground_truths": [
        "Janet and La Toya,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f3de28833bb54a6d993201db9cee19aa",
      "question": "Which item did the doctors say was safe in the home?",
      "prediction": "surgical anesthetic propofol",
      "ground_truths": [
        "the surgical anesthetic propofol"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a9257f4722bf4248ae7f2bdcd0fd3079",
      "question": "what did jackson tell the nurse",
      "prediction": "\"You build the house and I will put on the door and paint it,\"",
      "ground_truths": [
        "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9501472c0039417f9f574fe8656627ea",
      "question": "Who attended court?",
      "prediction": "Randy Jackson",
      "ground_truths": [
        "Jackson's mother, Katherine, sisters Janet and La Toya, and brother Randy Jackson"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "a242910379e44bcb85f26a05fa459d87",
      "question": "what would jackson have had to pay",
      "prediction": "for all of the production and rehearsal costs,",
      "ground_truths": [
        "all of the production and rehearsal costs,"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "d70fb3550f2d4140b3b8531591ca46f0",
      "question": "What costs would Jackson have incurred if the tour was cancelled?",
      "prediction": "$40 million,",
      "ground_truths": [
        "production and rehearsal"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a2b8b0d02074b15bc4b3cd8d9d28750",
      "question": "who attended court on tuesday",
      "prediction": "Jackson",
      "ground_truths": [
        "Jackson's mother, Katherine, sisters Janet"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962c66a71546468d8f6a0ece3c9a4b91",
      "question": "Who assured Jackson that propofol was safe?",
      "prediction": "Dr. Murray",
      "ground_truths": [
        "doctors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c75b242082a488dbbf0c6feb4739e69",
      "question": "what did the police do",
      "prediction": "tried to cover it up,\"",
      "ground_truths": [
        "covered up the murder of a British teenage girl in Goa"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "fbe718cccc7642fe807edbe160f1eb25",
      "question": "What was the girl's name?",
      "prediction": "Scarlett Keeling",
      "ground_truths": [
        "Scarlett Keeling"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "310e44374a0c4af285ee82e916f46590",
      "question": "Who covered up the murder of a UK girl?",
      "prediction": "Police",
      "ground_truths": [
        "Police"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "909be8a03d3640099f1f5094b47d9aa9",
      "question": "Where was the murder covered up?",
      "prediction": "Goa",
      "ground_truths": [
        "Goa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bde94c1ca900410fb83728d7a7a81621",
      "question": "Where did the killer find the UK girl?",
      "prediction": "Goa",
      "ground_truths": [
        "Goa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "33729b7290694eb380e57b5083f50f26",
      "question": "What age was the murdered girl?",
      "prediction": "15-year-old's",
      "ground_truths": [
        "15-year-old's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "757bb36f5eec45e3ae20ae5d5b9800c4",
      "question": "what did the mother say",
      "prediction": "she also believed police were trying to cover up the truth behind her daughter's murder, and that they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case.",
      "ground_truths": [
        "said she also believed police were trying to cover up the truth behind her daughter's murder,"
      ],
      "em": 0,
      "f1": 0.5833333333333333
    },
    {
      "id": "3db7c3d6835546f38741ee9abb96dc77",
      "question": "What age was the girl?",
      "prediction": "15-year-old's",
      "ground_truths": [
        "15-year-old's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3067ebacbf8e45f693b0cc93838def01",
      "question": "What are the mother's reasons?",
      "prediction": "she believed police were trying to cover up the truth behind her daughter's murder, and that they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case.",
      "ground_truths": [
        "police were trying to cover up the truth"
      ],
      "em": 0,
      "f1": 0.358974358974359
    },
    {
      "id": "f4583e6b781d42aca1bbc9360932b5b5",
      "question": "Who thinks police arrested wrong man?",
      "prediction": "Fiona MacKeown",
      "ground_truths": [
        "Fiona MacKeown"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "072051235c1f43a1b0c1d37fb84beef3",
      "question": "what did the police cover up",
      "prediction": "murder of a British teenage girl in Goa",
      "ground_truths": [
        "the murder of a British teenage girl in Goa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "902802036d9346bba8f3742fa55bce01",
      "question": "What was the man's name?",
      "prediction": "Samson D'Souza,",
      "ground_truths": [
        "Samson D'Souza,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1554e7b389d54ff7904f0bf2d82e269c",
      "question": "What team set a record?",
      "prediction": "Bayern Munich",
      "ground_truths": [
        "Munich"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b20e7a05673a4b3c8ee29beaba23f73a",
      "question": "Who did Bayern Munich defeat?",
      "prediction": "Sporting Lisbon",
      "ground_truths": [
        "Sporting Lisbon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "805a6afa477c45d18a26ebba495ac1b7",
      "question": "What was the score in the match between Bayern and Portugal?",
      "prediction": "5-0",
      "ground_truths": [
        "12-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8da9a487491d4861815c860882b46f1b",
      "question": "Who did Bayern defeat on the first leg?",
      "prediction": "Sporting Lisbon",
      "ground_truths": [
        "Sporting Lisbon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "08aee7c44c404089b766cbe0fa2d55f5",
      "question": "In what year did Lyon defeat Werder Bremen?",
      "prediction": "2005.",
      "ground_truths": [
        "2005."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3a33ecb066b9443e85b68c446f45b287",
      "question": "Who defeated Sporting Lisbon?",
      "prediction": "Bayern Munich",
      "ground_truths": [
        "Munich"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "199178bd55f849279f0e163c691d0903",
      "question": "What was the final score of the Munich/Lisbon game?",
      "prediction": "12-1",
      "ground_truths": [
        "12-1"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ccb81b6865484234ab334d34412a6b35",
      "question": "What was the score of the Bayern match?",
      "prediction": "12-1",
      "ground_truths": [
        "12-1"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f70de0169eb42c3a4b40cd1e5f1be61",
      "question": "What country's unemployment rate is highest in Europe?",
      "prediction": "Spain",
      "ground_truths": [
        "Latvia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a7fafb4013664c78a931293de688bedb",
      "question": "By what percentage has Spanish unemployment grown over the previous quarter?",
      "prediction": "20 percent,",
      "ground_truths": [
        "19 percent"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "10e96facf13e4091ba5c8cee71226ff0",
      "question": "What country has the highest unemployment rate in Europe?",
      "prediction": "Spain",
      "ground_truths": [
        "Latvia,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "738dc2a651c7477abe2efc0e5547054a",
      "question": "How many people are out of work in Spain?",
      "prediction": "4.6 million",
      "ground_truths": [
        "4.6 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2cdea969eb1441afb20bee7330453eac",
      "question": "what are the investments",
      "prediction": "Four hundred and sixty one million square feet",
      "ground_truths": [
        "$3 billion,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3de977d9d8c142c79231c332dad7cfdf",
      "question": "where Work will start?",
      "prediction": "Kurdistan Gas City",
      "ground_truths": [
        "on the project,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a018a8b09d14b4cb164c0abfd1594bb",
      "question": "What wil Kurdistan Gas City will include?",
      "prediction": "industrial, residential and commercial buildings",
      "ground_truths": [
        "industrial, residential and commercial buildings"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "489edeed2f20435b9fc8ab429d160239",
      "question": "The project will start on what date?",
      "prediction": "September 21.",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6bf1ecc9589d4447b31afde347505919",
      "question": "when will work begin",
      "prediction": "September 21.",
      "ground_truths": [
        "September 21."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bf26f3035ec45e5874eb0de76a8d903",
      "question": "Significant investments are being made in Kurdistan by whom?",
      "prediction": "Two United Arab Emirates",
      "ground_truths": [
        "Two United Arab Emirates based companies"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "62ccc3554230408fb85b346dedaeffa9",
      "question": "for how much is the investment?",
      "prediction": "$3 billion,",
      "ground_truths": [
        "$3 billion,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bccab12e798e428b8869c19eb4feac2c",
      "question": "what does this affect?",
      "prediction": "Iraq's autonomous region of Kurdistan.",
      "ground_truths": [
        "Iraqi economy.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bb2c93be295348eb933e57a16325af76",
      "question": "what Kurdistan Gas City will include?",
      "prediction": "industrial, residential and commercial buildings",
      "ground_truths": [
        "industrial, residential and commercial buildings"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21c894485b7a4cb0b66313dd1802a3c2",
      "question": "what is the investment?",
      "prediction": "$3 billion,",
      "ground_truths": [
        "$40 billion during"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "93170fdc966948419d9801fc0b63e513",
      "question": "who is amitabh bachman?",
      "prediction": "Bollywood superstar",
      "ground_truths": [
        "Bollywood superstar"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2811395b18944f3ca0b3ab89d737581a",
      "question": "How many times has an oscar winner starred in a bollywood film?",
      "prediction": "first",
      "ground_truths": [
        "the first"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "38d0c3d6862d423d86168e554aa1e80c",
      "question": "Is this high profile?",
      "prediction": "most",
      "ground_truths": [
        "In the most high-profile"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "b4773dd0dc5d4e45bf36d736162c184f",
      "question": "Who does Ben Kingsley star with?",
      "prediction": "Amitabh Bachchan",
      "ground_truths": [
        "Amitabh Bachchan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3183fdced356412498a9ad782f2e79d3",
      "question": "What is Kingsley's first return to Indian filmaking after Gandhi?",
      "prediction": "Academy Award-winner",
      "ground_truths": [
        "\"Teen Patti\" (\"Card Game\")"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e9c90ac121b476f95a1d297bbf5393d",
      "question": "which of the companies is targeted",
      "prediction": "Stratfor,",
      "ground_truths": [
        "Stratfor,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cbf033bb39e4bd4aa343ba4419ffd6a",
      "question": "Which company is targeted?",
      "prediction": "Stratfor",
      "ground_truths": [
        "Stratfor,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7368b08b0dc04144b2d606ab0788ed32",
      "question": "What do the hackers have access to?",
      "prediction": "credit card information",
      "ground_truths": [
        "information on 4,000 credit cards and the company's \"private client\" list,"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "da396eb93e554ffb8ce05e3f21581409",
      "question": "Who was involved",
      "prediction": "Stratfor",
      "ground_truths": [
        "activist hacking group Anonymous."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ca77c1bdd464d58ae82f2ad3a689436",
      "question": "Which hacking group may have been involved?",
      "prediction": "Anonymous.",
      "ground_truths": [
        "Anonymous."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5a34379c3fa84c2c8fe02de997c17f78",
      "question": "Where was targeted",
      "prediction": "Stratfor,",
      "ground_truths": [
        "Stratfor's website"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4ce8a055d1d6472ca8edf2b84eb75e7d",
      "question": "what was the information on the client list about",
      "prediction": "4,000 credit cards",
      "ground_truths": [
        "credit card"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "182724fabd2f4119ba9f5fcab23d4001",
      "question": "What was released",
      "prediction": "the breach and apparent",
      "ground_truths": [
        "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "febb62323b274b678dc47bb096688f4c",
      "question": "Who followed the bombers?",
      "prediction": "NATO fighters",
      "ground_truths": [
        "NATO fighters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb0a6c5287fc41d08464e736a987e53b",
      "question": "What did Interfax report?",
      "prediction": "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"",
      "ground_truths": [
        "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ce3ecd44941f4597ae927b61841b860a",
      "question": "Who did the Venezuelan president say he'd welcome?",
      "prediction": "the Russian air force,",
      "ground_truths": [
        "Russian air force,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "595d19e84b3d4ab181e545880eeb4aef",
      "question": "Who is the president of Venezuela?",
      "prediction": "Hugo Chavez",
      "ground_truths": [
        "Hugo Chavez"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21976ca866684313b86aa9450c61406c",
      "question": "Who would welcome the Russian air force?",
      "prediction": "Venezuela",
      "ground_truths": [
        "Venezuela"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c992708e6d004bedb81d57b076e33286",
      "question": "Who will use airfield for training?",
      "prediction": "Two",
      "ground_truths": [
        "Russian bombers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7baaa5545ac44114b79c5420d561d85e",
      "question": "Who did NATO fighters follow?",
      "prediction": "Bolouchers",
      "ground_truths": [
        "the bombers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962454a456cc4bbb9a8d5bfba6abf79c",
      "question": "What highway did the bus overturn at?",
      "prediction": "U.S. 93",
      "ground_truths": [
        "U.S. 93"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cdec7a0d975642d38b3b2d449bffc3e8",
      "question": "Where were many of the victims being flown?",
      "prediction": "Kingman Regional Medical Center,",
      "ground_truths": [
        "Kingman Regional Medical Center,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d86c4a3e9c944a5595ffd076817380c9",
      "question": "The tour bus overturns where?",
      "prediction": "near Hoover Dam,",
      "ground_truths": [
        "near Hoover Dam,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "59cca53e121949e39742ee34a89cffab",
      "question": "Who was flown to Las Vegas?",
      "prediction": "five victims by helicopter,",
      "ground_truths": [
        "five victims"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7951ebb574024ca7958ac8e4f56cc046",
      "question": "What country were the passengers on the bus from?",
      "prediction": "Chinese",
      "ground_truths": [
        "Chinese"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a084e061c5614a088e39a8d8468caf71",
      "question": "The dead lay where?",
      "prediction": "on the roadway near the bus,",
      "ground_truths": [
        "in body bags on the roadway near the bus,"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "116602c25c3e4b33b7a5a111f9174cbe",
      "question": "Where did the bus overturn?",
      "prediction": "near Hoover Dam,",
      "ground_truths": [
        "White Hills, Arizona, near Hoover Dam."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "23d1c9c515f3417ab70aba3c3bc43ec8",
      "question": "Bus passengers were from what country?",
      "prediction": "Chinese",
      "ground_truths": [
        "Chinese nationals."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4182285804154f018d4bb9735a8dc3e1",
      "question": "As many as how many victims are being flown to Vegas?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03905a27a5b841e59153c2e3de3c99f4",
      "question": "What foreign nationality were several of the passengers?",
      "prediction": "Chinese",
      "ground_truths": [
        "Chinese nationals."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4453276e770a4ed7b7051d9d65d40975",
      "question": "Who were bus passengers?",
      "prediction": "Chinese",
      "ground_truths": [
        "Chinese tourists"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c1e18f0bb23a4243b70eec0f92d21f71",
      "question": "What is Maradona's relation to Caceres?",
      "prediction": "former Boca Juniors teammate and national coach",
      "ground_truths": [
        "Boca Juniors teammate"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "ff9c4b6aa34a4cc1ac06e9671dd284c2",
      "question": "What is wrong with Fernando Caceres?",
      "prediction": "being shot in the head",
      "ground_truths": [
        "shot in the head"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "2b614698b55045c78b57cc7ed925f504",
      "question": "Who is in grave condition?",
      "prediction": "Fernando Caceres",
      "ground_truths": [
        "Fernando Caceres"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9715d425d7044c5aa81dcf972628c47d",
      "question": "What did Caceres enjoy?",
      "prediction": "playing for Argentina,",
      "ground_truths": [
        "Argentine league"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "03bccc9e5f964e718c8aa85f321068ce",
      "question": "Who visited Caceres?",
      "prediction": "Diego Maradona,",
      "ground_truths": [
        "by his former Boca Juniors teammate and national coach Diego Maradona,"
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "ea7c8bc9cc85442091d22b7aa2b11cb3",
      "question": "What did Caceres do for a career?",
      "prediction": "played 24 games",
      "ground_truths": [
        "Argentina in the 1994 World Cup,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "144ba08c90054d9f864538e8113ac853",
      "question": "What is used to lure students?",
      "prediction": "unique perks",
      "ground_truths": [
        "free services."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb1c311b507c464b903c7b32210e9319",
      "question": "Do students at Davidson College pay for laundry?",
      "prediction": "a free laundry service.",
      "ground_truths": [
        "don't have to visit laundromats because they enjoy the luxury of a free"
      ],
      "em": 0,
      "f1": 0.14285714285714288
    },
    {
      "id": "8238f44246824f5f8bce4ace866087d8",
      "question": "What is given out for free at some colleges?",
      "prediction": "laundry service.",
      "ground_truths": [
        "laundry service."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b4be388317541d6b4d43c9efdc0a094",
      "question": "Which ones have free laundry?",
      "prediction": "Davidson college",
      "ground_truths": [
        "Davidson college students"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "e266e50c7eb94d938f889f8e66aa10c8",
      "question": "What lures students to colleges?",
      "prediction": "free services.",
      "ground_truths": [
        "free services."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91cdd493d501438c868c9f4a43ea3f06",
      "question": "Where can I get a free computer?",
      "prediction": "Michigan Technological University",
      "ground_truths": [
        "Wake Forest,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "66ccd850630b4a069b80af81f0d8fa0c",
      "question": "Who works with Stella McCartney?",
      "prediction": "Adidas",
      "ground_truths": [
        "Adidas"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba9e926ece6b4b6e844f51d0b7b0f056",
      "question": "What will be the main priority for Adidas?",
      "prediction": "sportswear,",
      "ground_truths": [
        "sportswear,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b0d95637bcb44afb96457d3053631d5f",
      "question": "who is main priority will always be sportswear?",
      "prediction": "Herbert Hainer",
      "ground_truths": [
        "Adidas,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85ff90e4fd9c4c8790b6414520150e51",
      "question": "What allowed adidas to navigate Europe's economic difficulties?",
      "prediction": "brand name and the diversity of its product portfolio,",
      "ground_truths": [
        "strength of its brand name and the diversity of its product portfolio,"
      ],
      "em": 0,
      "f1": 0.8421052631578948
    },
    {
      "id": "a2374c9022504b94b1a832c91ab535fe",
      "question": "With who Adidas now has been working with?",
      "prediction": "top designers,",
      "ground_truths": [
        "top designers, such as Stella McCartney,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "afdf5764dddf4d858cff8be47f91fba7",
      "question": "What will the main priority be?",
      "prediction": "sportswear,",
      "ground_truths": [
        "sportswear,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cf40be104bfe45338fb4e03c699a2f7f",
      "question": "who was the guest speaker?",
      "prediction": "President Obama",
      "ground_truths": [
        "President Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "629ca0ba60d84e678789916c527e5e74",
      "question": "What is essential to democracy?",
      "prediction": "\"Across the country, there are extraordinary, hardworking journalists who have lost their jobs in recent days, recent weeks, recent months,\"",
      "ground_truths": [
        "ultimate success as an industry"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5c48b89357aa48fd8a1aafaeabe73c4c",
      "question": "What did Obama deliver?",
      "prediction": "laughs",
      "ground_truths": [
        "some one-liners at the White House Correspondents' Association dinner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e62ce90267b149f7a65dc0901c614317",
      "question": "Which party does not qualify for a bailout?",
      "prediction": "Republican",
      "ground_truths": [
        "Republican"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "147cef44c6e64257a962d7ed7e7df893",
      "question": "when did this happen?",
      "prediction": "Saturday,",
      "ground_truths": [
        "on Saturday."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b999b0cedcd1447b822ddccf34cabec4",
      "question": "Who recounted details of Winehouse's last hours?",
      "prediction": "Andrew Morris,",
      "ground_truths": [
        "Andrew Morris,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cbb53f49dd5746a19dc95b2ac047a687",
      "question": "What age was Winehouse when she died?",
      "prediction": "27-year-old's",
      "ground_truths": [
        "27-year-old's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8fd0fb454968435f93da377bce88d65c",
      "question": "How many vodka bottles were found?",
      "prediction": "three empty",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "558dea0334cf4e81ab5f22365e79ab5b",
      "question": "who talked about her last hours",
      "prediction": "Andrew Morris,",
      "ground_truths": [
        "Andrew Morris,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7d83d818f3ba4db19f40eb155fd6e616",
      "question": "What age was the person who died?",
      "prediction": "27-year-old's",
      "ground_truths": [
        "27-year-old's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a103fee179d24e01afac355e24433e16",
      "question": "What was found in Winehouse's home?",
      "prediction": "three empty vodka bottles,",
      "ground_truths": [
        "three empty vodka bottles,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f3edccdb629a473aa46b74b5be302b9b",
      "question": "When did she die?",
      "prediction": "7 p.m.",
      "ground_truths": [
        "July 23."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9e037717c1240859e5593a0a6586ffa",
      "question": "what was found at winehouse's home",
      "prediction": "three empty vodka bottles,",
      "ground_truths": [
        "three empty vodka bottles,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e381e6c7f5947febd026980b77ba9b1",
      "question": "What does Sahaab craft?",
      "prediction": "poems telling of the pain and suffering of children",
      "ground_truths": [
        "poems"
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "8bc4173ee48b4782a06d0cb9b6560f42",
      "question": "Who is Tuba Sahaab?",
      "prediction": "girl",
      "ground_truths": [
        "She is a slight girl of 11,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "80bd1a6acb4648c5978c1536ca9b87c1",
      "question": "What does Tuba craft?",
      "prediction": "poems",
      "ground_truths": [
        "poems"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ebf019efe7f1441f9b913bc872202133",
      "question": "What does she do?",
      "prediction": "crafting poems",
      "ground_truths": [
        "crafts poems telling of the pain and suffering of children just like her;"
      ],
      "em": 0,
      "f1": 0.14285714285714285
    },
    {
      "id": "7eed7785f1af4c2a80fab37a4cb64027",
      "question": "What is Tuba Sahaab's age?",
      "prediction": "11,",
      "ground_truths": [
        "11,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "475e7a194700436280b109877f5938eb",
      "question": "Where have girls been banned from?",
      "prediction": "school,",
      "ground_truths": [
        "school,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "40b316579aa445ebbf8dedce7b5722fb",
      "question": "What does Sahaab refuse to do?",
      "prediction": "give her life for her country.",
      "ground_truths": [
        "be silent."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9f0cc93d1bb54d99aac6cc95a24a1c39",
      "question": "How old is Tuba Sahaab?",
      "prediction": "11,",
      "ground_truths": [
        "11,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "736c1b0a529245568a535a39619721cf",
      "question": "How old is Sahaab?",
      "prediction": "11,",
      "ground_truths": [
        "11,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a9572e6dd5424d60b2fe711663259df2",
      "question": "What have girls been banned from?",
      "prediction": "school,",
      "ground_truths": [
        "school,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b99d58f4d2f4bd9bdd4ab030850c3c1",
      "question": "Who has been banned fro school?",
      "prediction": "girls",
      "ground_truths": [
        "girls"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "464ae299794f477b86ee4ed46db930ec",
      "question": "What have poets long written about?",
      "prediction": "mother",
      "ground_truths": [
        "Mother's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b9a879f717b9402d983846f5b7d875aa",
      "question": "What does Robert Louis Stevenson suggest his mother listen out for",
      "prediction": "unforgotten times,",
      "ground_truths": [
        "The little feet along the floor."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4427862bd5fc445a92b61bb409830823",
      "question": "What did Louis Stevensons poem say?",
      "prediction": "\"To My Mother\"",
      "ground_truths": [
        "You too, my mother, read my rhymes For love of unforgotten times, And you may chance to hear once more The little feet along the floor."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "25c4f5cc2f944db88c3f41e4b8e9ce52",
      "question": "What are the poems about?",
      "prediction": "Mother Nature",
      "ground_truths": [
        "Mother's Day"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4d1f3d2d289044d683e89b0166d6c11a",
      "question": "To whom have poets long written about",
      "prediction": "the poet's memories of his mother.",
      "ground_truths": [
        "mother."
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "0433b48954e94a5d96a3fe71cbd2e145",
      "question": "What is it that Julia's Kasdorf's mother taught her to do",
      "prediction": "offer healing and \"the blessing of your voice, your chaste touch.\"",
      "ground_truths": [
        "comfort those in mourning,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "36b302d3fb454f10b36bffcc08451bbe",
      "question": "Who asked for filming to be stop?",
      "prediction": "Graham",
      "ground_truths": [
        "Alan Graham"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a2fc4e20d2244d3ea34c2b03ebac828e",
      "question": "Who says the Barbados-born singer went topless",
      "prediction": "Mrs. Graham,",
      "ground_truths": [
        "Graham's wife"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc63ecaff9e64b1889456cdbf14d252c",
      "question": "What did his wife say?",
      "prediction": "said that Rihanna had gone topless.",
      "ground_truths": [
        "Alan was particularly objecting to"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e3ca0e8b58b64a9b8c4f4026946b648e",
      "question": "What song was the music video for?",
      "prediction": "\"We Found Love\"",
      "ground_truths": [
        "\"We Found Love\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9d7993ab762b4739b2da434533f6bab9",
      "question": "What did the farmer ask them to do?",
      "prediction": "stop,",
      "ground_truths": [
        "to cover up"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d18a11868b6e470f91c9abb1e338f3cb",
      "question": "Who stripped to bikin top",
      "prediction": "Rihanna",
      "ground_truths": [
        "Rihanna"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0aca308fce9b4cb4b883b413dfad75b4",
      "question": "What was the pop singer filming?",
      "prediction": "a music video",
      "ground_truths": [
        "a music video"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3d4805f37b84f3092a97f8978bc43cf",
      "question": "Where was the Pop singer filming",
      "prediction": "his land.",
      "ground_truths": [
        "Belfast."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2b3ef7f242c47b0858142af1dc91c2e",
      "question": "What type is Chuck Bass?",
      "prediction": "types (CBTs)",
      "ground_truths": [
        "Moody and sinister,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e1a72b05da3c4bb9b0bd99452f34df94",
      "question": "Who is the aging party boy?",
      "prediction": "Hank Moody",
      "ground_truths": [
        "Hank Moody"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0db987c2ec664e7a820ac63141d1a84e",
      "question": "Hank Moody is from what tv show?",
      "prediction": "\"Californication\"):",
      "ground_truths": [
        "(\"Californication\"):"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ad250ef9a417474c954df4e6e1fc9501",
      "question": "How should one learn who not to date?",
      "prediction": "Chuck Bass",
      "ground_truths": [
        "Television"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d3542dface0247cdb0e57be88b21c2e0",
      "question": "Which character from Mad Men is mentioned?",
      "prediction": "Don Draper",
      "ground_truths": [
        "Don Draper"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "750fe50e52844d08b695bedd58eeb642",
      "question": "Who is a smoldering liar and cheat?",
      "prediction": "Don Draper",
      "ground_truths": [
        "Don Draper"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fb400d0b145641c5b055f45c29050377",
      "question": "What show is Chuck Bass from?",
      "prediction": "\"Gossip Girl\"):",
      "ground_truths": [
        "(\"Gossip Girl\"):"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88c6ef9d9a3c436abbad77bf67d0902e",
      "question": "What show is the character Don Draper from?",
      "prediction": "(\"Mad Men\"):",
      "ground_truths": [
        "(\"Mad Men\"):"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "26782a326a394d688dda2441308bf00a",
      "question": "When was the first of the impeachment chargers brought against Arroyo?",
      "prediction": "2005",
      "ground_truths": [
        "vote-tampering."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e62ec634d4b44a50842d1913910877a5",
      "question": "What did the opposition parties call into question?",
      "prediction": "the legitimacy of that race.",
      "ground_truths": [
        "the legitimacy of that race."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "642ade1eebc04179b6ee599028cbb9b0",
      "question": "What does the complaint allege?",
      "prediction": "that Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm.",
      "ground_truths": [
        "Arroyo and her husband were directly involved in an Internet broadband deal with a Chinese firm."
      ],
      "em": 0,
      "f1": 0.9655172413793104
    },
    {
      "id": "8b9a2f29b35b41e38cb94347237d59d7",
      "question": "what was being called into question",
      "prediction": "the legitimacy of that race.",
      "ground_truths": [
        "legitimacy of that race."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "775656e96e4747f5a17cc2b57edd12b7",
      "question": "when were the impeachment charges bought",
      "prediction": "2005",
      "ground_truths": [
        "2005"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "194bd503229c46c3b308896e770e1eb1",
      "question": "What did the complaint allege that Arroyo and husband do?",
      "prediction": "involved in an Internet broadband deal with a Chinese firm.",
      "ground_truths": [
        "directly involved in an Internet broadband deal with a Chinese firm."
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "1cf1d0fdb27f4ba6a2399a4a86b09b8e",
      "question": "What type of charge was brought against Arroyo?",
      "prediction": "large-scale corruption",
      "ground_truths": [
        "corruption"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5b94852dff984b19bf8264cd8bc1caa3",
      "question": "Who called into question the legitimacy of Arroyo's win?",
      "prediction": "opposition parties",
      "ground_truths": [
        "opposition parties"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5a154a6b5c364e79b30a1bd2c9e47e0c",
      "question": "who were involved in the deal",
      "prediction": "President Gloria Macapagal Arroyo and her husband",
      "ground_truths": [
        "Arroyo and her husband"
      ],
      "em": 0,
      "f1": 0.7272727272727273
    },
    {
      "id": "41d5a6d229964b50b3c73d5bc9a5d1fa",
      "question": "How many cases have been reported in 26 states",
      "prediction": "133 people",
      "ground_truths": [
        "133"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c5559fa0b9a0438a9b843d4207061a3d",
      "question": "The outbreak has been deadliest since when",
      "prediction": "1998.",
      "ground_truths": [
        "1998."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fcf679e5b27f454aa3da8e1ecf8226bb",
      "question": "What was recalled?",
      "prediction": "Rocky Ford brand cantaloupes",
      "ground_truths": [
        "Rocky Ford brand cantaloupes"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1416c66b2eb543aea04972d5780b31d6",
      "question": "In how many states were the cases reported?",
      "prediction": "26",
      "ground_truths": [
        "26"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dd66178403e8487382fdc1b84bb2881c",
      "question": "What is not over",
      "prediction": "the outbreak",
      "ground_truths": [
        "the outbreak"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ac55787c46dc4d7b84b2856b8bf2dc93",
      "question": "How many cases have been reported?",
      "prediction": "133 people",
      "ground_truths": [
        "133 people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e8ac0eb458ea4dcabf252184b9da9bae",
      "question": "What the activist animal rights says?",
      "prediction": "\"A good vegan cupcake has the power to transform everything for the better,\"",
      "ground_truths": [
        "\"A good vegan cupcake has the power to transform everything for the better,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6961695f789f4da9a9c3fa718b135d0e",
      "question": "When does the sale run?",
      "prediction": "April 24 through May 2.",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95fd83262d2f47cfbb801bd5ca266c1b",
      "question": "How many groups there are about?",
      "prediction": "More than 120",
      "ground_truths": [
        "More than 120"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "031e600d36e74011aa477308544ebc84",
      "question": "What do the tasty treats not contain?",
      "prediction": "animal products.",
      "ground_truths": [
        "animal products."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa666bde7da647179528a5814dac9fb7",
      "question": "What are 120 groups selling?",
      "prediction": "vegan bake sales",
      "ground_truths": [
        "vegan bake sales"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "df292cfff75d46a8963765ee12c341f0",
      "question": "On what date runs Vegan Bake Sale",
      "prediction": "April 24 through May 2.",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "58224fe8e4ed454f9c42ea3331a9a46f",
      "question": "What did animal rights activist say?",
      "prediction": "\"A good vegan cupcake has the power to transform everything for the better,\"",
      "ground_truths": [
        "\"A good vegan cupcake has the power to transform everything for the better,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef463e57439c4b01af0a1f501f2a074f",
      "question": "When does the bake sale run?",
      "prediction": "April 24 through May 2.",
      "ground_truths": [
        "April 24 through May 2."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "70131d6d1a9a4f74a8005efd5d92fb47",
      "question": "What has helped his church according to preacher?",
      "prediction": "gifts from God",
      "ground_truths": [
        "natural gas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5aa43a0714d54bc99836dfbd5049fe40",
      "question": "What was the score at Bremen?",
      "prediction": "2-1",
      "ground_truths": [
        "3-3"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21d9ee49ecc144ab9ca0a426c203d559",
      "question": "Which team won against Hamburg?",
      "prediction": "Schalke",
      "ground_truths": [
        "Schalke"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2761f819ecb5477f91696a9510a7c2d8",
      "question": "Who scored against Hamburg?",
      "prediction": "Kevin Kuranyi",
      "ground_truths": [
        "Kevin Kuranyi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "622bd69fdf1a42e28e638b0fbbae1e21",
      "question": "Which position is Schalke now level in?",
      "prediction": "fifth",
      "ground_truths": [
        "joint second"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ac1d6afd28024e4bba04d2fb8ae65b61",
      "question": "Who scored an early goal for Schalke?",
      "prediction": "Kevin Kuranyi",
      "ground_truths": [
        "Kevin Kuranyi"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9adce0e50b4f440f847b00061534308c",
      "question": "Who does Bayern have to beat in Sunday?",
      "prediction": "Stuttgart",
      "ground_truths": [
        "Stuttgart"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8c87c6cbfac04e7b9b1cc82b5ba7c679",
      "question": "Who are Schalke now in second position with?",
      "prediction": "Werder Bremen,",
      "ground_truths": [
        "Bayern Munich"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f4325e8d9dc2437c86c45614ec0c210b",
      "question": "What position are Schalke in?",
      "prediction": "second",
      "ground_truths": [
        "joint second"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b8e2019642b940749d6bcd95b2acfa06",
      "question": "When is the next game?",
      "prediction": "Sunday.",
      "ground_truths": [
        "Sunday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "136d8cfca1384d459b264083a4f98710",
      "question": "Who won the game?",
      "prediction": "Schalke",
      "ground_truths": [
        "Schalke"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0beffe10acae4b82a1e79284189e07c8",
      "question": "When will Bayern Munich play Stuttgart to all but clinch the title?",
      "prediction": "Sunday.",
      "ground_truths": [
        "Sunday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c5a96f6032194077b291d81e38925761",
      "question": "Where is the judge who needs to sign bail paperwork?",
      "prediction": "Iran",
      "ground_truths": [
        "on vacation"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09f2baa54b7c4f12b1775f1f1814cb3d",
      "question": "Josh and Shane were held for being what?",
      "prediction": "spies",
      "ground_truths": [
        "as spies"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "4ae54b7d35b3414c94f430c3ef5dd589",
      "question": "What country is  Ahmadinejad president of?",
      "prediction": "Iran",
      "ground_truths": [
        "Iran's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76927bab1bc841c1b200dd9810afcb06",
      "question": "Who has requested their release?",
      "prediction": "delegation of American Muslim and Christian leaders",
      "ground_truths": [
        "a delegation of American Muslim and Christian leaders"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f872a701af41474e84deef08fea13528",
      "question": "How long have Fattal and Bauer been held?",
      "prediction": "more than two years,",
      "ground_truths": [
        "more than two years,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bd2e0c1318f4375ac049bd71a8b6876",
      "question": "Who asks President Ahmadinejad for their release?",
      "prediction": "American Muslim and Christian leaders",
      "ground_truths": [
        "a delegation of American Muslim and Christian leaders"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "4d8a017ec36c420984653e239a12606b",
      "question": "What did billboards use to encourage GOP votes?",
      "prediction": "image of the burning World Trade Center",
      "ground_truths": [
        "an image of the burning World Trade Center"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eae081ec9fb347678ec391e2394f7418",
      "question": "What was said about president Clinton?",
      "prediction": "should have put a stop to Osama bin Laden and al Qaeda before 9/11. He said a Republican",
      "ground_truths": [
        "should have put a stop to Osama bin Laden and al Qaeda before 9/11."
      ],
      "em": 0,
      "f1": 0.896551724137931
    },
    {
      "id": "be9df164a96b4ae8a5cad6d0bdffb257",
      "question": "What did billboards use to encourage votes?",
      "prediction": "image of the burning World Trade Center",
      "ground_truths": [
        "an image of the burning World Trade Center"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0718876d8bd9412b8a546bb3df09e72f",
      "question": "WHat are used to encourage GOP voters?",
      "prediction": "billboards",
      "ground_truths": [
        "using billboards with an image of the burning World Trade Center"
      ],
      "em": 0,
      "f1": 0.19999999999999998
    },
    {
      "id": "71ed6dfba6704b05b0d0aac8c79665d4",
      "question": "Who says the image is wrong?",
      "prediction": "Bill Robinson,",
      "ground_truths": [
        "The local Republican Party"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3aa16adb30c463f9d467da047b09044",
      "question": "What was the Best Picture winner?",
      "prediction": "\"Slumdog Millionaire\"",
      "ground_truths": [
        "\"Slumdog Millionaire\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "177ef67e2d3241cb88c1f65a90e1d9b8",
      "question": "What was the amount it grossed?",
      "prediction": "$55.7 million",
      "ground_truths": [
        "$55.7 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "589652b567004418a17c2aedea6f6ddd",
      "question": "What was the gross for Tyler Perry's \"Madea Goes to Jail\" (No. 2)?",
      "prediction": "$8.8 million",
      "ground_truths": [
        "$8.8 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e22b09d953fc4594906f1d83f2194a99",
      "question": "What premiered in more theaters than any other R-rated movie in history?",
      "prediction": "\"Watchmen\"",
      "ground_truths": [
        "\"Watchmen\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4c74a0f5264d4bb8bb2b63898af2333a",
      "question": "What was the movie rated?",
      "prediction": "B",
      "ground_truths": [
        "R-rated"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8b873841cfbc42a793a7a1ba9987a415",
      "question": "Which slot the \"Slumdog Millionaire\" take?",
      "prediction": "(No. 4)",
      "ground_truths": [
        "(No. 4)"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "94b10c393a7e4c55959a283da2c4d78f",
      "question": "What did Madea Goes to Jail gross?",
      "prediction": "$8.8 million",
      "ground_truths": [
        "$8.8 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d9d36c6631284d1091e578b9813afe6f",
      "question": "What did Watchmen do?",
      "prediction": "grossed $55.7 million during its first frame,",
      "ground_truths": [
        "grossed $55.7 million during its first frame,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "34fa5a8aa22d49fb90a1eabfaaeaf1c0",
      "question": "What was slumdog millionaire?",
      "prediction": "\"Paul Blart: Mall Cop\"",
      "ground_truths": [
        "Best Picture winner \"Slumdog Millionaire\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b3d2d83d34a42848409272d4c9248b9",
      "question": "How many weeks did the pus by Al-Shaabab and Hisb-ul-Islam militias last?",
      "prediction": "eight-week",
      "ground_truths": [
        "eight-week"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c170e4dcb7d4be0a567d55d531ba222",
      "question": "Where is the U.N. conflict happening?",
      "prediction": "Mogadishu",
      "ground_truths": [
        "Mogadishu"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e9ac1c4bf0845eab105c8b1b9265db2",
      "question": "Where is the conflict?",
      "prediction": "Mogadishu",
      "ground_truths": [
        "Somali capital of Mogadishu"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "94ae84b9e54947f9973d73153d50c57f",
      "question": "Has the conflict in Mogadishu had a positive impact on the city's polulation",
      "prediction": "\"The escalating",
      "ground_truths": [
        "is having a devastating"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "58c9a110235b4879950e679babb53318",
      "question": "how many people were displaced in Somalia?",
      "prediction": "more than 1.2 million",
      "ground_truths": [
        "more than 1.2 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f57595f77502405fbe6baf093d8baea0",
      "question": "The push by Al-Shabab and Hisb-ul-Islam lasted how long?",
      "prediction": "eight-week",
      "ground_truths": [
        "eight-week"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "857c29add0314727bee40735ec864631",
      "question": "Approximately how many people were internally displaced in Somalia according to the U.N.?",
      "prediction": "more than 1.2 million",
      "ground_truths": [
        "1.2 million"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5d6123571c4f4e5cb365ed1f2f6453f6",
      "question": "When did the Somali civil war start?",
      "prediction": "1991,\"",
      "ground_truths": [
        "in 1991,\""
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c00bbb93fc2947108a477eccccd8d900",
      "question": "Who is the Republican representative",
      "prediction": "Jason Chaffetz",
      "ground_truths": [
        "Jason Chaffetz"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8e6c47b5fac44a56814f058af7cd3121",
      "question": "Who documented their experince",
      "prediction": "Two new U.S. representatives",
      "ground_truths": [
        "Jason Chaffetz"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e250c9e8c70843569a6bc75aa759e58a",
      "question": "what is the name or the Democrat representig Colorado's Second district?",
      "prediction": "Jared Polis",
      "ground_truths": [
        "Jared Polis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9cae7898982c428aa4712f0d6964c67b",
      "question": "Who is the Democratic representative",
      "prediction": "Rep. Jared Polis",
      "ground_truths": [
        "Jared Polis"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "559100c2d1c24e8fae0ba6dcea6e7a69",
      "question": "two fresman representatives documented what?",
      "prediction": "their \"Freshman Year\"",
      "ground_truths": [
        "\"Freshman Year\""
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "839679ea2bc546d3aa71072f7edfd81b",
      "question": "what's the name of the Republican representig Utah's Third disctrict?",
      "prediction": "Rep. Jason Chaffetz",
      "ground_truths": [
        "Chaffetz"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9ad601900986433a9bd99b3a7af2dbb5",
      "question": "Who does Jason Chaffetz represent?",
      "prediction": "the 3rd District of Utah.",
      "ground_truths": [
        "conservative Republican"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a790d391a6c74fdfadcdeee082a871eb",
      "question": "What are the two freshman documenting?",
      "prediction": "their \"Freshman Year\" experience",
      "ground_truths": [
        "it really like to be a new member of the world's most powerful legislature?"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "567c70fec2634f439e055ef5fb30135d",
      "question": "Who is the president?",
      "prediction": "Sheikh Sharif Sheikh Ahmed",
      "ground_truths": [
        "Sheikh Sharif Sheikh Ahmed"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ecccb9b15eb3413d8f3a5ad8d8983716",
      "question": "Who is fighting in Somalia?",
      "prediction": "Islamic",
      "ground_truths": [
        "rebels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5098bc1ebaeb4c25831309d74b7461b6",
      "question": "who won't agree to a strict interpretation of Islamic law?",
      "prediction": "President Sheikh Sharif",
      "ground_truths": [
        "President Sheikh Sharif Sheikh Ahmed"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "9238d14529524f97a206ee0d51f157b8",
      "question": "What is the aim of the concession?",
      "prediction": "halt fighting between Somali forces and Islamic insurgents.",
      "ground_truths": [
        "to halt fighting between Somali forces and Islamic insurgents."
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "dca240ce71894a9798451f919b5d9df0",
      "question": "who has asked African peacekeepers to stand down?",
      "prediction": "President Sheikh Sharif Sheikh Ahmed",
      "ground_truths": [
        "The president,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "89079a330c38458eb5b697271e1e2f0f",
      "question": "What did he ask African peacekeepers to do?",
      "prediction": "stand down.",
      "ground_truths": [
        "stand down."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68a10085a6d942d090063841c1d024d3",
      "question": "What will President Ahmed not agree to?",
      "prediction": "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television.",
      "ground_truths": [
        "a strict interpretation of the law, which forbids girls from attending school, requires veils for women and beards for men, and bans music and television."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3dab0763a0764e6bb2bd2215fda60e7b",
      "question": "What did the Defense Secretary report?",
      "prediction": "U.S. forces in Afghanistan are doing everything possible to free Bergdahl,",
      "ground_truths": [
        "U.S. forces in Afghanistan are doing everything possible to free Bergdahl,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d418c84f766f459b90af95983b15f522",
      "question": "What captured soldier said about his future?",
      "prediction": "\"Scared I won't be able to go home.\"",
      "ground_truths": [
        "might never see them again"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dcd7d07dad674809bb1ab707f47914d5",
      "question": "What U.S. Defence Secretary said about finding Bergdahl?",
      "prediction": "\"My personal reaction was one of disgust at the exploitation of this young soldier,\"",
      "ground_truths": [
        "forces in Afghanistan are doing everything possible to free"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0b03da7faf474f56bb9d4447f62f0c9d",
      "question": "What friend to soldier said?",
      "prediction": "his",
      "ground_truths": [
        "\"stand tall, stand firm.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76a9bca3223248d393c03b2583aa76fb",
      "question": "What is one soldier from Idaho afraid of?",
      "prediction": "I won't be able to go home.\"",
      "ground_truths": [
        "I might never see them again and that I'll never be able to tell them I love them again. I'll never be able to hug them."
      ],
      "em": 0,
      "f1": 0.24242424242424246
    },
    {
      "id": "ff0b3796a6dc40fc8d87d3532065b3f1",
      "question": "Who was captured?",
      "prediction": "Pfc. Bowe Bergdahl",
      "ground_truths": [
        "U.S. soldier"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a940d9cc02a74c0ea64944ea9d6c4842",
      "question": "Where is the soldier from?",
      "prediction": "Ketchum, Idaho.",
      "ground_truths": [
        "Ketchum, Idaho."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7643157f5e54f3f8195949fe0804a3b",
      "question": "what are the results based on",
      "prediction": "data from the federal government's 2007 national survey of children's health.",
      "ground_truths": [
        "a national telephone survey"
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "747b8f3339c94fa293f30e648670e0f1",
      "question": "What kind of survey was used?",
      "prediction": "national telephone",
      "ground_truths": [
        "national telephone"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4864c159e354a4cbc7b2e4fdfd2c4a2",
      "question": "At what age is autism a huge risk?",
      "prediction": "3 to 17",
      "ground_truths": [
        "3 to 17"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c83e142be48c493d995b27b01eaeda65",
      "question": "who took part in the survey",
      "prediction": "Health Resources and Services Administration,",
      "ground_truths": [
        "78,000 parents of children ages 3 to 17.iReport.com:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a737b734b5b949009898da99b81e82ac",
      "question": "What percentage of children have autism?",
      "prediction": "1 percent",
      "ground_truths": [
        "about 1 percent"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "22536e61437f4cc59db250397061e8db",
      "question": "who has autism or related disorder",
      "prediction": "1 percent of children ages 3 to 17",
      "ground_truths": [
        "3 to 17"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "34532b23c4624e6e92c4faf30085297d",
      "question": "What did the DA's office say about the suspects?",
      "prediction": "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,",
      "ground_truths": [
        "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0.16
    },
    {
      "id": "0986a62e4b4b48be90e2c9172a5b9617",
      "question": "What did the suspects do according to the DA's office?",
      "prediction": "forged credit cards and identity theft",
      "ground_truths": [
        "identity theft"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "d69a7141ec1c4110ac124dd58d19b8b4",
      "question": "What did the DA say?",
      "prediction": "A New York City crackdown on",
      "ground_truths": [
        "Several suspects are believed to have engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2f56933ee8024412b6ef05faf87e7a46",
      "question": "What did the suspects do?",
      "prediction": "inquired in forged credit cards and identity theft",
      "ground_truths": [
        "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\""
      ],
      "em": 0,
      "f1": 0.23255813953488372
    },
    {
      "id": "7d4f644a529f41c4b257f9ffb9ce9c87",
      "question": "Where did the credit information come from?",
      "prediction": "conversations in Arabic, Russian and Mandarin",
      "ground_truths": [
        "American and European consumers,"
      ],
      "em": 0,
      "f1": 0.2
    },
    {
      "id": "407b7cab4614445f991abdc1b724f793",
      "question": "Who was a popular casting choice?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "be452005b70e4c44b514569be76510c2",
      "question": "What does Mears say of the iconic role?",
      "prediction": "I've got a lot of issues. I've got a whole lot of daddy issues.",
      "ground_truths": [
        "It's so weird. There's two different versions. There's my version of how it went about, and there's the producer's"
      ],
      "em": 0,
      "f1": 0.06666666666666667
    },
    {
      "id": "6c6bfb3e518f4c11a1215c7a6c21ddc6",
      "question": "Who takes on role of Jason in new \"Friday the 13th\"?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d94bbc9c885346efbf9fed0c63ad7ff4",
      "question": "Who takes on role of Jason?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1c429c7cffb24fa0afbdef4ca48e33d1",
      "question": "Who is taking on the role of Jason?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "666120b705a140c191a4f26d3827bd7d",
      "question": "Who says he tried to \"definitely make it my own\"?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03b048d6c9c7420997c632cb9b29c106",
      "question": "Who heard from producers that he was a popular casting choice?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a41630aeea734939ad1f2f6d7b44a9b5",
      "question": "Who trid to \"definitely make it my own\"?",
      "prediction": "Derek Mears",
      "ground_truths": [
        "Derek Mears"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b466d0d5acdb4c96bb1d4bd6d19548c4",
      "question": "Who are not doing great?",
      "prediction": "Knox's parents, Curt Knox and Edda Mellas,",
      "ground_truths": [
        "Knox's parents"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "023e088f89274e2abd8a49edd96a408d",
      "question": "Who is Janet Huff?",
      "prediction": "aunt of Amanda Knox's",
      "ground_truths": [
        "Amanda Knox's aunt"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "a8972465dad04d5f8b2fcf659f8aeb25",
      "question": "who was convicted of murder",
      "prediction": "Amanda Knox's",
      "ground_truths": [
        "Amanda Knox's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21eca67432494831b67960018d045fd7",
      "question": "who was murdered",
      "prediction": "Meredith Kercher.",
      "ground_truths": [
        "Meredith Kercher."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fca02df414b8412b83e2f9d34be46341",
      "question": "what did huff say",
      "prediction": "\"It was terrible, it was gut-wrenching just to hear them",
      "ground_truths": [
        "\"It was terrible, it was gut-wrenching just to hear them"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0cee4e0245b54d429412963ad22f68ab",
      "question": "What the police investigating?",
      "prediction": "allegations",
      "ground_truths": [
        "investigator into the deadliest terrorist attack in Argentine history that he was kidnapped and tortured by men who said they were national intelligence agents."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ebf6a7cef1a4f75aaa2ffb7510a3c8f",
      "question": "Who does he say tortured him?",
      "prediction": "men who said they were national intelligence agents.",
      "ground_truths": [
        "said they were national intelligence agents."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "3a32771820f14982b983662bca90f254",
      "question": "What is the name of the lawyer?",
      "prediction": "Lifschitz,",
      "ground_truths": [
        "Lifschitz,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3dd74943ebab4b1d828661d36a868dab",
      "question": "The bombing of a Jewish center in Buenos Aires took place in which year?",
      "prediction": "1994,",
      "ground_truths": [
        "1994,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "35dc385df2a9486a8a23a951e5f249d7",
      "question": "What day was he abducted?",
      "prediction": "Friday",
      "ground_truths": [
        "Friday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8eb81e9e5f84de29e31354f7288d548",
      "question": "What did she win?",
      "prediction": "bronze medal",
      "ground_truths": [
        "Olympic medal"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cb3ba559580b4d6d8123dd7de545c1de",
      "question": "What has Joannie Rochelle earn?",
      "prediction": "bronze medal",
      "ground_truths": [
        "bronze medal in the women's figure skating final,"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "2a015deb51034910a9b0c15e43c72c5f",
      "question": "what does rochette says abouth her mother?",
      "prediction": "to talk about",
      "ground_truths": [
        "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "55e0a00ae647444abe645a821f0cd737",
      "question": "What does Rochette say of her mother?",
      "prediction": "\"It feels good for me to talk about",
      "ground_truths": [
        "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,"
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "a2211516832146c59692b15720041f8f",
      "question": "When did she skate?",
      "prediction": "Thursday",
      "ground_truths": [
        "Tuesday night's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ec9acff63b5e401e8ba12f7828de0cfe",
      "question": "What is her sport?",
      "prediction": "figure skating",
      "ground_truths": [
        "women's figure skating"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "9e2e3e8fa88145c9b7c4b4b98e54bcbb",
      "question": "Which group did North Korea participate in?",
      "prediction": "2",
      "ground_truths": [
        "2"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c396c2292f664fddbd09c3dfcb7d85f0",
      "question": "How many goals were there in the Australia-Japan match of Group 1 of Asian ?",
      "prediction": "0-0",
      "ground_truths": [
        "2-1"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f64f4338826d4bd194bb0209f0d73afd",
      "question": "What was the final score between Australia and Japan?",
      "prediction": "2-1",
      "ground_truths": [
        "2-1"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4c0aa93109c14bec9d9250084876e556",
      "question": "What was the score that allows North Korea to qualify ?",
      "prediction": "0-0",
      "ground_truths": [
        "0-0 draw"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a081bc6140dd47baba7768168a3bb4f6",
      "question": "What country qualified from group 2 with 0 to 0 draw against Saudi Arabia?",
      "prediction": "North Korea",
      "ground_truths": [
        "North Korea"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4918b1401f964c9dadb693941ca7e05c",
      "question": "Who clinched the playoff spot from Group 1 with 1-0 win?",
      "prediction": "Australia",
      "ground_truths": [
        "Bahrain"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6d3dc82139f04c41a99e454f2f6eeed2",
      "question": "What group qualified with 2 to 1 win over Japan?",
      "prediction": "Korea",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "02da09b14bdb44abb93daf65aa4e4c09",
      "question": "Against what nation does Bahrain played and win ?",
      "prediction": "Iran",
      "ground_truths": [
        "Uzbekistan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "21532189e740426194a08808390eb2b4",
      "question": "The American was taken into custody after entering from where?",
      "prediction": "China",
      "ground_truths": [
        "China"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "23f830d9e6c24104852473a27360624c",
      "question": "Who is believed to be held in North Korea?",
      "prediction": "Robert Park",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f97a45e3576644eda7c95849e50babef",
      "question": "Who was trying to sneak in to bring message of \"Christ's love and forgiveness\"?",
      "prediction": "Robert Park",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "912cb6aecbec4dc2b44ee22a076c3e2a",
      "question": "Who was taken into custody after entering from China?",
      "prediction": "Robert",
      "ground_truths": [
        "a Korean-American missionary"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f811a0a4db94a648917355fabbd40ae",
      "question": "Where is he believed to be held?",
      "prediction": "isolated communist state",
      "ground_truths": [
        "North Korea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f28f7be13ad43cda7681b7fa874843c",
      "question": "What is the name of the Korean-American missionary?",
      "prediction": "Robert Park",
      "ground_truths": [
        "Robert Park"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "902f161fe57f4998b07fd0de6d95270d",
      "question": "What Park said?",
      "prediction": "\"He is a very special member of our family. We miss having his love and compassion in our home,\"",
      "ground_truths": [
        "he was trying to sneak into the isolated communist state to bring a message of \"Christ's love and forgiveness\" to North Korean leader Kim Jong Il."
      ],
      "em": 0,
      "f1": 0.1904761904761905
    },
    {
      "id": "8ce493334e064c2f97343909d7ddb91e",
      "question": "What did rescue workers do?",
      "prediction": "found the body",
      "ground_truths": [
        "have pulled a body from underneath"
      ],
      "em": 0,
      "f1": 0.28571428571428575
    },
    {
      "id": "287086d87ca540629ea9a5d1f3e42827",
      "question": "What hasn't been determined?",
      "prediction": "It is still not clear",
      "ground_truths": [
        "caused the collapse of the building"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1b4f5f6d643456aad1526324d39c259",
      "question": "Who was pulled from the rubble?",
      "prediction": "a body",
      "ground_truths": [
        "body"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa55b1829c9c41e28f3998d32da669ec",
      "question": "Who pulled the body from the collapsed building?",
      "prediction": "Rescue",
      "ground_truths": [
        "Rescue"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "06752c5554924c339e8cc99f3e4776aa",
      "question": "What collapsed?",
      "prediction": "apartment building in Cologne,",
      "ground_truths": [
        "apartment building"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f540c016913344238538408d0b3ac731",
      "question": "What happened in Cologne?",
      "prediction": "The apartment building collapsed together with two other buildings on Tuesday afternoon.",
      "ground_truths": [
        "workers have pulled a body from underneath the rubble of a collapsed apartment building"
      ],
      "em": 0,
      "f1": 0.2727272727272727
    },
    {
      "id": "d031593cc6f142f6bba7e64ea9224dea",
      "question": "Where did the building collapse?",
      "prediction": "Köln,",
      "ground_truths": [
        "Cologne, Germany,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "963f7654e83b425bbeadf56527740ba0",
      "question": "Have authorities determined what brought down the structure?",
      "prediction": "The remains of Cologne's archive building",
      "ground_truths": [
        "still not clear"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "19fdbf5cb17843f1b6afc14de8604661",
      "question": "Who is facing misdemeanor assault charges?",
      "prediction": "Bryant Purvis",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c390a19b735414bbf4928f8c37aa879",
      "question": "Teen is awaiting trial for what?",
      "prediction": "second-degree aggravated battery.",
      "ground_truths": [
        "second-degree aggravated battery."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "28dbd333877249afbb95296f3404dfd7",
      "question": "Who is facing the charges?",
      "prediction": "Bryant Purvis",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "56aa271b35074e4fba0d82728d99c5c0",
      "question": "Who did Purvis allegedly choke?",
      "prediction": "student",
      "ground_truths": [
        "An 18-year-old student"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "538b9b0a443147089da51e3cbd31de77",
      "question": "What is Bryant Purvis facing?",
      "prediction": "misdemeanor assault",
      "ground_truths": [
        "misdemeanor assault charges"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "63002f70e2f74001b710cc5b6c97c79c",
      "question": "What does the fight not appear to be?",
      "prediction": "racially motivated.",
      "ground_truths": [
        "racially motivated."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec7ec97bf1624e75b422a32cfa68c104",
      "question": "who allegedly choked a teen?",
      "prediction": "Bryant Purvis",
      "ground_truths": [
        "Bryant Purvis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d679f2ddc15e438591bc73863dfc23cf",
      "question": "Where were the bodies discovered?",
      "prediction": "inside their home in the capital of Villahermosa",
      "ground_truths": [
        "inside their home in the capital of Villahermosa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a09179766c3345a1a9d54f959ba12b7c",
      "question": "How many sons did Jose Francisco Fuentes Esperson and his wife have?",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "39b334c095e8467981db2a6689f92914",
      "question": "Who did Schlesselman threaten to kill on October 23, 2008?",
      "prediction": "then-Sen. Obama",
      "ground_truths": [
        "then-Sen. Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2683ec5ca85f422da7ebd56b903a78ab",
      "question": "What did Schlesselman threaten to do?",
      "prediction": "kill then-Sen. Obama",
      "ground_truths": [
        "kill then-Sen. Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c6d223e334824da48b2c3885756f06d2",
      "question": "Schlesselman made a deal with federal prosecutors from what state?",
      "prediction": "Tennessee.",
      "ground_truths": [
        "Tennessee."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ad67beb0c1c4f9eb667ab93e3f4c322",
      "question": "How many years in prison does Schlesselman face?",
      "prediction": "10",
      "ground_truths": [
        "10"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2b9f8d5e6f234a8ab99b25b44a529086",
      "question": "What racial group did Schlesselman focus on targeting?",
      "prediction": "African-Americans",
      "ground_truths": [
        "African-Americans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "553920fb5b924a5896e590136039b57c",
      "question": "Who did Schlesselman threaten to kill?",
      "prediction": "African-Americans",
      "ground_truths": [
        "then-Sen. Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "747f1582934e443b9ce8b6a8a35718d7",
      "question": "How many years does Schlesselman face in prison?",
      "prediction": "10",
      "ground_truths": [
        "10"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fbbce6b6fec84eaf8c77bf9182971bef",
      "question": "Who made a plea deal?",
      "prediction": "Paul Schlesselman",
      "ground_truths": [
        "Paul Schlesselman"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cb390e324ea348b19dd922a431b57c93",
      "question": "Who do officials blame for attacks on police?",
      "prediction": "Taliban and their al Qaeda associates",
      "ground_truths": [
        "Taliban and their al Qaeda associates"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "37a00147dbc44e0c8b9c7693bea1d3f3",
      "question": "When were dozens of Taliban militants killed?",
      "prediction": "Sunday,",
      "ground_truths": [
        "Sunday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4445a2b47bb1498fa032751e2d3f0224",
      "question": "For what reason were extra police deployed around Kabul?",
      "prediction": "89th observance of Afghanistan's independence from Great Britain.",
      "ground_truths": [
        "Monday's 89th observance of Afghanistan's independence from Great Britain."
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "1cb0b177021d4e22aba08856e9a81ee3",
      "question": "Independence from what?",
      "prediction": "Great Britain.",
      "ground_truths": [
        "Great Britain."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "66485f565d3c4c248cafd2ef5708d9d7",
      "question": "How many officers were killed?",
      "prediction": "5",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad75ca9e4c034d1f99fcf36517260cc5",
      "question": "Who is to blame for the bomb?",
      "prediction": "the Taliban and their al Qaeda associates",
      "ground_truths": [
        "Taliban and their al Qaeda associates"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f513129872a41228cb7a0aada0f81d5",
      "question": "What city has extra police officers?",
      "prediction": "Kabul",
      "ground_truths": [
        "Kabul"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1958d14b7db54bffbbdd57e3677c08e7",
      "question": "What was his novel called?",
      "prediction": "\"Empire of the Sun\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eeef45fe8f664128b6f49991e5695c3e",
      "question": "What did he die from?",
      "prediction": "cancer",
      "ground_truths": [
        "cancer"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "96b2b16f84e8465d9de71f9fa9718941",
      "question": "When did he start writing fiction?",
      "prediction": "the 1950s,",
      "ground_truths": [
        "1950s,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f1a03dd30304609a4e646069b0deffd",
      "question": "What was the name of the film",
      "prediction": "\"Empire of the Sun,\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6071cc99d5744a17814ad38e72b53f3b",
      "question": "What did he write about?",
      "prediction": "his boyhood experience in a World War II internment camp became the novel and film \"Empire of the Sun,\"",
      "ground_truths": [
        "boyhood experience in a World War II internment camp"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "d5012bef02214f0c9698b7a828959e39",
      "question": "What book was made into a film?",
      "prediction": "\"Empire of the Sun\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8ff3ddfe7f3a41e294a831e452189dd5",
      "question": "What books did he wrote",
      "prediction": "\"Empire of the Sun\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c46c0d1751554ccea747cb3825bdbe66",
      "question": "Who died from cancer?",
      "prediction": "J.G. Ballard,",
      "ground_truths": [
        "British"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18cc95b228434b23a3969ac0d65687fa",
      "question": "What did they write",
      "prediction": "\"Empire of the Sun\"",
      "ground_truths": [
        "\"Empire of the Sun,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "504074fdd18a4bfdb3c4d8b3d6a4b589",
      "question": "which player was the favourite",
      "prediction": "Venus Williams",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc9ff2ab888d45b79fa4959911c65e59",
      "question": "which title number was this",
      "prediction": "23-year-old",
      "ground_truths": [
        "44th"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c5b80b9f943d4bd3b11c84700bff495f",
      "question": "When did the Madrid Open final take place?",
      "prediction": "Sunday",
      "ground_truths": [
        "Sunday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "368647e758554203bcd6310b0cb41a9f",
      "question": "what was the victors seed",
      "prediction": "Aravane Rezai",
      "ground_truths": [
        "Unseeded"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c551f0e759174fbc80c4d851c51c31fd",
      "question": "What age is Rezai?",
      "prediction": "23-year-old",
      "ground_truths": [
        "23-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "52f45355af90482cb6a950e995783d4b",
      "question": "Who was favourite to win?",
      "prediction": "Venus Williams",
      "ground_truths": [
        "Venus Williams"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba4602ee540d4fcfb52af541b3e7dfd8",
      "question": "What Aravane Rezai claims the player?",
      "prediction": "WTA Tour titles",
      "ground_truths": [
        "WTA Tour titles"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b997384c17614f7ba1954363aa86dade",
      "question": "where was the shooting",
      "prediction": "Little Rock military recruiting center",
      "ground_truths": [
        "Arkansas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50cc5fd5df684303b18d21732c69c85e",
      "question": "what was the motive?",
      "prediction": "\"political and religious\"",
      "ground_truths": [
        "\"political and religious\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "acc714436403401f87f020ab900438cd",
      "question": "What happened in Monday's shooting?",
      "prediction": "killing one soldier and wounded another",
      "ground_truths": [
        "killed one soldier and wounded another at a Little"
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "df4b2f48ebd849209e17392d1f220418",
      "question": "What do the police say?",
      "prediction": "The suspect had been under investigation after visiting Yemen, a federal law enforcement official said. The official declined to provide further information.",
      "ground_truths": [
        "He faces one count of capital murder and 16 counts of engaging in a terrorist act,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49db254b761e4e4199cd037cf89f3575",
      "question": "what was the motives",
      "prediction": "\"political and religious\"",
      "ground_truths": [
        "angry over the treatment of Muslims,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7e4e50ab633a4bd79a08dcc39d7a6b00",
      "question": "where did this happen?",
      "prediction": "Little Rock",
      "ground_truths": [
        "Rock military recruiting center"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "1e425208f97249e5a81e6454cc03f5e1",
      "question": "What television showed footage?",
      "prediction": "CNN",
      "ground_truths": [
        "Turkish"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e806b10c581643d7aec4fe0c185c60e8",
      "question": "Did the women sign contracts?",
      "prediction": "required them to pay fines of more then $30,000 if they left the show before it completed filming.",
      "ground_truths": [
        "were signed with the contestant girls,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c149bbaa57fb45f1b8dcf83ce591934f",
      "question": "How many young girls were returned?",
      "prediction": "eight or nine",
      "ground_truths": [
        "eight or nine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f7d5f07f09814ae6bde8fd535228203f",
      "question": "What is name of the place with the best burger?",
      "prediction": "Booches Billiard Hall,",
      "ground_truths": [
        "Booches Billiard Hall,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "daf9ef11c5214622aff43957272a6899",
      "question": "What sport is Edwards in?",
      "prediction": "NASCAR",
      "ground_truths": [
        "NASCAR"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2321a5071574ea5bc97d8ee97b3358f",
      "question": "Which state does Edwards talk about his favorite spots?",
      "prediction": "Missouri.",
      "ground_truths": [
        "Columbia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6a3cfa2f24494994bb285aa0a444492e",
      "question": "Where is the best burger?",
      "prediction": "Booches Billiard Hall,",
      "ground_truths": [
        "Booches Billiard Hall,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f28b7801ac5e4385a6e8999492e8974f",
      "question": "What did the Tech crunch founder call silicone valley?",
      "prediction": "pure meritocracy,",
      "ground_truths": [
        "a pure meritocracy,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc82210865b84c0fba91c2ec5b5f534d",
      "question": "What Hank Williams said about founder of TechCrunch?",
      "prediction": "\"You people don't make good CEOs.\"",
      "ground_truths": [
        "I think Arrington truly believes everything he has"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6f86cbec527c4c75862e16b938872404",
      "question": "Who has been involved in a Twitter spat?",
      "prediction": "Michael Arrington,",
      "ground_truths": [
        "Michael Arrington,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2c9ff035684f49099a2147c4f17a2992",
      "question": "Who called Silicon Valley a meritorcracy?",
      "prediction": "Vivek Wadhwa,",
      "ground_truths": [
        "Michael Arrington,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "349744fdc69c490ba50ba01f9d31f9a3",
      "question": "Who said it was not so?",
      "prediction": "Vivek Wadhwa,",
      "ground_truths": [
        "Vivek Wadhwa,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "24809353158b4ac89851907981f74f68",
      "question": "Who turned fire hoses on vessel?",
      "prediction": "Impeccable",
      "ground_truths": [
        "The Impeccable"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f7de5ba2f53f4a35af0e9cfad20792e5",
      "question": "What was the Navy surveillance vessel  violating?",
      "prediction": "Chinese laws",
      "ground_truths": [
        "Chinese and international laws"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0f06f03e9d4d42aca200b9604dc7efc6",
      "question": "On what date did the fire hose incident take place?",
      "prediction": "March 8",
      "ground_truths": [
        "March 8"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a7d83f75668d49f4ba1ee28570d5b09d",
      "question": "What did China say was violating Chinese and International law?",
      "prediction": "Impeccable,",
      "ground_truths": [
        "the ship"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ef734b3df1aa44bd9e1f32f8f4da6f19",
      "question": "who has more marine power",
      "prediction": "U.S. ocean surveillance ship",
      "ground_truths": [
        "Chinese"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fb2d1f54601e4f89be70d07393db45f4",
      "question": "Who launched the Nook Tablet?",
      "prediction": "Barnes & Noble",
      "ground_truths": [
        "Barnes & Noble CEO William Lynch"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "17a72d69d7fd4e2fb9c562451e5434ef",
      "question": "Who has launched the Nook tablet?",
      "prediction": "Barnes & Noble",
      "ground_truths": [
        "Barnes & Noble"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5b7387ec945d47b8b624c2b55b6a80c1",
      "question": "Which tablet is the most popular?",
      "prediction": "Kindle Fire",
      "ground_truths": [
        "Apple iPad,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "30913ce681084f3f8357bef6147fe049",
      "question": "How much does the Nook cost?",
      "prediction": "$249",
      "ground_truths": [
        "$249"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6d34f597fdc54dd2aa1a8a9e32e596cb",
      "question": "Is the Kindle Fire cheaper than the Nook Tablet?",
      "prediction": "$199",
      "ground_truths": [
        "costs $50 less,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5f0273d41ad45ab9a9819cb49e29b02",
      "question": "How much cheaper is Amazon's Kindle fire?",
      "prediction": "$50",
      "ground_truths": [
        "$50"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "92100c8dcef141f69ab22ef3ad13d9d5",
      "question": "Name Ali's wife?",
      "prediction": "Lonnie",
      "ground_truths": [
        "Lonnie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "112542d377134435a5e6e79b2e9a5c1a",
      "question": "What did Ali have to say?",
      "prediction": "\"Louisville Lip.\"",
      "ground_truths": [
        "not speak"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6029989106ef4c32bc7eb342670dc8c1",
      "question": "What did Ali unveil in town?",
      "prediction": "a plaque",
      "ground_truths": [
        "plaque"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8eacd2be836f472a83d7008f33ed8506",
      "question": "Where is Muhammad Ali's ancestral town?",
      "prediction": "County Clare",
      "ground_truths": [
        "Ennis, County Clare"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "01b98eb2ff194b198d052f759f6f12bb",
      "question": "What do nutritionists warn?",
      "prediction": "against using injectable vitamin supplements",
      "ground_truths": [
        "against using injectable vitamin supplements"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3257fb95269946feab9eeec2ebcd3522",
      "question": "What do the injections target?",
      "prediction": "thougthththththththththththththththththththththththththththth",
      "ground_truths": [
        "a particular health ailment or beauty concern."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0e8a39c6c38742ef85f518e5746185c5",
      "question": "What are nutritionists warning of?",
      "prediction": "cautions against using injectable vitamin supplements because the quantities are not regulated.",
      "ground_truths": [
        "using injectable vitamin supplements"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "9c4719555e46425496a22559eff66115",
      "question": "What is happening in Japan?",
      "prediction": "IV cafe.",
      "ground_truths": [
        "customers are lining up for vitamin injections that promise"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "547765c1158b4a5a8100d6a300aa88b1",
      "question": "Where are intravenous vitamin boosts the latest fad?",
      "prediction": "Tokyo",
      "ground_truths": [
        "Japan:"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "97cb3c77ddba4ab9ae6bfdc00eb09c42",
      "question": "What is the health fad in Japan?",
      "prediction": "intravenous vitamin \"drips\"",
      "ground_truths": [
        "intravenous vitamin \"drips\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e504aa46a10248e8b18cb4639dce0467",
      "question": "Who are vitamin injections popular among?",
      "prediction": "celebrities with hectic lifestyles and little time to sleep,",
      "ground_truths": [
        "Tokyo customers are lining up for"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89abc7236e6941ee88a263d88d6d299a",
      "question": "What do vitamin injections do?",
      "prediction": "to improve health and beauty.",
      "ground_truths": [
        "improve health and beauty."
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "94175e0177bc46c1929d3ecf1b017fa9",
      "question": "Who uses vitamin injections?",
      "prediction": "Japanese businessmen",
      "ground_truths": [
        "Tokyo customers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3e064f85e8d242b485d48b6b7d0c45b9",
      "question": "Who called the father/",
      "prediction": "Jennifer",
      "ground_truths": [
        "David Russ,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "440e77465a4c487a86f1eda1727f70a2",
      "question": "Who says inmate's information is nothing new?",
      "prediction": "Sgt. Jones",
      "ground_truths": [
        "Sgt. Barbara Jones"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "93a06e7bf23e44ce95a1bbf50f8604c9",
      "question": "when did she disappear?",
      "prediction": "January 24, 2006.",
      "ground_truths": [
        "January 24, 2006."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c134778e95754456b71869ea92b80b5d",
      "question": "Who was told to change course prior to crash?",
      "prediction": "pilot of an Ethiopian Airlines flight",
      "ground_truths": [
        "flight"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "248d9e97bb0543c9a394d488ac6b48ce",
      "question": "How many bodies have been found so far?",
      "prediction": "14",
      "ground_truths": [
        "14"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5d5743c3626d45d5934e0dddc83e958d",
      "question": "What do you tell the pilot before crashing?",
      "prediction": "change course",
      "ground_truths": [
        "to change course"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "058520090325403b9c133b6f1b9c2e49",
      "question": "How many bodies were found so far?",
      "prediction": "14",
      "ground_truths": [
        "14"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60dbe06ef944496d8be8be4bed4b5b12",
      "question": "The pilot was told to do what, prior to the crash?",
      "prediction": "change course",
      "ground_truths": [
        "change course"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e444494f4df6449e89599fdaea3f84f7",
      "question": "What combs Lebanese coast following Ethiopian Airlines plane crash?",
      "prediction": "An international search team",
      "ground_truths": [
        "international search team"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bd84ca081b0c45ea8d37bee8cec35ce2",
      "question": "Which airline owns the plain which crashed?",
      "prediction": "Ethiopian",
      "ground_truths": [
        "Ethiopian"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f67d24b577114f6aac716d82d82371ae",
      "question": "How many people have been rescued?",
      "prediction": "90",
      "ground_truths": [
        "No survivors"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d5228e62244542428fd65303ec23dd11",
      "question": "How many life-size models are ther?",
      "prediction": "three",
      "ground_truths": [
        "three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e385644e99a94097915a45a834d84dbb",
      "question": "What can visitors explore?",
      "prediction": "18th-century sights, sounds, and scents.",
      "ground_truths": [
        "the estate"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4e7c7187fe8a4636ac095b31bfc91433",
      "question": "how many people",
      "prediction": "visitors",
      "ground_truths": [
        "700"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "416153d5b74943cf889a0e53fc6c7857",
      "question": "At what ages do the models depict Washington?",
      "prediction": "19,",
      "ground_truths": [
        "19,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5c7129352d6443beb845ecd46a492c0a",
      "question": "who is depicted in the new models",
      "prediction": "George Washington",
      "ground_truths": [
        "George Washington"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a420f8f42204d968a137faeb1d73784",
      "question": "Where could the weapons reach?",
      "prediction": "Alaska or Hawaii.",
      "ground_truths": [
        "U.S. military bases in the Pacific Ocean territory of Guam"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aece58a299d84cd88579bca72c73da46",
      "question": "How many miles?",
      "prediction": "about 4,200",
      "ground_truths": [
        "(1,900 miles),"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7f00e9c077324f689c8b0746c6381e0e",
      "question": "How far can the missiles travel?",
      "prediction": "about 3,000 kilometers (1,900 miles),",
      "ground_truths": [
        "3,000 kilometers (1,900 miles),"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "692352f2fd7d4a389c1f1ca96570be32",
      "question": "Where are tensions running high",
      "prediction": "between Pyongyang and Seoul",
      "ground_truths": [
        "Pyongyang and Seoul"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "0754e951855e46df80b0269253e4acf7",
      "question": "Weapons could reach what places?",
      "prediction": "Alaska or Hawaii.",
      "ground_truths": [
        "U.S. military bases in the Pacific Ocean territory of Guam"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8f3827c3491247c7ab4d7706cbb8ef6d",
      "question": "Tensions are running high where?",
      "prediction": "between Pyongyang and Seoul",
      "ground_truths": [
        "between Pyongyang and Seoul"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "da54c67b92b74a2c91d2227d09a41438",
      "question": "Who has the weapons?",
      "prediction": "North Korea",
      "ground_truths": [
        "United States, Japan, Russia, South Korea"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "dc5438680eeb4547b962a6d05a58125d",
      "question": "Which US state could the missile reach",
      "prediction": "Alaska or Hawaii.",
      "ground_truths": [
        "Alaska or Hawaii."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4dd31c6180e64c4eb59983819779cf2c",
      "question": "How far can the new missiles travel",
      "prediction": "3,000 kilometers",
      "ground_truths": [
        "3,000 kilometers (1,900 miles),"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "eed4baf04e844c1a88bac329ce16c664",
      "question": "How many were wounded?",
      "prediction": "30",
      "ground_truths": [
        "30"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2f351229f6cd4761b7b0d45e3594031e",
      "question": "How many were killed?",
      "prediction": "10",
      "ground_truths": [
        "a dozen people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c0580a5ffe5e41cd8e035466823d8d9b",
      "question": "Where the bomb was detonated?",
      "prediction": "near the village of Dara Bazar in the Bajaur Agency,",
      "ground_truths": [
        "Quetta,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "786765cfe8b44449af887152402aaa29",
      "question": "In what cities were the protests?",
      "prediction": "Montreal, Toronto,",
      "ground_truths": [
        "various"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8cd035648dd64edb912f01d1b01e6b89",
      "question": "What languages were used",
      "prediction": "Arabic, French",
      "ground_truths": [
        "Arabic, French and English,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8444096a84e9455287180d0b1a928142",
      "question": "In what languages were the protest slogans?",
      "prediction": "Arabic, French",
      "ground_truths": [
        "Arabic, French and English,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a2f796f7536947518cb11c541c564c83",
      "question": "Protests are held in London, Geneva and where else?",
      "prediction": "Egypt",
      "ground_truths": [
        "Canada."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "60f332bc68e64d8680e179e359b0d6fa",
      "question": "In Geneva, people chant slogans in Arabic, French and which other language?",
      "prediction": "English,",
      "ground_truths": [
        "English,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a1747749a3904c53b124b17ba86b1df6",
      "question": "Texas is awarded cash relief for what?",
      "prediction": "series of wildfires from late summer through autumn in Bastrop County.",
      "ground_truths": [
        "a series of wildfires"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "29b82a59344f4d38854c476a31827ba7",
      "question": "What are the funds based on?",
      "prediction": "hurricanes",
      "ground_truths": [
        "housing, business and infrastructure repairs,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "894cfdb45c9149f6bb42906c842ec632",
      "question": "What states receive aid for tornadoes?",
      "prediction": "Missouri",
      "ground_truths": [
        "New York"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "527a875dd5504c12b1e26cec3ff31fb6",
      "question": "Which states received aid for tornadoes?",
      "prediction": "Alabama",
      "ground_truths": [
        "Alabama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cf94611bbdb74f89b38fd1ac06c27777",
      "question": "What year's disasters are the basis for the funds?",
      "prediction": "2011.",
      "ground_truths": [
        "2011."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab7f7e70dde447fcaa29fdc20ba659db",
      "question": "what were The funds are based on?",
      "prediction": "natural disasters in 2011.",
      "ground_truths": [
        "natural disasters"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "500e35b80a13424c8eb9c6e6a0b9af15",
      "question": "Alabama and Missouri receive aid for what?",
      "prediction": "natural disasters",
      "ground_truths": [
        "natural disasters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6bdd81ddfa4d41f9a8dac2c1ce206945",
      "question": "What is Texas receiving aid for?",
      "prediction": "a series of wildfires from late summer through autumn in Bastrop County.",
      "ground_truths": [
        "a series of wildfires"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "7f271dc40b1540828bc913ffc1d7dbf4",
      "question": "Who ran his political campaigns?",
      "prediction": "Gov. Mark Sanford,",
      "ground_truths": [
        "Jenny Sanford"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "68aacaf01d6848be9738b0e986cb1236",
      "question": "who is Jenny Sanford?",
      "prediction": "wife of Gov. Mark",
      "ground_truths": [
        "the wife of Gov. Mark"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "861c2f0524104e6291b91efcea22adc2",
      "question": "What is he avoiding?",
      "prediction": "public exposure.\"",
      "ground_truths": [
        "public exposure.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7ded71c940b4408794baf52361a3e87d",
      "question": "Who is avoiding media?",
      "prediction": "Gov. Mark Sanford,",
      "ground_truths": [
        "Jenny Sanford,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c12ef3e9f94740edb9565c036be79d3e",
      "question": "What was called \"novel that you would be embarrassed to buy\"?",
      "prediction": "e-mails",
      "ground_truths": [
        "\"[The e-mails]"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "890f0fb25e9545798b2f743b121d3c69",
      "question": "What were the e-mails called?",
      "prediction": "\"email\"",
      "ground_truths": [
        "romantic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "897068e062c44e01add9c98a80eb63d0",
      "question": "Who died of cancer at age 86?",
      "prediction": "Bea Arthur,",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8bd2d3430c07471e886b1dfe30b061e8",
      "question": "Who has died at 86?",
      "prediction": "Arthur,",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6c779c5d72af4eb69b69977fde49e8ff",
      "question": "Who is survived by two sons and grandchildren?",
      "prediction": "Arthur,",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d16493e3db7c4bd5b494934febcb82b4",
      "question": "What did she die from?",
      "prediction": "cancer,",
      "ground_truths": [
        "cancer,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "49ab77dcd0394a898aec0b35440b572a",
      "question": "What tv show did she star in?",
      "prediction": "\"Maude\"",
      "ground_truths": [
        "\"The Golden Girls,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9fdc5aba0df54f148cc213911a81cd37",
      "question": "who was at home with family in Los Angeles?",
      "prediction": "Bea Arthur,",
      "ground_truths": [
        "Arthur,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1041e2ddf5454bc9abd397681fbaf1e4",
      "question": "What did Burnette say he threw at the shotter?",
      "prediction": "folding table",
      "ground_truths": [
        "folding table"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bdb30a392d79453992b27a1cae25efec",
      "question": "Where is Burnette being housed?",
      "prediction": "Fort Hood,",
      "ground_truths": [
        "Fort Hood, Texas,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "02bb9745c7f8413fa1c809402b8ea81f",
      "question": "Where did the nightmare day take place?",
      "prediction": "Fort Hood, Texas,",
      "ground_truths": [
        "Fort Hood,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "732c42970ce142f8a28b7f3d5e3695d5",
      "question": "How many people were shot on Spc. Logan Burnette's original day at Fort Hood?",
      "prediction": "Four in intensive care,",
      "ground_truths": [
        "Twelve soldiers and one civilian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ec338260290400aa6b3ae6685666a80",
      "question": "Where did the shooting take place?",
      "prediction": "Fort Hood, Texas,",
      "ground_truths": [
        "Fort Hood"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "6368dd06f186478dae5ba3271569ef66",
      "question": "What says official about opposition?",
      "prediction": "the power-sharing deal with the MDC offshoot is part of larger deal that has not been signed by anyone.",
      "ground_truths": [
        "deciding the duties of the new prime minister has been a sticking point in the negotiations."
      ],
      "em": 0,
      "f1": 0.20689655172413793
    },
    {
      "id": "d313b671236c48f3b029c72fbc46971b",
      "question": "Who denies discussions?",
      "prediction": "Zimbabwean President Robert Mugabe",
      "ground_truths": [
        "Mugabe's opponents"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "36ab12ff07cf46ed9af03891964f20de",
      "question": "Who is not part of the deal?",
      "prediction": "Morgan Tsvangirai.",
      "ground_truths": [
        "Thabo Mbeki,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d468e9a904314a5a92582bcfcc7b7635",
      "question": "Who makes power-share deal with splinter group?",
      "prediction": "Mugabe's",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "374f14d949354e9aa9bdcfdc090433d7",
      "question": "Who denies he's out of power-sharing discussions?",
      "prediction": "Mugabe's",
      "ground_truths": [
        "Mugabe's opponents"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7e7a5a32f59b49ccacc7c1408799a32a",
      "question": "Who made a power-share deal with a splinter group?",
      "prediction": "Zimbabwean President Robert Mugabe",
      "ground_truths": [
        "Robert Mugabe"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "6eeccb736c5f4d72944f4c46e4859c9d",
      "question": "Who is  not part of deal, official says?",
      "prediction": "Mugabe's",
      "ground_truths": [
        "Morgan Tsvangirai."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa15e1b710b94eac8122358024750aa8",
      "question": "What was Hopper dying of?",
      "prediction": "prostate cancer,",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88be8aa0f5fc4d13b318a5925f63eddf",
      "question": "What illness is Hopper stricken with?",
      "prediction": "prostate cancer,",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed21c1e730a24590bd0d16f6125a1af1",
      "question": "What did Hopper's doctor recommend?",
      "prediction": "radical chemotherapy",
      "ground_truths": [
        "has to do with his estranged wife at this time, the more likely he is to have his life extended.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cc24ab9ce0b40c5a4adec0fc985ea81",
      "question": "Which cable network paid for Hopper's star?",
      "prediction": "Starz",
      "ground_truths": [
        "Starz"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "65786f4f7af04a3bb1bfff6044b8d526",
      "question": "What is Hopper dying of?",
      "prediction": "prostate cancer,",
      "ground_truths": [
        "prostate cancer,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "75962804944c4efcb636e1c78acfd974",
      "question": "What country does Bill Gates thinks has made great progress in recent times?",
      "prediction": "Italy,",
      "ground_truths": [
        "Africa."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b4774d59215b4aa5935ff8e40927f723",
      "question": "Which couple sends billions of dollars to developing nations?",
      "prediction": "Bill Gates",
      "ground_truths": [
        "Bill & Melinda Gates"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "0b05e48090e341199366a661b7723199",
      "question": "what does the bill & melinda gates foundation do",
      "prediction": "help nations trapped by hunger and extreme poverty,",
      "ground_truths": [
        "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid"
      ],
      "em": 0,
      "f1": 0.6956521739130436
    },
    {
      "id": "3e039dbfc29f4a16ba51c79e9be4d0d6",
      "question": "Was she abducted alone?",
      "prediction": "captured",
      "ground_truths": [
        "The gunmen also took hostage Lunsmann's 14-year-old son, Kevin, and her 19-year-old Filipino nephew, Romnick Jakaria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6916c3d355a940bb9458167bb9826ff8",
      "question": "When was she abducted?",
      "prediction": "July",
      "ground_truths": [
        "July"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "03c09d9f7c2f45138f9f2d5063ed55f0",
      "question": "where were they abducted from?",
      "prediction": "Philippines",
      "ground_truths": [
        "Philippines"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b27e0d27887439e8efe5b6a46bbc236",
      "question": "when was she taken?",
      "prediction": "July",
      "ground_truths": [
        "July"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eb7f292ef67f47e1bb188b922f6b4cab",
      "question": "what island was she released on?",
      "prediction": "Basilan",
      "ground_truths": [
        "Suba Kampong township"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "85924e5341834bbaa0625029a2d0e1a8",
      "question": "They paid ransom for the liberation of women?",
      "prediction": "was not immediately known",
      "ground_truths": [
        "declined to confirm whether officials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "78c8514c2c314cfabb1b15b88add2d64",
      "question": "What country is this show being aired?",
      "prediction": "UK",
      "ground_truths": [
        "UK"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a00ed0e35e5443f2b7916e3da4dc143e",
      "question": "What has been compared to reality show Big Brother?",
      "prediction": "Boys And Girls Alone",
      "ground_truths": [
        "Boys And Girls Alone"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0c5863f36ce4b2f9ffd2a1207d65f11",
      "question": "what kind of show is it",
      "prediction": "Boys And Girls Alone",
      "ground_truths": [
        "reality"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b52a67e9b1f3432aa139bd8fad4c99d5",
      "question": "What is the name of the show?",
      "prediction": "Boys And Girls Alone",
      "ground_truths": [
        "Boys And Girls Alone"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b90c21ab99a94720afca525bb58500b7",
      "question": "What does program show?",
      "prediction": "how children as young as eight would cope without their parents for two weeks.",
      "ground_truths": [
        "allows 10 boys and 10 girls between the age of eight and 11 to create their own mini-societies, organizing everything from"
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "34541edc94984ce2b60e211b15de0acc",
      "question": "is this show legal",
      "prediction": "\"This is not a project for commercial gain. It is done with the parents' full consent,\"",
      "ground_truths": [
        "the program was made with the parents' full consent."
      ],
      "em": 0,
      "f1": 0.38095238095238093
    },
    {
      "id": "a08a4ab897de471ba93b91ce672e62c3",
      "question": "What type of show is this being compared to?",
      "prediction": "Big Brother.",
      "ground_truths": [
        "adult reality"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ffe71377d09d4ca3acfdf061885f6496",
      "question": "who is making the show",
      "prediction": "Channel 4",
      "ground_truths": [
        "Channel 4"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a838e3905c2641b6a15817e1647641be",
      "question": "Who traveled to India?",
      "prediction": "Christiane Amanpour",
      "ground_truths": [
        "Christiane Amanpour"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "62644011bef24c2d963961879c34902c",
      "question": "When did the Dalai Lama flee Tibet?",
      "prediction": "1959.",
      "ground_truths": [
        "1959."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a10f8733a55144f2b2c4fe81f2478cf1",
      "question": "what does the dalai lama do",
      "prediction": "cracked jokes and chatted",
      "ground_truths": [
        "cracked jokes and chatted"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b0a22b1eed43466983c5b9edaf1fa825",
      "question": "In what year did Dalai Lama flee?",
      "prediction": "1959.",
      "ground_truths": [
        "1959."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c0514fdee611444382522aa5246f03e2",
      "question": "Where did Amanpour travel?",
      "prediction": "Dharamsala, India.",
      "ground_truths": [
        "Dharamsala, India."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "12710fddf22f49c388bb51066604fe09",
      "question": "Who made recent attacks?",
      "prediction": "The Movement for the Emancipation of the Niger Delta",
      "ground_truths": [
        "Militants from the Movement for the Emancipation of the Niger Delta,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "0e8ea585b9e94a22bde67b4af2d1af8e",
      "question": "Which country is Africa largest oil producer?",
      "prediction": "Nigeria,",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af0ad17709c94941a8928f11ff29d3c6",
      "question": "where is the game played?",
      "prediction": "the Niger Delta region",
      "ground_truths": [
        "Nigeria"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "da6f0e91b9044dc89759497adae15cf1",
      "question": "when is the tournament to take place?",
      "prediction": "between October 24 and November 15,",
      "ground_truths": [
        "between October 24 and November 15,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c2821cf5d9a04d9187f12a4087a1cd72",
      "question": "Which country is Africa's largest oil producer?",
      "prediction": "Nigeria",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "79ec6637b0cf49ae8a75b26bbfb11ff8",
      "question": "Attacks by who have limited supplies?",
      "prediction": "Militant",
      "ground_truths": [
        "MEND"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ffa942875124cafb8cf0eb5a87392ef",
      "question": "where is oil produced?",
      "prediction": "Niger Delta",
      "ground_truths": [
        "Nigeria,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0de2319530fa45f99a79b4c4333c9ff1",
      "question": "What is five hours from Berlin by train?",
      "prediction": "Sylt",
      "ground_truths": [
        "Sylt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c7a1cbd3aa6e458db4b66664e2107e0c",
      "question": "What is Sylt known for?",
      "prediction": "its nude beaches.",
      "ground_truths": [
        "tranquil beaches,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b68565e5c8a54a72889714bc0ded15d8",
      "question": "What attracts German celebrities to the island?",
      "prediction": "The island's dining scene",
      "ground_truths": [
        "dining scene"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "d524dd73ccf34b32a7c8c5bbb6935f55",
      "question": "What  is five hours from Berlin by train?",
      "prediction": "Sylt",
      "ground_truths": [
        "Sylt"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6f9255058a62405f911a081a2ee6e251",
      "question": "What attracts a number of German celebrities?",
      "prediction": "The island's dining scene",
      "ground_truths": [
        "The island's dining scene"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "91ee88ab11164eb9ae6b7520537baffb",
      "question": "Have the teens charged pled guilty?",
      "prediction": "not guilty",
      "ground_truths": [
        "All three pleaded not"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "b14a0ef7a9484617b9a6551c0f7f802d",
      "question": "What happens to 15-year old Michael Brewer?",
      "prediction": "being burned over 65 percent of his body",
      "ground_truths": [
        "burned over 65 percent of his body after being set on fire,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "dd2beb51267c4da6823dcdd446ab8e49",
      "question": "What is he healing from?",
      "prediction": "second- and third-degree burns over about two-thirds of his body,",
      "ground_truths": [
        "burned over 65 percent of his body"
      ],
      "em": 0,
      "f1": 0.47058823529411764
    },
    {
      "id": "e8ea4ee3633f4b46bbe40f20c3c015be",
      "question": "What teen uses to get through painful therapy?",
      "prediction": "Ozzy Osbourne",
      "ground_truths": [
        "\"He focuses on Ozzy,"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "0302a6e98184410dbc50f961abf3486c",
      "question": "Whose CD does the teen use during therapy?",
      "prediction": "Ozzy Osbourne",
      "ground_truths": [
        "Ozzy Osbourne"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88a599b815dd4e5898e8b2e840fef6ef",
      "question": "How many attacked Michael Brewer?",
      "prediction": "13-year-old",
      "ground_truths": [
        "three"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6520a4ec91834f1db374c4bded23c4cf",
      "question": "What is observation of mother of burned teen?",
      "prediction": "pain is excruciating at times.",
      "ground_truths": [
        "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\""
      ],
      "em": 0,
      "f1": 0.17647058823529413
    },
    {
      "id": "e9e6e952876b4c2b864f9c5fd00a3d8a",
      "question": "Which commodity will have its price frozen?",
      "prediction": "petroleum",
      "ground_truths": [
        "gasoline"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fcccb4baf6314a589f6725cf1dc37979",
      "question": "What size companies will be helped?",
      "prediction": "small- and medium-size",
      "ground_truths": [
        "small- and medium-size"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2bffb804aab041ee9605adbff83fe141",
      "question": "Who is the Mexican leader that's being quoted?",
      "prediction": "President Felipe Calderon",
      "ground_truths": [
        "President Felipe Calderon"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6803dc0693214791b3058a87480b15d6",
      "question": "What prices will be frozen for the rest of the year?",
      "prediction": "gasoline",
      "ground_truths": [
        "gasoline"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "299726f939f0420aaeb6e5f33a325d39",
      "question": "Who will be aided by the plan?",
      "prediction": "Mexicans",
      "ground_truths": [
        "unemployed Mexicans,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e412730b37274c11bad105b38ed9e196",
      "question": "Number of pesos that will be spent?",
      "prediction": "2.2 billion",
      "ground_truths": [
        "570 billion"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "a645bb94c6194b119832d8e395b47a0c",
      "question": "What else did the plan also call for?",
      "prediction": "in better shape this year to fight off recession than it was in previous instances.",
      "ground_truths": [
        "rebuild the nation's highways, bridges and other public-use facilities."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2ea97cfbe22340cdabec8b67b75b0f0e",
      "question": "How many pesos will be spent?",
      "prediction": "570 billion",
      "ground_truths": [
        "570 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1f2ee8a4ce5d448e8ed30e7ca27b2b32",
      "question": "How long will the prices be frozen?",
      "prediction": "the rest of the year",
      "ground_truths": [
        "for the rest of the year"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "7d08f62337054caa8e0104c19a653158",
      "question": "What is Mexican currency called?",
      "prediction": "Pesos",
      "ground_truths": [
        "pesos"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1aa0b7493d9b47bbb4625d39922576b6",
      "question": "How much money will be spent?",
      "prediction": "570 billion pesos ($42 billion).",
      "ground_truths": [
        "570 billion pesos ($42 billion)."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f6d66062548343c2a3fa727cb5f78e2e",
      "question": "What happened to the ice jam?",
      "prediction": "explosives",
      "ground_truths": [
        "Explosives are set off in the Missouri River"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "f7d944ddbf964356a88835f02f839e78",
      "question": "When are they hoping the sandbag goal in Fargo will be met?",
      "prediction": "Thursday.",
      "ground_truths": [
        "Thursday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d966325ec53b4bc6b6351a5f7b0ee6e6",
      "question": "Where were residents rescued after residential dike gives way?",
      "prediction": "Oxbow, a town of about 238 people,",
      "ground_truths": [
        "Oxbow,"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "f09bb7c330874e1493fe95436c2566dc",
      "question": "What goal is Fargo halfway to?",
      "prediction": "shore up.\"",
      "ground_truths": [
        "volunteers had filled more than 1 million sandbags out of the needed 2 million."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "776c4ccd1f4e44a1a89ddbf4aacdd8c2",
      "question": "What complicates preparations?",
      "prediction": "Snow,",
      "ground_truths": [
        "Snow,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6530fe0dc1774470b59562813af55aab",
      "question": "Where was the ice jam?",
      "prediction": "Missouri River",
      "ground_truths": [
        "south of"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5fdc81c791924ab6875608a54b1b6581",
      "question": "What gave way south of Fargo?",
      "prediction": "a residential dike",
      "ground_truths": [
        "a residential dike"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a6e3b10812dd41d488e85954b27fb762",
      "question": "What is the sandbag goal?",
      "prediction": "2 million.",
      "ground_truths": [
        "2 million."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "856b14030ac342bb979ca5091377cff0",
      "question": "Who is the prime minister?",
      "prediction": "Manmohan Singh's",
      "ground_truths": [
        "Manmohan Singh's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2e7792c3711b4a139cf99e7dec1b7e26",
      "question": "who is seeking re-election?",
      "prediction": "Prime Minister Manmohan Singh's Congress party,",
      "ground_truths": [
        "Prime Minister Manmohan Singh's"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "8dbad2970e8945f0a4595ccae78e57e9",
      "question": "what is the prime minister name?",
      "prediction": "Manmohan Singh's",
      "ground_truths": [
        "Manmohan Singh's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f8b5c43adc974d82aaab3f81649ca754",
      "question": "What is the number of states that voting was spread across?",
      "prediction": "28",
      "ground_truths": [
        "28"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b2a92b434a344e49b3a6911bfd86e98f",
      "question": "Whose party is seeking re-election?",
      "prediction": "Prime Minister Manmohan Singh's Congress",
      "ground_truths": [
        "Prime Minister Manmohan Singh's Congress"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0db9e6fa3f21435996d0bf1e3b3f8adb",
      "question": "What country is voting?",
      "prediction": "India",
      "ground_truths": [
        "India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "044418fd6fd646ec89a54a7013712c9d",
      "question": "till how far Voting in the month-long exercise was spread ?",
      "prediction": "28 states and seven federal territories.",
      "ground_truths": [
        "across 28 states and seven federal territories."
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "2324b8d221fa41d58267b515b1a1109a",
      "question": "How many states voted during the month long exercise?",
      "prediction": "28",
      "ground_truths": [
        "28"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "092f821b8fd243efb2bfd2f200cb99ee",
      "question": "What brand were most of the sport cars?",
      "prediction": "Ferrari,",
      "ground_truths": [
        "Ferraris,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5a1d3756a44c4a5ba9b30237dd1c807f",
      "question": "what was the tag of Lamborghini?",
      "prediction": "\"BADBUL,\"",
      "ground_truths": [
        "\"BADBUL,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "02e40f5399fb4b2386f784865610ff5a",
      "question": "how many exotic sport cars?",
      "prediction": "six",
      "ground_truths": [
        "six"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "109740b4edfa49c5961632436d1df48c",
      "question": "What is the fine for speed racing?",
      "prediction": "$627,",
      "ground_truths": [
        "$627,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9c03696110b94d56bbfbc83746f79a94",
      "question": "what kind of cars?",
      "prediction": "luxury",
      "ground_truths": [
        "exotic sports"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "11452fbd4fdb40d8bd8f10d24a8087e2",
      "question": "where were they going",
      "prediction": "an annual road trip,",
      "ground_truths": [
        "Grand Ronde, Oregon."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0099081da3eb4d36bc020201d4f97a27",
      "question": "what speeds were they travelling?",
      "prediction": "running about 100 mph in a 55-mph zone,",
      "ground_truths": [
        "about 100 mph"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "83966e58471549e2adc73c25f58c846b",
      "question": "What were they making?",
      "prediction": "annual road trip,",
      "ground_truths": [
        "an annual road trip,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8383bee1823d4a178d9746f9c5425d24",
      "question": "what time did this event happen?",
      "prediction": "5:20 p.m.",
      "ground_truths": [
        "5:20 p.m."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3b7e037cf27d46fbbc6fc50bb7f30e35",
      "question": "What terminal was closed?",
      "prediction": "C",
      "ground_truths": [
        "C"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0423f569713641c5827ce4a31cda24fc",
      "question": "where has the officer been reassigned to?",
      "prediction": "Transportation Security Administration",
      "ground_truths": [
        "performing non-screening duties"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "90aaf736a3284105817eef47e461fec6",
      "question": "When did security breach happen?",
      "prediction": "Sunday's",
      "ground_truths": [
        "Sunday's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95588acac903452eb46741382017346b",
      "question": "What did man do improperly at security screening?",
      "prediction": "walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,",
      "ground_truths": [
        "walked through an exit on the public side to the secure \"sterile\" side"
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "03987c7b8e9f44f7a471610e6de64f6c",
      "question": "When was the breach?",
      "prediction": "Sunday's",
      "ground_truths": [
        "Sunday evening"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "373bec1f959948aba63fe708ef5aaf4d",
      "question": "How long was it closed for?",
      "prediction": "about 5:20 p.m. at Terminal C",
      "ground_truths": [
        "hours"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c067406d1ca644f68fcc91b86cbd302a",
      "question": "What forced closure of Terminal C?",
      "prediction": "security breach",
      "ground_truths": [
        "security breach"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "672a0eb014e2448d8dd9fd945f587d23",
      "question": "what did human rights watch official say",
      "prediction": "Obama should have met with the Dalai Lama.",
      "ground_truths": [
        "Obama should have met with the Dalai Lama."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "14e32051f99643d788c0471074d4597c",
      "question": "An Ex UN official said what about Obama's strategy",
      "prediction": "human rights issues to one side,\"",
      "ground_truths": [
        "\"gotten the balance right\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bf0ef6764dd64e91a7f16852e78867b2",
      "question": "who followed the bombers",
      "prediction": "NATO fighters",
      "ground_truths": [
        "NATO fighters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f86dfe94c7784de99ef22445f3d8d371",
      "question": "what did spokeman say",
      "prediction": "NATO fighters followed the bombers on their 13-hour flight over the Arctic Ocean and the Atlantic,",
      "ground_truths": [
        "\"We exercise all around the globe and have joint exercises with countries all over"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "d745bee67227422b824140f2734b0621",
      "question": "what does news agency say",
      "prediction": "\"If Russian long-range bombers should need to land in Venezuela, we would not object to that either. We will also welcome them,\" Chavez said",
      "ground_truths": [
        "Venezuelan President Hugo Chavez, whose comments have frequently antagonized Washington, said it would welcome the Russian air force,"
      ],
      "em": 0,
      "f1": 0.24390243902439027
    },
    {
      "id": "027b6f1bd664457db6e6063da4fcfe6b",
      "question": "What would Venezuelan president welcome?",
      "prediction": "the Russian air force,",
      "ground_truths": [
        "Russian air force,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "86a8ab5a4a8046d49add6140bbcac0a6",
      "question": "who bombed the country",
      "prediction": "Russian bombers",
      "ground_truths": [
        "Russian bombers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "58487a8f39d745abb6c2823ba3bcd2ab",
      "question": "What will help less fortunate tribes?",
      "prediction": "group is intended to",
      "ground_truths": [
        "Keeping the dollar within Indian Country, Bowers hopes,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1cf7fd231c584939838ed6ec3f192f26",
      "question": "Which tribe owns the property?",
      "prediction": "Seminole",
      "ground_truths": [
        "Seminole"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "36243b6f2e134d4f94c63ff7f739150e",
      "question": "How many Native-owned businesses are a part of The Native American Group?",
      "prediction": "100",
      "ground_truths": [
        "more than 100"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "cacc201f681d4e33ac294b7210b3f030",
      "question": "How many businesses are owned by the Native American Group?",
      "prediction": "more than 100",
      "ground_truths": [
        "more than 100"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "894e88efeef14e0e87486904b79bc5aa",
      "question": "Who owns Hard Rock properties?",
      "prediction": "Tribes of Florida Inc.,",
      "ground_truths": [
        "The Seminole Tribe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cab4cf53e8694388b5596d14d729756a",
      "question": "What Native American tribe owns Hard Rock properties?",
      "prediction": "of Florida",
      "ground_truths": [
        "Seminole Indian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e76d0d1755a446e398a114d49c9cd6cb",
      "question": "What is the 19th century bikini made of?",
      "prediction": "either heavy flannel or wool",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "74fe5cbec55e4c79b40ae84889093d27",
      "question": "What ode has Brian Hyland`s 1960 hit single?",
      "prediction": "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"",
      "ground_truths": [
        "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e846a76cf896416fa46846010b343285",
      "question": "In what way in 19th-century bikinis has been made?",
      "prediction": "either heavy flannel or wool",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "631d516c9682428ba9adab73196e9b74",
      "question": "When Bikinis first appeared?",
      "prediction": "1960",
      "ground_truths": [
        "1946."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "67e2faa59e874c85954b0c7f05378a61",
      "question": "What appeared for the first time in Roman mosaics?",
      "prediction": "1. The oldest documented bikinis",
      "ground_truths": [
        "oldest documented bikinis"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "9e3d546814314fab91adebab3557fb1e",
      "question": "When did bikinis first appear?",
      "prediction": "1960",
      "ground_truths": [
        "1,700 year old Roman mosaic"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ff89789e7aab4c31b15fed6597f67770",
      "question": "What was the bikini of the version of the 19th century made?",
      "prediction": "heavy flannel or wool",
      "ground_truths": [
        "heavy flannel or wool"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "17ba9abcc2d34d24b383137f643a09c0",
      "question": "What did Brian Hyland release in 1960?",
      "prediction": "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"",
      "ground_truths": [
        "hit single, \"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\""
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "c64f2789544c45c8bf9dceebb35cfa53",
      "question": "What has he won?",
      "prediction": "WBO welterweight",
      "ground_truths": [
        "WBO welterweight title"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "6616d9ae236f46acb990d2a879bf346f",
      "question": "Who returned home after his win?",
      "prediction": "Manny Pacquiao",
      "ground_truths": [
        "Manny Pacquiao"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ea6918ad41e84412b642d9ac01b6d839",
      "question": "Where has Manny Pacquiano gone?",
      "prediction": "Philippines",
      "ground_truths": [
        "Philippines"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1b99dffb92c14a838fdb91967645cbf0",
      "question": "To where does Manny Pacquiao return?",
      "prediction": "home",
      "ground_truths": [
        "Philippines"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37c0ef1f9eb3412594ff30f21611b882",
      "question": "Who is his next opponent?",
      "prediction": "Floyd Mayweather Jr.",
      "ground_truths": [
        "American Floyd Mayweather"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7fa327c5d6d443e8b5528c521db7bf65",
      "question": "Who did Pacman beat?",
      "prediction": "Miguel Cotto",
      "ground_truths": [
        "Miguel Cotto"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e98ac3de59a646c69b6eb17d3e3a4d0d",
      "question": "What did he win?",
      "prediction": "WBO welterweight title",
      "ground_truths": [
        "WBO welterweight title"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a23bfc2ed0ea4b19b736c30f73f7e406",
      "question": "Who did he beat?",
      "prediction": "Cotto",
      "ground_truths": [
        "Miguel Cotto"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3038ae539727412fa4c113ae79580b91",
      "question": "How many others shared the room in Mumbai?",
      "prediction": "eight people",
      "ground_truths": [
        "eight people"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5d476ed59bc845d484ec9a8098d2a71d",
      "question": "Who played the quiz show host?",
      "prediction": "Anil Kapoor",
      "ground_truths": [
        "Anil Kapoor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf211b6bda2644e5801ce89678246fae",
      "question": "What did Kapoor say?",
      "prediction": "\"Slumdog,\" a love story about a boy from a Mumbai slum who wins a fortune on quiz show \"Who Wants To Be A Millionaire?,\" resembles his own life story.",
      "ground_truths": [
        "\"I also started from scratch, went from rags to riches,\""
      ],
      "em": 0,
      "f1": 0.11764705882352941
    },
    {
      "id": "c6b1a287f63345cd9df7f78f9b9e8d9a",
      "question": "Who did the bollywood star play?",
      "prediction": "\"Slumdog,\"",
      "ground_truths": [
        "the creepy quiz show host"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18ed9e010d6d41c8b1d69f1cf1834dc8",
      "question": "where did he grow up",
      "prediction": "in a tenement in the Mumbai suburb of Chembur,",
      "ground_truths": [
        "a tenement in the Mumbai suburb of Chembur,"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "282d649f73264f2a97d76f8a7f3a0b9c",
      "question": "who is a bollywood legend",
      "prediction": "Anil Kapoor",
      "ground_truths": [
        "Anil Kapoor."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d3fdb0181051401cab348f6162104860",
      "question": "who is the bollywood star",
      "prediction": "Anil Kapoor",
      "ground_truths": [
        "Anil Kapoor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e10fb0470ea641d8924989aeca3b9c67",
      "question": "who quit the board",
      "prediction": "Brown-Waite",
      "ground_truths": [
        "Brown-Waite"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f92118c87d7c45b6beb825d91b0cbeba",
      "question": "who is recommending \"corrective actions\"?",
      "prediction": "The House Ethics Committee",
      "ground_truths": [
        "and"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ae0abe6f6b7641e0977cb649ed87be36",
      "question": "who got fired for alleged oral sex in public?",
      "prediction": "Two pages",
      "ground_truths": [
        "Two pages"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a2956c65db9643d6868d9a15adf1850a",
      "question": "who were \"enablers\"?",
      "prediction": "\"It",
      "ground_truths": [
        "observers and other page participants"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e2ff35af7519458ab90fd7816840c6b2",
      "question": "how many farmers have been affected since 2000?",
      "prediction": "400",
      "ground_truths": [
        "4,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6c47056d75f4421589fe4dc9bf60f5ac",
      "question": "What are farmers fighting over?",
      "prediction": "land",
      "ground_truths": [
        "to hold onto his land"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "c0890f632248463e98bfe78ec9ff750e",
      "question": "Since 2000, how many commercial farmers have been driven off their land?",
      "prediction": "4,000",
      "ground_truths": [
        "4,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "832ba598aeab4cd082be7dba8f62ce4b",
      "question": "What policies enable controversy?",
      "prediction": "land reform program",
      "ground_truths": [
        "President Robert Mugabe's policy of redistributing white-owned farms to landless blacks."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "09909b2908834b4f871ae1e0e7bd63f1",
      "question": "Who is battling continuing policy of land redistribution?",
      "prediction": "Zimbabweans",
      "ground_truths": [
        "400 farmers"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29d9d1f8844c429db35562d9d85d1a27",
      "question": "What does one farmer show CNN?",
      "prediction": "a year after the country's political rivals pledged",
      "ground_truths": [
        "workers were standing idle."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e7cf0ed6ffcc456a9c49c01cff79384f",
      "question": "Where and when was Harry Patch wounded in the World War One?",
      "prediction": "Ypres, Belgium,",
      "ground_truths": [
        "Ypres, Belgium, in 1917"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "f0026bd57477438b9abfca1e085c890f",
      "question": "last British survivor of",
      "prediction": "World War I",
      "ground_truths": [
        "World War I"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fd8b0bbb4f084371a16c57f4bb73bc47",
      "question": "Who is Harry Patch?",
      "prediction": "last surviving British soldier from World War I",
      "ground_truths": [
        "last surviving British soldier from World War I"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5853b72f18f44468b7a88b25e486ac22",
      "question": "What did Harry Patch try to do all his life?",
      "prediction": "speak of his memories,",
      "ground_truths": [
        "\"He tried"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "116e002d700e4d349581f1a4937dfba8",
      "question": "Where was the bus travelling to?",
      "prediction": "Matamoros, Mexico,",
      "ground_truths": [
        "Matamoros, Mexico,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "94b1187d3e5c4282983df4ab44d3d46b",
      "question": "What happened to the bus?",
      "prediction": "rolled over",
      "ground_truths": [
        "rolled over"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "549215ba9f05415f88e0c7b58148d436",
      "question": "Which company was operating the bus?",
      "prediction": "Greyhound,",
      "ground_truths": [
        "Americanos"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "75dc5bcd1b77473789e0fff4f89aafef",
      "question": "Where were the injured taken?",
      "prediction": "San Antonio's Brooke Army Medical Center and University Hospital,",
      "ground_truths": [
        "San Antonio's Brooke Army Medical Center and University Hospital,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7de2c568942c4160a93c7c80f264030f",
      "question": "How many people died in the accident?",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b25cbff83ef403ba7c2ee3b03a1d8be",
      "question": "How many people were rescued?",
      "prediction": "At least 15",
      "ground_truths": [
        "At least 15"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2fcf533dede444f2a0296b3bf112e0bd",
      "question": "Where did the military transport plae crash?",
      "prediction": "residential area in East Java",
      "ground_truths": [
        "East Java"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "9a9db144769c4ce29c4cdb431d756043",
      "question": "What did the crash destroy ?",
      "prediction": "four homes",
      "ground_truths": [
        "destroyed four homes"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "4f66952eeb534a109a11044fcfecf033",
      "question": "What is the new death toll?",
      "prediction": "19",
      "ground_truths": [
        "112 people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "06ad4f09434a47e7a9563d96bde0e71f",
      "question": "What did the Karas village leader says?",
      "prediction": "at least 15 people from the flight had been rescued,",
      "ground_truths": [
        "The crash destroyed four homes and killed two people who lived in at least one of the homes,"
      ],
      "em": 0,
      "f1": 0.24000000000000005
    },
    {
      "id": "dcb53cda7e6144fca04963ac7581bcdd",
      "question": "Whose transport plane crashed into residential area in East Java ?",
      "prediction": "An Indonesian military",
      "ground_truths": [
        "An Indonesian military"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f5627bd7cf9545e78204161253ca2e8e",
      "question": "What plane crashed into residential area?",
      "prediction": "An Indonesian military transport",
      "ground_truths": [
        "An Indonesian military transport"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ecbae504766b467fb0dd39213c22ff21",
      "question": "Which country owned the military transport plane?",
      "prediction": "Indonesia",
      "ground_truths": [
        "Indonesian"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "822d82601bda4b509b180096346c1885",
      "question": "How many people from the flight  wererescued ?",
      "prediction": "At Least 15",
      "ground_truths": [
        "At least 15"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "80489a118e244f1ab4b2843692ab1b34",
      "question": "who was arrested?",
      "prediction": "Vicente Carrillo Leyva,",
      "ground_truths": [
        "Vicente Carrillo Leyva,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "376e8a711bc649d285bde4d7b21f21b3",
      "question": "What is the name fo the person arested?",
      "prediction": "Vicente Carrillo Leyva,",
      "ground_truths": [
        "Vicente Carrillo Leyva,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "11c406199ecc4e92a99602a002316f9e",
      "question": "Who are authorities blaming for the surge in violence near the border?",
      "prediction": "drug cartels",
      "ground_truths": [
        "drug cartels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1bc7d549f7514a3fb84a2e0f5e68f90a",
      "question": "What age is Vicente Carrillo Leyva?",
      "prediction": "32,",
      "ground_truths": [
        "32,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d7f96c16afcc447fbf22d059a50abf08",
      "question": "Where was Leyva arrested according to officials?",
      "prediction": "Mexico City,",
      "ground_truths": [
        "in a park in a residential area of Mexico City,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "13957e04af1f45c9b2390e590a3024aa",
      "question": "Wher ewas the suspect arrested?",
      "prediction": "mexican authorities",
      "ground_truths": [
        "in a park in a residential area of Mexico City,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93c7f5400de8418caa74475873be170a",
      "question": "who is blamed for surge in violence?",
      "prediction": "drug cartels",
      "ground_truths": [
        "drug cartels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d9f8104092c54b1caab016d6280a0433",
      "question": "What does the professor think about the show's inspiration?",
      "prediction": "the overweight being held up for public ridicule.",
      "ground_truths": [
        "she wonders if part of the appeal of plus-sized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6ef4a3522b7c418eaaaa6970d6914b45",
      "question": "What drew over a million viewers?",
      "prediction": "\"Dance Your Ass Off.\"",
      "ground_truths": [
        "\"Dance\""
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "55aa567e83e84daf8f24f0b4464bd525",
      "question": "What is the show about?",
      "prediction": "\"Dance Your Ass Off.\"",
      "ground_truths": [
        "weight-loss"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f61cf4c0ecff40e294bf410889610098",
      "question": "What show drew more than a million viewers for its premier?",
      "prediction": "\"Mo'Nique's F.A.T. Chance,\"",
      "ground_truths": [
        "\"Dance Your Ass Off.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b34e08c6a5384be89492250bfb3e11bf",
      "question": "What company were they fired from?",
      "prediction": "Graziano Transmissioni",
      "ground_truths": [
        "Graziano Transmissioni"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa5ab129da0a48d0a29725bf34d1694e",
      "question": "Did India's labor minister apologize for his statement?",
      "prediction": "apologized,",
      "ground_truths": [
        "later apologized,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "676a7a901dc44a4d9d6768721b362662",
      "question": "what did the mob do",
      "prediction": "attacked L.K. Chaudhary,",
      "ground_truths": [
        "attacked L.K. Chaudhary,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b7366a8b06543e88c6498f81ebdaefb",
      "question": "where did the police charge 63 people",
      "prediction": "India",
      "ground_truths": [
        "India"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cde0a15396548a8acebfe775446d3c0",
      "question": "What crime were the 63 people charged with?",
      "prediction": "murder",
      "ground_truths": [
        "murder in the beating death of"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "5a9c1f83248e41e595cc49ca6321e207",
      "question": "for what did the minister apologize",
      "prediction": "his comments had been taken out of context.",
      "ground_truths": [
        "his comments"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "aab04244f10344df84ff56eaf98084d4",
      "question": "What was the apology issued by India's labor minister?",
      "prediction": "his comments had been taken out of context.",
      "ground_truths": [
        "comments had been taken out of context."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "0f5470b87d79473c8fa5c8d85822f01f",
      "question": "What type of company was run by the chief executive who was attacked?",
      "prediction": "Italian car parts manufacturing",
      "ground_truths": [
        "Italian car parts manufacturing"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "16d963b0845842e78d18606036243158",
      "question": "What were the sixty-three people charged with?",
      "prediction": "murder",
      "ground_truths": [
        "murder"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e68dcc48e03c4743b168512f576639af",
      "question": "In India, how many people did police charge with murdering their company boss?",
      "prediction": "63",
      "ground_truths": [
        "63"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ad096092524147ab9ea6e4cc7b3dc4fc",
      "question": "What number of people were charged?",
      "prediction": "63",
      "ground_truths": [
        "63"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "95b7241ff4724cebbdfdbb7908e03cee",
      "question": "How much did the US  provide?",
      "prediction": "$500,000",
      "ground_truths": [
        "million dollars"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6da32ee0b18a487192dc8897872719f4",
      "question": "How much money did the US provide?",
      "prediction": "$500,000",
      "ground_truths": [
        "additional million"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ccf4158795544791820f591e3ea877a0",
      "question": "How much did the U.S. provide to protect Haitian children from trafficking?",
      "prediction": "$500,000",
      "ground_truths": [
        "$500,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "40ca2fea196041abaa5ecb9644adeea7",
      "question": "How many people are laboring in bondage?",
      "prediction": "27 million,",
      "ground_truths": [
        "12.3 million"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9b78695b228247bc88e1c3680aaa1083",
      "question": "How many people does The International Labour Organization report are 'laboring in bondage'?",
      "prediction": "12.3 million",
      "ground_truths": [
        "12.3 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b57dc6e739304b1da3ec67de17fdf72e",
      "question": "Who has anecdotal evidence about the trafficking of Haitian children?",
      "prediction": "Ambassador Louis CdeBaca",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "38407a483d224ec1b4a43154b2a8a69b",
      "question": "who has anecdotal evidence of the trafficking of Haitian children?",
      "prediction": "Ambassador Louis CdeBaca",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4bcd1a9273904f42b28a195a0790d89d",
      "question": "Who has anecdotal evidence about the trafficking?",
      "prediction": "Ambassador Louis CdeBaca",
      "ground_truths": [
        "UNICEF\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a9d48615144a4cdbb8f41bf8e0bddbb5",
      "question": "How many states do lawmakers say wish to follow Arizona's lead regarding the bill?",
      "prediction": "four",
      "ground_truths": [
        "four"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5fc7ecf64ee84d9f8bebbc8762e8de37",
      "question": "Which state official has not signed the Arizona bill?",
      "prediction": "Republican Gov. Jan Brewer.",
      "ground_truths": [
        "Gov. Jan Brewer."
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "d4b92af0485149e2a48214b9df6fd9f0",
      "question": "What does the Arizona bill require?",
      "prediction": "immigrants to carry their alien registration documents at all times and",
      "ground_truths": [
        "orders immigrants to carry their alien registration documents at all times"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "e0b3dc2268b34678acf05945eee8298b",
      "question": "Who has not yet signed bill?",
      "prediction": "Republican Gov. Jan Brewer.",
      "ground_truths": [
        "Republican Gov. Jan Brewer."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "69ffbc465b9d4e9fa198209656825e2e",
      "question": "What does the Arizona bill require police to do?",
      "prediction": "to question people if there's reason to suspect they're in the United States illegally.",
      "ground_truths": [
        "question people if there's reason to suspect they're in the United States illegally."
      ],
      "em": 0,
      "f1": 0.9600000000000001
    },
    {
      "id": "bdb371e606c74ea8957172d6ba4f398c",
      "question": "What did the  ACLU attorney say?",
      "prediction": "following in Arizona's footsteps would take states in the wrong direction.",
      "ground_truths": [
        "following in Arizona's footsteps would take states in the wrong direction."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a0cdfbc45b3f4641b64a2dfd3bde4c2b",
      "question": "Who did the Coast Guard search for?",
      "prediction": "Two remaining crew members",
      "ground_truths": [
        "the two remaining crew members"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e4533ea2a1404bc285933f7355d58203",
      "question": "Where did the crash occur?",
      "prediction": "waters off San Diego, California,",
      "ground_truths": [
        "the waters off San Diego, California,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3c5e3c93b4f04892a42057005a46caaa",
      "question": "How many people are missing after the crash?",
      "prediction": "two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60dd6a100f3b42598ab0d32f254b1662",
      "question": "What are the Coast Guard using to search?",
      "prediction": "helicopters and boats,",
      "ground_truths": [
        "helicopters and boats, as well as vessels from other agencies,"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "f532114560ac430e8d8e5e9929829182",
      "question": "Where did the copter crash?",
      "prediction": "20 miles off the Mexican coast,",
      "ground_truths": [
        "It crashed about 20 miles off the Mexican coast,"
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "a5c81a5e5d3c42d089cd98f0b556de2f",
      "question": "How many people are still missing?",
      "prediction": "two",
      "ground_truths": [
        "two remaining crew members"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "1fcf01709c0b404da2b3024ac0a02eb0",
      "question": "Who is using helicopters?",
      "prediction": "Coast Guard",
      "ground_truths": [
        "U.S. Navy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9b01a7aac7fb40979eccbed5da004372",
      "question": "What day did the crash occur?",
      "prediction": "Tuesday",
      "ground_truths": [
        "11:30 p.m. Tuesday,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "89a41de7e5ca416cae37603aab8638c3",
      "question": "who was mentoring",
      "prediction": "Former Mobile County Circuit Judge Herman Thomas",
      "ground_truths": [
        "Thomas"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "fe662df179fe47f1a9fa2c634c58c26a",
      "question": "who was denied allegations",
      "prediction": "Former Mobile County Circuit Judge Herman Thomas",
      "ground_truths": [
        "Judge Herman Thomas"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "f2286074ceb04295a819efc7e503fe39",
      "question": "who has  denied allegations",
      "prediction": "Former Mobile County Circuit Judge Herman Thomas",
      "ground_truths": [
        "Former Mobile County Circuit Judge Herman Thomas"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b76738b25e7a44f89547dda91c1ce73b",
      "question": "who is accused",
      "prediction": "Former Mobile County Circuit Judge Herman Thomas",
      "ground_truths": [
        "Judge Herman Thomas"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "25cdce361ace48b99c156c6e70b5f03e",
      "question": "who is herman thomas",
      "prediction": "former Alabama judge",
      "ground_truths": [
        "Former Mobile County Circuit Judge"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2784aee1dd324e3f8945b73963b7785f",
      "question": "What did the township ban?",
      "prediction": "inflatable or portable signs and banners on public property.",
      "ground_truths": [
        "inflatable or portable signs and banners on public property."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "470161e89bbf46fcaeb483641c98ebb5",
      "question": "What is banned?",
      "prediction": "inflatable or portable signs and banners on public property.",
      "ground_truths": [
        "inflatable or portable signs and banners on public property."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "073515a682484ceaac87a83d1ab38a06",
      "question": "What do towns have the right to enforce?",
      "prediction": "an \"aesthetic environment\"",
      "ground_truths": [
        "an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "61d661961fb143578d6e07aeab16ae66",
      "question": "What did the court say the towns have the right to enforce?",
      "prediction": "10-foot-tall, black, rat-shaped balloon at a rally held outside a fitness center.",
      "ground_truths": [
        "maintain an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1d5e987319ba4158a12cab23543a04aa",
      "question": "What do towns have the right to do?",
      "prediction": "maintain an \"aesthetic environment\"",
      "ground_truths": [
        "maintain an \"aesthetic environment\" and ensure public safety,"
      ],
      "em": 0,
      "f1": 0.6
    },
    {
      "id": "969af4d0da444448906032c0784076e6",
      "question": "What's the name of the balloon displayed at union rallies?",
      "prediction": "Scabby",
      "ground_truths": [
        "Scabby"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4ab8ad68491d42d590acd464b7e19e82",
      "question": "Who can disrupt the Tennessee Valley Authority?",
      "prediction": "a skilled hacker",
      "ground_truths": [
        "skilled hacker"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5a0dab70352547cf92c768c78614532c",
      "question": "What was Congress told?",
      "prediction": "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,",
      "ground_truths": [
        "75 percent of utilities had taken steps to mitigate the Aurora vulnerability,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e747846267684ddeb167b085777bbd21",
      "question": "TVA supplies power to how many Americans?",
      "prediction": "9 million",
      "ground_truths": [
        "almost 9 million"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "4308b5c4c2af4b51b0d454474e139318",
      "question": "What does the TVA do?",
      "prediction": "supplies power to almost 9 million Americans,",
      "ground_truths": [
        "supplies power to almost 9 million Americans,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9a9585ebb3ef47898a0e074a4ab0d6b5",
      "question": "Who caused the blackouts?",
      "prediction": "cyberattacks,",
      "ground_truths": [
        "a skilled hacker"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7084bfe9b1b64089b8c1b38b38226424",
      "question": "What did representative say?",
      "prediction": "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"",
      "ground_truths": [
        "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "832db3f0a7264d689b6e7c8ca7869e9f",
      "question": "What percentage of utilities were fixed to combat attacks?",
      "prediction": "75 percent",
      "ground_truths": [
        "75 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "371c170f87a74d4ca39cbaf126b5172d",
      "question": "When did they find him",
      "prediction": "June 25.",
      "ground_truths": [
        "June 25."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2e96837b84e478c8c580e054383e693",
      "question": "What drug was found in Michael Jackson's home?",
      "prediction": "propofol,",
      "ground_truths": [
        "propofol,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8f5f6161afbd49f4aedb61e2aa24a75f",
      "question": "What kind of drug is Propofol?",
      "prediction": "a powerful anesthetic and sedative.",
      "ground_truths": [
        "powerful anesthetic and sedative."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "096b55148bea4c5c9368d672892e9447",
      "question": "What happened to him",
      "prediction": "found after Michael Jackson's death",
      "ground_truths": [
        "death of cardiac arrest"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "bc036900b2f747aa92b1cdf251b0a37c",
      "question": "What is propofol used?",
      "prediction": "a powerful anesthetic and sedative.",
      "ground_truths": [
        "anesthetic"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "c23796c4d75440d5a6e1200deb8c14e8",
      "question": "How long has the DEA been looking into it?",
      "prediction": "past two years,\"",
      "ground_truths": [
        "for the past two years,\""
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "4ba8b05dc3d54a498ab16e372ef71035",
      "question": "Has the DEA been looking into this?",
      "prediction": "it's considering tighter restrictions on propofol.",
      "ground_truths": [
        "have"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c3ec8eaa57d0418da0c66d7ad83761b3",
      "question": "What where the results of the autopsy?",
      "prediction": "could provide a better quality of life for his final years,\"",
      "ground_truths": [
        "inconclusive"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8ec79c66e284a45a63c7dcdcf4f9064",
      "question": "What is happening at the Georgia Aquarium?",
      "prediction": "is undergoing renovation.",
      "ground_truths": [
        "undergoing renovation."
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "6c6f0f2b83e546e39bc40d37b0ee9975",
      "question": "Where in the world did Nico die?",
      "prediction": "in a foreign park",
      "ground_truths": [
        "Atlanta, Georgia"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9759378e0cc44a1c839214bdaa26612d",
      "question": "Where did Lavau's car come to a rest?",
      "prediction": "an embankment in the Angeles National Forest",
      "ground_truths": [
        "500 feet down an embankment"
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "18cbdcbef93544acac9407596569a64f",
      "question": "Who began searching for Lavau?",
      "prediction": "family",
      "ground_truths": [
        "family"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "40f708bf9a744a90b62edfb3b44ad9cf",
      "question": "What did the California man survive on?",
      "prediction": "leaves and drinking water from a creek,",
      "ground_truths": [
        "by eating leaves and drinking water from a creek,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "4d9450e5a9ce4b2d93b7576b2dce59ce",
      "question": "Who searched for Lavau when he went missing?",
      "prediction": "His family",
      "ground_truths": [
        "family"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "2e9b048c06dc45be97da6116056f74f6",
      "question": "What did Lavau's car fall into?",
      "prediction": "a ravine",
      "ground_truths": [
        "down a steep embankment in the Angeles National Forest"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "49af423e020444c999bdbdaa7cc573bd",
      "question": "Who found David Lavau?",
      "prediction": "Sean",
      "ground_truths": [
        "Sean"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53278976c290499ab7f1decb9f7bf6a2",
      "question": "what did the chp say?",
      "prediction": "Lavau's accident and the one involving the dead driver are under investigation.",
      "ground_truths": [
        "Lavau's accident and the one involving the dead driver are under investigation."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "423f76b5864047f480599c7946546eb1",
      "question": "when did the family begin the search?",
      "prediction": "September 23,",
      "ground_truths": [
        "he failed to return home,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b32d5172d0b3451b83352d24deab59e6",
      "question": "Who will report to quinns office?",
      "prediction": "the Illinois Reform Commission",
      "ground_truths": [
        "Reform Commission"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "1405700ba6ce4058a4fa68e3c3d680e3",
      "question": "What does pat quinn want?",
      "prediction": "a review of state government practices completed in 100 days.",
      "ground_truths": [
        "a review of state government practices completed in 100 days."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "413339d30cc447a884eb601571769b06",
      "question": "Who will report to Quinn's office?",
      "prediction": "the Illinois Reform Commission",
      "ground_truths": [
        "commission, led by former U.S. Attorney Patrick Collins,"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "5372bffd269e45f5b80bfa12e294af8f",
      "question": "Who took over as Illinois governor?",
      "prediction": "Pat Quinn",
      "ground_truths": [
        "Pat Quinn"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "487cd2a2f5ee49bb96d33d0d56ea6a2b",
      "question": "What governor was ousted?",
      "prediction": "Blagojevich",
      "ground_truths": [
        "Rod Blagojevich,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fd58ed81f9ea41e2b803c29dc3d424d0",
      "question": "What character does he play on \"How I Met Your Mother\"?",
      "prediction": "Barney Stinson,",
      "ground_truths": [
        "Barney Stinson,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7ad7bbb7ca4e4e89bafebeb59f8ae349",
      "question": "What show does Neil Patrick Harris star in?",
      "prediction": "\"How I Met Your Mother,\"",
      "ground_truths": [
        "\"How I Met Your Mother,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba7ed34f4a424c8c9878ba4048de02a1",
      "question": "How long will it  take for the new season of Mother?",
      "prediction": "September 21.",
      "ground_truths": [
        "September 21."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a96963964374eddb50b591b59d53734",
      "question": "What show is Neil Patrick Harris on?",
      "prediction": "\"How I Met Your Mother,\"",
      "ground_truths": [
        "\"How I Met Your Mother,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0fafdbcc50b44df0aebc0d097d9e6541",
      "question": "What will he probably not do at the Emmys?",
      "prediction": "dancing",
      "ground_truths": [
        "dancing"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "928063d9906145b6b83392f5068efb4f",
      "question": "what is borwns new novel",
      "prediction": "\"The Lost Symbol\"",
      "ground_truths": [
        "\"The Lost Symbol,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0ad40f362389498382471e1f6171f996",
      "question": "who wrote novel da vinci code",
      "prediction": "Dan Brown's",
      "ground_truths": [
        "Dan Brown's"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "57c251b6218b4c1094f9dfb9f53f2a21",
      "question": "According to EW it doesn't equal his previous novel entitled what?",
      "prediction": "\"The Da Vinci Code\"",
      "ground_truths": [
        "\"The Da Vinci Code\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b322fdc7819546138582ebc07e8aa7a8",
      "question": "Dan Brown's new novel is called what?",
      "prediction": "\"The Lost Symbol,\"",
      "ground_truths": [
        "\"The Lost Symbol,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "aa4169b3de364c318562b0b09308caa0",
      "question": "who is dan brown",
      "prediction": "thriller writer",
      "ground_truths": [
        "rare thriller writer"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "dc530a1b42fc45af9b9112f391db3504",
      "question": "How much did Avatar gross in the 9th weekend?",
      "prediction": "$22 million,",
      "ground_truths": [
        "$22 million,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf7fc80bf1024d73b6f5cea672696789",
      "question": "will the film top $60 million by the time the holiday is over?",
      "prediction": "is likely to",
      "ground_truths": [
        "is likely to"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4cff857ac2f74c6094f26f596ba0cb1f",
      "question": "avatar grossed how much in its 9th weekend?",
      "prediction": "$22 million,",
      "ground_truths": [
        "$22 million,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "619869505781451abc4517f031a8b857",
      "question": "how much did valentine's day gross?",
      "prediction": "$52.4 million",
      "ground_truths": [
        "$52.4 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8a34a18a13e9487984c1d6839fed053e",
      "question": "How much did the film gross?",
      "prediction": "an estimated $52.4 million",
      "ground_truths": [
        "$52.4 million"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "b39eac495217430ebe3cfb37878882ee",
      "question": "Which movies are competing with \"Valentine's Day\"?",
      "prediction": "\"Dear John,\"",
      "ground_truths": [
        "\"Wolfman,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "efc438e4fd18417daa0e662499fbe956",
      "question": "How much will the film top?",
      "prediction": "$60 million",
      "ground_truths": [
        "$60 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "edd8922fe3ec43be851dee74c97ea861",
      "question": "where are the shows taking palce",
      "prediction": "20,000-capacity O2 Arena.",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a045cbc6199c4b9e8dbb8545febda365",
      "question": "When were the shows scheduled to start?",
      "prediction": "July 8",
      "ground_truths": [
        "10th July"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "1cc25c3729ab4627a29f3f11172af372",
      "question": "Several shows were postponed to which year because of because of \"sheer magnitude\"?",
      "prediction": "2009,\"",
      "ground_truths": [
        "2010,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1703025033a448048d6f688f9321e2a2",
      "question": "Several shows have been postponed until when?",
      "prediction": "next year",
      "ground_truths": [
        "next year"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7f022ca089d74d51a4be4625cb6191d5",
      "question": "what are the rumors",
      "prediction": "Jackson had skin cancer.",
      "ground_truths": [
        "diagnosed with skin cancer."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "77eaa1d176e14eb092f85c51020850ff",
      "question": "What was Michael Jackson scheduled to perform?",
      "prediction": "concerts in London",
      "ground_truths": [
        "comeback concerts in London"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "c524775cfdfd474691b06c6c05dc4cd2",
      "question": "How many died during the killing?",
      "prediction": "1 million, 10 percent of the population of the central African nation.",
      "ground_truths": [
        "800,000 dead."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6143a2a8fba6403bac9b1a61c403d540",
      "question": "Which is the most stable nation in Africa?",
      "prediction": "Rwanda",
      "ground_truths": [
        "Rwanda"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b6459e60add477cb7b5c6df85dcc727",
      "question": "What is the hatred between?",
      "prediction": "between Hutus and Tutsis",
      "ground_truths": [
        "Hutus and Tutsis"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "8e55899b8be04ddc97c6cae4f4d01699",
      "question": "What lasted 100 days?",
      "prediction": "killing rampage.",
      "ground_truths": [
        "killing rampage."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "97b0da00c8d64c7e9cf9ca56599a06e8",
      "question": "What is Rwanda considered?",
      "prediction": "one of Africa's most stable nations.",
      "ground_truths": [
        "one of Africa's most stable nations."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2546878366e744218972a0eb6c7c650b",
      "question": "What caused genocide between Tutsi and Hutu in 1994?",
      "prediction": "colonial times, when the Belgians considered Tutsis the privileged ethnicity,",
      "ground_truths": [
        "the privileged ethnicity,"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "f40c50b5bb8e4c7b9eb852a860a71d2f",
      "question": "How long was the rampage?",
      "prediction": "100-day",
      "ground_truths": [
        "100-day"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e957032916604030a309b46e81fd7382",
      "question": "For how long killing were between Tutsi and Hutu?",
      "prediction": "100-day",
      "ground_truths": [
        "100-day"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88289b7898cf44bdb3bd88d62391a03b",
      "question": "With what organization Al-Maliki has business talks?",
      "prediction": "the Iraqi Prime Minister Nouri",
      "ground_truths": [
        "U.S. Chamber of Commerce"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "70903ce347bf4339a8eb3f17576ddf6f",
      "question": "What will build the U.S.-Iraq relationship?",
      "prediction": "a much greater presence of U.S. companies in his country",
      "ground_truths": [
        "end of U.S. military operations"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "59d1e4a40a3f4aad9a0e5ae95b711322",
      "question": "What does Al-Maliki say Iraq wants to see?",
      "prediction": "a much greater presence of U.S. companies in his country",
      "ground_truths": [
        "a much greater presence of U.S. companies in his country to help spur greater spending and investment on the country's infrastructure"
      ],
      "em": 0,
      "f1": 0.6428571428571429
    },
    {
      "id": "1ffcd2e9e37749d7ac35f3229327a97e",
      "question": "When were Iraqi contracts auctioned?",
      "prediction": "by the Iraqi government two years ago.",
      "ground_truths": [
        "two years ago."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "936773dbfb6c42989c8090f992429d0d",
      "question": "Who talked business?",
      "prediction": "al-Maliki",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "a06f4131fee84de1b4b796edbfc4dbfa",
      "question": "Who talked business with the U.S. Chamber of Commerce?",
      "prediction": "Nouri al-Maliki",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "3a15cda812324bc0adabb898c0f18a04",
      "question": "Who said he wants to see more us companies in his country?",
      "prediction": "Al-Maliki",
      "ground_truths": [
        "Iraqi Prime Minister Nouri al-Maliki"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "42fc7a4cc2c24cdb843174c1f1314dab",
      "question": "He's been given what to travel?",
      "prediction": "private jet",
      "ground_truths": [
        "an 11-seat Challenger, courtesy of Steve Wynn,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dd9d78ffa4134b8a8a45c017e9706bd5",
      "question": "Who is coming out of retirement?",
      "prediction": "Garth Brooks",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bbae818236954eddb4473d9278983e2d",
      "question": "who was the one who did that",
      "prediction": "Garth Brooks",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68f5d6f4ac934efba4a292614334dc2b",
      "question": "When did he retire",
      "prediction": "two secret concerts",
      "ground_truths": [
        "2000."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52327052a18c48079fbc64f02e20a583",
      "question": "Who describes himself as an average guy?",
      "prediction": "Garth Brooks",
      "ground_truths": [
        "Garth Brooks"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8a8b75eb7c441ec9caad21622b73079",
      "question": "Where was the attack?",
      "prediction": "remotely part of northwestern Montana",
      "ground_truths": [
        "remote part of northwestern Montana"
      ],
      "em": 0,
      "f1": 0.8000000000000002
    },
    {
      "id": "c3f8ce9a9cfb41bf92e4bfda01b3ecc5",
      "question": "Is the man alive?",
      "prediction": "was dead,",
      "ground_truths": [
        "was killed"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7e9c99ffa50c40bf94f5306d8134e736",
      "question": "Who found the man?",
      "prediction": "Two men were killed this summer in Yellowstone National Park by grizzly bears,",
      "ground_truths": [
        "two hunters"
      ],
      "em": 0,
      "f1": 0.13333333333333336
    },
    {
      "id": "bb267b92a44f481d81b2d7aa6606847a",
      "question": "What was he attacked by?",
      "prediction": "grizzly bear",
      "ground_truths": [
        "grizzly bear"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "80109609b1f24dc0859af11d2bf33ca9",
      "question": "Who will he donate to?",
      "prediction": "the underprivileged.",
      "ground_truths": [
        "the underprivileged."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba3edfff481042f2a104ffdc278727c0",
      "question": "What kind of company was he the CEO of?",
      "prediction": "engineering and construction",
      "ground_truths": [
        "engineering and construction"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bf0512befa13468ca6a1763a6e032e82",
      "question": "Who is the South Korean leader?",
      "prediction": "Lee Myung-Bak",
      "ground_truths": [
        "Lee Myung-Bak"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7f5733184c134247ad79a6d2bc6f0081",
      "question": "Who will donate his salary to help the poor?",
      "prediction": "president",
      "ground_truths": [
        "Korea's new president"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5bbab85f5ac243a9b98914f8c7af6eb4",
      "question": "What city was he mayor of?",
      "prediction": "Seoul",
      "ground_truths": [
        "Seoul"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "de36cde140ee4ceda620f1518dd104bc",
      "question": "who has he donated to before",
      "prediction": "the underprivileged.",
      "ground_truths": [
        "children of street cleaners and firefighters."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa9daba7638b4fa9b1472bb648fe964e",
      "question": "what is lee a former ceo of",
      "prediction": "an engineering and construction company",
      "ground_truths": [
        "an engineering and construction company"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68ea789496074f56878ceddf5f0bb6c0",
      "question": "What position did Lee hold?",
      "prediction": "former CEO of an engineering and construction company",
      "ground_truths": [
        "CEO of an engineering and construction company"
      ],
      "em": 0,
      "f1": 0.923076923076923
    },
    {
      "id": "66d0e7da123d49fd8f8016dc27775052",
      "question": "Who did he donate to while mayor of Seoul?",
      "prediction": "children of street cleaners and firefighters.",
      "ground_truths": [
        "children of street cleaners and firefighters."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8b49dc87c93b45c48eb62e4a0b8ac604",
      "question": "What will Lee Myung-Bak donate?",
      "prediction": "his salary",
      "ground_truths": [
        "his salary"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b7f306b0682a46e6907b952794cca076",
      "question": "What is the third most popular religion in the US?",
      "prediction": "Buddhism",
      "ground_truths": [
        "Buddhism"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "349c6fea61d94b258a2b0b58783d96c3",
      "question": "what are the first two most popular religions?",
      "prediction": "Christianity and Judaism,",
      "ground_truths": [
        "Christianity and Judaism,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f114f6b090aa4b81abf5a6ee02f81928",
      "question": "Who are being educated about meditation and yoga?",
      "prediction": "inmates",
      "ground_truths": [
        "inmates"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "328149409f62438f88c73c618a418b3a",
      "question": "What are the programs educating inmates about?",
      "prediction": "meditation and yoga",
      "ground_truths": [
        "meditation"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8acf07893149496b980ed1f5f5421820",
      "question": "What has meditation helped to do?",
      "prediction": "how to cope in prison.",
      "ground_truths": [
        "stay on track and get me through prison,\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "d291eb71b93347c584b6b6aa8826cfb5",
      "question": "What is the third most popular religion in the United States?",
      "prediction": "Buddhism",
      "ground_truths": [
        "Buddhism"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "019a54c5394847ec81325e8bdaf8355f",
      "question": "what does the analyst say?",
      "prediction": "indicators such as inflation are underreported by as much as half.",
      "ground_truths": [
        "that indicators such as inflation are underreported by as much as half."
      ],
      "em": 0,
      "f1": 0.9565217391304348
    },
    {
      "id": "61509191d23a45baa208be884903f3b7",
      "question": "Who has \"been weakened by this latest economic crisis\"?",
      "prediction": "Argentina's",
      "ground_truths": [
        "\"The Kirchners"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f24c1ba50b04f8493f8663fd779846e",
      "question": "who stepped down?",
      "prediction": "President Nestor Kirchner",
      "ground_truths": [
        "Former Argentine President Nestor Kirchner"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "fea7e77ef2364dcdb4bbfa5d69b0c0ef",
      "question": "What party  lost majority in Argentina's Chamber of Deputies?",
      "prediction": "Justicialist",
      "ground_truths": [
        "Justicialist"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e2bc1abc94f141399b0b004676f84171",
      "question": "Who steped down as leader of Argentine's ruling party?",
      "prediction": "Former Argentine",
      "ground_truths": [
        "President Nestor Kirchner"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "aa7ddfbb8f3c403ba5b0a0ded76f2bff",
      "question": "where is this taking place",
      "prediction": "Buenos Aires",
      "ground_truths": [
        "Buenos Aires."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b0d0fee178644809ac252c7347aac58b",
      "question": "who lost the majority?",
      "prediction": "Justicialist Party,",
      "ground_truths": [
        "The ruling Justicialist Party, or PJ by its Spanish acronym,"
      ],
      "em": 0,
      "f1": 0.3636363636363636
    },
    {
      "id": "1cfc4441a86445fab7952656df2d64a0",
      "question": "who have been weakened by the economic crisis",
      "prediction": "Kirchners",
      "ground_truths": [
        "\"The Kirchners"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0d3ffcfb5097482881bfb029456e8e66",
      "question": "how many integrated it?",
      "prediction": "nine",
      "ground_truths": [
        "two"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "89be46d5676849c19dd4ce9d45bda064",
      "question": "what In 1957 school was all-white; today?",
      "prediction": "Little Rock Central High",
      "ground_truths": [
        "Little Rock Central High"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c53dc70e93264f18adcdcddb971ed893",
      "question": "what school was integrated?",
      "prediction": "Little Rock",
      "ground_truths": [
        "Little Rock Central High"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "32bcc94873a24815b3f84adbc715bdb5",
      "question": "How many people integrated Little Rock Central High School?",
      "prediction": "nine",
      "ground_truths": [
        "Nine"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a17fbfcea608490bad3b6a46e1127316",
      "question": "where LaNier hopes Americans will focus?",
      "prediction": "the school's history",
      "ground_truths": [
        "got to get past just the color of our skins being newsworthy. It's really about all the things we knew we could do for this country and now we have the opportunity to show it and it's going to come through his leadership,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "996996ce4b1347a6985e7e0351f40ee3",
      "question": "In what year was the school all-white?",
      "prediction": "1957,",
      "ground_truths": [
        "In 1957,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7464dad64f7e486ca7535d7e02f28586",
      "question": "who integrated the high school?",
      "prediction": "LaNier and eight other members of the Little Rock Nine",
      "ground_truths": [
        "Carlotta Walls LaNier and eight other members of the Little Rock Nine"
      ],
      "em": 0,
      "f1": 0.9
    },
    {
      "id": "19837fd3eddc462c9b178a83fddce010",
      "question": "What is the length of Susan Atkin's sentaence?",
      "prediction": "life",
      "ground_truths": [
        "life sentence"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "87ac9682d8b541de8ef5f78175be9615",
      "question": "Who says Susan has brain cancer?",
      "prediction": "James Whitehouse,",
      "ground_truths": [
        "Her husband and attorney, James Whitehouse,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "5626902cc76741eb9f4075d0a042016c",
      "question": "What was Susan Atkin's role in the Manson family murders?",
      "prediction": "actress",
      "ground_truths": [
        "slayings of actress Sharon Tate and four others."
      ],
      "em": 0,
      "f1": 0.2222222222222222
    },
    {
      "id": "864fba7b5c2744b8ad73ded96b8fef81",
      "question": "Who is serving a life sentence?",
      "prediction": "Susan Atkins",
      "ground_truths": [
        "Susan Atkins"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7e7da6521baa4d6bb18f6279522d0f43",
      "question": "Who has terminal brain cancer?",
      "prediction": "Susan Atkins",
      "ground_truths": [
        "Susan Atkins"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d219a6074cc74aea842c1a2a383e208f",
      "question": "What decade the murders occur?",
      "prediction": "1969",
      "ground_truths": [
        "1969"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d210306368a24de884557f43a1c4d48e",
      "question": "What is harder to control than food allergies?",
      "prediction": "Insect allergies",
      "ground_truths": [
        "Insect"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "a5f166a914df4c3b8f6afe0ec3462437",
      "question": "In rare cases, mosquito bites can lead to what?",
      "prediction": "anaphylaxis,",
      "ground_truths": [
        "anaphylaxis,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d98750850d5249e09d7bfbb43313513f",
      "question": "If you are allergic to insects, you should carry what?",
      "prediction": "antihistamine and an epinephrine auto-injector",
      "ground_truths": [
        "antihistamine"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "4ba7c330436f469b835be6ce879b08d6",
      "question": "How many people die in the U.S. from stings?",
      "prediction": "least 40",
      "ground_truths": [
        "40"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c4e4cc4c65dc49cb951576e334e7bf52",
      "question": "How many people die each year?",
      "prediction": "40",
      "ground_truths": [
        "At least 40"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7518999e54834c1389cf350e45975ed7",
      "question": "40 people in the U.S. die each year from what?",
      "prediction": "insect stings,",
      "ground_truths": [
        "At least 40 people in the United States die each year as the result of insect stings,"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "af0cc33542ac4440a243c4807d87fd37",
      "question": "What should you carry if you are allergic to insects?",
      "prediction": "an antihistamine and an epinephrine auto-injector",
      "ground_truths": [
        "an antihistamine and an epinephrine auto-injector"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41b01c0e5e764c59a65bfaace8628f0e",
      "question": "Who's death was declared a homicide?",
      "prediction": "missing Florida toddler Caylee Anthony,",
      "ground_truths": [
        "Caylee Anthony,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "cfd17679a12d453f8449f8dacf19eef2",
      "question": "What did the medical examiner declare?",
      "prediction": "the manner of Caylee's death -- an opinion based on factors including an examination of the body and circumstantial evidence -- was determined to be homicide.",
      "ground_truths": [
        "The cause of the child's death will be listed as homicide by undetermined means,"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "e907f75777b242b5b1ebbc4533d577c9",
      "question": "What was Casey Anthony notified about?",
      "prediction": "the results by a chaplain",
      "ground_truths": [
        "test results"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "cdbbd817090f476f8c805bc304c9ca46",
      "question": "What has Caylee's death been declared as?",
      "prediction": "homicide",
      "ground_truths": [
        "homicide"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a773d482def146618ecee4e25e39568a",
      "question": "What was found in the wooded area?",
      "prediction": "remains",
      "ground_truths": [
        "\"significant skeletal remains\""
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "23ae0c6768224c3f92a336904773a74d",
      "question": "What dpes a woman organize for food allergy sufferers?",
      "prediction": "Worry Free Dinners",
      "ground_truths": [
        "monthly meals"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6e7c609c8a824ca9bb3d4d55270d9441",
      "question": "Why are food allergies on the rise?",
      "prediction": "Food and Drug Administration's September 16 public hearing",
      "ground_truths": [
        "no one is sure"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5c99e7f2414645c1a212ce119fc38ef8",
      "question": "How many Americans have food allergies?",
      "prediction": "12 million",
      "ground_truths": [
        "about 12 million in America,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "ac9462160d8c4827aa0820c4b03c441e",
      "question": "What kind of food allergies?",
      "prediction": "peanuts, nuts, shellfish and fish",
      "ground_truths": [
        "peanuts, nuts, shellfish and fish"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0f7024ec6d9243b09541119fa1bfdb47",
      "question": "Who does she organize dinners for?",
      "prediction": "people with food allergies.",
      "ground_truths": [
        "people with food allergies."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f9f572cc061d4613b4ff2c08addaf855",
      "question": "What is the leading food allergy?",
      "prediction": "peanuts, nuts, shellfish",
      "ground_truths": [
        "peanuts."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "8de1e7f2f6a44cdd900cfcfc5ddbe016",
      "question": "What should you do before going to a restaurant if you have a food allergy?",
      "prediction": "take part in Worry Free Dinners,",
      "ground_truths": [
        "staff with special requests."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "340d572d148d4c298fe8a75a61d1625f",
      "question": "who was arrested?",
      "prediction": "Bani Dugal,",
      "ground_truths": [
        "of the religious minority"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "25dc8619c6e743be99772792c454db17",
      "question": "What did Bani Dugal say?",
      "prediction": "\"The allegations are not new, and the Iranian government knows well that they are untrue,\"",
      "ground_truths": [
        "\"The allegations are not new, and the Iranian government knows well that they are untrue,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4d0bbc08ef8a4c6984394b40103d47ec",
      "question": "The representative said the claim was what?",
      "prediction": "\"utterly baseless.\"",
      "ground_truths": [
        "\"utterly baseless.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a19d2fabcf3a4dc29f67b43b120b932b",
      "question": "Bani Dugal said the government is trying to destroy who?",
      "prediction": "Baha'is community,",
      "ground_truths": [
        "the Baha'i community,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2b1f8415d12b4b6c8b6e8f180e50da20",
      "question": "Where were the minority leaders arrested?",
      "prediction": "Acre/Haifa area in northern Israel",
      "ground_truths": [
        "Iran's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d0e9edf083cf46c58984f222be658296",
      "question": "What did Baha'i officials deny?",
      "prediction": "Iran's claim that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.",
      "ground_truths": [
        "held for security reasons and not because of their faith."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "0c163ddaaef945b7b3224f1a7c8eaac5",
      "question": "How many leaders were held for security reasons?",
      "prediction": "six",
      "ground_truths": [
        "six"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d2875b09841345acbc5c811ecc81947c",
      "question": "who is trying to destroy the community?",
      "prediction": "Iranian government",
      "ground_truths": [
        "President Mahmoud Ahmadinejad's government"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "261af3059f674136b75a9f1ffd6b0499",
      "question": "who denies nuclear program has military purposes?",
      "prediction": "Iran",
      "ground_truths": [
        "Iran"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21d5eb5ff6594bec8489a26dc30e5221",
      "question": "When did the vice prime minister speak?",
      "prediction": "Tuesday",
      "ground_truths": [
        "Tuesday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab6ae284b7a3413791e69f7f5f368192",
      "question": "who speaks on Holocaust remembrance day?",
      "prediction": "Israel's vice prime minister Silvan Shalom",
      "ground_truths": [
        "Silvan Shalom"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0760573d4a6644b5a9528bd111cc597a",
      "question": "what was iran's nuclear effort compared to?",
      "prediction": "Nazi Germany",
      "ground_truths": [
        "Nazi Germany"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "04aba1b206a74b468046427c8ba664a9",
      "question": "What does Iran deny the nuclear program is to be used for?",
      "prediction": "building bombs,",
      "ground_truths": [
        "denies its"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "74499298c6294de997f203b1415d21d4",
      "question": "What is Iran's nuclear effort being compared to?",
      "prediction": "Nazi Germany",
      "ground_truths": [
        "Nazi Germany"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5504cb34682f415d83c81e33fbc13214",
      "question": "What does Iran deny?",
      "prediction": "nuclear program is aimed at building bombs,",
      "ground_truths": [
        "denies its nuclear program is aimed at building bombs,"
      ],
      "em": 0,
      "f1": 0.8750000000000001
    },
    {
      "id": "201020dcae5841dfacf83d089035f079",
      "question": "Who will speak on Holocaust remembrance day?",
      "prediction": "Israel's vice prime minister Silvan Shalom",
      "ground_truths": [
        "Silvan Shalom"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "30fa17868c604959b1bca6654cb7b6e2",
      "question": "What is the effort compared with?",
      "prediction": "to what Hitler did to the Jewish people just 65 years ago,\"",
      "ground_truths": [
        "Hitler did to the Jewish people just 65 years ago,\""
      ],
      "em": 0,
      "f1": 0.9
    },
    {
      "id": "4c3cfd857b01453f9d193ba70e0f2257",
      "question": "Woods will make statement at PGA headquarters  in what city?",
      "prediction": "Vrede Beach, Florida,",
      "ground_truths": [
        "Ponte Vedra Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "d082708c14574406931567132712d955",
      "question": "who is woods' wife?",
      "prediction": "Elin Nordegren,",
      "ground_truths": [
        "Elin Nordegren,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "873ea1af23e548efb10a54c97edcdb6b",
      "question": "What is Woods' wife name?",
      "prediction": "Elin Nordegren,",
      "ground_truths": [
        "Elin Nordegren,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2e71514053a447d5bbd3f9625420ca43",
      "question": "what will woods announce",
      "prediction": "public appearance",
      "ground_truths": [
        "\"apologize for his behavior\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "415c375731994f69a116e415444abd99",
      "question": "what did steinberg state?",
      "prediction": "\"Tiger Woods will not answer questions.",
      "ground_truths": [
        "Woods would not answer questions."
      ],
      "em": 0,
      "f1": 0.7272727272727272
    },
    {
      "id": "1667240d3dd740ffa3c317106730a441",
      "question": "ho will not answer question?",
      "prediction": "Woods",
      "ground_truths": [
        "Woods"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3ffa9e00578043369a677dc8661bfc96",
      "question": "what does woods plan to discuss",
      "prediction": "his past and his future",
      "ground_truths": [
        "his past and his future"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d9f2855c8621457fb68bac161b7f811b",
      "question": "Will Woods will  answer questions?",
      "prediction": "would not",
      "ground_truths": [
        "would not"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bdb76a5fa0754cb68f9f2388fad40bca",
      "question": "Espacio Escultórico has a collection of what items?",
      "prediction": "sculptures by famous artists.",
      "ground_truths": [
        "sculptures"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "cdaaca079e2b4eef92a36b61ffa527d1",
      "question": "Zinco bar is on what street?",
      "prediction": "Cinco de Mayo",
      "ground_truths": [
        "Cinco de Mayo"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f3fbc3504f744c3f971dfd3ce6e0dcf8",
      "question": "What kind of music is played at the Zinco bar?",
      "prediction": "jazz",
      "ground_truths": [
        "jazz"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "68b543f7a3aa4839a39e514e4f56f83b",
      "question": "Zinco bar has great what kind of music?",
      "prediction": "jazz",
      "ground_truths": [
        "jazz"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e5412d7cf02047c3949b854b18c01770",
      "question": "On what street is the Zinco bar?",
      "prediction": "Cinco de Mayo",
      "ground_truths": [
        "Cinco de Mayo"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2aaa26fff7be41698806089edb69a0b9",
      "question": "Who is filing for divorce?",
      "prediction": "Ronald Cummings",
      "ground_truths": [
        "Ronald Cummings"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b27b26ced424638a3b9c75a67d6f49d",
      "question": "Who plans to file for divorce?",
      "prediction": "Ronald Cummings",
      "ground_truths": [
        "Ronald Cummings"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2866afbb73c248e7aadb04390c57124d",
      "question": "Where in Florida did the girl go missing?",
      "prediction": "Satsuma,",
      "ground_truths": [
        "her father's home in Satsuma,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "4f08712b06d7408189a0e846377feb85",
      "question": "Who went missing in Florida?",
      "prediction": "Haleigh Cummings,",
      "ground_truths": [
        "Haleigh Cummings,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "977a85dceaff4e9995034d133961a1ce",
      "question": "What do investigators think?",
      "prediction": "\"The police have been telling me that I've been keeping Misty under my wing and that's why she hasn't talked to the cops,\" Ronald Cummings told a \"Nancy Grace\" producer Tuesday.",
      "ground_truths": [
        "not feel Misty Cummings has told them everything she knows."
      ],
      "em": 0,
      "f1": 0.21052631578947364
    },
    {
      "id": "af5a0954d5ee4800bfc755fed87274db",
      "question": "What do investigators say?",
      "prediction": "they do not feel Misty Cummings has told them everything she knows.",
      "ground_truths": [
        "they"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "20fa1f36b57b47da812735a529e4117d",
      "question": "Who went missing?",
      "prediction": "Haleigh Cummings,",
      "ground_truths": [
        "Haleigh Cummings,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d274da2cc20d4759b02ef68cab5a2b5c",
      "question": "Who is Haleigh Cummings?",
      "prediction": "17-year-old wife.",
      "ground_truths": [
        "a Florida girl"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "497147b475924c7381b92e475b7de2c8",
      "question": "Who is Misty Cummings?",
      "prediction": "wife,",
      "ground_truths": [
        "his wife,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "3d14218e29634731bab87948106fd070",
      "question": "Who pays for all their clothes?",
      "prediction": "Both women",
      "ground_truths": [
        "Both women"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "97c580ecb105499e9fd68b24fb82db8b",
      "question": "Who wore J.Crew at the England summit?",
      "prediction": "Michelle Obama",
      "ground_truths": [
        "Michelle Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e821c5211ebf41148fd5e9db16e605d0",
      "question": "Who is Gordon Bronw's wife?",
      "prediction": "Sarah,",
      "ground_truths": [
        "Sarah,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ba173ea4f5ba499585c7ccd85a75e3ca",
      "question": "Whose wife is Sarah?",
      "prediction": "Prime Minister Gordon Brown's",
      "ground_truths": [
        "British Prime Minister Gordon Brown's"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "88822c4e2acc4ea4a5f713386de7ded3",
      "question": "Who wore J. Crew?",
      "prediction": "Michelle Obama",
      "ground_truths": [
        "Michelle Obama"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "decd6f636da04b8ca01d4353ece9323e",
      "question": "What did Michelle Obama wera?",
      "prediction": "J.Crew,",
      "ground_truths": [
        "J.Crew,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c38b1d313cba4a1c9dd244d09624945a",
      "question": "What type of clothes does Sarah wear to greet dignitaries?",
      "prediction": "An outfit from designer Britt Lintner",
      "ground_truths": [
        "outfit from designer"
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "074bf4298dbc432dbe7989c913948a6c",
      "question": "What does Michelle Obama wear?",
      "prediction": "J.Crew.",
      "ground_truths": [
        "J.Crew,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e24cdb1d7df47b2ae96758ad816aab7",
      "question": "How much is the course?",
      "prediction": "$995,",
      "ground_truths": [
        "$75 for full-day class,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c63aa88af92d4f87b216f060cad550a7",
      "question": "where in mexico it offers cooking classes Seasons of My Heart",
      "prediction": "Oaxaca,",
      "ground_truths": [
        "school in the Oaxacan countryside of southern"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "d1139bdf97f14af2b46dee7b9a3c52fa",
      "question": "Where is the school located?",
      "prediction": "Oaxaca, Mexico",
      "ground_truths": [
        "in the Oaxacan countryside of southern Mexico"
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "33e668576ef44beb833c68a31b422aab",
      "question": "that place offers Mexican cooking classes in Oaxaca",
      "prediction": "Oaxaca, Mexico",
      "ground_truths": [
        "Seasons of My Heart,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "bdd5472a6ccc4d3c92a719454775639d",
      "question": "Who was recognized as a Top 10 CNN Hero of 2009?",
      "prediction": "Army veteran Roy Foster's",
      "ground_truths": [
        "Roy Foster's"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "de6f95a86fe94cc0963cb044ffef3523",
      "question": "was Roy Foster recognized as a Top 10 CNN Hero of 2009?",
      "prediction": "Army veteran",
      "ground_truths": [
        "at \"CNN Heroes: An All-Star Tribute\" as a"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e80f59eb7ee4467ea2a693cc03f7fedd",
      "question": "Who did the organization expand to help?",
      "prediction": "women.",
      "ground_truths": [
        "women."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "83374abdcff843938dfe31e262431577",
      "question": "What do police say about it?",
      "prediction": "witnesses say a red minivan ran a red light and struck two vehicles at an intersection,",
      "ground_truths": [
        "a red minivan ran a red light and struck two vehicles at an intersection,"
      ],
      "em": 0,
      "f1": 0.9166666666666666
    },
    {
      "id": "5d85c0bd310045578980993d8fe218ab",
      "question": "Where was the fatal crash?",
      "prediction": "Anteco, California,",
      "ground_truths": [
        "Fullerton, California,"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "054f3f1eb5554431a707e3f984c4364e",
      "question": "what team is he on?",
      "prediction": "Los Angeles Angels",
      "ground_truths": [
        "Los Angeles Angels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "55a60815fccf47c2826b21d56c3a1923",
      "question": "Who pitched six scoreless innings?",
      "prediction": "Adenhart",
      "ground_truths": [
        "Nick Adenhart"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "e96066c9b99a4c9c8f6e37cc96a91e8d",
      "question": "Where was the venue of the crash?",
      "prediction": "in Fullerton, California,",
      "ground_truths": [
        "Fullerton, California,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "300da5c6b3624ba18d2945c0a64130ea",
      "question": "what was the score?",
      "prediction": "6-4",
      "ground_truths": [
        "scoreless six innings,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86da104406044f35822517a0c9c1b1ba",
      "question": "what are Americans, South Africans investigating?",
      "prediction": "allegations that a dorm parent mistreated students at the school.",
      "ground_truths": [
        "allegations that a dorm parent mistreated students at the school."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0445da1d201e4e53af90522a804d3c73",
      "question": "Dorm parent accused abuse where at?",
      "prediction": "South Africa",
      "ground_truths": [
        "school"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "86078e9a4b7f4384a221fa81c4fca7b6",
      "question": "where has dorm parent been accused of abuse?",
      "prediction": "South Africa.",
      "ground_truths": [
        "school in South Africa"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "569e3465485f4597a4aadeaf1d8b4379",
      "question": "Who are investigation the allegations?",
      "prediction": "South African police",
      "ground_truths": [
        "South African police"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2142d71d782c4fc784cf0e3fe37758e0",
      "question": "What did one parent call Winfrey?",
      "prediction": "\"Oprah is an angel, she is God-sent,\"",
      "ground_truths": [
        "\"Oprah is an angel, she is God-sent,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d4e76b13326f4512a6e0c8b7c7552a8d",
      "question": "what does one mother say about Winfrey?",
      "prediction": "\"Oprah is an angel, she is God-sent,\"",
      "ground_truths": [
        "\"Oprah is an angel, she is God-sent,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "db15047ec8e348f18e46a99e55e3ddec",
      "question": "What did one parent claim was going on at the school?",
      "prediction": "abuse",
      "ground_truths": [
        "abuse"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6a30b126060c4496aaae5e5ae4c37976",
      "question": "What did the judge order?",
      "prediction": "dismissed all charges",
      "ground_truths": [
        "the release of the four men"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3073f61a2dcf4d5c9032ae4dcc47f8c1",
      "question": "What was the judge's verdict?",
      "prediction": "dismissed all charges",
      "ground_truths": [
        "dismissed all charges"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8a87319c8e2f48808dd4c3934cf0c53e",
      "question": "What will the DA do?",
      "prediction": "dismissed all charges Wednesday night and ordered the release of the four men",
      "ground_truths": [
        "her office has launched a criminal investigation into the statements and reports given by the woman."
      ],
      "em": 0,
      "f1": 0.08333333333333334
    },
    {
      "id": "8d39120fa12b4f63a63e4a352b43a18e",
      "question": "What did the judge do?",
      "prediction": "dismissed all charges Wednesday night and ordered the release of the four men",
      "ground_truths": [
        "dismissed all charges"
      ],
      "em": 0,
      "f1": 0.42857142857142855
    },
    {
      "id": "dcc6d8c18da44839a470a13ab7556100",
      "question": "What did the Hofstra student say?",
      "prediction": "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.",
      "ground_truths": [
        "claims that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0.35714285714285715
    },
    {
      "id": "6f4d684ebab5450da56c94bf1f930cd7",
      "question": "What did the student claim?",
      "prediction": "recanted her allegations, prosecutors said.",
      "ground_truths": [
        "that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e62b6e9dfead44c3881da06f669aa294",
      "question": "What were investigators told?",
      "prediction": "The student had told Nassau County police that she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.",
      "ground_truths": [
        "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted."
      ],
      "em": 0,
      "f1": 0.8205128205128205
    },
    {
      "id": "19e435b52d654888be8a2dfa6b343e64",
      "question": "What is the Hofstra student claiming?",
      "prediction": "raped an 18-year-old",
      "ground_truths": [
        "that she was lured to a dorm and assaulted in a bathroom stall."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cc436260a2524c7097fd044bbc5b40c6",
      "question": "What did the woman tell investigators?",
      "prediction": "recanted her claims that she was lured to a dorm and assaulted in a bathroom stall.",
      "ground_truths": [
        "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted."
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "bdbc4d71d79846079ed0526ba43af217",
      "question": "What is Mohammed al-Qahtani  accused of?",
      "prediction": "of helping to plan the September 11, 2001, terror attacks,",
      "ground_truths": [
        "helping to plan the September 11, 2001, terror attacks,"
      ],
      "em": 0,
      "f1": 0.9411764705882353
    },
    {
      "id": "6c98a5be3830481da066866354dee5fe",
      "question": "What did the judge refuse to do?",
      "prediction": "refer the case of Mohammed al-Qahtani",
      "ground_truths": [
        "to refer the case of Mohammed al-Qahtani to prosecutors"
      ],
      "em": 0,
      "f1": 0.7692307692307693
    },
    {
      "id": "1b391680d94f4505905a8d575648f3d9",
      "question": "Which paper had the story?",
      "prediction": "Washington Post",
      "ground_truths": [
        "Washington Post"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "13f53b184a1d491ca79ab04591a29369",
      "question": "What was the White houses response?",
      "prediction": "\"It has never been the policy of this president or this administration to torture.",
      "ground_truths": [
        "\"It has never been the policy of this president or this administration to torture."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c40eca727a4340e4b4e66130b55e3704",
      "question": "What did Crawford say?",
      "prediction": "torture,",
      "ground_truths": [
        "\"We tortured (Mohammed al-) Qahtani,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "783dc9ff141f43229e359bd53b0f7f22",
      "question": "What did the judge do?",
      "prediction": "refused to refer the case",
      "ground_truths": [
        "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,"
      ],
      "em": 0,
      "f1": 0.47058823529411764
    },
    {
      "id": "f5f1913991544a3db364369d36b14e47",
      "question": "What did Susan Crawford say?",
      "prediction": "\"We tortured (Mohammed al-) Qahtani,\"",
      "ground_truths": [
        "\"We tortured (Mohammed al-) Qahtani,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "457356915ea04301a7384118b59ea88d",
      "question": "What was al-Qahtani accused of?",
      "prediction": "helped to plan the September 11, 2001, terror attacks,",
      "ground_truths": [
        "helping to plan the September 11, 2001,"
      ],
      "em": 0,
      "f1": 0.7142857142857143
    },
    {
      "id": "21a75de34c68413c9e418decc5c71d2f",
      "question": "What items were thrown?",
      "prediction": "Molotov cocktails, rocks and glass.",
      "ground_truths": [
        "Molotov cocktails, rocks and glass."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ee7adf8b775344429dbd224b9e06c7d3",
      "question": "how many people died",
      "prediction": "Six",
      "ground_truths": [
        "Six"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e09dea004dc14ca394d37b263e129ac3",
      "question": "what was thrown?",
      "prediction": "Molotov cocktails, rocks and glass.",
      "ground_truths": [
        "Molotov cocktails, rocks and glass."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "715592c05e2a41b8a221df4f7b12274b",
      "question": "Where did it happen?",
      "prediction": "Cairo,",
      "ground_truths": [
        "central Cairo,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "61df2ca95986481da92fc106c8a6a1f0",
      "question": "What caused the violence",
      "prediction": "pro-democracy activists",
      "ground_truths": [
        "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "dace90eb46944bc7aa77627525f618d9",
      "question": "How many people were injured?",
      "prediction": "At least 300",
      "ground_truths": [
        "at least 300"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d6ee51c48856447ebba5d4192903e56a",
      "question": "What sparked the violence?",
      "prediction": "military arrested one man,",
      "ground_truths": [
        "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\""
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "6e3f97b8799e442991ae2ec7b7090a71",
      "question": "How many were injured",
      "prediction": "at least 300",
      "ground_truths": [
        "at least 300"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "918d39163c934fb89811db623983a76c",
      "question": "How many died?",
      "prediction": "Six",
      "ground_truths": [
        "Six"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a766d005b5c240d1af21ccf7a2d0d0f1",
      "question": "What says only 9 percent of Turks had positive view of U.S. in 2007 ?",
      "prediction": "America,",
      "ground_truths": [
        "Pew Research Center"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1411ab49be5d4637a1e2a46c8ea6a69e",
      "question": "What percentage of Turks had positive view of the US in 2007?",
      "prediction": "9 percent",
      "ground_truths": [
        "9 percent"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1cc4aa5f329344e8919635f28f2ca39b",
      "question": "When did Obama go to Turkey?",
      "prediction": "in April.",
      "ground_truths": [
        "April."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "04fef76641e3400ba7c0c83195153a1c",
      "question": "President Obama tried to change what in April ?",
      "prediction": "dispel some of the pervasive suspicion in U.S.-Turkish relations",
      "ground_truths": [
        "public opinion in Turkey.\""
      ],
      "em": 0,
      "f1": 0.16666666666666666
    },
    {
      "id": "c6d9da1ec8ea4a86a2ff76bf9bc1855b",
      "question": "What was the main reason Obama`s visit to Turkey?",
      "prediction": "to remedy the situation of America wielding a big stick for the last eight years.\"",
      "ground_truths": [
        "to remedy the situation of America wielding a big stick for the last eight years.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "94a79e4abb8a4b2ebfec7589ba777553",
      "question": "What is percentage of Turks being positive towards U.S.?",
      "prediction": "9 percent",
      "ground_truths": [
        "9"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "fd467f3a57e2462388848bfd1a495c4b",
      "question": "where has the power being restored?",
      "prediction": "Northeast,",
      "ground_truths": [
        "Fargo, North Dakota,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a4979cd79024479abb28834bc6104114",
      "question": "How many sandbags did the National Guard fill?",
      "prediction": "a million",
      "ground_truths": [
        "a million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5ba0e7bd271342958ca73b52e8358c05",
      "question": "When did the River expect to crest?",
      "prediction": "the first or second week in April.",
      "ground_truths": [
        "this week,\""
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "fd9d280b011a4bfb8e128428c0ab216c",
      "question": "how many sand bags did the national guard fill?",
      "prediction": "million",
      "ground_truths": [
        "a million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7a0c35d82c8a40a0aac26bc6f94d00da",
      "question": "The river expected to crest at what height on Sunday?",
      "prediction": "38 feet",
      "ground_truths": [
        "38 feet"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ec3dcf0931384721a5e7a6525b32ae0a",
      "question": "What does resident say?",
      "prediction": "about 40,000 customers in New York, New Jersey and Connecticut remained without power early Thursday, down from a peak of more than 500,000.",
      "ground_truths": [
        "\"Otherwise I believe we would have washed out a considerable amount of our town,\""
      ],
      "em": 0,
      "f1": 0.05714285714285715
    },
    {
      "id": "1a461bec973b42b6b3e35a0dadd8dbdc",
      "question": "What did the volunteers do?",
      "prediction": "filled a million sandbags and place 700,000 around our city,\"",
      "ground_truths": [
        "fill a million sandbags and place 700,000 around our city,\""
      ],
      "em": 0,
      "f1": 0.8888888888888888
    },
    {
      "id": "90c0b9ccd0c24d71a26ab4e175bca802",
      "question": "To what does Mandisa credit her weight loss?",
      "prediction": "new exercise regimen and diet.",
      "ground_truths": [
        "faith in God"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1fd5919517f14c139d3c092367a33f5d",
      "question": "Amount of weight that Mandisa lost?",
      "prediction": "75 pounds.",
      "ground_truths": [
        "more than 75 pounds."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "59795dd9895a4538bfb6a31e427ee527",
      "question": "When is the album due out>",
      "prediction": "March 24,",
      "ground_truths": [
        "March 24,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c4f891cfa4e24a9f8c46748442169c71",
      "question": "What kind of music does Mandisa sing?",
      "prediction": "Christian pop.\"",
      "ground_truths": [
        "catchy Christian pop tunes"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5156a5e67de04c78a1a43f5ee9dd4347",
      "question": "When is Mandisa's second studio album out?",
      "prediction": "March 24,",
      "ground_truths": [
        "March 24,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "38bc152a0b8e433084e206fbb785e239",
      "question": "When was Phagan murdered?",
      "prediction": "April 26, 1913,",
      "ground_truths": [
        "April 26, 1913,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "565ba022cac0410e9e1ed653c54a25c7",
      "question": "Who was convicted of her murder",
      "prediction": "Leo Frank,",
      "ground_truths": [
        "Leo Frank,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fe2e9d3e09e54b969381827d1eddd4ff",
      "question": "Who was murdered in 1913?",
      "prediction": "Mary Phagan",
      "ground_truths": [
        "Mary Phagan"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "63dba556c6ea430890e1549f442c456e",
      "question": "What year was Mary Phagan's murder",
      "prediction": "1913.",
      "ground_truths": [
        "1913,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9c3f78e8e52f4dd988ad0b7f4aaa9e3b",
      "question": "Who was convicted?",
      "prediction": "Leo Frank,",
      "ground_truths": [
        "Leo Frank,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fef9627cdad84ad8ab8e138012e3e0ca",
      "question": "What hot buttons did the case hit",
      "prediction": "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.",
      "ground_truths": [
        "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "21dbbe66a7b649ea945b8b3d2a717063",
      "question": "How long was the boy with autism left in a cubicle?",
      "prediction": "at least two and a half hours.",
      "ground_truths": [
        "at least two and a half hours."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "41b02a0a6bcb4004ae5396399dd3395f",
      "question": "What was the outcome of the practices?",
      "prediction": "deaths",
      "ground_truths": [
        "some deaths"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "018cfb1991114779a856fcf97c8de8be",
      "question": "What report uncovered information on the discipline of children?",
      "prediction": "Government Accountability Office",
      "ground_truths": [
        "Government Accountability Office"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7ee8dc50324b435c8634db82fc95f921",
      "question": "For what is Buster Keaton saved by in \"Steamboat Bill Jr\"?",
      "prediction": "an open window",
      "ground_truths": [
        "an open window"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5088d10d4c834617b64b3b952a76df5f",
      "question": "What is Buster Keaton saved by in \"Steamboat Bill Jr.\"?",
      "prediction": "open window",
      "ground_truths": [
        "open window"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fef813f4b22a44e6a416d35ae878afd8",
      "question": "Where are they choosing the best stunts?",
      "prediction": "Indiana Jones",
      "ground_truths": [
        "\"The Screening Room\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4cbbf897c5a4ae58c7a3c50c0d257ec",
      "question": "Which Keaton film was chosen?",
      "prediction": "\"Steamboat Bill, Jr.\"",
      "ground_truths": [
        "\"Steamboat Bill, Jr.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "59566aea9daa470f97c3d205d76f4a39",
      "question": "What does the Screening Room choose?",
      "prediction": "the best stunts ever pulled off",
      "ground_truths": [
        "the best stunts ever pulled off"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e8c9439ad7b44b493274c5859920620",
      "question": "Which James Bond film was chosen?",
      "prediction": "\"Raiders of the Lost Ark.\"",
      "ground_truths": [
        "\"GoldenEye\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "18fc2fa9f7a24777954eab16627d62dd",
      "question": "You could say bungee jumping is an art form thanks to whom?",
      "prediction": "Terry Leonard",
      "ground_truths": [
        "Stuntman: Wayne Michaels"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a8f8b27573d46b7bd7256a603268a87",
      "question": "Who met at white house?",
      "prediction": "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici,",
      "ground_truths": [
        "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d24662abff4b4fcc8032254ae8e9b527",
      "question": "What did the CIA memo say about the Brazilian general?",
      "prediction": "the regime will not be above using the threat of intervention or tools of diplomacy and covert action to oppose leftist regimes, to keep friendly governments in office, or to help place them there.\"",
      "ground_truths": [
        "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\""
      ],
      "em": 0,
      "f1": 0.0851063829787234
    },
    {
      "id": "8c651c2134a6460db75b7805f0b7badf",
      "question": "When were the documents declassified?",
      "prediction": "July",
      "ground_truths": [
        "in July"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ba0dc5621c94472a9de2afe8c0c3cc7b",
      "question": "How many leaders met at the White House?",
      "prediction": "the two",
      "ground_truths": [
        "two"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a10302d3d03a4562827d729c1284abc7",
      "question": "What did Nixon offer?",
      "prediction": "money or other discreet aid",
      "ground_truths": [
        "money or other discreet aid"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "64307f80a70740eca2071c3fa6ac7e4a",
      "question": "What did the U.S. want Brazil to do?",
      "prediction": "defeat the socialist government",
      "ground_truths": [
        "overthrow the socialist government of Salvador Allende in Chile,"
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "b46e3738dddb49d3b21ad3bdb678df23",
      "question": "What does document show?",
      "prediction": "details collusion between the colossus of the North [the United States] and the colossus of the South [Brazil]],\"",
      "ground_truths": [
        "Nixon offered money or other discreet aid for the effort if it could be made available,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "848250b6500548f9a8ff55cf6f657c45",
      "question": "Who believe Damas boarded a flight?",
      "prediction": "authorities",
      "ground_truths": [
        "authorities"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "60e90f8b066f4921897a71ce5031b5ef",
      "question": "What day were the bodies found?",
      "prediction": "Saturday,",
      "ground_truths": [
        "Sunday"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "870f2e0c2b614c81b6cdf70cddc89a1d",
      "question": "What did the sheriff say there was?",
      "prediction": "eighteen killings",
      "ground_truths": [
        "any indication of an individual out in the neighborhoods committing additional crimes or homicides,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ade061d2af09443ca7431c42dc6d8bcd",
      "question": "What is Mesac Damas' age?",
      "prediction": "33-year-old",
      "ground_truths": [
        "33-year-old"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c1ae542670bf40bfb26367e761d2acfa",
      "question": "On what day were the bodies found?",
      "prediction": "Saturday,",
      "ground_truths": [
        "Saturday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "56d7ce14a3d640b3ac4df6f37f97e668",
      "question": "Where were the bodies found?",
      "prediction": "Naples home.",
      "ground_truths": [
        "Naples home."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "85fab2583c0348308eb98aa738583563",
      "question": "Who do police believe boarded a flight to Haiti on Friday morning?",
      "prediction": "Mesac Damas",
      "ground_truths": [
        "Mesac Damas,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "afdf027bc3b546eca6940aebedfd3718",
      "question": "Where did Mesac Damas board a flight to?",
      "prediction": "Haiti",
      "ground_truths": [
        "Haiti"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "043896b56ef744daa11eb4c6857bed35",
      "question": "Who could be reinstated?",
      "prediction": "President Jose Manuel Zelaya",
      "ground_truths": [
        "Honduran President Jose Manuel Zelaya"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "70ac70404d9342a58cf771326205dfa2",
      "question": "Did the negotiators reach a deal?",
      "prediction": "negotiators reached an agreement",
      "ground_truths": [
        "reached an agreement late Thursday to form a government of national reconciliation."
      ],
      "em": 0,
      "f1": 0.30769230769230765
    },
    {
      "id": "c3101f302e714d2881aded1522335b6c",
      "question": "Who is Manuel Zelaya replaced with?",
      "prediction": "Roberto Micheletti,",
      "ground_truths": [
        "Roberto Micheletti,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "cc2d08fa347241ec99e8c7120b839fde",
      "question": "For whom do the negotiators work?",
      "prediction": "the nation's congress,",
      "ground_truths": [
        "Zelaya and Roberto Micheletti,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2451b810994d4f1a9a28cc97350262e4",
      "question": "Manuel Zelaya is the president of what country?",
      "prediction": "Honduras,",
      "ground_truths": [
        "Honduran"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a8ff7b13f4b943d89f09cf03cbde580a",
      "question": "which nations are at odds",
      "prediction": "Argentina",
      "ground_truths": [
        "British"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "441f524567e9477a9b0d07f4a0907622",
      "question": "what does desire say",
      "prediction": "\"oil may be present in thin intervals but that reservoir quality is poor.\"",
      "ground_truths": [
        "\"oil may be present in thin intervals but that reservoir quality is poor.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "40d56efbcf5041ff9ab958e865e63171",
      "question": "What quality is the reservoir?",
      "prediction": "poor.\"",
      "ground_truths": [
        "poor.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e296d418c6614640b45afe738e0fe7ce",
      "question": "What is causing tension between the UK and Argentina?",
      "prediction": "ownership of the Falklands.",
      "ground_truths": [
        "gas"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "412a163387474317b4eb02196506d1f4",
      "question": "What did the Falklands government say?",
      "prediction": "natural resources around the islands should be protected,",
      "ground_truths": [
        "\"This has to do with the defense of the interests of Argentineans, not just about sovereignty,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2818d5e47a844bc9831496e2f587a803",
      "question": "Argentina is at odds with who over ownership of South Atlantic islands?",
      "prediction": "British",
      "ground_truths": [
        "London"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "435e49b5d4474366b64f5e29ee0c3f63",
      "question": "Who was accused of firing on Indian forces?",
      "prediction": "Pakistani militants",
      "ground_truths": [
        "separatist militants"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "6bc25fd1669d435fbb8b7d5fb1f08bb0",
      "question": "What is the death toll in the fighting in Kashmir?",
      "prediction": "at least 25 dead",
      "ground_truths": [
        "25 dead"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "5b31bab9ac6d462a8c65f38942f63649",
      "question": "What type of campaign has Kashmir been involved in?",
      "prediction": "violent separatist",
      "ground_truths": [
        "violent separatist"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fa2a50d9d16a449b9ecf338dcedd85ab",
      "question": "What campaign has waged for two decades?",
      "prediction": "violent separatist",
      "ground_truths": [
        "violent separatist"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "94d420e9c7754759a7391868d14e81fc",
      "question": "Who were involed in the gunfights?",
      "prediction": "Indian army and separatist militants",
      "ground_truths": [
        "the Indian army and separatist militants in Indian-administered"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "ab1542b2a7e64584ba41a72e10d3f1d4",
      "question": "Who was involved in the fighting in Kashmir?",
      "prediction": "Separatist militants",
      "ground_truths": [
        "the Indian army and separatist militants"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "d548f4b0a8014fa3bdc21f5d4e65665c",
      "question": "What weapon was used in the fighting?",
      "prediction": "\"minimum collateral damage to property,\"",
      "ground_truths": [
        "helicopter gunships"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a24d5cc2a8f44819ca35879d229e8bc",
      "question": "Where was the tournament?",
      "prediction": "Doral",
      "ground_truths": [
        "Florida"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0161b1de8e9944f58ba3663305c9759a",
      "question": "what Swedish star strips down to the bare essentials to play?",
      "prediction": "Henrik Stenson",
      "ground_truths": [
        "Henrik Stenson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c78e0c2903384595af84d1f0cf51a54f",
      "question": "Who create a stir with Striptease?",
      "prediction": "Henrik Stenson",
      "ground_truths": [
        "Henrik Stenson"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9cdb3be6523047f48d6ec5110de357e0",
      "question": "What did he strip down to?",
      "prediction": "the bare essentials -- a pair of white boxer shorts",
      "ground_truths": [
        "a pair of white boxer shorts"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "dafae23b13b148619784124572b26008",
      "question": "where Henrik Stenson creates a stir with 'striptease'?",
      "prediction": "Blue Monster course at Doral",
      "ground_truths": [
        "on the Blue Monster course at Doral"
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "ad0e4b1a103d400f9ef4dd6827e215b9",
      "question": "where was tournament?",
      "prediction": "Doral",
      "ground_truths": [
        "Blue Monster course at Doral"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "7611c1e36e6d43a991ba213c40fe5178",
      "question": "What could the discovery of water boost?",
      "prediction": "potential resources that could sustain future exploration of the moon and beyond.",
      "ground_truths": [
        "sustain future exploration of the moon and beyond."
      ],
      "em": 0,
      "f1": 0.7777777777777778
    },
    {
      "id": "fe08748775ae44a6ad801cbfba521bc4",
      "question": "What is the lunar orbiter taking pictures of?",
      "prediction": "over 50 areas on the moon,",
      "ground_truths": [
        "the moon's surface"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a3d418c59fe644388fccb6a470ec954b",
      "question": "What does the space agency say?",
      "prediction": "The lunar Crater Observation and Sensing Satellite, or L-CROSS, and its companion spacecraft crashed into a crater at the moon's south pole in October and discovered water in a very dark and very cold place. L-CROSS researchers said about 25 gallons of water were detected in the crater, which measured about 60 feet wide by a few feet deep.",
      "ground_truths": [
        "scientists know about Earth's closest neighbor."
      ],
      "em": 0,
      "f1": 0.03389830508474576
    },
    {
      "id": "591c9c6b06934a25ae6612de894283b6",
      "question": "What could the discovery of water in a lunar crater do?",
      "prediction": "provide you insight,",
      "ground_truths": [
        "sustain future exploration of the moon and beyond."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3392fcd067b4e50ba6b28a937389f6b",
      "question": "How many in numbers are there",
      "prediction": "83",
      "ground_truths": [
        "100,000"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cffde012e2ae45b5bc9c6cf569ce7657",
      "question": "What breeds quickly and travels quickly?",
      "prediction": "pythons,\"",
      "ground_truths": [
        "Burmese python."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ce020475bd42443c9a47498225879754",
      "question": "What does the expert patrol?",
      "prediction": "Burmese pythons,\"",
      "ground_truths": [
        "the Florida Keys,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "ad293f2dfc814c049ffc3d774d15033d",
      "question": "Where is a perfect place for Burmese pythons to live?",
      "prediction": "the Everglades,",
      "ground_truths": [
        "The Everglades,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9bdff30a8674448db3b028d425d6c16b",
      "question": "Where is this place",
      "prediction": "Florida's Everglades.",
      "ground_truths": [
        "Florida Everglades."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "0453e067ffbf4cf19accf2fb20bb7db0",
      "question": "What does the reptile expert do?",
      "prediction": "holds a Burmese python he found in the Florida Everglades.",
      "ground_truths": [
        "holds a Burmese python"
      ],
      "em": 0,
      "f1": 0.5454545454545454
    },
    {
      "id": "f5e5584880dc440c848418daf2ade992",
      "question": "what is happening with microsoft",
      "prediction": "EU continues to watch the company's behavior on the previous matters.",
      "ground_truths": [
        "round of investigations"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "858e77b8d0564a3486c079418a48fb5f",
      "question": "Who else is under investigation of the EC?",
      "prediction": "Microsoft.",
      "ground_truths": [
        "Microsoft."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "59a6f7f87da84cccb0a6bae21feb8de1",
      "question": "what EC says Intel has been abusing its dominant market position?",
      "prediction": "The European Commission fined Intel a record $1.45 billion for violating anti-trust laws.",
      "ground_truths": [
        "anti-trust laws."
      ],
      "em": 0,
      "f1": 0.3076923076923077
    },
    {
      "id": "2cf6668f6c3e413ab7da6c421b25b6b2",
      "question": "What was Intel fined for?",
      "prediction": "violating anti-trust laws.",
      "ground_truths": [
        "violating anti-trust laws."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "09e6ad72a4334ed7a46d3a12d1363b67",
      "question": "how much was the fine",
      "prediction": "$1.45 billion",
      "ground_truths": [
        "$1.45 billion"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6afa42cb52a342458ab65f1c2235ea11",
      "question": "What company initiated the investigation into Intel?",
      "prediction": "the European Commission",
      "ground_truths": [
        "AMD, a competitor, launched this in Europe (and in Japan and South Korea)"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9c6c32cae71b496c819d415dbd625fb9",
      "question": "what has the EC said about Intel",
      "prediction": "has been abusing its dominant market position in semiconductors for years.",
      "ground_truths": [
        "been abusing its dominant market position in semiconductors"
      ],
      "em": 0,
      "f1": 0.8421052631578948
    },
    {
      "id": "5bc0838118ce439fa15dc66813fed529",
      "question": "where did the election take place",
      "prediction": "Libreville, Gabon.",
      "ground_truths": [
        "Libreville, Gabon."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "047c102fd84d4e478afe36ce9a384265",
      "question": "when did voters go to the polls?",
      "prediction": "Sunday",
      "ground_truths": [
        "Sunday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "eb49e4d87a4a4b5e8fbc099316b18e95",
      "question": "Who is Ali Bongo?",
      "prediction": "son of Gabon's former president",
      "ground_truths": [
        "son of Gabon's former president"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6285e913cb0d4e5f9ab9b2f23383933e",
      "question": "who was declared winner of the election",
      "prediction": "Ali Bongo",
      "ground_truths": [
        "Ali Bongo"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6e5fcb94e6a14465b0a57e26e693271e",
      "question": "who is the winner",
      "prediction": "Gabonese",
      "ground_truths": [
        "son of Gabon's former president was declared the"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "869b6218682e464bae6c1328ea6b2a7e",
      "question": "who was elected?",
      "prediction": "Ali Bongo",
      "ground_truths": [
        "son of Gabon's former president"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "76d53c92aca14d1d9e78aa402b17ca18",
      "question": "who is Gabon's former president?",
      "prediction": "Omar Bongo,",
      "ground_truths": [
        "Omar Bongo,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0028e5ee411f4dd9837c601ad7f0cc4a",
      "question": "who is protesting?",
      "prediction": "Opposition supporters",
      "ground_truths": [
        "Opposition supporters"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "61f45cf18f2c47f18ba19d8e971f5712",
      "question": "on what day did the election take place",
      "prediction": "Sunday",
      "ground_truths": [
        "Thursday,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "204991cae42743b583fe6e336abb0b6a",
      "question": "who faces arrest?",
      "prediction": "Roberto Micheletti,",
      "ground_truths": [
        "Ousted Honduran President Jose Manuel Zelaya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fbca8a676a7449e8ad526ad6d5b8adb8",
      "question": "who do the unions support?",
      "prediction": "\"Armed forces! Armed forces!\"",
      "ground_truths": [
        "Zelaya"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0ca42356781e422d89c3767c3d711844",
      "question": "when will Jose Manuel Zelaya return home?",
      "prediction": "Thursday",
      "ground_truths": [
        "Thursday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ab546d56ffdb4c989b8fc4f76950eb1a",
      "question": "What did she remember?",
      "prediction": "when daughter Sasha exhibited signs of potentially deadly meningitis",
      "ground_truths": [
        "when daughter Sasha exhibited signs of potentially deadly meningitis"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "abdf1b5352e3426891d05c65e282677c",
      "question": "What is the first lady speaking about?",
      "prediction": "health care reform",
      "ground_truths": [
        "health care"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "2f19508f628f4cecaa6b3db9c3ad1c56",
      "question": "What will the speech be about",
      "prediction": "focusing on the issue by focusing on how health care can affect families.",
      "ground_truths": [
        "health care reform"
      ],
      "em": 0,
      "f1": 0.26666666666666666
    },
    {
      "id": "7cfa3ba0cc72417a8103369446fcaa87",
      "question": "what is the issue",
      "prediction": "health care",
      "ground_truths": [
        "how health care can affect families."
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "e2988ddd1e4943d3b0a35c070b6c2091",
      "question": "What did Obama call for",
      "prediction": "improved governance",
      "ground_truths": [
        "\"new chapter\" of improved governance in Afghanistan"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "156a8983efb5464299bb5ed262ae00e2",
      "question": "what has president obama called for",
      "prediction": "a \"new chapter\" of improved governance",
      "ground_truths": [
        "\"new chapter\" of improved governance in Afghanistan"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "62d99a30c21e4402b596e8f2885a42e7",
      "question": "What is President Obama calling for?",
      "prediction": "a \"new chapter\" of improved governance",
      "ground_truths": [
        "a \"new chapter\" of improved governance in Afghanistan now that Karzai's re-election as"
      ],
      "em": 0,
      "f1": 0.5882352941176471
    },
    {
      "id": "ce353d421545436daa25071d4ebdc870",
      "question": "Where should U.S. and Britain keep out of, warns Mugabe?",
      "prediction": "Zimbabwe",
      "ground_truths": [
        "election,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1ad512845a8249638d9780e7d6b9caee",
      "question": "what did the president Mugabe likens?",
      "prediction": "one American diplomat to a \"prostitute\"",
      "ground_truths": [
        "likening one American diplomat to a \"prostitute\""
      ],
      "em": 0,
      "f1": 0.9090909090909091
    },
    {
      "id": "84a848a62eed4fb7b121decd5db944d8",
      "question": "To what does Zimbabwe President Robert Mugabe liken U.S. diplomat?",
      "prediction": "\"prostitute\"",
      "ground_truths": [
        "\"prostitute\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3f078b8bb57d47119420af37d7529346",
      "question": "what was the warns provided by mugabe?",
      "prediction": "against outside influences in next month's run-off election,",
      "ground_truths": [
        "against outside influences in next month's run-off"
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "55b31d139f764bfe81829b76c62031a8",
      "question": "what does mugabe say",
      "prediction": "\"You saw the joy that the British had, that the Americans had, and saw them here through their representatives celebrating and acting as if we Zimbabwe are either an extension of Britain or ... America. You saw that little American girl [U. S. Assistant Secretary of State for African Affairs Jendayi Frazer] troting around the globe like a prostitute....\"",
      "ground_truths": [
        "\"Zimbabwe cannot be British, it cannot be American. Yes, it is African,\""
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "323ba733ecc64d9f93cb6c466ccc6f91",
      "question": "who are accuced for the murder",
      "prediction": "the violence targets opponents of Mugabe and his Zanu-PF party.",
      "ground_truths": [
        "Mugabe's supporters."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "91764d605f5f4795b322ed5e35bed029",
      "question": "For what did Tsvangirai accuse?",
      "prediction": "\"trying to steal the election\" and \"intimidating the population and election officials as well.\"",
      "ground_truths": [
        "kidnappings, torture and other violence, including the deaths of opposition party members."
      ],
      "em": 0,
      "f1": 0.08695652173913043
    },
    {
      "id": "cbb711ac029644ba8e66865d150f486c",
      "question": "Who attends funeral?",
      "prediction": "Tsvangirai",
      "ground_truths": [
        "Tsvangirai"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4515974f2776464ea560272d9179a08c",
      "question": "who is mugabe?",
      "prediction": "President",
      "ground_truths": [
        "Zimbabwe"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9a5e4762fc7642c8b6c0e3edb934091e",
      "question": "What was Cummins found alongside in a tub?",
      "prediction": "a man's",
      "ground_truths": [
        "man's lifeless, naked body"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "b6cfe0943b2e41438f69abee84b49a47",
      "question": "Who received all the inquiries?",
      "prediction": "Brett Cummins,",
      "ground_truths": [
        "\"The station"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fb8fd9786fa4bd38963ac00bde7e7eb",
      "question": "What was found in the tub a week ago?",
      "prediction": "a man's lifeless, naked body",
      "ground_truths": [
        "a man's lifeless, naked body"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b8d9791f00d34f9c803ce6000b087b97",
      "question": "what job did Brett Cummins resign from",
      "prediction": "meteorologist at KARK,",
      "ground_truths": [
        "meteorologist at KARK,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af49f9dece07440492b976d619ab4709",
      "question": "what was Brett Cummins  found alongside",
      "prediction": "dead man's",
      "ground_truths": [
        "man's lifeless, naked body"
      ],
      "em": 0,
      "f1": 0.3333333333333333
    },
    {
      "id": "a3b3f6f5ac1642db8cb48c5539e1bc21",
      "question": "Who resigned as a meteorologist?",
      "prediction": "Brett Cummins,",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "76d4150a11f94a0a9df3acb524379dc3",
      "question": "Who resigned on Friday",
      "prediction": "Brett Cummins,",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0b2ff0268cb64664b0b7a27b7f5355b8",
      "question": "Which day did Brett Cummins resign?",
      "prediction": "Friday",
      "ground_truths": [
        "Friday"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a2025daeb7184c2badca285a883ead1e",
      "question": "What is the name of the meterologist who resigned at KARK?",
      "prediction": "Brett Cummins,",
      "ground_truths": [
        "Brett Cummins,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e6cad506fcab4d979e90f4a9176b1408",
      "question": "What number of searches are planned for Monday?",
      "prediction": "Three",
      "ground_truths": [
        "Three"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9e32a1d3fd11455280d3aecde908c2e6",
      "question": "Where did the pilot and passengers take off from?",
      "prediction": "the Dominican Republic",
      "ground_truths": [
        "Casa de Campo International Airport in the Dominican Republic"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "d83aff530fd447b49138953308883ac2",
      "question": "How many searches have been made?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1d1a6ab35fe14fdd8bbe88f884d82ce6",
      "question": "What did the Coast Guard find?",
      "prediction": "debris",
      "ground_truths": [
        "debris"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2e613fb291864bdd8a0198c5df5ae238",
      "question": "Where is the debris from?",
      "prediction": "the area where the single-engine Cessna 206 went down,",
      "ground_truths": [
        "the single-engine Cessna 206 went down,"
      ],
      "em": 0,
      "f1": 0.8333333333333333
    },
    {
      "id": "62c441cc36584729bc6608d4f56ca4d6",
      "question": "When will the searches occur?",
      "prediction": "Monday",
      "ground_truths": [
        "Monday,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e52126edb5a499381fb1f064dd4377c",
      "question": "How many searches have taken place?",
      "prediction": "five",
      "ground_truths": [
        "five"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b91f7ce859194bf3a952f9948618cfaf",
      "question": "Where might the debris come from?",
      "prediction": "area",
      "ground_truths": [
        "the single-engine Cessna 206"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6973eb89fb1441328fc7a79248c2a0ec",
      "question": "What is planned for Monday?",
      "prediction": "Three searches",
      "ground_truths": [
        "Search operations"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "962fac5bf5424b35a4237c1af07ede3e",
      "question": "when did Wagner waited 4 hours to call the Coast Guard?",
      "prediction": "after Wood went missing off Catalina Island,",
      "ground_truths": [
        "1981"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "77357032f0954a87b6f12d01f073907a",
      "question": "What happened to Natalie Wood?",
      "prediction": "drowned",
      "ground_truths": [
        "drowned in the Pacific Ocean"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "39d86558a6424e1289089f1dc859b691",
      "question": "Who has not yet been interviewed by police?",
      "prediction": "Dennis Davern,",
      "ground_truths": [
        "Dennis Davern,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6179293e102d4898bee708ec10582472",
      "question": "Who drowned off Catalina Island in 1981?",
      "prediction": "Natalie Wood's",
      "ground_truths": [
        "Natalie"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "1050ec903b444d02bff6c0c0111a8351",
      "question": "What did police receive?",
      "prediction": "the presser, we got a number of calls,",
      "ground_truths": [
        "additional information"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "155783cc25514eb5b17116ed5eaed9c6",
      "question": "Whom have detectives yet to interview?",
      "prediction": "Dennis Davern,",
      "ground_truths": [
        "Dennis Davern,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "81d4be0064a24ca48f1bc8655f1d561c",
      "question": "What do police receive after a recent press conference?",
      "prediction": "number of calls,",
      "ground_truths": [
        "a number of calls,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e665fe3a57234e01ab82f67a205fd6fe",
      "question": "What was the reason for PlanetSolar to stops off in Hong Kong?",
      "prediction": "part of its 18-month journey around the world.",
      "ground_truths": [
        "as part of its 18-month journey around the world."
      ],
      "em": 0,
      "f1": 0.9333333333333333
    },
    {
      "id": "73ffe7e6a2624701aed02b60f8690bdd",
      "question": "Where did it stop off at?",
      "prediction": "Hong Kong's Victoria Harbor",
      "ground_truths": [
        "Hong Kong's Victoria Harbor"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "32d011bee7194d05b191109f63093a09",
      "question": "What can it produce?",
      "prediction": "on average 94 kilowatts of power",
      "ground_truths": [
        "on average 94 kilowatts of power"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bbe5fe704e1147bf8ad9fd90297f22ac",
      "question": "What are the measure of the Solar yacht?",
      "prediction": "31 meters (102 feet)",
      "ground_truths": [
        "31 meters (102 feet) long and 15 meters (49 feet) wide,"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "f1d31a7dd5fe48ec86396fe1c70594c8",
      "question": "What is the area of the boat?",
      "prediction": "31 meters (102 feet)",
      "ground_truths": [
        "31 meters (102 feet) long and 15 meters (49 feet) wide,"
      ],
      "em": 0,
      "f1": 0.5333333333333333
    },
    {
      "id": "11199ef38b1946959b670f145a229301",
      "question": "What will they embrace?",
      "prediction": "this technology",
      "ground_truths": [
        "technology"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b9e49c54d5cc47829e27ab441af295f1",
      "question": "What has been hope from Captain of Planet Solar to?",
      "prediction": "the sun,",
      "ground_truths": [
        "and renewable energy at home everyday,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "93df3e4cd7ed4859a3e67bb63fded5f5",
      "question": "Who was removed from duties?",
      "prediction": "The Rev. Alberto Cutie",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "50929bed80564062ad70230ddb02f956",
      "question": "Who says he is in love with the woman?",
      "prediction": "The Rev. Alberto Cutie",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c064d29477e543a8a9f10e1f063f08a8",
      "question": "Who was the woman?",
      "prediction": "The Rev. Alberto Cutie",
      "ground_truths": [
        "has not been publicly identified,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "99320c1449e54556ba293ed05d7f040d",
      "question": "Who was removed from his duties?",
      "prediction": "Rev. Alberto Cutie",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e65e84946d1d4f99819210a08cf3fd8b",
      "question": "Who struggled with relationship?",
      "prediction": "Cutie",
      "ground_truths": [
        "They have \"both struggled\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "216f9d42835642648e2f0a9ff9b922cd",
      "question": "Who says he is in love with the woman, considering his options?",
      "prediction": "Cutie",
      "ground_truths": [
        "Rev. Alberto Cutie"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "2d98ca69eb4344f2b7c4d8eaa52e85e7",
      "question": "In which archdiocese was the priest removed?",
      "prediction": "St. Francis De Sales Catholic Church in Miami Beach, Florida,",
      "ground_truths": [
        "Miami Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.4615384615384615
    },
    {
      "id": "abf52d1c0add48f28486a2a3840f760c",
      "question": "what did the rumours say",
      "prediction": "\"Sex and the City's\" Kim Cattrall isn't get along with her co-star Kristin Davis,",
      "ground_truths": [
        "would insinuate that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,"
      ],
      "em": 0,
      "f1": 0.8275862068965517
    },
    {
      "id": "5ff5d8f1083b442cbf759f1980f8cd15",
      "question": "What did he co-write",
      "prediction": "its signature song,\"The Devil Went Down to Georgia.\"",
      "ground_truths": [
        "its signature song,\"The Devil Went Down to Georgia.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3ded2ac8aff04250896711bfc070cd39",
      "question": "What is the name of the band?",
      "prediction": "The Charlie Daniels",
      "ground_truths": [
        "The Charlie Daniels"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc16e9799a154e94b2bcade2e9eacd72",
      "question": "Is DiGregorio an original member of the band?",
      "prediction": "keyboardist",
      "ground_truths": [
        "keyboardist and"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "63b89182644f454c92026c4ca6747140",
      "question": "Did he write the song on his own?",
      "prediction": "co-wrote",
      "ground_truths": [
        "co-wrote"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "352bbdac5d414a65a714e30a0fd97f42",
      "question": "Who is an original member of the band?",
      "prediction": "Joel \"Taz\" DiGregorio,",
      "ground_truths": [
        "\"Taz\" DiGregorio,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "76c6e9ef00884f509bc625e25b15cc85",
      "question": "What was DiGregorio a member of",
      "prediction": "The Charlie Daniels Band,",
      "ground_truths": [
        "The Charlie Daniels Band,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a3ff08899c474fcdaaae2dd2409ec7a0",
      "question": "what did the faa say",
      "prediction": "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"",
      "ground_truths": [
        "hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\""
      ],
      "em": 0,
      "f1": 0.9600000000000001
    },
    {
      "id": "5d464ecc16894c0cbc33ed95ac502289",
      "question": "what were US pilots told to watch for?",
      "prediction": "\"falling space debris,\"",
      "ground_truths": [
        "\"falling space debris,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "33d1e51367a3481086b0b8206010e16c",
      "question": "What types of sightings were reported?",
      "prediction": "fireball",
      "ground_truths": [
        "booms and at least one fireball"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "12cc92d88fd24d6697009b061d1769c2",
      "question": "When was the video shot?",
      "prediction": "Sunday morning.",
      "ground_truths": [
        "Sunday morning."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ed32ea00d52a492b93ce00ce06b93f4a",
      "question": "what collided in space?",
      "prediction": "two satellites",
      "ground_truths": [
        "two satellites"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "22ec9df97ce34bd3a6d514c8c2fd3c13",
      "question": "what did the spokesman say",
      "prediction": "the FAA received no reports from pilots in the air of any sightings but the agencyreceived \"numerous\" calls from people on the ground from Dallas, Texas, south to Austin, Texas.",
      "ground_truths": [
        "There were no reports of ground strikes or interference with aircraft in flight,"
      ],
      "em": 0,
      "f1": 0.2564102564102564
    },
    {
      "id": "ce37bce116da48499f15b84b3c158692",
      "question": "where was the video shot",
      "prediction": "Austin, Texas,",
      "ground_truths": [
        "Austin, Texas,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "57351304b16347d58b309e84d19b79aa",
      "question": "What were pilots told to watch out for?",
      "prediction": "\"falling space debris,\"",
      "ground_truths": [
        "\"falling space debris,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f126e81901054b3ba5fb4ce920b1df38",
      "question": "When will the book be released?",
      "prediction": "April 13,",
      "ground_truths": [
        "April 13,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "92216f1e211c4ddf968b958588255feb",
      "question": "Who else has the author written biographies on?",
      "prediction": "Oprah Winfrey.",
      "ground_truths": [
        "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3365b3e46e9240dca893042f6f34486f",
      "question": "What is the name of Oprah Winfrey's tell- all?",
      "prediction": "\"Oprah: A Biography,\"",
      "ground_truths": [
        "\"Oprah: A Biography,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "08b8fd7f109845eda0ffb6868479f8c2",
      "question": "its authorized or not?",
      "prediction": "will be released on April 13,",
      "ground_truths": [
        "unauthorized"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "50e8ae7a28b347edbef6a9604da6802a",
      "question": "Who will write the biography?",
      "prediction": "Kelley,",
      "ground_truths": [
        "Kelley,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "803441a3a59d42b293810212bf2d6ced",
      "question": "What other biographies has Kelley written?",
      "prediction": "\"Oprah: A Biography,\"",
      "ground_truths": [
        "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0dfeb9e7d43747bcac4e5ac700caec79",
      "question": "What was found capsized?",
      "prediction": "sailboat",
      "ground_truths": [
        "sailboat"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "0c1c6f6b44e3466eac1a68dadefd06ab",
      "question": "A sailboat matching the decription was found how?",
      "prediction": "overturned about 5:15 p.m. Saturday,",
      "ground_truths": [
        "overturned"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "d4b5dfb2769d4208804f62b71b6e71c3",
      "question": "What caused the capsize?",
      "prediction": "it took on water and capsized.",
      "ground_truths": [
        "took on water"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "eb0682a6d1694acf886b84a407d68416",
      "question": "What was taking part in regatta?",
      "prediction": "boat",
      "ground_truths": [
        "The sailboat, named Cynthia Woods,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "52e75f8056964c38b2a325a45841c089",
      "question": "Where was the sailboat headed?",
      "prediction": "Veracruz, Mexico,",
      "ground_truths": [
        "Veracruz, Mexico,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7002124cbe1043ecb021b8193037ab49",
      "question": "Who won the endorsement of South Carolina Gov. Nikki Haley?",
      "prediction": "Mitt Romney",
      "ground_truths": [
        "Romney"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "c9494792b6f44d0b86ee620dfee49502",
      "question": "Who praised Mitt Romneys credentials?",
      "prediction": "The Register's editorial board",
      "ground_truths": [
        "The board"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "c4fa4d65d44e4af08a95506f60889546",
      "question": "Who won endorsement of South Carolina Gov,?",
      "prediction": "Mitt Romney",
      "ground_truths": [
        "Mitt Romney"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f7904039eab64b9b8cb6271e7b8bf1eb",
      "question": "What came two weeks before the Iowa caucus?",
      "prediction": "endorsement",
      "ground_truths": [
        "The public endorsement"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "ba62f9a8709e43e6a4353462aed825f5",
      "question": "Bob Dole and The Des Moines Register's editoral praised who?",
      "prediction": "Mitt Romney",
      "ground_truths": [
        "Mitt Romney"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4c89b52721af4cfab0cb98a246f488eb",
      "question": "Whose endorsement did Romney win?",
      "prediction": "Bob Dole,",
      "ground_truths": [
        "Bob Dole,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fabb462af78845fc8a68a0b2a7a85607",
      "question": "The endorsements came about two weeks before what?",
      "prediction": "Iowa's critical presidential caucuses",
      "ground_truths": [
        "Iowa's critical presidential caucuses"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2d73cd5a9e4d47929655d94692a79355",
      "question": "How many million internet users are in china?",
      "prediction": "338",
      "ground_truths": [
        "338 million Internet users in"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "d4a8128476ac4d929bb12460282c1ed8",
      "question": "how many users has china?",
      "prediction": "338 million",
      "ground_truths": [
        "338 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dd14e1dc2f5544ec958f98850e4105bd",
      "question": "what did henry have removed",
      "prediction": "cancerous tumor.",
      "ground_truths": [
        "cancerous tumor."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c61125f708f444a08a74f67c940ed77a",
      "question": "How many eggs did Henry and his near 80 year old mate Mildred produce?",
      "prediction": "11 healthy",
      "ground_truths": [
        "11"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7e62f6dd07e44159a17dbf9e23722d70",
      "question": "What did Henry and Mildren produce?",
      "prediction": "11 healthy eggs",
      "ground_truths": [
        "11 healthy eggs"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bba8beff7e84467798bcc8877a8f8f24",
      "question": "how many eggs did henry and mildred produce",
      "prediction": "11",
      "ground_truths": [
        "11"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "88128dcfd0874369a2dee08ef86f6c1e",
      "question": "What did Henry do for the first time in forty years?",
      "prediction": "mate at Southland Museum,",
      "ground_truths": [
        "mated"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b56f8165ffa44477b6e69deef3533885",
      "question": "what is the name of henry's mate",
      "prediction": "Mildred,",
      "ground_truths": [
        "Mildred"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ffe44836297740d7a182a1a1c22a8868",
      "question": "What changed things for Henry?",
      "prediction": "a cancerous tumor.",
      "ground_truths": [
        "cancerous tumor."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "72753eabd02346189c361e07e5693b57",
      "question": "What amount are the stolen artworks worth?",
      "prediction": "$163 million",
      "ground_truths": [
        "$163 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8eb5cc20098f42998110b6ae272d3db5",
      "question": "what happened to picassos art",
      "prediction": "stolen in Switzerland",
      "ground_truths": [
        "stole"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fc23ad5bd40f4ed8874ef74776638b12",
      "question": "What country had other recent thefts?",
      "prediction": "Switzerland",
      "ground_truths": [
        "Switzerland"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1feb6429cf2b45d1818ea0a530e965e1",
      "question": "what was stolen",
      "prediction": "impressionist paintings",
      "ground_truths": [
        "famous paintings"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "7baeb822f72c486ab0ecda07d34f1a8a",
      "question": "what happened in zurich",
      "prediction": "police were scrambling",
      "ground_truths": [
        "Swiss art heist"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "301de41a38b141ba8fa519b780069911",
      "question": "What artists are included in the haul?",
      "prediction": "Claude Monet",
      "ground_truths": [
        "Paul Cezanne, Edgar Degas, Claude Monet"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "26946b22375240aca1f36b8280a59a11",
      "question": "What did Obama promise the girls?",
      "prediction": "to bringing a new puppy with them to the White House",
      "ground_truths": [
        "that they'd get to bring a new puppy with them to the White House"
      ],
      "em": 0,
      "f1": 0.761904761904762
    },
    {
      "id": "bb9aadd4bff44abd87cc35736d98cd41",
      "question": "What did Barack Obama promise the girls",
      "prediction": "their request to getting a dog;",
      "ground_truths": [
        "they'd get to bring a new puppy with them to the White House in January."
      ],
      "em": 0,
      "f1": 0.1111111111111111
    },
    {
      "id": "cfcb878ce5814bc9bc167d9c2984adba",
      "question": "What can lead to more safety and save cash?",
      "prediction": "Cash for Clunkers program",
      "ground_truths": [
        "fuel economy"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2032756f88a047c480c62dde0370296a",
      "question": "Which program is boosting economy and auto industry?",
      "prediction": "Cash for Clunkers",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5b8b16eec3b9472f93b5e0d16458de0b",
      "question": "What did Jack Hidary say about the Cash for Clunkers program ?",
      "prediction": "it is not just $3 billion of new money into the economy. It is injecting $21 billion",
      "ground_truths": [
        "promotes fuel economy and safety while boosting the economy."
      ],
      "em": 0,
      "f1": 0.08333333333333333
    },
    {
      "id": "581c4fe469a84f2f989aabaede600937",
      "question": "What is the name of the program?",
      "prediction": "Cash for Clunkers",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9abd942170454e78981c5d29d713f515",
      "question": "What does the  Cash for Clunkers program save?",
      "prediction": "helps autoworkers across the country.",
      "ground_truths": [
        "fuel economy and safety while boosting"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "57bffe27d20040699f85a6954fbb5bea",
      "question": "What is being saved?",
      "prediction": "jobs",
      "ground_truths": [
        "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers."
      ],
      "em": 0,
      "f1": 0.125
    },
    {
      "id": "9206b026187f4230b610c9a05a62edad",
      "question": "What is the reason as to why Cash for Clunkers is saving jobs?",
      "prediction": "helps autoworkers across the country.",
      "ground_truths": [
        "It's helping consumers move beyond these hard times and has reignited a whole industry."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "9d71016ff1924e6287251e9ae657d1d8",
      "question": "What will decrease dependence on foreign oil?",
      "prediction": "Cash for Clunkers",
      "ground_truths": [
        "Cash for Clunkers"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1082fd6599c245bba88c6c46ce86eef8",
      "question": "what is her age",
      "prediction": "64,",
      "ground_truths": [
        "64,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6cb904d598ce4f7ebb4e4283a24a2f63",
      "question": "Who is a member of the the Dalit class",
      "prediction": "Meira Kumar",
      "ground_truths": [
        "Meira Kumar"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "728220fe1b5f4ce886273ec524c6972b",
      "question": "Meira is a member of what?",
      "prediction": "the \"untouchable\" Dalit class,",
      "ground_truths": [
        "\"untouchable\" Dalit class,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "53fe1484646648969f42e40f550ab7ef",
      "question": "What amount of members will she preside over?",
      "prediction": "543",
      "ground_truths": [
        "543 elected"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "79668b02828141b4bc20fd9faba0e2d9",
      "question": "What age is Meira Kumar?",
      "prediction": "64,",
      "ground_truths": [
        "64,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d316c97abd1e4815a14a66b9b76df881",
      "question": "Mrs. Kumar will preside over how many members?",
      "prediction": "543",
      "ground_truths": [
        "543"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "2ff5deeb56834423b46cb59226137c85",
      "question": "What age is Mrs. Kumar?",
      "prediction": "64,",
      "ground_truths": [
        "64,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "669c0a0c0c0b49f4b8d647e1ae814cb9",
      "question": "What did Sanford do?",
      "prediction": "an extramarital affair with a woman",
      "ground_truths": [
        "admitted to an extramarital affair"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "28ca5c9e1476480a998289afcd8e3990",
      "question": "Who is the GOP chairwoman?",
      "prediction": "Karen Floyd",
      "ground_truths": [
        "Karen Floyd"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5f16043e697d41e185b901033f1fa369",
      "question": "Who may have to step down?",
      "prediction": "Gov. Mark Sanford",
      "ground_truths": [
        "Gov. Mark Sanford"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e45687f5ab9d4e7a8468c93bd2e555cc",
      "question": "Who is now calling on Sanford to resign?",
      "prediction": "GOP state senators",
      "ground_truths": [
        "South Carolina Republican Party Chairwoman Karen Floyd"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "e44e649e65d144f6b7f4570657697e64",
      "question": "Who called on Sanford to resign?",
      "prediction": "GOP state senators",
      "ground_truths": [
        "South Carolina Republican Party Chairwoman Karen Floyd"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "a902053fd61745bdb4850afb62bd6e84",
      "question": "How many senators are calling for resignation?",
      "prediction": "13.",
      "ground_truths": [
        "13."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7d1e3748c47143648832d63acda6de2b",
      "question": "Who suggests Sanford may have to step down?",
      "prediction": "South Carolina Republican Party Chairwoman Karen Floyd",
      "ground_truths": [
        "Karen Floyd"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "ceb8c6e838d347268492c88995a6fbc2",
      "question": "What was cancelled?",
      "prediction": "The rally",
      "ground_truths": [
        "rally at the State House"
      ],
      "em": 0,
      "f1": 0.4
    },
    {
      "id": "64575952b546437aa2013f3bd1aae455",
      "question": "What type of work are the firefighters doing?",
      "prediction": "cutting down underbrush",
      "ground_truths": [
        "digging ditches."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "238847addfa9408487251bff5d0e2c4a",
      "question": "What hour shifts do they do?",
      "prediction": "12-hour-plus",
      "ground_truths": [
        "12-hour-plus"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "40d046f0160c4124b666ebaeff5e2ead",
      "question": "Which group followed firefighters?",
      "prediction": "VBS.TV",
      "ground_truths": [
        "VBS.TV"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6f03ff04d77b4145a7f14d0613733117",
      "question": "What is the VBS following?",
      "prediction": "wildfires",
      "ground_truths": [
        "a crew of Grayback forest-firefighters"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3ebe149a32c74c02aa0a03cb1e1c6dda",
      "question": "What does the VBS call it?",
      "prediction": "\"project work\"",
      "ground_truths": [
        "\"Beats digging ditches.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "37ccaff71d2844b6ab69084e87b8d658",
      "question": "What type of burn was this?",
      "prediction": "wildfires,",
      "ground_truths": [
        "controlled"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "fa36a51494e24e6fa1703c519e13d09f",
      "question": "How long are the shifts?",
      "prediction": "12-hour-plus",
      "ground_truths": [
        "12-hour-plus"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9eacf1402c884680aa535a49eb50ec62",
      "question": "What does the 12 hr shift involved?",
      "prediction": "backbreaking labor,",
      "ground_truths": [
        "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry"
      ],
      "em": 0,
      "f1": 0.23529411764705882
    },
    {
      "id": "dd28d4ce2c5f4fac9c44006755c274ba",
      "question": "what is happening in brazil",
      "prediction": "HIV/AIDS fight",
      "ground_truths": [
        "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world."
      ],
      "em": 0,
      "f1": 0.25
    },
    {
      "id": "66aab61f3c5640b28c962d656127f1cf",
      "question": "what country has been hailed as a leader in the fight against the HIV/AIDS epidemic?",
      "prediction": "Brazil",
      "ground_truths": [
        "Brazil's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "2125661233694655a3ed2e4be0429f38",
      "question": "what happened in 1996",
      "prediction": "Brazil began guaranteeing free anti-retroviral treatment",
      "ground_truths": [
        "Brazil"
      ],
      "em": 0,
      "f1": 0.2857142857142857
    },
    {
      "id": "4b8d0b8de71c4c72be5ee5c11a8b367c",
      "question": "when did the Government start offering free anti-retroviral treatment to its citizens ?",
      "prediction": "1996",
      "ground_truths": [
        "1996"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "50211fc1b1cf4a00a980fdfab4e4424a",
      "question": "what did the government start offering to its citizens in 1996?",
      "prediction": "guaranteeing free anti-retroviral treatment",
      "ground_truths": [
        "free anti-retroviral treatment"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "72db4fe2adba411abff72eab29cfaf23",
      "question": "What was his reported biggest obstacle?",
      "prediction": "managing",
      "ground_truths": [
        "time."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a1f5df0db9f4f55a0cd291ba708e0c2",
      "question": "What did Kevin Evans buy?",
      "prediction": "treadmill",
      "ground_truths": [
        "treadmill"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "44ba26c0dbb346e2956a96d27ddb3ebf",
      "question": "Who bought a treadmill?",
      "prediction": "Evans",
      "ground_truths": [
        "Evans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af7d61da98fa436da3f6524e2b864b08",
      "question": "What type of eating plan did he follow?",
      "prediction": "1,800 calories a day.",
      "ground_truths": [
        "low-calorie meals"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0d29f96a620a43bd99661dbfb7072b59",
      "question": "How long did he work out daily?",
      "prediction": "fourteen",
      "ground_truths": [
        "45 minutes,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "129e731f7cf548eb8ade87485fd2cbd1",
      "question": "How long did he workout for?",
      "prediction": "45 minutes,",
      "ground_truths": [
        "45 minutes, five days a week."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "736df4db1462436a97d65428d4f1ac9b",
      "question": "What actually happens with the news?",
      "prediction": "seeped out.\"",
      "ground_truths": [
        "control and censorship"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "87aa1eb8cc90491c9b80a38c7751a6f9",
      "question": "What governments want to control?",
      "prediction": "the media",
      "ground_truths": [
        "Web"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3f99c8aa8238493d9c2cc5e49f4eb371",
      "question": "what gets out via Internet, social media, despite governments' efforts?",
      "prediction": "news of a violent",
      "ground_truths": [
        "access to information and the outside world"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "b8320aeb13a4407eab7c1935e30ea4ee",
      "question": "where Governments seek to control news, sometimes by jailing?",
      "prediction": "Middle East and North Africa,",
      "ground_truths": [
        "Middle East and North Africa,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "45aceb7efa1141cb833f1187994e4881",
      "question": "who is controling news by jailling journalists?",
      "prediction": "authorities",
      "ground_truths": [
        "government"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0cb0af83d0024e0b9ddbe7fb7ffaff26",
      "question": "What mentions the International Press Institute report?",
      "prediction": "the IPI Press Review 2009 Focus on the Middle East and North Africa.",
      "ground_truths": [
        "Review 2009 Focus on the Middle East and North Africa."
      ],
      "em": 0,
      "f1": 0.9
    },
    {
      "id": "c9bcb9b8a8ba42f5bfd2538b4925a3ac",
      "question": "How much did South Africa beat Australia by?",
      "prediction": "nine-wicket",
      "ground_truths": [
        "2-0"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ae5ae5b607b4dbc86f7da5147a095c7",
      "question": "Who is leading the series?",
      "prediction": "Australia",
      "ground_truths": [
        "South Africa"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6019b29d903348c4a083f53c4551ac53",
      "question": "Who won the first Test of the series?",
      "prediction": "South Africa",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "bb720c2091ee4a94a85eb81aa5a7acc3",
      "question": "Who beat Australia?",
      "prediction": "South Africa",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "679586e746bf437fb4372790a38aaa81",
      "question": "By how many wickets did South Africa win?",
      "prediction": "nine-wicket",
      "ground_truths": [
        "nine-wicket"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9adfdf882a9a4f8bb69b96506ce7471c",
      "question": "Who beat Australia by nine wickets?",
      "prediction": "South Africa",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6489bd7df2d04a8aa23561eaed38c793",
      "question": "What was the winning edge in the second Test?",
      "prediction": "180-run ninth wicket partnership with pace bowler Dale Steyn.",
      "ground_truths": [
        "2-0"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "cceb8fa1b2a84757932a6f7ae86b3653",
      "question": "Who has the lead in the series?",
      "prediction": "South Africa",
      "ground_truths": [
        "South Africa"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1e7aa41fe6ff4d048cf6c314eaae526a",
      "question": "How much passengers had fallen?",
      "prediction": "86",
      "ground_truths": [
        "86"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "faca8fa5d6724e3f95ea8550bbb1a3c8",
      "question": "Of what did the elderly woman suffered?",
      "prediction": "diabetes and hypertension,",
      "ground_truths": [
        "diabetes and hypertension,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "384383fa2ce544db8f6eaf79bed69215",
      "question": "Who were shot and killed?",
      "prediction": "Brian Smith",
      "ground_truths": [
        "Two people"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "716a88bdce814241afaf7287239b1e51",
      "question": "Who was hospitalized in serious condition after a suicide attempt?",
      "prediction": "Brian Smith",
      "ground_truths": [
        "Brian Smith"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dab8ffafa5e04822a7c0f1c2cfd68e6f",
      "question": "Who was tied to at least three of four Dallas shootings?",
      "prediction": "Brian Smith.",
      "ground_truths": [
        "Brian Smith."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f31334cd5435401299951cd843c721a7",
      "question": "Who is the ex-Utah officer?",
      "prediction": "Brian Smith",
      "ground_truths": [
        "Brian Smith."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "656256192ad14bbf9f0cbb3ee3aa052d",
      "question": "What he painted in 1610?",
      "prediction": "William Shakespeare",
      "ground_truths": [
        "portrait"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7a1c968d39644e7eb1f0a27e3f70c70f",
      "question": "For how long was the painting on display?",
      "prediction": "several months",
      "ground_truths": [
        "several months starting April 23."
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "8af8f27867bd48a2b337b97001163115",
      "question": "what hakespeare group unveil \"only\" portrait of playwright?",
      "prediction": "William",
      "ground_truths": [
        "London's Shakespeare Birthplace Trust."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7027f9cf591c42c1aa99c460a581d1d1",
      "question": "In what year did Shakespeare die?",
      "prediction": "1616.",
      "ground_truths": [
        "1616."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5c77d779830a4f93b3261e066c7ea67d",
      "question": "When was this painted?",
      "prediction": "400 years ago",
      "ground_truths": [
        "400 years ago"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f083d754f3b54ab9bca200bfaa83b30d",
      "question": "What is the adventure about?",
      "prediction": "a curmudgeonly senior citizen, Carl, tries to cope with the enthusiasm of Russell, a young boy.",
      "ground_truths": [
        "a curmudgeonly senior citizen, Carl, tries to cope with the enthusiasm of Russell, a young boy."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "9b8c9419cfef4133b494c825c3cf6dfd",
      "question": "Which company created the movie \"Up\"?",
      "prediction": "Pixar",
      "ground_truths": [
        "Pixar's"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "338c52f639b64b4697831b74cbca529f",
      "question": "What is unusual about the dog in the story?",
      "prediction": "serve wine and play cards and fly biplanes.",
      "ground_truths": [
        "talking"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "454cbae23b864073955a1003858a86b7",
      "question": "Who won from Pixar?",
      "prediction": "Russell,",
      "ground_truths": [
        "\"Up,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8a3561154a454ca597d3a9a636a95a93",
      "question": "What have great material for film?",
      "prediction": "club-themed movies,",
      "ground_truths": [
        "the action in-and-around the golf course"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "6e1780414deb4cf9b49568f07b3bdc26",
      "question": "Which actors were on the list?",
      "prediction": "Dean Martin, Katharine Hepburn and Spencer Tracy",
      "ground_truths": [
        "Will Smith."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "3a1ba8b914d141eaac80d5de56ed0114",
      "question": "What films are on the list?",
      "prediction": "\"Three Little Beers,\"",
      "ground_truths": [
        "\"Follow the Sun,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f5855e26ffa944fab475fd077cb06a43",
      "question": "What sport has been good for film makers?",
      "prediction": "golf",
      "ground_truths": [
        "golf"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "b3d3a7f3fd00419c896e03a2dd70c2eb",
      "question": "Which two funny films lead the list?",
      "prediction": "\"Three Little Beers,\"",
      "ground_truths": [
        "\"Three Little Beers,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c564493e2442402ebb6d4a5af51f7c25",
      "question": "What has provided great material for film makers?",
      "prediction": "in-and-around the golf course",
      "ground_truths": [
        "drama of the action in-and-around the golf course"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8b1f4118d8cc46719e7be2e78f5bc20f",
      "question": "Which people have all starred down the years?",
      "prediction": "Dean Martin, Katharine Hepburn and Spencer Tracy",
      "ground_truths": [
        "Dean Martin, Katharine Hepburn and Spencer Tracy"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "dc07de7e20dc420f8313f86757aafd8a",
      "question": "where does the correspondent go",
      "prediction": "Russian farm",
      "ground_truths": [
        "at Star City, the Russian cosmonaut training facility."
      ],
      "em": 0,
      "f1": 0.22222222222222224
    },
    {
      "id": "c3f5e33aaf4e4f49a9d196f9cbe600b8",
      "question": "Who rides the train from artic to black sea?",
      "prediction": "Matthew Chance",
      "ground_truths": [
        "Senior International Correspondent Matthew Chance"
      ],
      "em": 0,
      "f1": 0.5714285714285715
    },
    {
      "id": "9e3d4569c31f4984af8ccbf4f789e438",
      "question": "Who features in the series?",
      "prediction": "Russian cosmonaut training facility.",
      "ground_truths": [
        "Matthew Chance"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "34bd133cbb6c42f5acdc5c38eb205ddb",
      "question": "A correspondent rides a train between which two points",
      "prediction": "Murmansk",
      "ground_truths": [
        "the Arctic north of Murmansk down to the southern climes of Sochi"
      ],
      "em": 0,
      "f1": 0.18181818181818182
    },
    {
      "id": "64a39248e7b54b1fbc995e26886a78b7",
      "question": "Which news channel launches week long  programming?",
      "prediction": "CNN",
      "ground_truths": [
        "CNN"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a70868a801ca4ed4be0c1e27a1dbb40b",
      "question": "Which tv station is launching a week long series on modern russis",
      "prediction": "CNN",
      "ground_truths": [
        "CNN"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ff22a081cf304ecba18fc33376e6eb4c",
      "question": "Name to people who are said to feature in the series",
      "prediction": "Nikolai Valuev",
      "ground_truths": [
        "Nikolai Valuev"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "5fcc6fb126e14d65ba4d0d86bb28a8be",
      "question": "What Polo club's comment?",
      "prediction": "\"A total of seven died on our property,\"",
      "ground_truths": [
        "\"A total of seven died on our property,\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "054b7315353146d4b0ad624514005ef2",
      "question": "How many horses died?",
      "prediction": "fourteen",
      "ground_truths": [
        "seven"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "29cf64bece6c424f80f4a55c46fc523f",
      "question": "Where were the races held?",
      "prediction": "near West Palm Beach,",
      "ground_truths": [
        "International Polo Club Palm Beach in Florida."
      ],
      "em": 0,
      "f1": 0.36363636363636365
    },
    {
      "id": "b75b64ca513e4ae3be64ff5d71a9b652",
      "question": "Where was the match set?",
      "prediction": "near West Palm Beach, Florida,",
      "ground_truths": [
        "West Palm Beach, Florida,"
      ],
      "em": 0,
      "f1": 0.888888888888889
    },
    {
      "id": "61bcc7e8df9841939729131fba4cd039",
      "question": "What was the reason that horses die?",
      "prediction": "not been determined,",
      "ground_truths": [
        "has not been determined,"
      ],
      "em": 0,
      "f1": 0.8571428571428571
    },
    {
      "id": "1054b70aff2947d58044fae8a9a048c0",
      "question": "How many horsed died?",
      "prediction": "Fourteen",
      "ground_truths": [
        "Fourteen"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c13da5dd5e2246d28ea136f9f7db0468",
      "question": "What show is Piers Morgan a judge on?",
      "prediction": "\"Britain's Got Talent\"",
      "ground_truths": [
        "\"Britain's Got Talent.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ff4db7c7ac2446b6939d36496da1f46f",
      "question": "What dance group won Britain's Got Talent?",
      "prediction": "Diversity,",
      "ground_truths": [
        "Diversity,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "590494beebfa4403b5ce68447f0499d2",
      "question": "Who is a show judge on Britain's Got Talent?",
      "prediction": "Simon Cowell",
      "ground_truths": [
        "Piers Morgan"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "5117de4cfa6c461babc5ee0b87c08fbf",
      "question": "Who came in second?",
      "prediction": "Susan Boyle",
      "ground_truths": [
        "Susan Boyle"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d140704e3b3a4639a0e0317b3ac3e726",
      "question": "What is the name of the dance group that won Britain's Got Talent?",
      "prediction": "Diversity,",
      "ground_truths": [
        "Diversity,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "af168f17b4e54de8b67d526a69269767",
      "question": "How many people are in the dance group Diversity?",
      "prediction": "a 10-person",
      "ground_truths": [
        "10-person"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "c8529b037aee493898daea2b1947af2b",
      "question": "What did Diversity win?",
      "prediction": "100,000 British pounds ($161,000) and will perform for Queen Elizabeth II in the Royal Variety Show.",
      "ground_truths": [
        "\"Britain's Got Talent\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "eddee4047dc14889afa69dd1bdd8b3b3",
      "question": "Who came in second to the dance group Diversite?",
      "prediction": "Susan Boyle",
      "ground_truths": [
        "Susan Boyle"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8df607a89ef24a34a5e5820f199dbc87",
      "question": "What did Johnny Depp say?",
      "prediction": "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"",
      "ground_truths": [
        "\"I always kind of admired him, oddly.\""
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "ed22ba9720d249ef8627f5038fb406aa",
      "question": "How many people did Dillinger's gang kill?",
      "prediction": "10 men",
      "ground_truths": [
        "10"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "8563cf12e0cc4059b56b8e37eb16e2d5",
      "question": "Who is the character Johnny Depp is playing?",
      "prediction": "John Dillinger,",
      "ground_truths": [
        "John Dillinger,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "98b1ffba0a8244a3bbcdbe600a1f75b8",
      "question": "Who was John Dillinger ?",
      "prediction": "Depression-era bank robber",
      "ground_truths": [
        "bank robber"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "f0906a40a6054651a230fdb9a5a557b1",
      "question": "Who played John Dillinger in Public Enemies?",
      "prediction": "Johnny Depp",
      "ground_truths": [
        "Johnny Depp"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "86f88ef6c5b74a548b18d51a0c738a28",
      "question": "What is Depp playing?",
      "prediction": "John Dillinger,",
      "ground_truths": [
        "John Dillinger,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "504f34a652e6456e993944c8716a518a",
      "question": "What is the surge of refugees to Yemen attributed to?",
      "prediction": "strive in Somalia,",
      "ground_truths": [
        "Intensifying"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "769f1ed90a044b4ebee75a26c7572ddb",
      "question": "What is the number of refugees that have come to Aden?",
      "prediction": "15,000",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "9e01adc0042e419787e823ca9d4dbdb9",
      "question": "Where have the refugess come to?",
      "prediction": "Yemen,",
      "ground_truths": [
        "Yemen,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "64bc1a2e9ed348e5abe5add0e6e7dcf0",
      "question": "How many refugees have come to port city of Aden?",
      "prediction": "More than 15,000",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "83b36cb41c7c4d748b38795c9e32c208",
      "question": "To what does the agency attribute a surging number of refugees?",
      "prediction": "strive in Somalia,",
      "ground_truths": [
        "violence, food shortages and widespread drought"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "39b831ae82844d62a7e981b94ddb37ef",
      "question": "How many refugees have come to Aden since January?",
      "prediction": "More than 15,000",
      "ground_truths": [
        "More than 15,000"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1535aa296b8842b281ff835cdfd0fc20",
      "question": "What else has contributed to increase of refugees to Yemen?",
      "prediction": "the number of boats landing in Aden",
      "ground_truths": [
        "New smuggling routes across the Red Sea"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "41807a040375438e8c42a2100c8aa8a3",
      "question": "What kind of new routes are contributing to the increase?",
      "prediction": "smuggling",
      "ground_truths": [
        "smuggling"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4a543ad4a77747a29d7258fff909d9d9",
      "question": "What is also contributing to the increase?",
      "prediction": "strive in Somalia,",
      "ground_truths": [
        "violence, food shortages and widespread drought"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "58659365bec34d19ad985f98b70d61f7",
      "question": "What did the report say?",
      "prediction": "\"The secretary started the meeting with an apology to me personally, to the American Legion and to the entire veterans community,\"",
      "ground_truths": [
        "returning combat veterans could be recruited by right-wing extremist groups."
      ],
      "em": 0,
      "f1": 0.07692307692307693
    },
    {
      "id": "5a8cae18097a4d7a995aa7f655f1d8af",
      "question": "Who did the apologising?",
      "prediction": "Department of Homeland Security Secretary Janet Napolitano",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "3a25fd6c20c84bb1abdda2294af223ce",
      "question": "Who apologizes?",
      "prediction": "Department of Homeland Security Secretary Janet Napolitano",
      "ground_truths": [
        "Department of Homeland Security Secretary Janet Napolitano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "780dff9431124442a4f67b48da10faaf",
      "question": "Who apologies?",
      "prediction": "Department of Homeland Security Secretary Janet Napolitano",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 0,
      "f1": 0.4444444444444445
    },
    {
      "id": "3016b4474ded45d1bbe6599cad972735",
      "question": "Which document was released too early?",
      "prediction": "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"",
      "ground_truths": [
        "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "30d529c2768142d09ec9ebf9e201e533",
      "question": "Who said document was released?",
      "prediction": "Janet Napolitano",
      "ground_truths": [
        "Janet Napolitano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ea483ff8e95c4daea3dd389f1c3eb646",
      "question": "Who could be recruited by right-wing extremists?",
      "prediction": "veterans",
      "ground_truths": [
        "returning combat veterans"
      ],
      "em": 0,
      "f1": 0.5
    },
    {
      "id": "29c6e634000b4e03af13cde0b5a8f09f",
      "question": "Wh says veterans could be recruited?",
      "prediction": "Department of Homeland Security Secretary Janet Napolitano",
      "ground_truths": [
        "Department of Homeland Security Secretary Janet Napolitano"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "f0a70a65608b43c79cf0a22d6d11a538",
      "question": "What were the men?",
      "prediction": "club managers,",
      "ground_truths": [
        "club managers,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3ed1845d695148d5b950471d255316d0",
      "question": "Where is the strip club?",
      "prediction": "Atlanta",
      "ground_truths": [
        "Atlanta"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "d0a6861253464596b3e82ef0691023bb",
      "question": "Who dies after fistfight?",
      "prediction": "Ashley \"A.J.\" Jewell,",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "47e4f33155cd4ec2818b85e57b859170",
      "question": "What is the strip club called?",
      "prediction": "Body Tap,",
      "ground_truths": [
        "Body Tap,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "fc6e9bcb0b6b4d6dafef94a3ef4fcc0b",
      "question": "Who was Kandi Burrus's fiancé?",
      "prediction": "Ashley \"A.J.\" Jewell,",
      "ground_truths": [
        "Ashley \"A.J.\" Jewell,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "33d88df314284b8885c0815ebf219da2",
      "question": "What was he charged with?",
      "prediction": "voluntary manslaughter",
      "ground_truths": [
        "voluntary manslaughter"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "238882a90dce41f6a69e2963ef23bc60",
      "question": "What does the advocate say?",
      "prediction": "people can't see that the oceans are filling up,",
      "ground_truths": [
        "planning processes are urgently needed"
      ],
      "em": 0,
      "f1": 0.15384615384615385
    },
    {
      "id": "b6ff077e9f204c3a8abb57b3f5a5e51b",
      "question": "What do advocats say?",
      "prediction": "these planning processes are urgently needed and have been a long time in coming.",
      "ground_truths": [
        "planning processes are urgently needed"
      ],
      "em": 0,
      "f1": 0.5555555555555556
    },
    {
      "id": "c6240a4476804dfa888951352348797c",
      "question": "What is growing crowded?",
      "prediction": "oceans",
      "ground_truths": [
        "The oceans"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "00e7c10ad5a14793a91667fede21506b",
      "question": "What is the purpose of the task force?",
      "prediction": "devoted to federal ocean planning.",
      "ground_truths": [
        "federal ocean planning."
      ],
      "em": 0,
      "f1": 0.7499999999999999
    },
    {
      "id": "5c1d81a2821841cd869fd127cb7374a4",
      "question": "WHo created the task force?",
      "prediction": "government",
      "ground_truths": [
        "The Obama administration"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "c4ce634f201d41d08fb34d4a94de2b0a",
      "question": "What did the advocate say about oceans?",
      "prediction": "are kind of the last frontier for use and development,\"",
      "ground_truths": [
        "growing crowded,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f3081bac0c7b4c31b5761dc00653c83e",
      "question": "What does he say about Turkey?",
      "prediction": "His address in the Turkish parliament was one of the greatest speeches made by an American leader",
      "ground_truths": [
        "can play an important role in Afghanistan as a reliable NATO ally. The question is: How can"
      ],
      "em": 0,
      "f1": 0.07142857142857142
    },
    {
      "id": "35a4d88131f24082abcbca3edd33d1cf",
      "question": "Who says Turkey can help rebuild Afghanistan?",
      "prediction": "Zeyno Baran",
      "ground_truths": [
        "Zeyno Baran"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a80dbed58b934f7b96ff0990a777255a",
      "question": "What did Zeyno Baran say about Obama's speech?",
      "prediction": "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.",
      "ground_truths": [
        "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6b7c6c083c484e6a9d3b260a2637ec7f",
      "question": "Whose speech shows deep understanding of Turkey?",
      "prediction": "Zeyno Baran",
      "ground_truths": [
        "President Obama"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f714562f1c5045cb99bb8335566a1697",
      "question": "What did Baran say would be a mistake?",
      "prediction": "pulling Turkey into any kind of engagement with the Taliban",
      "ground_truths": [
        "pulling Turkey into any kind of engagement with the Taliban -- either as part of NATO or bilaterally"
      ],
      "em": 0,
      "f1": 0.72
    },
    {
      "id": "51f6054c927f41a586b38361fc290fd1",
      "question": "When was she last seen?",
      "prediction": "October 3,",
      "ground_truths": [
        "10:30 p.m. October 3,"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "7046b4ee351b4200a16ba68fe210a839",
      "question": "In which location was Lisa last seen on October 3?",
      "prediction": "grocery store",
      "ground_truths": [
        "asleep in her crib,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "991bac9dc88e4388a02e3be07df05727",
      "question": "what does Bill Stanton want?",
      "prediction": "a \"happy ending\"",
      "ground_truths": [
        "a \"happy ending\" to the case."
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "b9e7d81f1bc443508c715a712692e8cb",
      "question": "according to a supermarket clerk what did the mom never seem",
      "prediction": "depressed",
      "ground_truths": [
        "looked depressed"
      ],
      "em": 0,
      "f1": 0.6666666666666666
    },
    {
      "id": "04cfc749138b4f3dbed1b1582beeac18",
      "question": "What is the name of the Investigator?",
      "prediction": "Bill Stanton",
      "ground_truths": [
        "Bill Stanton"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "8f5969735d204ef2ba9ccf9d3fb58164",
      "question": "What did the clerk say about her mom?",
      "prediction": "her baby daughter was reported missing,",
      "ground_truths": [
        "face, like she always does when she comes in here,\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "7d74cc069506478abb97e75ede635049",
      "question": "when was the missing child last seen",
      "prediction": "10:30 p.m. October 3,",
      "ground_truths": [
        "10:30 p.m. October 3,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1a633f3f6fcb4f12b1f8398070ac6c5f",
      "question": "What did Stanton say?",
      "prediction": "he wants a \"happy ending\" to the case.",
      "ground_truths": [
        "he wants a \"happy ending\" to the case."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "6ad74641727d460e9865833b76de85eb",
      "question": "Who suspended the lower courts order?",
      "prediction": "A Brazilian supreme",
      "ground_truths": [
        "judge"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8339e3b757714b379f7dd0b729529214",
      "question": "What happened after mom moved with son to Brazil in 2004?",
      "prediction": "David Goldman's attorney, Patricia Apy, did not immediately respond to a message seeking comment.",
      "ground_truths": [
        "divorced Goldman and married a Brazilian lawyer."
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "4fce650a72de4befaacbf85b7453ddde",
      "question": "Where did the mom move with the son?",
      "prediction": "Brazil.",
      "ground_truths": [
        "Brazil"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "ef3c40e233344e10bc154aa185f5463e",
      "question": "What did the Brazilian supreme court judge suspend?",
      "prediction": "lower court's order that would have given custody of a 9-year-old boy to the U.S. Consulate in",
      "ground_truths": [
        "would have given custody of a 9-year-old boy to the U.S. Consulate in Rio de Janeiro, where he was to be reunited with his American father."
      ],
      "em": 0,
      "f1": 0.5641025641025641
    },
    {
      "id": "58c0270a69db44d9a44ddb594f43cd63",
      "question": "Where did the lower court order the son to be taken?",
      "prediction": "Brazil",
      "ground_truths": [
        "U.S. Consulate in Rio de Janeiro,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "72d750ca2345477a80129e6b3e240072",
      "question": "When did the mother die?",
      "prediction": "September,",
      "ground_truths": [
        "September,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "77ef14762d78476ba0c20e11ea6184c1",
      "question": "When did mom die in childbirth?",
      "prediction": "September,",
      "ground_truths": [
        "September,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a1c71fcc9f6a468c9468d13d69e711a7",
      "question": "which is the project name?",
      "prediction": "Orbiting Carbon Observatory,",
      "ground_truths": [
        "Orbiting Carbon Observatory,"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "077524326f6a4e53a728f5341e89f79c",
      "question": "How much was spent on the study of greenhouse gases effects?",
      "prediction": "$273 million",
      "ground_truths": [
        "$273 million"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "e5d6b99f2e6d4efea5d77b6e01aa98a3",
      "question": "Where did the satellite crash?",
      "prediction": "back to Earth",
      "ground_truths": [
        "landed just short of Antarctica in the ocean.\""
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1af7aafeb2e24d348d72b7559a2ce754",
      "question": "What was the purpose of the rocket?",
      "prediction": "would have collected global measurements of carbon dioxide",
      "ground_truths": [
        "monitored greenhouse gases to study how they affect the Earth's climate,"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "f523ba87f5714a108e9bae698f676063",
      "question": "What was the $273 million project?",
      "prediction": "Satellite",
      "ground_truths": [
        "satellite, called the Orbiting Carbon Observatory,"
      ],
      "em": 0,
      "f1": 0.33333333333333337
    },
    {
      "id": "5837e1a0fc834500856bc94d81b68ac2",
      "question": "when it launched?",
      "prediction": "Tuesday.",
      "ground_truths": [
        "Tuesday."
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "a456ea0840534b98b49cd3ef388e3a2a",
      "question": "What year is the Bureau of Labor Statistics report?",
      "prediction": "(May 2008)",
      "ground_truths": [
        "(May 2008)"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "097faabf28a9492ab39094254f9046ec",
      "question": "how much do physics teachers earn",
      "prediction": "$81,880",
      "ground_truths": [
        "$81,880"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "7274cc5f0de84f7e9665d3234563a498",
      "question": "how much Physics and post-secondary biology teachers can earn?",
      "prediction": "$81,880",
      "ground_truths": [
        "$81,880"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "1c448b1bd5874c299526596d9f80641b",
      "question": "how many jobs are listed that pay at least 80,000",
      "prediction": "30 occupations",
      "ground_truths": [
        "30 occupations"
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "4e17f0ff7bed494b88dd631b6122c549",
      "question": "What does Obama revive?",
      "prediction": "a 2006 law.",
      "ground_truths": [
        "system of military trials"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "1eede7107c9e48419d330929137ee37b",
      "question": "What did the ACLU call the move?",
      "prediction": "\"a striking blow to due process and the rule of law.\"",
      "ground_truths": [
        "\"a striking blow to due process and the rule of law.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "3a866b6d298b41afb982db6c34e1cd12",
      "question": "What is the new system to include?",
      "prediction": "expanded legal protections",
      "ground_truths": [
        "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts."
      ],
      "em": 0,
      "f1": 0.11428571428571428
    },
    {
      "id": "27e4cdf5912b4f558979ab1c2427ed75",
      "question": "Who calls the move \"a striking blow to due process and the rule of law\"?",
      "prediction": "Acura chided the military commission decision as \"a striking blow",
      "ground_truths": [
        "The ACLU"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "474cf4bfc8ed41268f0e9d6b62bde784",
      "question": "What does ACLU call the move?",
      "prediction": "\"a striking blow to due process and the rule of law.\"",
      "ground_truths": [
        "\"a striking blow to due process and the rule of law.\""
      ],
      "em": 1,
      "f1": 1.0
    },
    {
      "id": "366758dc520c473db2b9157d39a2ad10",
      "question": "What did Obama revive of Bush's?",
      "prediction": "military trials for some Guantanamo Bay detainees.",
      "ground_truths": [
        "administration's controversial system of military trials for some Guantanamo Bay detainees."
      ],
      "em": 0,
      "f1": 0.7777777777777778
    },
    {
      "id": "2a993576ba994b8da9b170bdd1613c12",
      "question": "What will the new system include?",
      "prediction": "the idea of the military commissions but opposes the version of the law that had been governing such trials in recent years:",
      "ground_truths": [
        "give detainees greater latitude in selecting legal representation"
      ],
      "em": 0,
      "f1": 0.07692307692307691
    },
    {
      "id": "4441ed963a074e97ac0045a1560efe5f",
      "question": "who are growing?",
      "prediction": "John and Elizabeth Calvert",
      "ground_truths": [
        "friends"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "0c8cd47c97e545fb9b691546e8497d0e",
      "question": "Where did the couple live?",
      "prediction": "Atlanta.",
      "ground_truths": [
        "Hilton Head Island"
      ],
      "em": 0,
      "f1": 0
    },
    {
      "id": "8ef710fca1ae4712878511bcdea0af51",
      "question": "when was the last seen of John and Elizabeth?",
      "prediction": "March 3, 2008,",
      "ground_truths": [
        "March 3,"
      ],
      "em": 0,
      "f1": 0.8
    },
    {
      "id": "d06b179a42ad4291bf19955332ada7b1",
      "question": "Who were they suspicious of?",
      "prediction": "their business books",
      "ground_truths": [
        "Dennis Ray Gerwing"
      ],
      "em": 0,
      "f1": 0
    }
  ]
}